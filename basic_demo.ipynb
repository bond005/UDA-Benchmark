{
   "cells": [
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "# Basic Usage on UDA Benchmark Suite"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "The demonstration encompasses several essential steps:\n",
            "\n",
            "* Prepare the question-answer-document triplet data-item\n",
            "* Extract and segment the document content\n",
            "* Build indexes and retrieve data segments\n",
            "* Generate answering reponse with LLMs\n",
            "* Evaluate the accuracy of reponses using the specific metric."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Load and view the Q&A labels\n",
            "\n",
            "The Q&A labels are accessible through the csv files in the `dataset/qa` directory or by loading the dataset from the HuggingFace repository `qinchuanhui/UDA-QA`."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 1,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Get the q&a labels from the csv file\n",
            "import pandas as pd\n",
            "from uda.utils import preprocess\n",
            "\n",
            "DATASET_NAME = \"fin\"\n",
            "\n",
            "csv_file_path = f\"./dataset/qa/{DATASET_NAME}_qa.csv\"\n",
            "df = pd.read_csv(csv_file_path, sep=\"|\",na_filter=False)\n",
            "qas_dict = preprocess.qa_df_to_dict(DATASET_NAME, df)\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "Or you can also get the q&a labels from the huggingface dataset"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 2,
         "metadata": {},
         "outputs": [],
         "source": [
            "# # Or you can also get the q&a labels from the huggingface dataset\n",
            "\n",
            "# import pandas as pd\n",
            "# from uda.utils import preprocess\n",
            "# from datasets import load_dataset\n",
            "\n",
            "# DATASET_NAME = \"tat\"\n",
            "\n",
            "# hf_dataset = load_dataset(\"qinchuanhui/UDA-QA\", DATASET_NAME)\n",
            "# hf_data = hf_dataset[\"test\"]\n",
            "# df = hf_data.to_pandas()\n",
            "# qas_dict = preprocess.qa_df_to_dict(DATASET_NAME, df)\n"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 3,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Document Name:  ADI_2009\n",
                  "Its Q&A pairs:  [{'question': 'what is the the interest expense in 2009?', 'answers': {'str_answer': '380', 'exe_answer': '3.8'}, 'q_uid': 'ADI/2009/page_49.pdf-1'}, {'question': 'what is the expected growth rate in amortization expense in 2010?', 'answers': {'str_answer': '-27.0%', 'exe_answer': '-0.26689'}, 'q_uid': 'ADI/2009/page_59.pdf-2'}]\n",
                  "=========================================\n",
                  "Document Name:  ABMD_2012\n",
                  "Its Q&A pairs:  [{'question': 'during the 2012 year , did the equity awards in which the prescribed performance milestones were achieved exceed the equity award compensation expense for equity granted during the year?', 'answers': {'str_answer': '', 'exe_answer': 'yes'}, 'q_uid': 'ABMD/2012/page_75.pdf-1'}, {'question': 'for equity awards where the performance criteria has been met in 2012 , what is the average compensation expense per year over which the cost will be expensed?', 'answers': {'str_answer': '1719526', 'exe_answer': '1714285.71429'}, 'q_uid': 'ABMD/2012/page_75.pdf-2'}]\n",
                  "=========================================\n"
               ]
            }
         ],
         "source": [
            "# View the snapshot of qas_dict\n",
            "for key in list(qas_dict.keys())[:2]:\n",
            "    print(\"Document Name: \", key)\n",
            "    print(\"Its Q&A pairs: \", qas_dict[key][:2])\n",
            "    print(\"=========================================\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Prepare the document data"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 4,
         "metadata": {},
         "outputs": [],
         "source": [
            "# Get the local path of the example pdf file\n",
            "example_doc_name = list(qas_dict.keys())[0]\n",
            "example_qa = qas_dict[example_doc_name][1] # or set the index to 0\n",
            "pdf_path = preprocess.get_example_pdf_path(\n",
            "    DATASET_NAME, example_doc_name\n",
            ")  # the function can be exchangable with get_pdf_path()\n",
            "if pdf_path is None:\n",
            "    print(\"No pdf found for this document\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Basic data extraction:\n",
            "\n",
            "Leverage the library `PyPDF` to extract the raw text data from the pdf_files. The tabular structure are presented as the structural markers, such as line-breakers and space."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 5,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "signiﬁcantly decreasing their\n",
                  "inventories. In response to these\n",
                  "unprecedented revenue declines, we\n",
                  "substantially decreased production levels to\n",
                  "reduce our inventory levels. This action, of\n",
                  "course, had the effect of temporarily lowering\n",
                  "our gross margins, which reached a trough of\n",
                  "54.1% in the third quarter.  We reacted\n",
                  "quickly to the business environment, reducingPresident’s Letter\n",
                  "0.000.100.200.300.400.500.60FY2009 Product Revenue and Diluted EPS\n",
                  "From Continuing Operations by Quarter\n",
                  "$0$100$200\n"
               ]
            }
         ],
         "source": [
            "import PyPDF2\n",
            "# Extract text from pdf\n",
            "pdf_text = \"\"\n",
            "with open(pdf_path, \"rb\") as file:\n",
            "    # Create a PDF file reader object\n",
            "    reader = PyPDF2.PdfReader(file, strict=False)\n",
            "    for page_num in range(len(reader.pages)):\n",
            "        page = reader.pages[page_num]\n",
            "        pdf_text += page.extract_text()\n",
            "# Show a snapshot of the text\n",
            "print(pdf_text[8000:8500])\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Data segmentation\n",
            "\n",
            "Utilize the `langchain.text_splitter` to recursively segment text into overlapping chunks, maintaining a 10% overlap and taking explicit separators such as `\\n\\n` and `\\n` into account."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 6,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "chunk_num: 141\n",
                  "chunk_word_counts: 436.5531914893617\n"
               ]
            }
         ],
         "source": [
            "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
            "text_splitter = RecursiveCharacterTextSplitter(\n",
            "    chunk_size=3000,  # chunk size in characters not in words\n",
            "    chunk_overlap=300,  # no overlap\n",
            ")\n",
            "text_chunks = text_splitter.split_text(pdf_text)\n",
            "print(\"chunk_num:\", len(text_chunks))\n",
            "avg_chunk_word_counts = sum([len(chunk.split()) for chunk in text_chunks]) / len(text_chunks)\n",
            "print(\"chunk_word_counts:\", avg_chunk_word_counts)\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Conduct indexing and retrieval\n",
            "In this demo, we utilize the traditional dense embedding approach, utilizing the prevalent `SentenceTransformer` framework, specifically the `all-MiniLM-L6` model, within the vector database `ChromaDB`. \n",
            "\n",
            "Both queries and document segments are embedded into vectors, upon which cosine similarity measures are computed to retrieve the segments with the highest relevance."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 7,
         "metadata": {},
         "outputs": [],
         "source": [
            "import chromadb\n",
            "import torch\n",
            "import chromadb.utils.embedding_functions as embedding_functions\n",
            "\n",
            "# Create the vector_db collection \n",
            "# and store the embeddings\n",
            "model_name = \"all-MiniLM-L6-v2\"\n",
            "chroma_client = chromadb.Client()\n",
            "device_info = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
            "ef = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
            "    model_name=model_name, device=device_info\n",
            ")\n",
            "collection = chroma_client.create_collection(\n",
            "    \"demo_vdb\", embedding_function=ef, metadata={\"hnsw:space\": \"cosine\"}\n",
            ")\n",
            "id_list = [str(i) for i in range(len(text_chunks))]\n",
            "collection.add(documents=text_chunks, ids=id_list)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 8,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "The most relevant contexts to the question: what is the expected growth rate in amortization expense in 2010?\n",
                  "===== Context 1 =======\n",
                  "Amortization expense from continuing operations, related to intangibles was $7.4 million, $9.3 milli ...\n",
                  "===== Context 2 =======\n",
                  "Service (cost), interest (cost), and expected return on assets ............................ (338)\n",
                  "d. ...\n",
                  "===== Context 3 =======\n",
                  "2009 2008\n",
                  "Discount rate .............................................................. 6.60% 5.64%\n",
                  "E ...\n",
                  "===== Context 4 =======\n",
                  "Interest expense . . . ....................................... 4,094 — —\n",
                  "Interest income . . . ..... ...\n",
                  "===== Context 5 =======\n",
                  "Amortization or curtailment recognition of prior service cost .................... ( 5 ) ( 9 )\n",
                  "Amort ...\n"
               ]
            }
         ],
         "source": [
            "# Fetch the top_k most similar chunks according to the query\n",
            "top_k = 5\n",
            "question = example_qa[\"question\"]\n",
            "fetct_res = collection.query(query_texts=[question], n_results=top_k)\n",
            "contexts = fetct_res[\"documents\"][0]\n",
            "\n",
            "# Show a snapshot of the context\n",
            "print(f\"The most relevant contexts to the question: {question}\")\n",
            "for idx,context in enumerate(contexts):\n",
            "    print(f\"===== Context {idx+1} =======\")\n",
            "    print(context[:100], \"...\")\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Perform LLM answering\n",
            "We input the combination of contexts and the question into a LLM to generate the final response. This process was illustrated utilizing locally-hosted open-source LLMs as well as commercially available GPT models."
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "We access the GPT-4 model through AzureOpenAI-API, and access the local LLM through HuggingFace. You should set up them with your own api-key or token, in the file [uda/utils/access_config.py](./uda/utils/access_config.py)"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Demonstration of GPT-4 Model"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 9,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "Question: what is the expected growth rate in amortization expense in 2010?\n",
                  "Ground Truth Reference: {'str_answer': '-27.0%', 'exe_answer': '-0.26689'}\n",
                  "LLM Response: The expected amortization expense for fiscal year 2010 is $5,425, and for fiscal year 2009 it was $7.4 million. To find the expected growth rate, we can use the formula:\n",
                  "\n",
                  "Growth Rate = (New Value - Old Value) / Old Value * 100%\n",
                  "\n",
                  "Plugging in the values:\n",
                  "\n",
                  "Growth Rate = ($5,425 - $7,400,000) / $7,400,000 * 100%\n",
                  "\n",
                  "First, we convert $5,425 to the same scale as $7,400,000, which is $5,425,000 (since the values in the report are in millions and the 2010 value is likely also meant to be in millions but is missing the appropriate notation).\n",
                  "\n",
                  "Growth Rate = ($5,425,000 - $7,400,000) / $7,400,000 * 100%\n",
                  "Growth Rate = (-$1,975,000) / $7,400,000 * 100%\n",
                  "Growth Rate = -26.69%\n",
                  "\n",
                  "The expected growth rate in amortization expense in 2010 is a decrease of 26.69%.\n",
                  "\n",
                  "The answer is: -26.69%\n"
               ]
            }
         ],
         "source": [
            "# Example on GPT model\n",
            "from uda.utils import llm\n",
            "from openai import AzureOpenAI\n",
            "from uda.utils import access_config\n",
            "\n",
            "llm_type = \"gpt-4\"\n",
            "# Create the prompt tailored for different datasets and LLMs  \n",
            "context_text = \"\\n\".join(contexts)\n",
            "llm_message = llm.make_prompt(question=question, context=context_text, task_name=DATASET_NAME, llm_type=llm_type)\n",
            "\n",
            "# Call GPT-4/GPT-3.5 through Azure OpenAI API\n",
            "# You should replace the following parameters with your own configurations\n",
            "# You can also use other API platforms here\n",
            "\n",
            "client = AzureOpenAI(\n",
            "    api_key = access_config.GPT_API_KEY,\n",
            "    api_version = \"2024-04-01-preview\",\n",
            "    azure_endpoint = access_config.GPT_ENDPOINT,\n",
            ")\n",
            "raw_response = client.chat.completions.create(\n",
            "    model = access_config.GPT_MODEL,\n",
            "    messages = llm_message,\n",
            "    temperature = 0.1,\n",
            ")\n",
            "\n",
            "gpt_response = raw_response.choices[0].message.content\n",
            "\n",
            "# Show the response\n",
            "print(f\"Question: {question}\")\n",
            "print(f\"Ground Truth Reference: {example_qa['answers']}\")\n",
            "print(f\"LLM Response: {gpt_response}\")"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "### Demonstration of Local-LLM (Llama-3-8B)"
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 10,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "2024-07-05 12:22:09 ====== Init LLM =======\n"
               ]
            },
            {
               "data": {
                  "application/vnd.jupyter.widget-view+json": {
                     "model_id": "06925342e72a45c89b9b2cc19eff1de3",
                     "version_major": 2,
                     "version_minor": 0
                  },
                  "text/plain": [
                     "Loading checkpoint shards:   0%|          | 0/4 [00:00<?, ?it/s]"
                  ]
               },
               "metadata": {},
               "output_type": "display_data"
            },
            {
               "name": "stderr",
               "output_type": "stream",
               "text": [
                  "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
                  "Setting `pad_token_id` to `eos_token_id`:128009 for open-end generation.\n"
               ]
            },
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "2024-07-05 12:23:34 ====== LLM Service Started =======\n",
                  "2024-07-05 12:23:34 CallLLM\n",
                  "Question: what is the expected growth rate in amortization expense in 2010?\n",
                  "Ground Truth Reference: {'str_answer': '-27.0%', 'exe_answer': '-0.26689'}\n",
                  "LLM Response: Based on the provided information, the amortization expense for intangible assets is expected to decrease from $9.3 million in 2008 to $5.425 million in 2010.\n",
                  "\n",
                  "The expected growth rate in amortization expense in 2010 would be:\n",
                  "\n",
                  "((5.425 - 9.3) / 9.3) * 100% ≈ -42.1%\n",
                  "\n",
                  "So, the expected growth rate in amortization expense in 2010 is approximately -42.1%.\n"
               ]
            }
         ],
         "source": [
            "# Example on Llama model\n",
            "from uda.utils import llm\n",
            "from uda.utils import inference\n",
            "\n",
            "llm_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
            "llm_type = \"llama-8B\"\n",
            "# Create the prompt tailored for different datasets and LLMs  \n",
            "context_text = \"\\n\".join(contexts)\n",
            "llm_message = llm.make_prompt(question=question, context=context_text, task_name=DATASET_NAME, llm_type=llm_type)\n",
            "\n",
            "# Local Inference\n",
            "llm_service = inference.LLM(llm_name)\n",
            "llm_service.init_llm()\n",
            "llama_response = llm_service.infer(llm_message)\n",
            "\n",
            "# Show the response\n",
            "print(f\"Question: {question}\")\n",
            "print(f\"Ground Truth Reference: {example_qa['answers']}\")\n",
            "print(f\"LLM Response: {llama_response}\")\n"
         ]
      },
      {
         "cell_type": "markdown",
         "metadata": {},
         "source": [
            "## Evaluation of the Responses\n",
            "\n",
            "To assess the precision of the LLM-responses, we employ a variety of metrics and evaluative techniques. \n",
            "\n",
            "The results should be organized into a series of dictionaries that encapsulate the response and the ground_truth answers. \n",
            "\n",
            "For an in-depth examination of the evaluative codes and their functionality, refer to the `uda.eval` module within our repository."
         ]
      },
      {
         "cell_type": "code",
         "execution_count": 12,
         "metadata": {},
         "outputs": [
            {
               "name": "stdout",
               "output_type": "stream",
               "text": [
                  "[{'question': 'what is the expected growth rate in amortization expense in 2010?', 'response': 'The expected amortization expense for fiscal year 2010 is $5,425, and for fiscal year 2009 it was $7.4 million. To find the expected growth rate, we can use the formula:\\n\\nGrowth Rate = (New Value - Old Value) / Old Value * 100%\\n\\nPlugging in the values:\\n\\nGrowth Rate = ($5,425 - $7,400,000) / $7,400,000 * 100%\\n\\nFirst, we convert $5,425 to the same scale as $7,400,000, which is $5,425,000 (since the values in the report are in millions and the 2010 value is likely also meant to be in millions but is missing the appropriate notation).\\n\\nGrowth Rate = ($5,425,000 - $7,400,000) / $7,400,000 * 100%\\nGrowth Rate = (-$1,975,000) / $7,400,000 * 100%\\nGrowth Rate = -26.69%\\n\\nThe expected growth rate in amortization expense in 2010 is a decrease of 26.69%.\\n\\nThe answer is: -26.69%', 'doc': 'ADI_2009', 'q_uid': 'ADI/2009/page_59.pdf-2', 'answers': {'str_answer': '-27.0%', 'exe_answer': '-0.26689'}}]\n",
                  "Exact-match accuracy: 100.00\n"
               ]
            }
         ],
         "source": [
            "# Format the result\n",
            "res_dict = {\n",
            "    \"question\": question,\n",
            "    \"response\": gpt_response,\n",
            "    \"doc\": example_doc_name,\n",
            "    \"q_uid\": example_qa[\"q_uid\"],\n",
            "    \"answers\": example_qa[\"answers\"],\n",
            "}\n",
            "res_data = [res_dict]\n",
            "\n",
            "print(res_data)\n",
            "\n",
            "# Evaluate the result\n",
            "from uda.eval.my_eval import eval_main\n",
            "eval_main(DATASET_NAME, res_data)\n"
         ]
      }
   ],
   "metadata": {
      "kernelspec": {
         "display_name": "langchain",
         "language": "python",
         "name": "python3"
      },
      "language_info": {
         "codemirror_mode": {
            "name": "ipython",
            "version": 3
         },
         "file_extension": ".py",
         "mimetype": "text/x-python",
         "name": "python",
         "nbconvert_exporter": "python",
         "pygments_lexer": "ipython3",
         "version": "3.11.0"
      }
   },
   "nbformat": 4,
   "nbformat_minor": 2
}
