{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Basic Usage on UDA Benchmark Suite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and view the Q&A labels\n",
    "\n",
    "The Q&A labels are accessible through the csv files in the `dataset/qa` directory or by loading the dataset from the HuggingFace repository `qinchuanhui/UDA-QA`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the q&a labels from the csv file\n",
    "import pandas as pd\n",
    "from uda.utils import preprocess\n",
    "\n",
    "DATASET_NAME = \"tat\"\n",
    "\n",
    "csv_file_path = f\"./dataset/qa/{DATASET_NAME}_qa.csv\"\n",
    "df = pd.read_csv(csv_file_path, sep=\"|\",na_filter=False)\n",
    "qas_dict = preprocess.qa_df_to_dict(DATASET_NAME, df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Or you can also get the q&a labels from the huggingface dataset\n",
    "\n",
    "# import pandas as pd\n",
    "# from uda.utils import preprocess\n",
    "# from datasets import load_dataset\n",
    "\n",
    "# DATASET_NAME = \"tat\"\n",
    "\n",
    "# hf_dataset = load_dataset(\"qinchuanhui/UDA-QA\", DATASET_NAME)\n",
    "# hf_data = hf_dataset[\"test\"]\n",
    "# df = hf_data.to_pandas()\n",
    "# qas_dict = preprocess.qa_df_to_dict(DATASET_NAME, df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Document Name:  overseas-shipholding-group-inc_2019\n",
      "Its Q&A pairs:  [{'question': 'What benefits are provided by the company to qualifying domestic retirees and their eligible dependents?', 'answers': {'answer': ['certain postretirement health care and life insurance benefits'], 'answer_type': 'span', 'scale': ''}, 'q_uid': 'bbdcf6da614f34fdb63995661c81613f'}, {'question': 'What is the change in Interest cost on benefit obligation for pension benefits from December 31, 2018 and 2019?', 'answers': {'answer': ['129'], 'answer_type': 'arithmetic', 'scale': ''}, 'q_uid': '0bf2a781ac6044d4d9dd94bd6cc1f790'}]\n",
      "=========================================\n",
      "Document Name:  lifeway-foods-inc_2019\n",
      "Its Q&A pairs:  [{'question': 'What was the low sale price per share for each quarters in 2018 in chronological order?', 'answers': {'answer': ['$ 5.99', '$ 4.79', '$ 2.66', '$ 1.88'], 'answer_type': 'multi-span', 'scale': ''}, 'q_uid': 'f4c8e2d0155ac338249d0fe6feba49ac'}, {'question': \"What is the symbol of the company's common stock that is listed on the Nasdaq Global Market?\", 'answers': {'answer': ['LWAY'], 'answer_type': 'span', 'scale': ''}, 'q_uid': '871af62021e2bd9a6ff15f9b1ba26d79'}]\n",
      "=========================================\n"
     ]
    }
   ],
   "source": [
    "# View the snapshot of qas_dict\n",
    "for key in list(qas_dict.keys())[:2]:\n",
    "    print(\"Document Name: \", key)\n",
    "    print(\"Its Q&A pairs: \", qas_dict[key][:2])\n",
    "    print(\"=========================================\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare the document data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the local path of the example pdf file\n",
    "example_doc_name = list(qas_dict.keys())[0]\n",
    "example_qa = qas_dict[example_doc_name][0]\n",
    "pdf_path = preprocess.get_example_pdf_path(\n",
    "    DATASET_NAME, example_doc_name\n",
    ")  # the function can be exchangable with get_pdf_path()\n",
    "if pdf_path is None:\n",
    "    print(\"No pdf found for this document\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic data extraction:\n",
    "\n",
    "Leverage the library `PyPDF` to extract the raw text data from the pdf_files. The tabular structure are presented as the structural markers, such as line-breakers and space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hington, D.C. 20549 (information on the operation of the Public Reference Room is available by calling the SEC\n",
      "at 1-800-SEC-0330). The SEC also maintains a website that contains reports, proxy and information statements, and other\n",
      "information regarding issuers that file electronically with the SEC at http://www.sec.gov.\n",
      " \n",
      "The Company also makes available on its website its corporate governance guidelines, its code of business conduct, insider trading\n",
      "policy, anti-bribery and corruption policy an\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "# Extract text from pdf\n",
    "pdf_text = \"\"\n",
    "with open(pdf_path, \"rb\") as file:\n",
    "    # Create a PDF file reader object\n",
    "    reader = PyPDF2.PdfReader(file, strict=False)\n",
    "    for page_num in range(len(reader.pages)):\n",
    "        page = reader.pages[page_num]\n",
    "        pdf_text += page.extract_text()\n",
    "# Show a snapshot of the text\n",
    "print(pdf_text[8000:8500])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data segmentation\n",
    "\n",
    "Utilize the `langchain.text_splitter` to recursively segment text into overlapping chunks, maintaining a 10% overlap and taking explicit separators such as `\\n\\n` and `\\n` into account."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "chunk_num: 143\n",
      "chunk_word_counts: 444.2237762237762\n"
     ]
    }
   ],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=3000,  # chunk size in characters not in words\n",
    "    chunk_overlap=300,  # no overlap\n",
    ")\n",
    "text_chunks = text_splitter.split_text(pdf_text)\n",
    "print(\"chunk_num:\", len(text_chunks))\n",
    "avg_chunk_word_counts = sum([len(chunk.split()) for chunk in text_chunks]) / len(text_chunks)\n",
    "print(\"chunk_word_counts:\", avg_chunk_word_counts)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conduct indexing and retrieval\n",
    "In this demo, we utilize the traditional dense embedding approach, utilizing the prevalent `SentenceTransformer` framework, specifically the `all-MiniLM-L6` model, within the vector database `ChromaDB`. \n",
    "\n",
    "Both queries and document segments are embedded into vectors, upon which cosine similarity measures are computed to retrieve the segments with the highest relevance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import chromadb\n",
    "import torch\n",
    "import chromadb.utils.embedding_functions as embedding_functions\n",
    "\n",
    "# Create the vector_db collection \n",
    "# and store the embeddings\n",
    "model_name = \"all-MiniLM-L6-v2\"\n",
    "chroma_client = chromadb.Client()\n",
    "device_info = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "ef = embedding_functions.SentenceTransformerEmbeddingFunction(\n",
    "    model_name=model_name, device=device_info\n",
    ")\n",
    "collection = chroma_client.create_collection(\n",
    "    \"demo_vdb\", embedding_function=ef, metadata={\"hnsw:space\": \"cosine\"}\n",
    ")\n",
    "id_list = [str(i) for i in range(len(text_chunks))]\n",
    "collection.add(documents=text_chunks, ids=id_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The most relevant contexts to the question: What benefits are provided by the company to qualifying domestic retirees and their eligible dependents?\n",
      "===== Context 1 =======\n",
      "the five consecutive plan years that produce the highest results.\n",
      " \n",
      "Multiemployer Pension and Postre ...\n",
      "===== Context 2 =======\n",
      "employee contributions and matching contributions to the plans. All contributions to the plans are a ...\n",
      "===== Context 3 =======\n",
      "underfunded multiemployer pension plan would require us to make payments to the plan for our proport ...\n",
      "===== Context 4 =======\n",
      "estimates and key assumptions, including those related to the discount rates, the rates expected to  ...\n",
      "===== Context 5 =======\n",
      "withdrawal liability would have been approximately $19,591 had the Company elected to withdraw from  ...\n"
     ]
    }
   ],
   "source": [
    "# Fetch the top_k most similar chunks according to the query\n",
    "top_k = 5\n",
    "question = example_qa[\"question\"]\n",
    "fetct_res = collection.query(query_texts=[question], n_results=top_k)\n",
    "contexts = fetct_res[\"documents\"][0]\n",
    "\n",
    "# Show a snapshot of the context\n",
    "print(f\"The most relevant contexts to the question: {question}\")\n",
    "for idx,context in enumerate(contexts):\n",
    "    print(f\"===== Context {idx+1} =======\")\n",
    "    print(context[:100], \"...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Perform LLM answering\n",
    "We input the combination of contexts and the question into a LLM to generate the final response. This process was illustrated utilizing locally-hosted open-source LLMs as well as commercially available GPT models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: What benefits are provided by the company to qualifying domestic retirees and their eligible dependents?\n",
      "Ground Truth Reference: {'answer': ['certain postretirement health care and life insurance benefits'], 'answer_type': 'span', 'scale': ''}\n",
      "LLM Response: The answer is: Certain postretirement health care and life insurance benefits.\n"
     ]
    }
   ],
   "source": [
    "# Example on GPT model\n",
    "from uda.utils import llm\n",
    "from openai import AzureOpenAI\n",
    "\n",
    "llm_type = \"gpt-4\"\n",
    "# Create the prompt tailored for different datasets and LLMs  \n",
    "context_text = \"\\n\".join(contexts)\n",
    "llm_message = llm.make_prompt(question=question, context=context_text, task_name=DATASET_NAME, llm_type=llm_type)\n",
    "\n",
    "# Call GPT-4/GPT-3.5 through Azure OpenAI API\n",
    "# You should replace the following parameters with your own configurations\n",
    "# You can also use other API platforms here\n",
    "\n",
    "client = AzureOpenAI(\n",
    "    api_key = \"abcdf\",\n",
    "    api_version = \"2024-04-01-preview\",\n",
    "    azure_endpoint = \"https://abcd.openai.azure.com/\",\n",
    ")\n",
    "raw_response = client.chat.completions.create(\n",
    "    model = \"abcde\", # deploy_name\n",
    "    messages = llm_message,\n",
    "    temperature = 0.1,\n",
    ")\n",
    "\n",
    "response = raw_response.choices[0].message.content\n",
    "\n",
    "# Show the response\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Ground Truth Reference: {example_qa['answers']}\")\n",
    "print(f\"LLM Response: {response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example on Llama model\n",
    "from uda.utils import llm\n",
    "from uda.utils import inference\n",
    "\n",
    "llm_name = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "llm_type = \"llama-8B\"\n",
    "# Create the prompt tailored for different datasets and LLMs  \n",
    "context_text = \"\\n\".join(contexts)\n",
    "llm_message = llm.make_prompt(question=question, context=context_text, task_name=DATASET_NAME, llm_type=llm_type)\n",
    "\n",
    "# Local Inference\n",
    "llm_service = inference.LLM(llm_name)\n",
    "llm_service.init_llm()\n",
    "response = llm_service.infer(llm_message)\n",
    "\n",
    "# Show the response\n",
    "print(f\"Question: {question}\")\n",
    "print(f\"Ground Truth Reference: {example_qa['answers']}\")\n",
    "print(f\"LLM Response: {response}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation of the Responses\n",
    "\n",
    "To assess the precision of the LLM-responses, we employ a variety of metrics and evaluative techniques. \n",
    "\n",
    "The results should be organized into a series of dictionaries that encapsulate the response and the ground_truth answers. \n",
    "\n",
    "For an in-depth examination of the evaluative codes and their functionality, refer to the `uda.eval` module within our repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numerical F1 score: 100.00\n"
     ]
    }
   ],
   "source": [
    "# Format the result\n",
    "res_dict = {\n",
    "    \"question\": question,\n",
    "    \"response\": response,\n",
    "    \"doc\": example_doc_name,\n",
    "    \"q_uid\": example_qa[\"q_uid\"],\n",
    "    \"answers\": example_qa[\"answers\"],\n",
    "}\n",
    "res_data = [res_dict]\n",
    "\n",
    "# Evaluate the result\n",
    "from uda.eval.my_eval import eval_main\n",
    "eval_main(DATASET_NAME, res_data)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
