{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Evaluations\n",
    "\n",
    "We construct an end-to-end RAG pipeline: \n",
    "* Parse unstructured data from PDFs leveraging the raw-text extraction approach\n",
    "* Employ OpenAIâ€™s text-embedding-3-large model to index and retrieve relevant data chunks. \n",
    "* In the generation phase, we incorporate the Chain-of-Thought approach to handle arithmetic-intensive tasks.\n",
    "\n",
    " Based on this end-to-end pipeline, we evaluate 8 LLMs spanning various model sizes and architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the project root directory\n",
    "root_dir = Path(os.path.abspath(\"\")).resolve().parents[1]\n",
    "sys.path.append(str(root_dir))\n",
    "# Change the working directory to the project root\n",
    "os.chdir(root_dir)\n",
    "\n",
    "# Set up the configs for this demo\n",
    "DEMO_SIZE = 2\n",
    "res_dir = f\"experiment/e2e/res/\"\n",
    "if not os.path.exists(res_dir):\n",
    "    os.makedirs(res_dir)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our paper, we use the powerful `text-embedding-3-large-model` model with AzureOpenAI-API . But you need to set up with your own api-key, endpoint and deploy-model in [uda/utils/retrieve.py (line-80)](../../uda/utils/retrieve.py#L81). \n",
    "\n",
    "For convenience, we choose the `BM25` retriever here, while you can also select other strategies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental Configurations\n",
    "\n",
    "# Available retrieval model_name: \"bm25\", \"all-MiniLM-L6-v2\", \"all-mpnet-base-v2\", \"openai\", \"colbert\"\n",
    "# We choose bm25 for convenience\n",
    "RT_MODEL = \"bm25\" \n",
    "\n",
    "\n",
    "DATASET_NAME_LIST = [\"paper_tab\", \"paper_text\", \"nq\", \"feta\",\"tat\",\"fin\"]\n",
    "\n",
    "LOCAL_LLM_DICT = {\n",
    "    \"llama-8B\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    \"qwen-7B\": \"Qwen/Qwen1.5-7B-Chat\",\n",
    "    \"mistral\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    \"qwen-32B\": \"Qwen/Qwen1.5-32B-Chat\",\n",
    "    \"mixtral\": \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    \"llama-70B\": \"meta-llama/Meta-Llama-3-70B-Instruct\",\n",
    "}\n",
    "LLM_LIST = [\"gpt4\", \"llama-8B\", \"mistral\", \"qwen-32B\", \"mixtral\", \"llama-70B\", \"qwen-7B\"]\n",
    "# You can sample a subset of LLMs for faster demo\n",
    "LLM_LIST = LLM_LIST[:1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our implementation, the AzureOpenAI-API serves as the interface for accessing GPT models. Users should set up the service with their own api-key and endpoint within the `call_gpt()` function, contained in the following codes.\n",
    "\n",
    "If you want to use alternative platforms, the `call_gpt()` can be replaced by the target model-calling function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from uda.utils import retrieve as rt\n",
    "from uda.utils import retrieve_exp as rt_exp\n",
    "from uda.utils import preprocess as pre\n",
    "import pandas as pd\n",
    "from uda.utils import llm\n",
    "from uda.utils import inference\n",
    "import json\n",
    "\n",
    "for DATASET_NAME in DATASET_NAME_LIST:\n",
    "    for LLM_MODEL in LLM_LIST:\n",
    "        print(f\"=== Start {DATASET_NAME} on {LLM_MODEL} ===\")\n",
    "        res_file = os.path.join(res_dir, f\"{DATASET_NAME}_{LLM_MODEL}_{RT_MODEL}.jsonl\")\n",
    "\n",
    "        # If use the local LLM, initialize the model\n",
    "        if LLM_MODEL in LOCAL_LLM_DICT:\n",
    "            llm_name = LOCAL_LLM_DICT[LLM_MODEL]\n",
    "            llm_service = inference.LLM(llm_name)\n",
    "            llm_service.init_llm()\n",
    "\n",
    "        # Load the benchmark data\n",
    "        bench_json_file = pre.meta_data[DATASET_NAME][\"bench_json_file\"]\n",
    "        with open(bench_json_file, \"r\") as f:\n",
    "            bench_data = json.load(f)\n",
    "\n",
    "        # Run experiments on the demo docs\n",
    "        doc_list = list(bench_data.keys())\n",
    "        for doc in doc_list[:DEMO_SIZE]:\n",
    "            pdf_path = pre.get_example_pdf_path(DATASET_NAME, doc)\n",
    "            if pdf_path is None:\n",
    "                continue\n",
    "            for qa_item in bench_data[doc]:\n",
    "                question = qa_item[\"question\"]\n",
    "                q_uid = qa_item[\"q_uid\"]\n",
    "                collection_name = f\"{DATASET_NAME}_vector_db\"\n",
    "                # Prepare the index\n",
    "                collection = rt.prepare_collection(pdf_path, collection_name, RT_MODEL)\n",
    "                # Retrieve the contexts\n",
    "                contexts = rt.get_contexts(collection, question, RT_MODEL)\n",
    "                context_text = '\\n'.join(contexts)\n",
    "                # Create the prompt\n",
    "                llm_message = llm.make_prompt(question, context_text, DATASET_NAME, LLM_MODEL)\n",
    "                # Generate the answer\n",
    "                if LLM_MODEL in LOCAL_LLM_DICT:\n",
    "                    response = llm_service.infer(llm_message)\n",
    "                elif LLM_MODEL == \"gpt4\":\n",
    "                    # Set up with your own GPT4 service\n",
    "                    response = llm.call_gpt(\n",
    "                        messages=llm_message,\n",
    "                        api_key=\"abcd\",\n",
    "                        endpoint=\"https://abcd\",\n",
    "                        deployment_name=\"abcd\",\n",
    "                    )\n",
    "                    if response is None:\n",
    "                        print(\"Make sure your gpt4 service is set up correctly.\")\n",
    "                        raise Exception(\"GPT4 service\")\n",
    "\n",
    "                # log the results\n",
    "                res_dict = {\"model\": LLM_MODEL, \"question\": question, \"response\": response, \"doc\": doc, \"q_uid\": q_uid, \"answers\": qa_item[\"answers\"]}\n",
    "                print(res_dict)\n",
    "                with open(res_file, \"a\") as f:\n",
    "                    f.write(json.dumps(res_dict) + \"\\n\")\n",
    "\n",
    "    print(f\"=== Finish {DATASET_NAME} ===\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the end-to-end results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact-match accuracy: 66.67\n"
     ]
    }
   ],
   "source": [
    "dataset_name=\"fin\"\n",
    "llm_model=\"gpt4\"\n",
    "rt_model=\"bm25\"\n",
    "res_file_name=f\"experiment/e2e/res/{dataset_name}_{llm_model}_{rt_model}.jsonl\"\n",
    "\n",
    "from uda.eval.my_eval import eval_from_file\n",
    "eval_from_file(dataset_name, res_file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
