{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# End-to-End Evaluations\n",
    "\n",
    "We construct an end-to-end RAG pipeline: \n",
    "* Parse unstructured data from PDFs leveraging the raw-text extraction approach\n",
    "* Employ OpenAIâ€™s text-embedding-3-large model to index and retrieve relevant data chunks. \n",
    "* In the generation phase, we incorporate the Chain-of-Thought approach to handle arithmetic-intensive tasks.\n",
    "\n",
    " Based on this end-to-end pipeline, we evaluate 8 LLMs spanning various model sizes and architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the project root directory\n",
    "root_dir = Path(os.path.abspath(\"\")).resolve().parents[1]\n",
    "sys.path.append(str(root_dir))\n",
    "# Change the working directory to the project root\n",
    "os.chdir(root_dir)\n",
    "\n",
    "res_dir = f\"experiment/e2e/res/\"\n",
    "if not os.path.exists(res_dir):\n",
    "    os.makedirs(res_dir)\n",
    "    \n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our paper, we use the powerful `text-embedding-3-large-model` model with AzureOpenAI-API . But you need to set up with your own api-key, endpoint and deploy-model in the config_file [uda/utils/access_config.py](../../uda/utils/access_config.py). \n",
    "\n",
    "If you want to use the API from **other alternative platforms** please change the codes in [uda/utils/retrieve.py (line-80)](../../uda/utils/retrieve.py#L81). \n",
    "\n",
    "For convenient demonstration, we choose the `BM25` retriever here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental Configurations\n",
    "\n",
    "# Available retrieval model_name: \"bm25\", \"all-MiniLM-L6-v2\", \"all-mpnet-base-v2\", \"openai\", \"colbert\"\n",
    "# We choose bm25 for convenience\n",
    "RT_MODEL = \"bm25\" \n",
    "\n",
    "DATASET_NAME_LIST = [\"fin\", \"feta\", \"tat\",\"paper_text\", \"nq\", \"paper_tab\"]\n",
    "LOCAL_LLM_DICT = {\n",
    "    \"llama-8B\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    \"qwen-7B\": \"Qwen/Qwen1.5-7B-Chat\",\n",
    "    \"mistral\": \"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "    \"qwen-32B\": \"Qwen/Qwen1.5-32B-Chat\",\n",
    "    \"mixtral\": \"mistralai/Mixtral-8x7B-Instruct-v0.1\",\n",
    "    \"llama-70B\": \"meta-llama/Meta-Llama-3-70B-Instruct\",\n",
    "}\n",
    "LLM_LIST = [\"gpt4\", \"llama-8B\", \"mistral\", \"qwen-32B\", \"mixtral\", \"llama-70B\", \"qwen-7B\"]\n",
    "\n",
    "# Sample a subset for faster demo\n",
    "DATASET_NAME_LIST = DATASET_NAME_LIST[:2]\n",
    "LLM_LIST = LLM_LIST[:1]\n",
    "DEMO_DOC_NUM = 2\n",
    "DEMO_QA_NUM = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our implementation, the **AzureOpenAI-API** serves as the interface for accessing GPT models. Users should set up the gpt-service with their own api-key and endpoint in the config_file [uda/utils/access_config.py](../../uda/utils/access_config.py). These configurations will be used in the `call_gpt()` function in the following codes.\n",
    "\n",
    "\n",
    "If you want to use **other alternative platforms**, the `call_gpt()` can be replaced by the corresponding model-calling function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Start fin on gpt4 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 12:34:41,146 - INFO - HTTP Request: POST https://qinchuan-hui.openai.azure.com/openai/deployments/yulong-4/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt4', 'question': 'what percentage of total long-term assets under supervision are comprised of fixed income in 2015?', 'response': 'In 2015, the total long-term assets under supervision were $946 billion, and the fixed income assets under supervision were $546 billion.\\n\\nTo calculate the percentage of fixed income assets in the total long-term assets under supervision for 2015:\\n\\nFixed income assets / Total long-term assets under supervision * 100\\n= $546 billion / $946 billion * 100\\n= 57.72%\\n\\nThe answer is: 57.72%', 'doc': 'GS_2016', 'q_uid': 'GS/2016/page_79.pdf-3', 'answers': {'str_answer': '57%', 'exe_answer': 0.57484}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 12:34:47,282 - INFO - HTTP Request: POST https://qinchuan-hui.openai.azure.com/openai/deployments/yulong-4/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt4', 'question': 'what percentage of total long-term assets under supervision are comprised of fixed income in 2016?', 'response': 'In 2016, the total long-term assets under supervision are $1,021 billion, and the fixed income assets under supervision are $601 billion. To find the percentage of fixed income assets in the total long-term assets under supervision:\\n\\nPercentage = (Fixed Income Assets / Total Long-Term Assets) * 100\\nPercentage = ($601 billion / $1,021 billion) * 100\\nPercentage = 58.86%\\n\\nThe answer is: 58.86%', 'doc': 'GS_2016', 'q_uid': 'GS/2016/page_79.pdf-1', 'answers': {'str_answer': '59%', 'exe_answer': 0.588}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 12:34:50,967 - INFO - HTTP Request: POST https://qinchuan-hui.openai.azure.com/openai/deployments/yulong-4/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt4', 'question': 'what was the percentage change in the 5 year annual performance of the peer group stock from 2010 to 2011', 'response': 'The answer is: 36.78%', 'doc': 'JKHY_2015', 'q_uid': 'JKHY/2015/page_20.pdf-2', 'answers': {'str_answer': '8.3%', 'exe_answer': 0.08276}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 12:34:53,362 - INFO - HTTP Request: POST https://qinchuan-hui.openai.azure.com/openai/deployments/yulong-4/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt4', 'question': 'in 2010 , what was the cumulative total return of the s&p 500?', 'response': 'The answer is: 100.00', 'doc': 'JKHY_2015', 'q_uid': 'JKHY/2015/page_20.pdf-3', 'answers': {'str_answer': '30.69', 'exe_answer': 30.69}}\n",
      "=== Finish fin ===\n",
      "\n",
      "=== Start feta on gpt4 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 12:34:59,611 - INFO - HTTP Request: POST https://qinchuan-hui.openai.azure.com/openai/deployments/yulong-4/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt4', 'question': \"Which season of Smallville performed the best during it's airing? \", 'response': 'The answer is: The first season of Smallville performed the best during its airing, with the highest rank of 115 and an average of 5.90 million viewers per episode.', 'doc': 'Smallville', 'q_uid': 12844, 'answers': \"Over ten seasons the Smallville averaged, million viewers per episode, is with season two's highest rating of 6.3 million.\"}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 12:35:05,499 - INFO - HTTP Request: POST https://qinchuan-hui.openai.azure.com/openai/deployments/yulong-4/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt4', 'question': 'In which film did Jennifer Jones star in 1995 and in which consequent film did she take on a role in 1956? ', 'response': 'The answer is: Jennifer Jones did not star in any film in 1995 as her acting career was active from 1939 to 1974. However, in 1956, she starred in \"The Man in the Gray Flannel Suit.\"', 'doc': 'Jennifer Jones', 'q_uid': 19050, 'answers': 'Jennifer Jones starred in Good Morning, Miss Dove in 1955, followed by a role in The Man in the Gray Flannel Suit.'}\n",
      "=== Finish feta ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from uda.utils import retrieve as rt\n",
    "from uda.utils import preprocess as pre\n",
    "import pandas as pd\n",
    "from uda.utils import llm\n",
    "from uda.utils import inference\n",
    "import json\n",
    "\n",
    "for DATASET_NAME in DATASET_NAME_LIST:\n",
    "    for LLM_MODEL in LLM_LIST:\n",
    "        print(f\"=== Start {DATASET_NAME} on {LLM_MODEL} ===\")\n",
    "        res_file = os.path.join(res_dir, f\"{DATASET_NAME}_{LLM_MODEL}_{RT_MODEL}.jsonl\")\n",
    "\n",
    "        # If use the local LLM, initialize the model\n",
    "        if LLM_MODEL in LOCAL_LLM_DICT:\n",
    "            llm_name = LOCAL_LLM_DICT[LLM_MODEL]\n",
    "            llm_service = inference.LLM(llm_name)\n",
    "            llm_service.init_llm()\n",
    "\n",
    "        # Load the benchmark data\n",
    "        bench_json_file = pre.meta_data[DATASET_NAME][\"bench_json_file\"]\n",
    "        with open(bench_json_file, \"r\") as f:\n",
    "            bench_data = json.load(f)\n",
    "\n",
    "        # Run experiments on the demo docs\n",
    "        doc_list = list(bench_data.keys())\n",
    "        for doc in doc_list[:DEMO_DOC_NUM]:\n",
    "            pdf_path = pre.get_example_pdf_path(DATASET_NAME, doc)\n",
    "            if pdf_path is None:\n",
    "                continue\n",
    "            # Prepare the index for the document\n",
    "            collection_name = f\"{DATASET_NAME}_vector_db\"\n",
    "            collection = rt.prepare_collection(pdf_path, collection_name, RT_MODEL)\n",
    "            for qa_item in bench_data[doc][:DEMO_QA_NUM]:\n",
    "                question = qa_item[\"question\"]\n",
    "                # Retrieve the contexts\n",
    "                contexts = rt.get_contexts(collection, question, RT_MODEL)\n",
    "                context_text = '\\n'.join(contexts)\n",
    "                # Create the prompt\n",
    "                llm_message = llm.make_prompt(question, context_text, DATASET_NAME, LLM_MODEL)\n",
    "                # Generate the answer\n",
    "                if LLM_MODEL in LOCAL_LLM_DICT:\n",
    "                    response = llm_service.infer(llm_message)\n",
    "                elif LLM_MODEL == \"gpt4\":\n",
    "                    # Set up with your own GPT4 service using environment variables\n",
    "                    response = llm.call_gpt(messages=llm_message)\n",
    "                    if response is None:\n",
    "                        print(\"Make sure your gpt4 service is set up correctly.\")\n",
    "                        raise Exception(\"GPT4 service\")\n",
    "\n",
    "                # log the results\n",
    "                res_dict = {\"model\": LLM_MODEL, \"question\": question, \"response\": response, \"doc\": doc, \"q_uid\": qa_item[\"q_uid\"], \"answers\": qa_item[\"answers\"]}\n",
    "                print(res_dict)\n",
    "                with open(res_file, \"a\") as f:\n",
    "                    f.write(json.dumps(res_dict) + \"\\n\")\n",
    "            rt.reset_collection(collection_name, RT_MODEL)\n",
    "\n",
    "    print(f\"=== Finish {DATASET_NAME} ===\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the end-to-end results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Answer F1': 0.44500000000000006, 'Missing predictions': 0}\n"
     ]
    }
   ],
   "source": [
    "dataset_name=\"feta\"\n",
    "llm_model=\"gpt4\"\n",
    "rt_model=\"bm25\"\n",
    "res_file_name=f\"experiment/e2e/res/{dataset_name}_{llm_model}_{rt_model}.jsonl\"\n",
    "\n",
    "from uda.eval.my_eval import eval_from_file\n",
    "eval_from_file(dataset_name, res_file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
