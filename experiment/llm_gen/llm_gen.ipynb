{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluating Chain-of-Thought and Code Interpreters\n",
    "\n",
    "LLMs may utilize diverse generating strategies for complex queries.\n",
    "\n",
    "We evaluate the performance of Chain-of-Thought, Code Interpreters and basic generation on the numerical reasoning tasks (with dataset FinHybrid)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the project root directory\n",
    "root_dir = Path(os.path.abspath(\"\")).resolve().parents[1]\n",
    "sys.path.append(str(root_dir))\n",
    "# Change the working directory to the project root\n",
    "os.chdir(root_dir)\n",
    "\n",
    "res_dir = f\"experiment/llm_gen/res/\"\n",
    "if not os.path.exists(res_dir):\n",
    "    os.makedirs(res_dir)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our paper, we use the powerful `text-embedding-3-large-model` model with AzureOpenAI-API . But you need to set up with your own api-key, endpoint and deploy-model in the config_file [uda/utils/access_config.py](../../uda/utils/access_config.py). \n",
    "\n",
    "If you want to use the API from **other alternative platforms** please change the codes in [uda/utils/retrieve.py (line-80)](../../uda/utils/retrieve.py#L81). \n",
    "\n",
    "For convenient demonstration, we choose the `BM25` retriever here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental Configurations\n",
    "\n",
    "# Available retrieval model_name: \"bm25\", \"all-MiniLM-L6-v2\", \"all-mpnet-base-v2\", \"openai\", \"colbert\"\n",
    "# We choose bm25 for convenience\n",
    "RT_MODEL = \"bm25\"\n",
    "\n",
    "# This experiment is conducted solely on the FinHybrid dataset\n",
    "DATASET_NAME = \"fin\"\n",
    "\n",
    "LOCAL_LLM_DICT = {\n",
    "    \"llama-8B\": \"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "    \"code-llama-7b\": \"codellama/CodeLlama-7b-Instruct-hf\",\n",
    "    \"code-llama-13b\": \"codellama/CodeLlama-13b-Instruct-hf\",\n",
    "}\n",
    "LLM_LIST = [\"gpt4\", \"llama-8B\", \"code-llama-7b\", \"code-llama-13b\"]\n",
    "\n",
    "LLM_LIST = LLM_LIST[:1]\n",
    "DEMO_DOC_NUM = 2\n",
    "DEMO_QA_NUM = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the **AzureOpenAI-API** serves as the interface for accessing GPT models. Users should set up the gpt-service with their own api-key and endpoint in the config_file [uda/utils/access_config.py](../../uda/utils/access_config.py). These configurations will be used in the `call_gpt()` function in the following codes.\n",
    "\n",
    "\n",
    "If you want to use **other alternative platforms**, the `call_gpt()` can be replaced by the corresponding model-calling function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Start fin on gpt4 with strategy: code ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 12:36:32,981 - INFO - HTTP Request: POST https://qinchuan-hui.openai.azure.com/openai/deployments/yulong-4/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt4', 'question': 'what percentage of total long-term assets under supervision are comprised of fixed income in 2015?', 'response': \"To calculate the percentage of total long-term assets under supervision that are comprised of fixed income in 2015, we need to extract the relevant information from the context:\\n\\n- Fixed income assets under supervision in 2015: $546 billion\\n- Total long-term assets under supervision in 2015: $946 billion\\n\\nNow we can calculate the percentage using the formula:\\n\\n\\\\( \\\\text{Percentage} = \\\\left( \\\\frac{\\\\text{Fixed Income Assets}}{\\\\text{Total Long-Term Assets}} \\\\right) \\\\times 100 \\\\)\\n\\nLet's do the calculation:\\n\\n```python\\nfixed_income_assets_2015 = 546\\ntotal_long_term_assets_2015 = 946\\n\\npercentage_fixed_income_2015 = (fixed_income_assets_2015 / total_long_term_assets_2015) * 100\\nprint(percentage_fixed_income_2015)\\n```\", 'doc': 'GS_2016', 'q_uid': 'GS/2016/page_79.pdf-3', 'answers': {'str_answer': '57%', 'exe_answer': 0.57484}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 12:36:40,450 - INFO - HTTP Request: POST https://qinchuan-hui.openai.azure.com/openai/deployments/yulong-4/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt4', 'question': 'what percentage of total long-term assets under supervision are comprised of fixed income in 2016?', 'response': \"To calculate the percentage of total long-term assets under supervision that are comprised of fixed income in 2016, we need to use the following information from the context:\\n\\n- Fixed income assets under supervision in 2016: $601 billion\\n- Total long-term assets under supervision in 2016: $1,021 billion\\n\\nNow, we can calculate the percentage using the formula:\\n\\n\\\\( \\\\text{Percentage} = \\\\left( \\\\frac{\\\\text{Fixed Income Assets}}{\\\\text{Total Long-Term Assets}} \\\\right) \\\\times 100 \\\\)\\n\\nLet's do the calculation:\\n\\n```python\\nfixed_income_assets = 601\\ntotal_long_term_assets = 1021\\n\\npercentage_fixed_income = (fixed_income_assets / total_long_term_assets) * 100\\nprint(percentage_fixed_income)\\n```\", 'doc': 'GS_2016', 'q_uid': 'GS/2016/page_79.pdf-1', 'answers': {'str_answer': '59%', 'exe_answer': 0.588}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 12:36:49,856 - INFO - HTTP Request: POST https://qinchuan-hui.openai.azure.com/openai/deployments/yulong-4/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt4', 'question': 'what was the percentage change in the 5 year annual performance of the peer group stock from 2010 to 2011', 'response': \"To calculate the percentage change in the 5-year annual performance of the peer group stock from 2010 to 2011, we need to use the values provided for those years:\\n\\nPeer Group:\\n- 2010: 100.00\\n- 2011: 136.78\\n\\nThe formula to calculate the percentage change is:\\n\\n```python\\npercentage_change = ((value_end - value_start) / value_start) * 100\\n```\\n\\nNow, let's plug in the values:\\n\\n```python\\nvalue_start = 100.00\\nvalue_end = 136.78\\n\\npercentage_change = ((value_end - value_start) / value_start) * 100\\nprint(percentage_change)\\n```\\n\\nRunning this code will give us the percentage change from 2010 to 2011 for the peer group stock.\", 'doc': 'JKHY_2015', 'q_uid': 'JKHY/2015/page_20.pdf-2', 'answers': {'str_answer': '8.3%', 'exe_answer': 0.08276}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 12:36:53,932 - INFO - HTTP Request: POST https://qinchuan-hui.openai.azure.com/openai/deployments/yulong-4/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt4', 'question': 'in 2010 , what was the cumulative total return of the s&p 500?', 'response': 'The cumulative total return of the S&P 500 in 2010 is given as 100.00. This is the baseline value for comparison, and it represents an assumed $100 investment at the beginning of the period.\\n\\nThe answer is: 100.00', 'doc': 'JKHY_2015', 'q_uid': 'JKHY/2015/page_20.pdf-3', 'answers': {'str_answer': '30.69', 'exe_answer': 30.69}}\n",
      "=== Finish code on gpt4 ===\n",
      "\n",
      "=== Start fin on gpt4 with strategy: cot ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 12:37:05,662 - INFO - HTTP Request: POST https://qinchuan-hui.openai.azure.com/openai/deployments/yulong-4/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt4', 'question': 'what percentage of total long-term assets under supervision are comprised of fixed income in 2015?', 'response': 'In 2015, the total long-term assets under supervision were $946 billion, and the fixed income assets under supervision were $546 billion.\\n\\nTo calculate the percentage of fixed income assets in the total long-term assets under supervision for 2015:\\n\\n($546 billion / $946 billion) * 100 = 57.72%\\n\\nThe answer is: 57.72%', 'doc': 'GS_2016', 'q_uid': 'GS/2016/page_79.pdf-3', 'answers': {'str_answer': '57%', 'exe_answer': 0.57484}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 12:37:11,086 - INFO - HTTP Request: POST https://qinchuan-hui.openai.azure.com/openai/deployments/yulong-4/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt4', 'question': 'what percentage of total long-term assets under supervision are comprised of fixed income in 2016?', 'response': 'In 2016, the total long-term assets under supervision are $1,021 billion, and the fixed income assets under supervision are $601 billion.\\n\\nTo find the percentage of total long-term assets under supervision that are comprised of fixed income, we use the formula:\\n\\n(Fixed Income Assets / Total Long-Term Assets) * 100\\n\\nSo, the calculation is:\\n\\n($601 billion / $1,021 billion) * 100 = 58.86%\\n\\nThe answer is: 58.86%', 'doc': 'GS_2016', 'q_uid': 'GS/2016/page_79.pdf-1', 'answers': {'str_answer': '59%', 'exe_answer': 0.588}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 12:37:15,802 - INFO - HTTP Request: POST https://qinchuan-hui.openai.azure.com/openai/deployments/yulong-4/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt4', 'question': 'what was the percentage change in the 5 year annual performance of the peer group stock from 2010 to 2011', 'response': 'The answer is: 36.78%', 'doc': 'JKHY_2015', 'q_uid': 'JKHY/2015/page_20.pdf-2', 'answers': {'str_answer': '8.3%', 'exe_answer': 0.08276}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 12:37:18,585 - INFO - HTTP Request: POST https://qinchuan-hui.openai.azure.com/openai/deployments/yulong-4/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt4', 'question': 'in 2010 , what was the cumulative total return of the s&p 500?', 'response': 'The answer is: 100.00', 'doc': 'JKHY_2015', 'q_uid': 'JKHY/2015/page_20.pdf-3', 'answers': {'str_answer': '30.69', 'exe_answer': 30.69}}\n",
      "=== Finish cot on gpt4 ===\n",
      "\n",
      "=== Start fin on gpt4 with strategy: basic ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 12:37:24,130 - INFO - HTTP Request: POST https://qinchuan-hui.openai.azure.com/openai/deployments/yulong-4/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt4', 'question': 'what percentage of total long-term assets under supervision are comprised of fixed income in 2015?', 'response': 'The answer is: 57.7%', 'doc': 'GS_2016', 'q_uid': 'GS/2016/page_79.pdf-3', 'answers': {'str_answer': '57%', 'exe_answer': 0.57484}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 12:37:26,722 - INFO - HTTP Request: POST https://qinchuan-hui.openai.azure.com/openai/deployments/yulong-4/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt4', 'question': 'what percentage of total long-term assets under supervision are comprised of fixed income in 2016?', 'response': 'The answer is: 58.76%', 'doc': 'GS_2016', 'q_uid': 'GS/2016/page_79.pdf-1', 'answers': {'str_answer': '59%', 'exe_answer': 0.588}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 12:37:30,355 - INFO - HTTP Request: POST https://qinchuan-hui.openai.azure.com/openai/deployments/yulong-4/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt4', 'question': 'what was the percentage change in the 5 year annual performance of the peer group stock from 2010 to 2011', 'response': 'The answer is: 36.78%.', 'doc': 'JKHY_2015', 'q_uid': 'JKHY/2015/page_20.pdf-2', 'answers': {'str_answer': '8.3%', 'exe_answer': 0.08276}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 12:37:32,679 - INFO - HTTP Request: POST https://qinchuan-hui.openai.azure.com/openai/deployments/yulong-4/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt4', 'question': 'in 2010 , what was the cumulative total return of the s&p 500?', 'response': 'The answer is: 100.00', 'doc': 'JKHY_2015', 'q_uid': 'JKHY/2015/page_20.pdf-3', 'answers': {'str_answer': '30.69', 'exe_answer': 30.69}}\n",
      "=== Finish basic on gpt4 ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from uda.utils import retrieve as rt\n",
    "from uda.utils import retrieve_exp as rt_exp\n",
    "from uda.utils import preprocess as pre\n",
    "import pandas as pd\n",
    "from uda.utils import llm\n",
    "from uda.utils import inference\n",
    "import json\n",
    "\n",
    "# Strategies for LLM-generating \n",
    "strategeis = [\"code\", \"cot\", \"basic\"]\n",
    "\n",
    "for LLM_MODEL in LLM_LIST:\n",
    "    for strategy in strategeis:\n",
    "        print(f\"=== Start {DATASET_NAME} on {LLM_MODEL} with strategy: {strategy} ===\")\n",
    "        res_file = os.path.join(res_dir, f\"{strategy}_{LLM_MODEL}_{RT_MODEL}.jsonl\")\n",
    "\n",
    "        # If use the local LLM, initialize the model\n",
    "        if LLM_MODEL in LOCAL_LLM_DICT:\n",
    "            llm_name = LOCAL_LLM_DICT[LLM_MODEL]\n",
    "            llm_service = inference.LLM(llm_name)\n",
    "            llm_service.init_llm()\n",
    "\n",
    "        # Load the benchmark data\n",
    "        bench_json_file = pre.meta_data[DATASET_NAME][\"bench_json_file\"]\n",
    "        with open(bench_json_file, \"r\") as f:\n",
    "            bench_data = json.load(f)\n",
    "\n",
    "        # Run experiments on the demo docs\n",
    "        doc_list = list(bench_data.keys())\n",
    "        for doc in doc_list[:DEMO_DOC_NUM]:\n",
    "            pdf_path = pre.get_example_pdf_path(DATASET_NAME, doc)\n",
    "            if pdf_path is None:\n",
    "                continue\n",
    "            # Prepare the index for the document\n",
    "            collection_name = f\"{DATASET_NAME}_vector_db\"\n",
    "            collection = rt.prepare_collection(pdf_path, collection_name, RT_MODEL)\n",
    "            for qa_item in bench_data[doc][:DEMO_QA_NUM]:\n",
    "                question = qa_item[\"question\"]\n",
    "                # Retrieve the contexts\n",
    "                contexts = rt.get_contexts(collection, question, RT_MODEL)\n",
    "                context_text = '\\n'.join(contexts)\n",
    "\n",
    "                # Create the prompt for different generating strategies\n",
    "                if strategy == \"code\":\n",
    "                    llm_message = llm.make_prompt_code(question, context_text, DATASET_NAME, LLM_MODEL)\n",
    "                elif strategy == \"cot\":\n",
    "                    llm_message = llm.make_prompt(question, context_text, DATASET_NAME, LLM_MODEL)\n",
    "                elif strategy == \"basic\":\n",
    "                    llm_message = llm.make_prompt_basic(question, context_text, DATASET_NAME, LLM_MODEL)\n",
    "                \n",
    "                # Generate the answer\n",
    "                if LLM_MODEL in LOCAL_LLM_DICT:\n",
    "                    response = llm_service.infer(llm_message)\n",
    "                elif LLM_MODEL == \"gpt4\":\n",
    "                    # Set up with your own GPT4 service\n",
    "                    response = llm.call_gpt(messages=llm_message)\n",
    "                    if response is None:\n",
    "                        print(\"Make sure your gpt4 service is set up correctly.\")\n",
    "                        raise Exception(\"GPT4 service\")\n",
    "\n",
    "                # log the results\n",
    "                res_dict = {\"model\": LLM_MODEL, \"question\": question, \"response\": response, \"doc\": doc, \"q_uid\": qa_item[\"q_uid\"], \"answers\": qa_item[\"answers\"]}\n",
    "                print(res_dict)\n",
    "                with open(res_file, \"a\") as f:\n",
    "                    f.write(json.dumps(res_dict) + \"\\n\")\n",
    "            rt.reset_collection(collection_name, RT_MODEL)\n",
    "        print(f\"=== Finish {strategy} on {LLM_MODEL} ===\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the results with different strategies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact-match accuracy: 50.00\n"
     ]
    }
   ],
   "source": [
    "strategy=\"cot\"\n",
    "llm_model=\"gpt4\"\n",
    "rt_model=\"bm25\"\n",
    "res_file_name=f\"experiment/llm_gen/res/{strategy}_{llm_model}_{rt_model}.jsonl\"\n",
    "\n",
    "from uda.eval.my_eval import eval_from_file\n",
    "if strategy == \"code\":\n",
    "    code_gen_flag = True\n",
    "else:\n",
    "    code_gen_flag = False\n",
    "eval_from_file(\"fin\", res_file_name, code_gen_flag)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
