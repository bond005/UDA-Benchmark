{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Long-context LLMs\n",
    "We use long-context LLMs compared with RAG-based methods , utilizing `GPT-4-Turbo` with a 128k context window and `Qwen-1.5-7B` with a 32k context window."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the project root directory\n",
    "root_dir = Path(os.path.abspath(\"\")).resolve().parents[1]\n",
    "sys.path.append(str(root_dir))\n",
    "# Change the working directory to the project root\n",
    "os.chdir(root_dir)\n",
    "\n",
    "# Set up the configs for this demo\n",
    "res_dir = f\"experiment/long_context/res/\"\n",
    "if not os.path.exists(res_dir):\n",
    "    os.makedirs(res_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In our paper, we use the powerful `text-embedding-3-large-model` model with AzureOpenAI-API . But you need to set up with your own api-key, endpoint and deploy-model in the config_file [uda/utils/access_config.py](../../uda/utils/access_config.py). \n",
    "\n",
    "If you want to use the API from **other alternative platforms** please change the codes in [uda/utils/retrieve.py (line-80)](../../uda/utils/retrieve.py#L81). \n",
    "\n",
    "For convenient demonstration, we choose the `BM25` retriever here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental Configurations\n",
    "\n",
    "# Available retrieval model_name: \"bm25\", \"all-MiniLM-L6-v2\", \"all-mpnet-base-v2\", \"openai\", \"colbert\"\n",
    "# We choose bm25 for convenience\n",
    "RT_MODEL = \"bm25\" \n",
    "\n",
    "\n",
    "DATASET_NAME_LIST = [\"paper_text\", \"fin\", \"paper_tab\",  \"feta\", \"nq\", \"fin\"]\n",
    "\n",
    "LOCAL_LLM_DICT = {\"qwen-7B\": \"Qwen/Qwen1.5-7B-Chat\"}\n",
    "LLM_LIST = [\"gpt4\", \"qwen-7B\"]\n",
    "\n",
    "# Sample a subset of LLMs for faster demo\n",
    "DATASET_NAME_LIST = DATASET_NAME_LIST[:2]\n",
    "LLM_LIST = LLM_LIST[:1]\n",
    "DEMO_DOC_NUM = 2\n",
    "DEMO_QA_NUM = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We utilized the Qwen-1.5-7B-32k and GPT4-Turbo-128k models, capable of processing larger text blocks. However, it may still be insufficient when dealing with quite lengthy documents, like financial reports that surpass 100k words (over 150k tokens). \n",
    "\n",
    "For such cases, full-context will reverts to a retrieval-based approach, selecting the 30 most relevant chunks to construct the input context."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Functions to implement full-context policy\n",
    "from uda.utils.retrieve import extract_text_from_pdf\n",
    "from uda.utils import preprocess as pre\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "def __get_token_length(text):\n",
    "    # use the llama tokenizer to calculate the approximate token length\n",
    "    model_name = \"meta-llama/Meta-Llama-3-8B-Instruct\" \n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        model_name,\n",
    "        trust_remote_code=True,\n",
    "        resume_download=True,\n",
    "        use_fast=True,\n",
    "    )\n",
    "    token_res = tokenizer(text)[\"input_ids\"]\n",
    "    return len(token_res)\n",
    "\n",
    "\n",
    "def get_full_context(pdf_path, llm_type):\n",
    "    if llm_type == \"gpt4\":\n",
    "        CONTEXT_SIZE = 127 * 1024  # 1k tokens for prompt\n",
    "    if llm_type == \"qwen-7B\":\n",
    "        CONTEXT_SIZE = 31 * 1024\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    token_length = __get_token_length(text)\n",
    "    if token_length < CONTEXT_SIZE:\n",
    "        return text\n",
    "    else:       \n",
    "        print(f\"Token length of {pdf_path}: {token_length}, exceeds the limit. Full-context falls back to top-30 relevant chunks.\") \n",
    "        return None # the caller code will implement top-30 RAG policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We use the **AzureOpenAI-API** serves as the interface for accessing GPT models. Users should set up the gpt-service with their own api-key and endpoint in the config_file [uda/utils/access_config.py](../../uda/utils/access_config.py). These configurations will be used in the `call_gpt()` function in the following codes.\n",
    "\n",
    "\n",
    "If you want to use **other alternative platforms**, the `call_gpt()` can be replaced by the corresponding model-calling function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Local Inference with Long-Context LLMs\n",
    "\n",
    "Additionally, executing local inference on long-context LLMs can be memory-intensive, often necessitating significant GPU memory.\n",
    "\n",
    " To alleviate this, you should install the `flashattention` [library](https://github.com/Dao-AILab/flash-attention). This optimization can be enabled by adding the parameter `attn_implementation=\"flash_attention_2\"` within the `inference.py` script, which can be found at [uda/utils/inference.py, line 27](../../uda/utils/inference.py#L27)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Start paper_text on gpt4 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "2024-07-05 12:39:43,856 - INFO - HTTP Request: POST https://qinchuan-hui.openai.azure.com/openai/deployments/yulong-4/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt4', 'question': 'Do they perform error analysis?', 'response': 'The answer is: No, they do not perform error analysis.', 'doc': '2001.03131', 'q_uid': '133eb4aa4394758be5f41744c60c99901b2bc01c', 'answers': [{'answer': 'No', 'type': 'boolean'}, {'answer': 'No', 'type': 'boolean'}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 12:39:51,074 - INFO - HTTP Request: POST https://qinchuan-hui.openai.azure.com/openai/deployments/yulong-4/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt4', 'question': 'What is the Random Kitchen Sink approach?', 'response': 'The answer is: The Random Kitchen Sink (RKS) approach is a method that explicitly maps data vectors to a space where linear separation is possible, and in that space, regularized least-square based classification (RLSC) is performed.', 'doc': '2001.03131', 'q_uid': 'a778b8204a415b295f73b93623d09599f242f202', 'answers': [{'answer': 'Random Kitchen Sink method uses a kernel function to map data vectors to a space where linear separation is possible.', 'type': 'abstractive'}, {'answer': 'explicitly maps data vectors to a space where linear separation is possible, RKS method provides an approximate kernel function via explicit mapping', 'type': 'extractive'}]}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "2024-07-05 12:39:58,179 - INFO - HTTP Request: POST https://qinchuan-hui.openai.azure.com/openai/deployments/yulong-4/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt4', 'question': 'how are multiple answers from multiple reformulated questions aggregated?', 'response': 'The answer is: using a convolutional neural network to select the best answer from the set of answers returned for each reformulation.', 'doc': '1705.07830', 'q_uid': '33d2919f3400cd3c6fbb6960d74187ec80b41cd6', 'answers': [{'answer': 'The selection model selects the best answer from the set $\\\\lbrace a_i\\\\rbrace _{i=1}^N$ observed during the interaction by predicting the difference of the F1 score to the average F1 of all variants.', 'type': 'extractive'}]}\n",
      "=== Finish paper_text ===\n",
      "\n",
      "=== Start fin on gpt4 ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Token length of dataset/src_doc_files_example/fin_docs/GS_2016.pdf: 215029, exceeds the limit. Full-context falls back to top-30 relevant chunks.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 12:40:15,450 - INFO - HTTP Request: POST https://qinchuan-hui.openai.azure.com/openai/deployments/yulong-4/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt4', 'question': 'what percentage of total long-term assets under supervision are comprised of fixed income in 2015?', 'response': 'In 2015, the total long-term assets under supervision were $946 billion, and fixed income assets under supervision were $546 billion.\\n\\nTo calculate the percentage of total long-term assets under supervision comprised of fixed income, we use the formula:\\n\\n(Fixed Income Assets / Total Long-Term Assets) * 100\\n\\nSo, the calculation is:\\n\\n($546 billion / $946 billion) * 100 = 57.72%\\n\\nThe answer is: 57.72%', 'doc': 'GS_2016', 'q_uid': 'GS/2016/page_79.pdf-3', 'answers': {'str_answer': '57%', 'exe_answer': 0.57484}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 12:40:26,796 - INFO - HTTP Request: POST https://qinchuan-hui.openai.azure.com/openai/deployments/yulong-4/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt4', 'question': 'what percentage of total long-term assets under supervision are comprised of fixed income in 2016?', 'response': 'The total long-term assets under supervision in 2016 are $1,021 billion, and the fixed income assets under supervision are $601 billion. To calculate the percentage:\\n\\n(601 / 1,021) * 100 = 58.86%\\n\\nThe answer is: 58.86%', 'doc': 'GS_2016', 'q_uid': 'GS/2016/page_79.pdf-1', 'answers': {'str_answer': '59%', 'exe_answer': 0.588}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "2024-07-05 12:40:41,081 - INFO - HTTP Request: POST https://qinchuan-hui.openai.azure.com/openai/deployments/yulong-4/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt4', 'question': 'what was the percentage change in the 5 year annual performance of the peer group stock from 2010 to 2011', 'response': 'The percentage change in the 5-year annual performance of the peer group stock from 2010 to 2011 was 36.78% - 100.00% = -63.22%.\\n\\nThe answer is: -63.22%', 'doc': 'JKHY_2015', 'q_uid': 'JKHY/2015/page_20.pdf-2', 'answers': {'str_answer': '8.3%', 'exe_answer': 0.08276}}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 12:40:52,010 - INFO - HTTP Request: POST https://qinchuan-hui.openai.azure.com/openai/deployments/yulong-4/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt4', 'question': 'in 2010 , what was the cumulative total return of the s&p 500?', 'response': 'The cumulative total return of the S&P 500 in 2010 was 130.69. \\n\\nThe answer is: 130.69', 'doc': 'JKHY_2015', 'q_uid': 'JKHY/2015/page_20.pdf-3', 'answers': {'str_answer': '30.69', 'exe_answer': 30.69}}\n",
      "=== Finish fin ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from uda.utils import retrieve as rt\n",
    "from uda.utils import preprocess as pre\n",
    "import pandas as pd\n",
    "from uda.utils import llm\n",
    "from uda.utils import inference\n",
    "import json\n",
    "\n",
    "for DATASET_NAME in DATASET_NAME_LIST:\n",
    "    for LLM_MODEL in LLM_LIST:\n",
    "        print(f\"=== Start {DATASET_NAME} on {LLM_MODEL} ===\")\n",
    "        res_file = os.path.join(res_dir, f\"{DATASET_NAME}_{LLM_MODEL}_{RT_MODEL}.jsonl\")\n",
    "\n",
    "        # If use the local LLM, initialize the model\n",
    "        if LLM_MODEL in LOCAL_LLM_DICT:\n",
    "            llm_name = LOCAL_LLM_DICT[LLM_MODEL]\n",
    "            llm_service = inference.LLM(llm_name)\n",
    "            llm_service.init_llm()\n",
    "\n",
    "        # Load the benchmark data\n",
    "        bench_json_file = pre.meta_data[DATASET_NAME][\"bench_json_file\"]\n",
    "        with open(bench_json_file, \"r\") as f:\n",
    "            bench_data = json.load(f)\n",
    "\n",
    "        # Run experiments on the demo docs\n",
    "        doc_list = list(bench_data.keys())\n",
    "        for doc in doc_list[:DEMO_DOC_NUM]:\n",
    "            pdf_path = pre.get_example_pdf_path(DATASET_NAME, doc)\n",
    "            if pdf_path is None:\n",
    "                continue\n",
    "\n",
    "            # Get full context\n",
    "            full_context = get_full_context(pdf_path, LLM_MODEL)\n",
    "            # If exceed the context window, fall back to the RAG with Top30 chunks\n",
    "            if full_context is None:\n",
    "                # Prepare the index for RAG\n",
    "                collection_name = f\"{DATASET_NAME}_vector_db\"\n",
    "                collection = rt.prepare_collection(pdf_path, collection_name, RT_MODEL)\n",
    "\n",
    "            for qa_item in bench_data[doc][:DEMO_QA_NUM]:\n",
    "                question = qa_item[\"question\"]\n",
    "                q_uid = qa_item[\"q_uid\"]\n",
    "                if full_context is None:\n",
    "                    # Retrieve the Top30 contexts\n",
    "                    contexts = rt.get_contexts(collection, question, RT_MODEL,top_k=30)\n",
    "                    context_text = '\\n'.join(contexts)\n",
    "                else:\n",
    "                    context_text = full_context\n",
    "                # Create the prompt\n",
    "                llm_message = llm.make_prompt(question, context_text, DATASET_NAME, LLM_MODEL)\n",
    "                # Generate the answer\n",
    "                if LLM_MODEL in LOCAL_LLM_DICT:\n",
    "                    response = llm_service.infer(llm_message)\n",
    "                elif LLM_MODEL == \"gpt4\":\n",
    "                    # Set up with your own GPT4 service\n",
    "                    response = llm.call_gpt(messages=llm_message)\n",
    "                    if response is None:\n",
    "                        print(\"Make sure your gpt4 service is set up correctly.\")\n",
    "                        raise Exception(\"GPT4 service\")\n",
    "\n",
    "                # log the results\n",
    "                res_dict = {\"model\": LLM_MODEL, \"question\": question, \"response\": response, \"doc\": doc, \"q_uid\": q_uid, \"answers\": qa_item[\"answers\"]}\n",
    "                print(res_dict)\n",
    "                with open(res_file, \"a\") as f:\n",
    "                    f.write(json.dumps(res_dict) + \"\\n\")\n",
    "            \n",
    "            if full_context is None:\n",
    "                rt.reset_collection(collection_name, RT_MODEL)\n",
    "            \n",
    "\n",
    "    print(f\"=== Finish {DATASET_NAME} ===\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the generating results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Answer F1': 0.6114498644986449, 'Missing predictions': 0}\n"
     ]
    }
   ],
   "source": [
    "dataset_name=\"paper_text\"\n",
    "llm_model=\"gpt4\"\n",
    "rt_model=\"bm25\"\n",
    "res_file_name=f\"experiment/long_context/res/{dataset_name}_{llm_model}_{rt_model}.jsonl\"\n",
    "\n",
    "from uda.eval.my_eval import eval_from_file\n",
    "eval_from_file(dataset_name, res_file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
