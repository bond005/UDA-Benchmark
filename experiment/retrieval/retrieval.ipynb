{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluations on indexing and retrieval\n",
    "<!-- We want to evalute:\n",
    "* The performance of different indexing and retrieval strategies, spanning sparse retrieval, classic dense embedding, and advanced retrieval model.\n",
    "\n",
    "* The influence of precise retrieval on the quality of LLM interpretation -->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Models\n",
    "\n",
    "We consider the following 5 indexing and retrieval methods:\n",
    "\n",
    "1) BM-25, a lightweight sparse retrieval method without complex neural networks, ranking document segments based on the appearing frequency of query terms.\n",
    "\n",
    "2) all-MiniLM-L6, from SentenceTransformer, a prevalent dense embedding model, mapping sentences to a 384-dimensional dense vector space. \n",
    "\n",
    "3) all-mpnet-base, another widely utilized embedding model from SentenceTransformer, noted for its larger architecture and improved performance. \n",
    "\n",
    "4) text-embedding-3-large-model, the latest embedding model from OpenAI, with enhanced capability. \n",
    "\n",
    "5) ColBERT, an advanced retrieval model, relying on token-level embedding and fine-grained contextual late interaction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the project root directory\n",
    "root_dir = Path(os.path.abspath(\"\")).resolve().parents[1]\n",
    "sys.path.append(str(root_dir))\n",
    "# Change the working directory to the project root\n",
    "os.chdir(root_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the configs for this demo\n",
    "DEMO_SIZE = 2\n",
    "res_dir = f\"experiment/retrieval/res/\"\n",
    "if not os.path.exists(res_dir):\n",
    "    os.makedirs(res_dir)\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the retrieval experiments, utilizing the functional implementation provided within the `uda.utils` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda-11.8'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Start fin on bm25 ===\n",
      "Retrieval-Match-Scores {'doc_name': 'GS_2016', 'q_uid': 'GS/2016/page_79.pdf-3', 'Top-1': 0.7555555555555555, 'Top-5': 0.9333333333333333, 'Top-10': 0.9333333333333333, 'Top-20': 0.9333333333333333, 'Top-30': 0.9333333333333333}\n",
      "Retrieval-Match-Scores {'doc_name': 'GS_2016', 'q_uid': 'GS/2016/page_79.pdf-1', 'Top-1': 0.7555555555555555, 'Top-5': 0.9333333333333333, 'Top-10': 0.9333333333333333, 'Top-20': 0.9333333333333333, 'Top-30': 0.9333333333333333}\n",
      "Retrieval-Match-Scores {'doc_name': 'GS_2016', 'q_uid': 'GS/2016/page_161.pdf-1', 'Top-1': 0.631578947368421, 'Top-5': 0.9836065573770492, 'Top-10': 0.9836065573770492, 'Top-20': 0.9836065573770492, 'Top-30': 0.9836065573770492}\n",
      "Retrieval-Match-Scores {'doc_name': 'GS_2016', 'q_uid': 'GS/2016/page_183.pdf-3', 'Top-1': 0.2857142857142857, 'Top-5': 1.0, 'Top-10': 1.0, 'Top-20': 1.0, 'Top-30': 1.0}\n",
      "Retrieval-Match-Scores {'doc_name': 'GS_2016', 'q_uid': 'GS/2016/page_186.pdf-2', 'Top-1': 0.9411764705882353, 'Top-5': 0.9411764705882353, 'Top-10': 0.9411764705882353, 'Top-20': 0.9411764705882353, 'Top-30': 0.9411764705882353}\n",
      "Retrieval-Match-Scores {'doc_name': 'GS_2016', 'q_uid': 'GS/2016/page_79.pdf-4', 'Top-1': 0.6666666666666666, 'Top-5': 0.7192982456140351, 'Top-10': 0.7368421052631579, 'Top-20': 0.7719298245614035, 'Top-30': 0.7719298245614035}\n",
      "=== Finish fin ===\n",
      "\n",
      "=== Start paper_tab on bm25 ===\n",
      "Retrieval-Match-Scores {'doc_name': '1809.01202', 'q_uid': '4cbe5a36b492b99f9f9fea8081fe4ba10a7a0e94', 'Top-1': 0.24107142857142858, 'Top-5': 0.6800595238095238, 'Top-10': 0.7514880952380952, 'Top-20': 0.8705357142857143, 'Top-30': 0.8705357142857143}\n",
      "=== Finish paper_tab ===\n",
      "\n",
      "=== Start paper_text on bm25 ===\n",
      "No human-anotated evidence for file 2001.03131 and qa_id 133eb4aa4394758be5f41744c60c99901b2bc01c\n",
      "Retrieval-Match-Scores {'doc_name': '2001.03131', 'q_uid': 'a778b8204a415b295f73b93623d09599f242f202', 'Top-1': 0.25757575757575757, 'Top-5': 0.8825757575757576, 'Top-10': 0.8825757575757576, 'Top-20': 0.8825757575757576, 'Top-30': 0.8825757575757576}\n",
      "=== Finish paper_text ===\n",
      "\n",
      "=== Start nq on bm25 ===\n",
      "Retrieval-Match-Scores {'doc_name': 'Hannah John-Kamen', 'q_uid': -6718102858366318183, 'Top-1': 0.23157894736842105, 'Top-5': 0.8105263157894737, 'Top-10': 0.8105263157894737, 'Top-20': 0.8105263157894737, 'Top-30': 0.8105263157894737}\n",
      "=== Finish nq ===\n",
      "\n",
      "=== Start feta on bm25 ===\n",
      "Retrieval-Match-Scores {'doc_name': 'Smallville', 'q_uid': 12844, 'Top-1': 0.7878787878787878, 'Top-5': 0.851619644723093, 'Top-10': 0.8725182863113897, 'Top-20': 0.9487983281086729, 'Top-30': 0.9885057471264368}\n",
      "=== Finish feta ===\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from uda.utils import retrieve as rt\n",
    "from uda.utils import retrieve_exp as rt_exp\n",
    "from uda.utils import preprocess as pre\n",
    "import json\n",
    "\n",
    "DATASET_NAME_LIST = [\"fin\", \"paper_tab\", \"paper_text\", \"nq\", \"feta\"]\n",
    "RT_MODEL_LIST = [\"bm25\", \"all-MiniLM-L6-v2\", \"all-mpnet-base-v2\", \"openai\", \"colbert\"]\n",
    "# The procedure of complex models may be time-consuming, you can choose to run a sub-list of models and datasets\n",
    "DATASET_NAME_LIST = DATASET_NAME_LIST[:]\n",
    "RT_MODEL_LIST = RT_MODEL_LIST[:1]\n",
    "\n",
    "\n",
    "for DATASET_NAME in DATASET_NAME_LIST:\n",
    "    for RT_MODEL in RT_MODEL_LIST:\n",
    "        print(f\"=== Start {DATASET_NAME} on {RT_MODEL} ===\")\n",
    "        res_file = os.path.join(res_dir, f\"{DATASET_NAME}_{RT_MODEL}.jsonl\")\n",
    "        bench_json_file = pre.meta_data[DATASET_NAME][\"bench_json_file\"]\n",
    "        with open(bench_json_file, \"r\") as f:\n",
    "            bench_data = json.load(f)\n",
    "        doc_list = list(bench_data.keys())\n",
    "        for doc in doc_list[:1]:\n",
    "            pdf_path = pre.get_example_pdf_path(DATASET_NAME, doc)\n",
    "            if pdf_path is None:\n",
    "                continue\n",
    "            for qa_item in bench_data[doc]:\n",
    "                question = qa_item[\"question\"]\n",
    "                q_uid = qa_item[\"q_uid\"]\n",
    "                collection_name = f\"{DATASET_NAME}_vector_db\"\n",
    "                # Prepare the index\n",
    "                collection = rt.prepare_collection(pdf_path, collection_name, RT_MODEL)\n",
    "                # Retrieve the contexts\n",
    "                contexts = rt.get_contexts(collection, question, RT_MODEL)\n",
    "                # Save the results\n",
    "                rt_exp.log_score(\n",
    "                    contexts, doc, q_uid, DATASET_NAME, res_file, bench_json_file\n",
    "                )\n",
    "    print(f\"=== Finish {DATASET_NAME} ===\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Get the averaged retrieval matching scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===== bm25 on fin =====\n",
      "   avg_1_score  avg_5_score  avg_10_score  avg_20_score\n",
      "0     0.672708     0.918458      0.921382       0.92723\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "\n",
    "def get_avg_score(file_path):\n",
    "    with open(file_path,\"r\") as f:\n",
    "        lines=f.readlines()\n",
    "        data_list=[json.loads(l) for l in lines]\n",
    "    df=pd.DataFrame(data_list)\n",
    "    avg_1_score=df[\"Top-1\"].mean()\n",
    "    avg_5_score=df[\"Top-5\"].mean()\n",
    "    avg_10_score=df[\"Top-10\"].mean()\n",
    "    avg_20_score=df[\"Top-20\"].mean()\n",
    "    res_df=pd.DataFrame({\"avg_1_score\":[avg_1_score],\"avg_5_score\":[avg_5_score],\"avg_10_score\":[avg_10_score],\"avg_20_score\":[avg_20_score]})\n",
    "    return res_df\n",
    "\n",
    "# rt_models=[\"bm25\",\"all-MiniLM-L6-v2\",\"all-mpnet-base-v2\",\"openai\",\"colbert\"]\n",
    "dataset_name=\"fin\"\n",
    "rt_model=\"bm25\"\n",
    "# relative path based on the project root\n",
    "res_file_name=f\"experiment/retrieval/res/{dataset_name}_{rt_model}.jsonl\" \n",
    "res_df=get_avg_score(res_file_name)\n",
    "print(f\"===== {rt_model} on {dataset_name} =====\")\n",
    "print(res_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
