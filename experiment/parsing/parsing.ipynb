{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluations on table parsing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate various parsing methods to extract tabular information from PDF files and analyze their influence on the downstream Q&A tasks. \n",
    "\n",
    "When measure the table-parsing methods, each question is paired with a PDF page contains the clue tables; doing so prevents inaccurate retrievals. \n",
    "\n",
    "We utilize the question set from PaperTab and the table-based questions from FinHybrid."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preparation\n",
    "import sys\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Get the project root directory\n",
    "root_dir = Path(os.path.abspath(\"\")).resolve().parents[1]\n",
    "sys.path.append(str(root_dir))\n",
    "# Change the working directory to the project root\n",
    "os.chdir(root_dir)\n",
    "\n",
    "res_dir = f\"experiment/parsing/res/\"\n",
    "if not os.path.exists(res_dir):\n",
    "    os.makedirs(res_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experimental Configurations\n",
    "\n",
    "DATASET_NAME_LIST = [\"fin\", \"paper_tab\"]\n",
    "LOCAL_LLM_DICT = {\"llama-8B\": \"meta-llama/Meta-Llama-3-8B-Instruct\"}\n",
    "LLM_LIST = [\"gpt4\", \"llama-8B\"]\n",
    "\n",
    "LLM_LIST = LLM_LIST[:1]\n",
    "DEMO_DOC_NUM = 1\n",
    "DEMO_QA_NUM = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We evaluate several varied approaches of table parsing:\n",
    "* **Raw text extraction**, which employs a PDF text extractor `PyPDF` to extract all the characters. \n",
    "  \n",
    "*  Classic Computer Vision (**CV**) based approach, which often performs layout detection and OCR extraction at the same time. We use `Unstructured` library to use Yolox, Tesseract  and TableTransformer models together. \n",
    "  \n",
    "*  **CV + LLM** method, which further employs an LLM to transform the outputs of (2) into Markdown tables. \n",
    "  \n",
    "*  For the advanced multi-modal approach, we employ the latest **GPT-4-Omni** to convert image-based\n",
    "document tables into Markdown format. \n",
    "\n",
    "* The FinHybrid dataset provides the verified **well-parsed** tables, which serve as the parsing ground truth."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Detailded parsing strategies are implemented in [uda/utils/parsing_exp.py](../../uda/utils/parsing_exp.py), and the following codes just call the encapsulated functions from it.\n",
    "\n",
    "You need to install the [unstructured](https://docs.unstructured.io/open-source/installation/full-installation) library to run the CV and CV-LLM strategies.\n",
    "\n",
    "We use the **AzureOpenAI-API**  as the interface for accessing GPT-4 and GPT-4o models. Users should set up the gpt-service with their own api-key and endpoint in the config_file [uda/utils/access_config.py](../../uda/utils/access_config.py). \n",
    "\n",
    "If you want to use **other alternative platforms**, the following `call_gpt()` function and the [get_omni_table](../../uda/utils/parsing_exp.py#L185) function can be replaced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/langchain/lib/python3.11/site-packages/torch/cuda/__init__.py:611: UserWarning: Can't initialize NVML\n",
      "  warnings.warn(\"Can't initialize NVML\")\n",
      "No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda-11.8'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== ðŸŒŸ Start fin on gpt4 with parsing strategy raw_extract ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 13:03:44,980 - INFO - HTTP Request: POST https://qinchuan-hui.openai.azure.com/openai/deployments/yulong-4/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt4', 'question': 'what percentage of total long-term assets under supervision are comprised of fixed income in 2015?', 'response': 'The total long-term assets under supervision in 2015 were $922 billion, and the fixed income assets under supervision were $530 billion.\\n\\nTo calculate the percentage of fixed income assets in the total long-term assets under supervision for 2015:\\n\\n( Fixed Income / Total Long-Term Assets ) * 100\\n( $530 billion / $922 billion ) * 100 = 57.48%\\n\\nThe answer is: 57.48%', 'doc': 'GS_2016', 'q_uid': 'GS/2016/page_79.pdf-3', 'answers': {'str_answer': '57%', 'exe_answer': 0.57484}}\n",
      "=== ðŸŒŸ Start fin on gpt4 with parsing strategy cv ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 13:03:46,174 - INFO - Reading PDF for file: ./experiment/parsing/parsing_tmp/fin_tmp.pdf ...\n",
      "2024-07-05 13:03:46,342 - INFO - Detecting page elements ...\n",
      "2024-07-05 13:03:47,609 - INFO - Processing entire page OCR with tesseract...\n",
      "2024-07-05 13:03:50,660 - INFO - Loading the table structure model ...\n",
      "2024-07-05 13:03:51,265 - INFO - Loading pretrained weights from Hugging Face hub (timm/resnet18.a1_in1k)\n",
      "2024-07-05 13:03:51,482 - INFO - [timm/resnet18.a1_in1k] Safe alternative available for 'pytorch_model.bin' (as 'model.safetensors'). Loading weights using safetensors.\n",
      "Some weights of the model checkpoint at microsoft/table-transformer-structure-recognition were not used when initializing TableTransformerForObjectDetection: ['model.backbone.conv_encoder.model.layer2.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer3.0.downsample.1.num_batches_tracked', 'model.backbone.conv_encoder.model.layer4.0.downsample.1.num_batches_tracked']\n",
      "- This IS expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TableTransformerForObjectDetection from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "2024-07-05 13:03:51,929 - INFO - Processing entire page OCR with tesseract...\n",
      "2024-07-05 13:03:52,196 - INFO - padding image by 20 for structure detection\n",
      "2024-07-05 13:03:58,437 - INFO - HTTP Request: POST https://qinchuan-hui.openai.azure.com/openai/deployments/yulong-4/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n",
      "2024-07-05 13:03:58,524 - INFO - Reading PDF for file: ./experiment/parsing/parsing_tmp/fin_tmp.pdf ...\n",
      "2024-07-05 13:03:58,621 - INFO - Detecting page elements ...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt4', 'question': 'what percentage of total long-term assets under supervision are comprised of fixed income in 2015?', 'response': 'To calculate the percentage of total long-term assets under supervision comprised of fixed income in 2015, we need to divide the average fixed income assets under supervision by the total long-term assets under supervision for the same year.\\n\\nFrom the table provided:\\n- Fixed income assets under supervision for 2015: $530 billion\\n- Total long-term assets under supervision for 2015: $922 billion\\n\\nPercentage calculation:\\n(530 / 922) * 100 = 57.48%\\n\\nThe answer is: 57.48%', 'doc': 'GS_2016', 'q_uid': 'GS/2016/page_79.pdf-3', 'answers': {'str_answer': '57%', 'exe_answer': 0.57484}}\n",
      "=== ðŸŒŸ Start fin on gpt4 with parsing strategy cv_llm ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 13:03:59,865 - INFO - Processing entire page OCR with tesseract...\n",
      "2024-07-05 13:04:02,860 - INFO - Processing entire page OCR with tesseract...\n",
      "2024-07-05 13:04:03,114 - INFO - padding image by 20 for structure detection\n",
      "2024-07-05 13:04:17,407 - INFO - HTTP Request: POST https://qinchuan-hui.openai.azure.com/openai/deployments/yulong-4/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n",
      "2024-07-05 13:04:22,451 - INFO - HTTP Request: POST https://qinchuan-hui.openai.azure.com/openai/deployments/yulong-4/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt4', 'question': 'what percentage of total long-term assets under supervision are comprised of fixed income in 2015?', 'response': 'In 2015, the total long-term assets under supervision were $922 billion, and the fixed income assets under supervision were $530 billion.\\n\\nTo calculate the percentage of fixed income assets in the total long-term assets under supervision for 2015:\\n\\n($530 billion / $922 billion) * 100 = 57.48%\\n\\nThe answer is: 57.48%', 'doc': 'GS_2016', 'q_uid': 'GS/2016/page_79.pdf-3', 'answers': {'str_answer': '57%', 'exe_answer': 0.57484}}\n",
      "=== ðŸŒŸ Start fin on gpt4 with parsing strategy well_parsed ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 13:04:27,761 - INFO - HTTP Request: POST https://qinchuan-hui.openai.azure.com/openai/deployments/yulong-4/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt4', 'question': 'what percentage of total long-term assets under supervision are comprised of fixed income in 2015?', 'response': 'To calculate the percentage of total long-term assets under supervision comprised of fixed income in 2015, we need to divide the average fixed income assets under supervision by the total long-term assets under supervision for the same year.\\n\\nFrom the table provided:\\n- Fixed income assets under supervision in 2015: $530 billion\\n- Total long-term assets under supervision in 2015: $922 billion\\n\\nPercentage calculation:\\n(530 / 922) * 100 = 57.48%\\n\\nThe answer is: 57.48%', 'doc': 'GS_2016', 'q_uid': 'GS/2016/page_79.pdf-3', 'answers': {'str_answer': '57%', 'exe_answer': 0.57484}}\n",
      "=== ðŸŒŸ Start fin on gpt4 with parsing strategy omni ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 13:04:37,557 - INFO - HTTP Request: POST https://yulong-eu2.openai.azure.com//openai/deployments/yulong-4o/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n",
      "2024-07-05 13:04:52,775 - INFO - HTTP Request: POST https://qinchuan-hui.openai.azure.com/openai/deployments/yulong-4/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt4', 'question': 'what percentage of total long-term assets under supervision are comprised of fixed income in 2015?', 'response': \"To calculate the percentage of total long-term assets under supervision comprised of fixed income in 2015, we need to divide the fixed income assets by the total long-term assets under supervision for that year.\\n\\nFrom the table provided:\\nFixed income assets in 2015 = $530 billion\\nTotal long-term assets under supervision in 2015 = $922 billion\\n\\nPercentage = (Fixed income assets / Total long-term assets under supervision) * 100\\nPercentage = ($530 billion / $922 billion) * 100\\n\\nNow, let's calculate the percentage:\\n\\nPercentage = (530 / 922) * 100\\nPercentage â‰ˆ 57.48%\\n\\nThe answer is: 57.48%\", 'doc': 'GS_2016', 'q_uid': 'GS/2016/page_79.pdf-3', 'answers': {'str_answer': '57%', 'exe_answer': 0.57484}}\n",
      "======= Finish fin =======\n",
      "\n",
      "=== ðŸŒŸ Start paper_tab on gpt4 with parsing strategy raw_extract ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 13:04:56,216 - INFO - HTTP Request: POST https://qinchuan-hui.openai.azure.com/openai/deployments/yulong-4/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt4', 'question': 'What baselines did they consider?', 'response': 'The answer is: off-the-shelf PDTB parsers, linear SVM, RBF SVM, Random forest, and LSTM classifiers.', 'doc': '1809.01202', 'q_uid': '4cbe5a36b492b99f9f9fea8081fe4ba10a7a0e94', 'answers': [{'answer': 'state-of-the-art PDTB taggers', 'type': 'extractive'}, {'answer': 'Linear SVM, RBF SVM, and Random Forest', 'type': 'abstractive'}]}\n",
      "=== ðŸŒŸ Start paper_tab on gpt4 with parsing strategy cv ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 13:04:56,421 - INFO - Reading PDF for file: ./experiment/parsing/parsing_tmp/paper_tmp.pdf ...\n",
      "2024-07-05 13:04:56,537 - INFO - Detecting page elements ...\n",
      "2024-07-05 13:04:57,780 - INFO - Processing entire page OCR with tesseract...\n",
      "2024-07-05 13:05:00,721 - INFO - Processing entire page OCR with tesseract...\n",
      "2024-07-05 13:05:00,908 - INFO - padding image by 20 for structure detection\n",
      "2024-07-05 13:05:01,441 - INFO - Processing entire page OCR with tesseract...\n",
      "2024-07-05 13:05:01,604 - INFO - padding image by 20 for structure detection\n",
      "2024-07-05 13:05:02,415 - INFO - Processing entire page OCR with tesseract...\n",
      "2024-07-05 13:05:02,581 - INFO - padding image by 20 for structure detection\n",
      "2024-07-05 13:05:05,451 - INFO - HTTP Request: POST https://qinchuan-hui.openai.azure.com/openai/deployments/yulong-4/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt4', 'question': 'What baselines did they consider?', 'response': 'The answer is: off-the-shelf PDTB parsers, linear SVM, RBF SVM, Random forest, and LSTM classifiers.', 'doc': '1809.01202', 'q_uid': '4cbe5a36b492b99f9f9fea8081fe4ba10a7a0e94', 'answers': [{'answer': 'state-of-the-art PDTB taggers', 'type': 'extractive'}, {'answer': 'Linear SVM, RBF SVM, and Random Forest', 'type': 'abstractive'}]}\n",
      "=== ðŸŒŸ Start paper_tab on gpt4 with parsing strategy cv_llm ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 13:05:05,894 - INFO - Reading PDF for file: ./experiment/parsing/parsing_tmp/paper_tmp.pdf ...\n",
      "2024-07-05 13:05:06,010 - INFO - Detecting page elements ...\n",
      "2024-07-05 13:05:07,258 - INFO - Processing entire page OCR with tesseract...\n",
      "2024-07-05 13:05:10,210 - INFO - Processing entire page OCR with tesseract...\n",
      "2024-07-05 13:05:10,395 - INFO - padding image by 20 for structure detection\n",
      "2024-07-05 13:05:10,875 - INFO - Processing entire page OCR with tesseract...\n",
      "2024-07-05 13:05:11,037 - INFO - padding image by 20 for structure detection\n",
      "2024-07-05 13:05:11,778 - INFO - Processing entire page OCR with tesseract...\n",
      "2024-07-05 13:05:11,944 - INFO - padding image by 20 for structure detection\n",
      "2024-07-05 13:05:23,645 - INFO - HTTP Request: POST https://qinchuan-hui.openai.azure.com/openai/deployments/yulong-4/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n",
      "2024-07-05 13:05:27,245 - INFO - HTTP Request: POST https://qinchuan-hui.openai.azure.com/openai/deployments/yulong-4/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt4', 'question': 'What baselines did they consider?', 'response': 'The answer is: off-the-shelf PDTB parsers, linear SVM, RBF SVM, Random forest, and LSTM classifiers.', 'doc': '1809.01202', 'q_uid': '4cbe5a36b492b99f9f9fea8081fe4ba10a7a0e94', 'answers': [{'answer': 'state-of-the-art PDTB taggers', 'type': 'extractive'}, {'answer': 'Linear SVM, RBF SVM, and Random Forest', 'type': 'abstractive'}]}\n",
      "There is no well-parsed data for paper_tab dataset. Skip the strategy.\n",
      "=== ðŸŒŸ Start paper_tab on gpt4 with parsing strategy omni ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-07-05 13:05:41,441 - INFO - HTTP Request: POST https://yulong-eu2.openai.azure.com//openai/deployments/yulong-4o/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n",
      "2024-07-05 13:05:43,902 - INFO - HTTP Request: POST https://qinchuan-hui.openai.azure.com/openai/deployments/yulong-4/chat/completions?api-version=2024-04-01-preview \"HTTP/1.1 200 OK\"\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'model': 'gpt4', 'question': 'What baselines did they consider?', 'response': 'The answer is: (Biran and McKeown, 2015), (Lin et al., 2014), Linear SVM, RBF SVM, Random Forest, LSTM.', 'doc': '1809.01202', 'q_uid': '4cbe5a36b492b99f9f9fea8081fe4ba10a7a0e94', 'answers': [{'answer': 'state-of-the-art PDTB taggers', 'type': 'extractive'}, {'answer': 'Linear SVM, RBF SVM, and Random Forest', 'type': 'abstractive'}]}\n",
      "======= Finish paper_tab =======\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from uda.utils.parsing_exp import get_fin_context, get_paper_context\n",
    "from uda.utils import preprocess as pre\n",
    "import pandas as pd\n",
    "from uda.utils import llm\n",
    "from uda.utils import inference\n",
    "import json\n",
    "\n",
    "strategies = [\"raw_extract\", \"cv\", \"cv_llm\", \"well_parsed\", \"omni\"]\n",
    "\n",
    "for LLM_MODEL in LLM_LIST:\n",
    "    for DATASET_NAME in DATASET_NAME_LIST:\n",
    "        for strategy in strategies:\n",
    "            if strategy == \"well_parsed\" and DATASET_NAME == \"paper_tab\":\n",
    "                print(f\"There is no well-parsed data for paper_tab dataset. Skip the strategy.\")\n",
    "                continue\n",
    "            print(f\"=== ðŸŒŸ Start {DATASET_NAME} on {LLM_MODEL} with parsing strategy {strategy} ===\")\n",
    "            res_file = os.path.join(res_dir, f\"{DATASET_NAME}_{LLM_MODEL}_{strategy}.jsonl\")\n",
    "\n",
    "            # If use the local LLM, initialize the model\n",
    "            if LLM_MODEL in LOCAL_LLM_DICT:\n",
    "                llm_name = LOCAL_LLM_DICT[LLM_MODEL]\n",
    "                llm_service = inference.LLM(llm_name)\n",
    "                llm_service.init_llm()\n",
    "\n",
    "            # Load the benchmark data\n",
    "            bench_json_file = pre.meta_data[DATASET_NAME][\"bench_json_file\"]\n",
    "            with open(bench_json_file, \"r\") as f:\n",
    "                bench_data = json.load(f)\n",
    "\n",
    "            # Run experiments on the demo docs\n",
    "            doc_list = list(bench_data.keys())\n",
    "            for doc in doc_list[:DEMO_DOC_NUM]:\n",
    "                pdf_path = pre.get_example_pdf_path(DATASET_NAME, doc)\n",
    "                if pdf_path is None:\n",
    "                    continue\n",
    "                for qa_item in bench_data[doc][:DEMO_QA_NUM]:\n",
    "                    question = qa_item[\"question\"]\n",
    "                    # Parse the tables from the document and get the context\n",
    "                    if DATASET_NAME == \"fin\":\n",
    "                        context = get_fin_context(qa_item, strategy, pdf_path)\n",
    "                    elif DATASET_NAME == \"paper_tab\":                            \n",
    "                        context = get_paper_context(qa_item, strategy, pdf_path)\n",
    "                    ## Show the context if needed\n",
    "                    # print(context)\n",
    "                    \n",
    "                    # Create the prompt\n",
    "                    llm_message = llm.make_prompt(question, context, DATASET_NAME, LLM_MODEL)\n",
    "                    # Generate the answer\n",
    "                    if LLM_MODEL in LOCAL_LLM_DICT:\n",
    "                        response = llm_service.infer(llm_message)\n",
    "                    elif LLM_MODEL == \"gpt4\":\n",
    "                        # Set up with your own GPT4 service using environment variables\n",
    "                        response = llm.call_gpt(messages=llm_message)\n",
    "                        if response is None:\n",
    "                            print(\"Make sure your gpt4 service is set up correctly.\")\n",
    "                            raise Exception(\"GPT4 service\")\n",
    "\n",
    "                    # log the results\n",
    "                    res_dict = {\n",
    "                        \"model\": LLM_MODEL,\n",
    "                        \"question\": question,\n",
    "                        \"response\": response,\n",
    "                        \"doc\": doc,\n",
    "                        \"q_uid\": qa_item[\"q_uid\"],\n",
    "                        \"answers\": qa_item[\"answers\"],\n",
    "                    }\n",
    "                    print(res_dict)\n",
    "                    with open(res_file, \"a\") as f:\n",
    "                        f.write(json.dumps(res_dict) + \"\\n\")\n",
    "\n",
    "        print(f\"======= Finish {DATASET_NAME} =======\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate the parsing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Exact-match accuracy: 100.00\n"
     ]
    }
   ],
   "source": [
    "dataset_name=\"fin\"\n",
    "llm_model=\"gpt4\"\n",
    "parsing_strategy=\"raw_extract\"\n",
    "res_file_name=f\"experiment/parsing/res/{dataset_name}_{llm_model}_{parsing_strategy}.jsonl\"\n",
    "\n",
    "from uda.eval.my_eval import eval_from_file\n",
    "eval_from_file(dataset_name, res_file_name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "langchain",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
