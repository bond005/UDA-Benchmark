{"q_uid": "085147cd32153d46dd9901ab0f9195bfdbff6a85", "table": "\nTable 1: Results mean (min, max) achieved with each method. w2v:word2vec. Glv:GloVe. Syn: Syntactic embedding. Note that we experiment with using two and three sets of embeddings jointly, e.g., w2v+Syn+Glv indicates that we use all three of these.\n| Model                   | Subj                | SST-1               | SST-2               | TREC                | Irony                |\n|-------------------------|---------------------|---------------------|---------------------|---------------------|----------------------|\n| CNN(w2v)                | 93.14 (92.92,93.39) | 46.99 (46.11,48.28) | 87.03 (86.16,88.08) | 93.32 (92.40,94.60) | 67.15 (66.53,68.11)  |\n| CNN(Glv)                | 93.41 (93.20,93.51) | 46.58 (46.11,47.06) | 87.36 (87.20,87.64) | 93.36 (93.30,93.60) | 67.84 (67.29,68.38)  |\n| CNN(Syn)                | 93.24 (93.01,93.45)  | 45.48(44.67,46.24)  | 86.04 (85.28,86.77) | 94.68 (94.00,95.00) | 67.93 (67.30,68.38)  |\n| MVCNN (Yin and Sch\u00fctze, 2015)  |      93.9     | 49.6               | 89.4                |  -                  |  -  |\n| C-CNN(w2v+Glv)          | 93.72 (93.68,93.76) | 47.02(46.24,47.69)  | 87.42(86.88,87.81)  | 93.80 (93.40,94.20) | 67.70 (66.97,68.35)  |\n| C-CNN(w2v+Syn)          | 93.48 (93.43,93.52) | 46.91(45.97,47.81)  | 87.17 (86.55,87.42) | 94.66 (94.00,95.20) | 68.08 (67.63,68.57)  |\n| C-CNN(w2v+Syn+Glv)      | 93.61 (93.47,93.77) | 46.52 (45.02,47.47) | 87.55 (86.77,88.58) | 95.20 (94.80,65.60) | 68.38 (67.66,69.23)  |\n| MG-CNN(w2v+Glv)         | 93.84 (93.66,94.35) | 48.24 (47.60,49.05) | 87.90 (87.48,88.30) | 94.09 (93.60,94.80) | 69.40 (66.35,72.30)  |\n| MG-CNN(w2v+Syn)         | 93.78 (93.62,93.98) | 48.48(47.78,49.19)  | 87.47(87.10,87.70)  | 94.87 (94.00,95.60) | 68.28 (66.44,69.97)  |\n| MG-CNN(w2v+Syn+Glv)     | 94.11 (94.04,94.17) | 48.01 (47.65,48.37) | 87.63(87.04,88.36)  | 94.68 (93.80,95.40) | 69.19 (67.06,72.30)  |\n| MGNC-CNN(w2v+Glv)       | 93.93 (93.79,94.14) | 48.53 (47.92,49.37) | 88.35(87.86,88.74)  | 94.40 (94.00,94.80) | 69.15 (67.25,71.10)  |\n| MGNC-CNN(w2v+Syn)       | 93.95 (93.75,94.21) | 48.51 (47.60,49.41) | 87.88(87.64,88.19)  | 95.12 (94.60,95.60) | 69.35 (67.40,70.86)  |\n| MGNC-CNN(w2v+Syn+Glv)   | 94.09 (93.98,94.18) | 48.65 (46.92,49.19) | 88.30 (87.83,88.65) | 95.52 (94.60,96.60) | 71.53 (69.74,73.06)  | \n", "text": "\nWe compared our proposed approaches to a standard CNN that exploits a single set of word embeddings BIBREF3 . We also compared to a baseline of simply concatenating embeddings for each word to form long vector inputs. We refer to this as Concatenation-CNN C-CNN. For all multiple embedding approaches (C-CNN, MG-CNN and MGNC-CNN), we explored two combined sets of embedding: word2vec+Glove, and word2vec+syntactic, and one three sets of embedding: word2vec+Glove+syntactic. For all models, we tuned the l2 norm constraint INLINEFORM0 over the range INLINEFORM1 on a validation set. For instantiations of MGNC-CNN in which we exploited two embeddings, we tuned both INLINEFORM2 , and INLINEFORM3 ; where we used three embedding sets, we tuned INLINEFORM4 and INLINEFORM5 .\n"}
{"q_uid": "c0035fb1c2b3de15146a7ce186ccd2e366fb4da2", "table": "\nTable 1: Results mean (min, max) achieved with each method. w2v:word2vec. Glv:GloVe. Syn: Syntactic embedding. Note that we experiment with using two and three sets of embeddings jointly, e.g., w2v+Syn+Glv indicates that we use all three of these.\n| Model                   | Subj                | SST-1               | SST-2               | TREC                | Irony                |\n|-------------------------|---------------------|---------------------|---------------------|---------------------|----------------------|\n| CNN(w2v)                | 93.14 (92.92,93.39) | 46.99 (46.11,48.28) | 87.03 (86.16,88.08) | 93.32 (92.40,94.60) | 67.15 (66.53,68.11)  |\n| CNN(Glv)                | 93.41 (93.20,93.51) | 46.58 (46.11,47.06) | 87.36 (87.20,87.64) | 93.36 (93.30,93.60) | 67.84 (67.29,68.38)  |\n| CNN(Syn)                | 93.24 (93.01,93.45)  | 45.48(44.67,46.24)  | 86.04 (85.28,86.77) | 94.68 (94.00,95.00) | 67.93 (67.30,68.38)  |\n| MVCNN (Yin and Sch\u00fctze, 2015)  |      93.9     | 49.6               | 89.4                |  -                  |  -  |\n| C-CNN(w2v+Glv)          | 93.72 (93.68,93.76) | 47.02(46.24,47.69)  | 87.42(86.88,87.81)  | 93.80 (93.40,94.20) | 67.70 (66.97,68.35)  |\n| C-CNN(w2v+Syn)          | 93.48 (93.43,93.52) | 46.91(45.97,47.81)  | 87.17 (86.55,87.42) | 94.66 (94.00,95.20) | 68.08 (67.63,68.57)  |\n| C-CNN(w2v+Syn+Glv)      | 93.61 (93.47,93.77) | 46.52 (45.02,47.47) | 87.55 (86.77,88.58) | 95.20 (94.80,65.60) | 68.38 (67.66,69.23)  |\n| MG-CNN(w2v+Glv)         | 93.84 (93.66,94.35) | 48.24 (47.60,49.05) | 87.90 (87.48,88.30) | 94.09 (93.60,94.80) | 69.40 (66.35,72.30)  |\n| MG-CNN(w2v+Syn)         | 93.78 (93.62,93.98) | 48.48(47.78,49.19)  | 87.47(87.10,87.70)  | 94.87 (94.00,95.60) | 68.28 (66.44,69.97)  |\n| MG-CNN(w2v+Syn+Glv)     | 94.11 (94.04,94.17) | 48.01 (47.65,48.37) | 87.63(87.04,88.36)  | 94.68 (93.80,95.40) | 69.19 (67.06,72.30)  |\n| MGNC-CNN(w2v+Glv)       | 93.93 (93.79,94.14) | 48.53 (47.92,49.37) | 88.35(87.86,88.74)  | 94.40 (94.00,94.80) | 69.15 (67.25,71.10)  |\n| MGNC-CNN(w2v+Syn)       | 93.95 (93.75,94.21) | 48.51 (47.60,49.41) | 87.88(87.64,88.19)  | 95.12 (94.60,95.60) | 69.35 (67.40,70.86)  |\n| MGNC-CNN(w2v+Syn+Glv)   | 94.09 (93.98,94.18) | 48.65 (46.92,49.19) | 88.30 (87.83,88.65) | 95.52 (94.60,96.60) | 71.53 (69.74,73.06)  | \n", "text": "\nWe repeated each experiment 10 times and report the mean and ranges across these. This replication is important because training is stochastic and thus introduces variance in performance BIBREF4 . Results are shown in Table TABREF2 , and the corresponding best norm constraint value is shown in Table TABREF2 . \n"}
{"q_uid": "34dd0ee1374a3afd16cf8b0c803f4ef4c6fec8ac", "table": "\nTable 1: Results mean (min, max) achieved with each method. w2v:word2vec. Glv:GloVe. Syn: Syntactic embedding. Note that we experiment with using two and three sets of embeddings jointly, e.g., w2v+Syn+Glv indicates that we use all three of these.\n| Model                   | Subj                | SST-1               | SST-2               | TREC                | Irony                |\n|-------------------------|---------------------|---------------------|---------------------|---------------------|----------------------|\n| CNN(w2v)                | 93.14 (92.92,93.39) | 46.99 (46.11,48.28) | 87.03 (86.16,88.08) | 93.32 (92.40,94.60) | 67.15 (66.53,68.11)  |\n| CNN(Glv)                | 93.41 (93.20,93.51) | 46.58 (46.11,47.06) | 87.36 (87.20,87.64) | 93.36 (93.30,93.60) | 67.84 (67.29,68.38)  |\n| CNN(Syn)                | 93.24 (93.01,93.45)  | 45.48(44.67,46.24)  | 86.04 (85.28,86.77) | 94.68 (94.00,95.00) | 67.93 (67.30,68.38)  |\n| MVCNN (Yin and Sch\u00fctze, 2015)  |      93.9     | 49.6               | 89.4                |  -                  |  -  |\n| C-CNN(w2v+Glv)          | 93.72 (93.68,93.76) | 47.02(46.24,47.69)  | 87.42(86.88,87.81)  | 93.80 (93.40,94.20) | 67.70 (66.97,68.35)  |\n| C-CNN(w2v+Syn)          | 93.48 (93.43,93.52) | 46.91(45.97,47.81)  | 87.17 (86.55,87.42) | 94.66 (94.00,95.20) | 68.08 (67.63,68.57)  |\n| C-CNN(w2v+Syn+Glv)      | 93.61 (93.47,93.77) | 46.52 (45.02,47.47) | 87.55 (86.77,88.58) | 95.20 (94.80,65.60) | 68.38 (67.66,69.23)  |\n| MG-CNN(w2v+Glv)         | 93.84 (93.66,94.35) | 48.24 (47.60,49.05) | 87.90 (87.48,88.30) | 94.09 (93.60,94.80) | 69.40 (66.35,72.30)  |\n| MG-CNN(w2v+Syn)         | 93.78 (93.62,93.98) | 48.48(47.78,49.19)  | 87.47(87.10,87.70)  | 94.87 (94.00,95.60) | 68.28 (66.44,69.97)  |\n| MG-CNN(w2v+Syn+Glv)     | 94.11 (94.04,94.17) | 48.01 (47.65,48.37) | 87.63(87.04,88.36)  | 94.68 (93.80,95.40) | 69.19 (67.06,72.30)  |\n| MGNC-CNN(w2v+Glv)       | 93.93 (93.79,94.14) | 48.53 (47.92,49.37) | 88.35(87.86,88.74)  | 94.40 (94.00,94.80) | 69.15 (67.25,71.10)  |\n| MGNC-CNN(w2v+Syn)       | 93.95 (93.75,94.21) | 48.51 (47.60,49.41) | 87.88(87.64,88.19)  | 95.12 (94.60,95.60) | 69.35 (67.40,70.86)  |\n| MGNC-CNN(w2v+Syn+Glv)   | 94.09 (93.98,94.18) | 48.65 (46.92,49.19) | 88.30 (87.83,88.65) | 95.52 (94.60,96.60) | 71.53 (69.74,73.06)  | \n", "text": "\nWe compared our proposed approaches to a standard CNN that exploits a single set of word embeddings BIBREF3 . We also compared to a baseline of simply concatenating embeddings for each word to form long vector inputs. We refer to this as Concatenation-CNN C-CNN. For all multiple embedding approaches (C-CNN, MG-CNN and MGNC-CNN), we explored two combined sets of embedding: word2vec+Glove, and word2vec+syntactic, and one three sets of embedding: word2vec+Glove+syntactic. \nMore similar to our work, Yin and Sch\u00fctze yin-schutze:2015:CoNLL proposed MVCNN for sentence classification. This CNN-based architecture accepts multiple word embeddings as inputs. \n"}
{"q_uid": "d8de12f5eff64d0e9c9e88f6ebdabc4cdf042c22", "table": "\nTable 3: Test set results of our CNN model against other methods.\n|   | Model                                      | Binary | Fine-grained | Senti140 | Subj |\n|------|--------------------------------------------|--------|--------------|----------|------|\n| baselines | 1 RAE (Socher et al., 2011b)               | 82.4   | 43.2         | -        | -    |\n| baselines | 2 MV-RNN (Socher et al., 2012)             | 82.9   | 44.4         | -        | -    |\n| baselines | 3 RNTN (Socher et al., 2013)               | 85.4   | 45.7         | -        | -    |\n| baselines | 4 DCNN (Kalchbrenner et al., 2014)         | 86.8   | 48.5         | 87.4     | -    |\n| baselines | 5 Paragraph-Vec (Le and Mikolov, 2014)     | 87.7   | 48.7         | -        | -    |\n| baselines | 6 CNN-rand (Kim, 2014)                     | 82.7   | 45.0         | -        | 89.6 |\n| baselines | 7 CNN-static (Kim, 2014)                   | 86.8   | 45.5         | -        | 93.0 |\n| baselines | 8 CNN-non-static (Kim, 2014)               | 87.2   | 48.0         | -        | 93.4 |\n| baselines | 9 CNN-multichannel (Kim, 2014)             | 88.1   | 47.4         | -        | 93.2 |\n| baselines | 10 NBSVM (Wang and Manning, 2012)          | -      | -            | -        | 93.2 |\n| baselines | 11 MNB (Wang and Manning, 2012)            | -      | -            | -        | 93.6 |\n| baselines | 12 G-Dropout (Wang and Manning, 2013)      | -      | -            | -        | 93.4 |\n| baselines | 13 F-Dropout (Wang and Manning, 2013)      | -      | -            | -        | 93.6 |\n| baselines | 14 SVM (Go et al., 2009)                   | -      | -            | 81.6     | -    |\n| baselines | 15 BINB (Go et al., 2009)                   | -      | -            | 82.7     | -    |\n| baselines | 16 MAX-TDNN (Kalchbrenner et al., 2014)    | -      | -            | 78.8     | -    |\n| baselines | 17 NBOW (Kalchbrenner et al., 2014)        | -      | -            | 80.9     | -    |\n| baselines | 18 MAXENT (Go et al., 2009)                | -      | -            | 83.0     | -    |\n|          |                                            |        |              |          |      |\n| versions | 19 MVCNN (-HLBL)                           | 88.5   | 48.7         | 88.0     | 93.6 |\n| versions | 20 MVCNN (-Huang)                          | 89.2   | 49.2         | 88.1     | 93.7 |\n| versions | 21 MVCNN (-Glove)                          | 88.3   | 48.6         | 87.4     | 93.6 |\n| versions | 22 MVCNN (-SENNA)                          | 89.3   | 49.1         | 87.9     | 93.4 |\n| versions | 23 MVCNN (-Word2Vec)                       | 88.4   | 48.2         | 87.6     | 93.4 |\n|          |                                            |        |              |          |      |\n| filters | 24 MVCNN (-3)                              | 89.1   | 49.2         | 88.0     | 93.6 |\n| filters | 25 MVCNN (-5)                              | 88.7   | 49.0         | 87.5     | 93.4 |\n| filters | 26 MVCNN (-7)                              | 87.8   | 48.9         | 87.5     | 93.1 |\n| filters | 27 MVCNN (-9)                              | 88.6   | 49.2         | 87.8     | 93.3 |\n|         |                                           |        |              |          |      |\n| tricks | 28 MVCNN (-mutual-learning)                | 88.2   | 49.2         | 87.8     | 93.5 |\n| tricks | 29 MVCNN (-pretraining)                    | 87.6   | 48.9         | 87.6     | 93.2 |\n|        |                                            |        |              |          |      |\n| layers | 30 MVCNN (1)                               | 89.0   | 49.3         | 86.8     | 93.8 |\n| layers | 31 MVCNN (2)                               | 89.4   | 49.6         | 87.6     | 93.9 |\n| layers | 32 MVCNN (3)                               | 88.6   | 48.6         | 88.2     | 93.1 |\n| layers | 33 MVCNN (4)                               | 87.9   | 48.2         | 88.0     | 92.4 |\n|        |                                            |        |              |          |      |\n|        | 34 MVCNN (overall)                         | 89.4   | 49.6         | 88.2     | 93.9 |\n", "text": "\n"}
{"q_uid": "9cba2ee1f8e1560e48b3099d0d8cf6c854ddea2e", "table": "\nTable 3: Test set results of our CNN model against other methods.\n|   | Model                                      | Binary | Fine-grained | Senti140 | Subj |\n|------|--------------------------------------------|--------|--------------|----------|------|\n| baselines | 1 RAE (Socher et al., 2011b)               | 82.4   | 43.2         | -        | -    |\n| baselines | 2 MV-RNN (Socher et al., 2012)             | 82.9   | 44.4         | -        | -    |\n| baselines | 3 RNTN (Socher et al., 2013)               | 85.4   | 45.7         | -        | -    |\n| baselines | 4 DCNN (Kalchbrenner et al., 2014)         | 86.8   | 48.5         | 87.4     | -    |\n| baselines | 5 Paragraph-Vec (Le and Mikolov, 2014)     | 87.7   | 48.7         | -        | -    |\n| baselines | 6 CNN-rand (Kim, 2014)                     | 82.7   | 45.0         | -        | 89.6 |\n| baselines | 7 CNN-static (Kim, 2014)                   | 86.8   | 45.5         | -        | 93.0 |\n| baselines | 8 CNN-non-static (Kim, 2014)               | 87.2   | 48.0         | -        | 93.4 |\n| baselines | 9 CNN-multichannel (Kim, 2014)             | 88.1   | 47.4         | -        | 93.2 |\n| baselines | 10 NBSVM (Wang and Manning, 2012)          | -      | -            | -        | 93.2 |\n| baselines | 11 MNB (Wang and Manning, 2012)            | -      | -            | -        | 93.6 |\n| baselines | 12 G-Dropout (Wang and Manning, 2013)      | -      | -            | -        | 93.4 |\n| baselines | 13 F-Dropout (Wang and Manning, 2013)      | -      | -            | -        | 93.6 |\n| baselines | 14 SVM (Go et al., 2009)                   | -      | -            | 81.6     | -    |\n| baselines | 15 BINB (Go et al., 2009)                   | -      | -            | 82.7     | -    |\n| baselines | 16 MAX-TDNN (Kalchbrenner et al., 2014)    | -      | -            | 78.8     | -    |\n| baselines | 17 NBOW (Kalchbrenner et al., 2014)        | -      | -            | 80.9     | -    |\n| baselines | 18 MAXENT (Go et al., 2009)                | -      | -            | 83.0     | -    |\n|          |                                            |        |              |          |      |\n| versions | 19 MVCNN (-HLBL)                           | 88.5   | 48.7         | 88.0     | 93.6 |\n| versions | 20 MVCNN (-Huang)                          | 89.2   | 49.2         | 88.1     | 93.7 |\n| versions | 21 MVCNN (-Glove)                          | 88.3   | 48.6         | 87.4     | 93.6 |\n| versions | 22 MVCNN (-SENNA)                          | 89.3   | 49.1         | 87.9     | 93.4 |\n| versions | 23 MVCNN (-Word2Vec)                       | 88.4   | 48.2         | 87.6     | 93.4 |\n|          |                                            |        |              |          |      |\n| filters | 24 MVCNN (-3)                              | 89.1   | 49.2         | 88.0     | 93.6 |\n| filters | 25 MVCNN (-5)                              | 88.7   | 49.0         | 87.5     | 93.4 |\n| filters | 26 MVCNN (-7)                              | 87.8   | 48.9         | 87.5     | 93.1 |\n| filters | 27 MVCNN (-9)                              | 88.6   | 49.2         | 87.8     | 93.3 |\n|         |                                           |        |              |          |      |\n| tricks | 28 MVCNN (-mutual-learning)                | 88.2   | 49.2         | 87.8     | 93.5 |\n| tricks | 29 MVCNN (-pretraining)                    | 87.6   | 48.9         | 87.6     | 93.2 |\n|        |                                            |        |              |          |      |\n| layers | 30 MVCNN (1)                               | 89.0   | 49.3         | 86.8     | 93.8 |\n| layers | 31 MVCNN (2)                               | 89.4   | 49.6         | 87.6     | 93.9 |\n| layers | 32 MVCNN (3)                               | 88.6   | 48.6         | 88.2     | 93.1 |\n| layers | 33 MVCNN (4)                               | 87.9   | 48.2         | 88.0     | 92.4 |\n|        |                                            |        |              |          |      |\n|        | 34 MVCNN (overall)                         | 89.4   | 49.6         | 88.2     | 93.9 |\n", "text": "\nThe block \u201cfilters\u201d indicates the contribution of each filter size. The system benefits from filters of each size. Sizes 5 and 7 are most important for high performance, especially 7 (rows 25 and 26).\nThis work presented MVCNN, a novel CNN architecture for sentence classification. It combines multichannel initialization - diverse versions of pretrained word embeddings are used - and variable-size filters - features of multigranular phrases are extracted with variable-size convolution filters. \n"}
{"q_uid": "7975c3e1f61344e3da3b38bb12e1ac6dcb153a18", "table": "\nTable 3: Test set results of our CNN model against other methods.\n|   | Model                                      | Binary | Fine-grained | Senti140 | Subj |\n|------|--------------------------------------------|--------|--------------|----------|------|\n| baselines | 1 RAE (Socher et al., 2011b)               | 82.4   | 43.2         | -        | -    |\n| baselines | 2 MV-RNN (Socher et al., 2012)             | 82.9   | 44.4         | -        | -    |\n| baselines | 3 RNTN (Socher et al., 2013)               | 85.4   | 45.7         | -        | -    |\n| baselines | 4 DCNN (Kalchbrenner et al., 2014)         | 86.8   | 48.5         | 87.4     | -    |\n| baselines | 5 Paragraph-Vec (Le and Mikolov, 2014)     | 87.7   | 48.7         | -        | -    |\n| baselines | 6 CNN-rand (Kim, 2014)                     | 82.7   | 45.0         | -        | 89.6 |\n| baselines | 7 CNN-static (Kim, 2014)                   | 86.8   | 45.5         | -        | 93.0 |\n| baselines | 8 CNN-non-static (Kim, 2014)               | 87.2   | 48.0         | -        | 93.4 |\n| baselines | 9 CNN-multichannel (Kim, 2014)             | 88.1   | 47.4         | -        | 93.2 |\n| baselines | 10 NBSVM (Wang and Manning, 2012)          | -      | -            | -        | 93.2 |\n| baselines | 11 MNB (Wang and Manning, 2012)            | -      | -            | -        | 93.6 |\n| baselines | 12 G-Dropout (Wang and Manning, 2013)      | -      | -            | -        | 93.4 |\n| baselines | 13 F-Dropout (Wang and Manning, 2013)      | -      | -            | -        | 93.6 |\n| baselines | 14 SVM (Go et al., 2009)                   | -      | -            | 81.6     | -    |\n| baselines | 15 BINB (Go et al., 2009)                   | -      | -            | 82.7     | -    |\n| baselines | 16 MAX-TDNN (Kalchbrenner et al., 2014)    | -      | -            | 78.8     | -    |\n| baselines | 17 NBOW (Kalchbrenner et al., 2014)        | -      | -            | 80.9     | -    |\n| baselines | 18 MAXENT (Go et al., 2009)                | -      | -            | 83.0     | -    |\n|          |                                            |        |              |          |      |\n| versions | 19 MVCNN (-HLBL)                           | 88.5   | 48.7         | 88.0     | 93.6 |\n| versions | 20 MVCNN (-Huang)                          | 89.2   | 49.2         | 88.1     | 93.7 |\n| versions | 21 MVCNN (-Glove)                          | 88.3   | 48.6         | 87.4     | 93.6 |\n| versions | 22 MVCNN (-SENNA)                          | 89.3   | 49.1         | 87.9     | 93.4 |\n| versions | 23 MVCNN (-Word2Vec)                       | 88.4   | 48.2         | 87.6     | 93.4 |\n|          |                                            |        |              |          |      |\n| filters | 24 MVCNN (-3)                              | 89.1   | 49.2         | 88.0     | 93.6 |\n| filters | 25 MVCNN (-5)                              | 88.7   | 49.0         | 87.5     | 93.4 |\n| filters | 26 MVCNN (-7)                              | 87.8   | 48.9         | 87.5     | 93.1 |\n| filters | 27 MVCNN (-9)                              | 88.6   | 49.2         | 87.8     | 93.3 |\n|         |                                           |        |              |          |      |\n| tricks | 28 MVCNN (-mutual-learning)                | 88.2   | 49.2         | 87.8     | 93.5 |\n| tricks | 29 MVCNN (-pretraining)                    | 87.6   | 48.9         | 87.6     | 93.2 |\n|        |                                            |        |              |          |      |\n| layers | 30 MVCNN (1)                               | 89.0   | 49.3         | 86.8     | 93.8 |\n| layers | 31 MVCNN (2)                               | 89.4   | 49.6         | 87.6     | 93.9 |\n| layers | 32 MVCNN (3)                               | 88.6   | 48.6         | 88.2     | 93.1 |\n| layers | 33 MVCNN (4)                               | 87.9   | 48.2         | 88.0     | 92.4 |\n|        |                                            |        |              |          |      |\n|        | 34 MVCNN (overall)                         | 89.4   | 49.6         | 88.2     | 93.9 |\n", "text": "\nIn the block \u201cversions\u201d, we see that each embedding version is crucial for good performance: performance drops in every single case.\n"}
{"q_uid": "08333e4dd1da7d6b5e9b645d40ec9d502823f5d7", "table": "\nTable 4: Compared with other systems (bold is best).\n| Task A | Task B | Task C |\n|--------|--------|--------|\n| Model  | MAP    | MAP    | MAP    |\n|--------|--------|--------|--------|\n| IR     | 0.538  | 0.714  | 0.307  |\n| Attention | 0.639 | 0.659 | 0.324 |\n| Feature-Rich & IR | 0.632 | 0.685 | 0.339 |\n| Attention & IR | 0.639 | 0.717 | 0.394 |\n", "text": "\n"}
{"q_uid": "bf52c01bf82612d0c7bbf2e6a5bb2570c322936f", "table": "\nTable 2: Correlation between variants of ROUGE and SERA, with human pyramid scores. All variants of ROUGE are displayed. F : F-Score; R: Recall; P : Precision; DIS: Discounted variant of SERA; KW: using Keyword query reformulation; NP: Using noun phrases for query reformulation. The numbers in front of the SERA metrics indicate the rank cut-off point.\n\n|       | Pyramid |    Pyramid |  Pyramid    |\n|----------------|---------|-------------|-------------|\n|   Metric      | Pearson(r) | Spearman(\u03c1) | Kendall(\u03c4) |\n|----------------|---------|-------------|-------------|\n| ROUGE-1-F      | 0.454   | 0.174       | 0.138       |\n| ROUGE-1-P      | 0.257   | 0.116       | 0      |\n| ROUGE-1-R      | 0.513   | 0.229       | 0.138       |\n| ROUGE-2-F      | 0.816   | 0.696       | 0.552       |\n| ROUGE-2-P      | 0.824   | 0.841       | 0.69        |\n| ROUGE-2-R      | 0.803   | 0.696       | 0.552       |\n| ROUGE-3-F      | 0.878   | 0.841       | 0.69        |\n| ROUGE-3-P      | 0.875   | 0.725       | 0.552       |\n| ROUGE-3-R      | 0.875   | 0.841       | 0.69        |\n| ROUGE-L-F      | 0.454   | 0.261       | 0.276       |\n| ROUGE-L-P      | 0.262   | 0.229       | 0.138       |\n| ROUGE-L-R      | 0.52    | 0.261       | 0.276       |\n| ROUGE-S-F      | 0.603   | 0.406       | 0.414       |\n| ROUGE-S-P      | 0.344   | 0.174       | 0.138       |\n| ROUGE-S-R      | 0.664   | 0.406       | 0.414       |\n| ROUGE-SU-F     | 0.601   | 0.493       | 0.462       |\n| ROUGE-SU-P     | 0.338   | 0.174       | 0.138       |\n| ROUGE-SU-R     | 0.662   | 0.406       | 0.414       |\n| ROUGE-W-1.2-F  | 0.607   | 0.493       | 0.414       |\n| ROUGE-W-1.2-P  | 0.418   | 0.377       | 0.276       |\n| ROUGE-W-1.2-R  | 0.626   | 0.667       | 0.552       |\n| SERA-5         | 0.823   | 0.941       | 0.857       |\n| SERA-10        | 0.788   | 0.647       | 0.429       |\n| SERA-KW-5      | 0.848   | 0.765       | 0.571       |\n| SERA-KW-10     | 0.641   | 0.618       | 0.486       |\n| SERA-NP-5      | 0.859   | 1.0         | 1.0         |\n| SERA-NP-10     | 0.806   | 0.941       | 0.857       |\n| SERA-DIS-5     | 0.631   | 0.824       | 0.714       |\n| SERA-DIS-10    | 0.687   | 0.824       | 0.714       |\n| SERA-DIS-KW-5  | 0.838   | 0.941       | 0.857       |\n| SERA-DIS-KW-10 | 0.766   | 0.712       | 0.729       |\n| SERA-DIS-NP-5  | 0.834   | 0.941       | 0.857       |\n| SERA-DIS-NP-10 | 0.86   | 0.941       | 0.857       |\n", "text": "\nTable TABREF23 shows the Pearson, Spearman and Kendall correlation of Rouge and Sera, with pyramid scores.\nInterestingly, we observe that many variants of Rouge scores do not have high correlations with human pyramid scores. The lowest F-score correlations are for Rouge-1 and Rouge-L (with INLINEFORM0 =0.454). Weak correlation of Rouge-1 shows that matching unigrams between the candidate summary and gold summaries is not accurate in quantifying the quality of the summary.\nFurthermore, we showed that different variants of Rouge result in different correlation values with human judgments, indicating that not all Rouge scores are equally effective. Among all variants of Rouge, Rouge-2 and Rouge-3 are better correlated with manual judgments in the context of scientific summarization.\n"}
{"q_uid": "6cd8bad8a031ce6d802ded90f9754088e0c8d653", "table": "\nTable 3: State-of-the-art results for relation classification\n| Classifier                                      | F1   |\n|-------------------------------------------------|------|\n| SVM (Rink and Harabagiu, 2010b)                 | 82.2 |\n| RNN (Socher et al., 2012)                       | 77.6 |\n| MVRNN (Socher et al., 2012)                     | 82.4 |\n| CNN (Zeng et al., 2014)                         | 82.7 |\n| FCM (Yu et al., 2014)                           | 83.0 |\n| bi-RNN (Zhang and Wang, 2015)                   | 82.5 |\n| CR-CNN (Dos Santos et al., 2015)                | 84.1 |\n| R-RNN                                           | 83.4 |\n| ER-CNN                                          | 84.2 |\n| ER-CNN + R-RNN                                  | 84.9 |\n", "text": "\nTable TABREF16 shows the results of our models ER-CNN (extended ranking CNN) and R-RNN (ranking RNN) in the context of other state-of-the-art models.\nOur proposed models obtain state-of-the-art results on the SemEval 2010 task 8 data set without making use of any linguistic features.\n"}
{"q_uid": "a02696d4ab728ddd591f84a352df9375faf7d1b4", "table": "\nTable 1: Data used in this paper. Tasks 1-5 were generated using our simulator and share the same KB. Task 6 was converted from the 2nd Dialog State Tracking Challenge. Concierge is made of chats extracted from a real online concierge service. (*) Tasks 1-5 have two test sets, one using the vocabulary of the training set and the other using out-of-vocabulary words.\n\n|                                                   | Tasks | T1 | T2 | T3 | T4 | T5 | T6 | Concierge |\n|-----------------------------|-----------------------------|----|----|----|----|----|----|-----------|\n| DIALOGS Average statistics  | Number of utterances:       | 12 | 17 | 43 | 15 | 55 | 54 | 8   |\n| DIALOGS Average statistics  | - user utterances           |  5  | 7  | 7  | 4  | 13 | 6  | 4        |\n| DIALOGS Average statistics  | - bot utterances            | 7  | 10 | 10 | 4  | 18 | 8  | 4         |\n| DIALOGS Average statistics  | - outputs from API calls    | 0  | 0  | 23 | 7  | 24 | 40 | 0         |\n\n| DATASETS Tasks 1-5 share the same data source |   T1  T2  T3  T4  T5   |  T6   | Concierge | \n|-----------------------------------------------|------------------------|-------|-------|\n| Vocabulary size                               |        3,747           | 1,229 | 8,629 |\n| Candidate set size                            |        4,212           | 2,406 | 11,482|\n| Training dialogs                              |        1,000           | 1,618 | 3,249 |\n| Validation dialogs                            |        1,000           | 500   | 403   |\n| Test dialogs                                  |        1,000           | 1117   | 402   |\n", "text": "\n"}
{"q_uid": "a49832c89a2d7f95c1fe6132902d74e4e7a3f2d0", "table": "\nTable 2: Results of our experiments with NNGLM and NNJM on the CoNLL 2014 test set (* indicates statistical significance with p < 0.01)\n| System                    | P     | R     | F<sub>0.5</sub> |\n|---------------------------|-------|-------|-----------------|\n| Baseline                  | 50.56 | 22.68 | 40.58           |\n| Baseline + NNGLM          | 50.73 | 23.21 | 41.01*          |\n| Baseline + NNJM           | 51.39 | 23.26 | 41.38*          |\n| Baseline + NNGLM + NNJM   | 52.34 | 23.07 | 41.75*          |\n", "text": "\nThe evaluation is performed similar to the CoNLL 2014 shared task setting using the the official test data of the CoNLL 2014 shared task with annotations from two annotators (without considering alternative annotations suggested by the participating teams). The test dataset consists of 1,312 error-annotated sentences with 30,144 tokens on the source side. We make use of the official scorer for the shared task, M INLINEFORM0 Scorer v3.2 BIBREF19 , for evaluation.\n"}
{"q_uid": "b3fcab006a9e51a0178a1f64d1d084a895bd8d5c", "table": "\nTable 2: Comparing with several state-of-the-art models (reported in percentage, higher is better).\n| Model                        | METEOR |\n|------------------------------|--------|\n| LSTM [26]                    | 26.9   |\n| Joint-LSTM unidirectional (ours) | 29.5   |\n| S2VT [23]                    |        |\n| -RGB (VGG)                   | 29.2   |\n| -RGB (VGG)+Flow (AlexNet)    | 29.8   |\n| LSTM-E (VGG) [15]            | 29.5   |\n| LSTM-E (C3D) [15]            | 29.9   |\n| Yao et al. [32]              | 29.6   |\n|Joint-BiLSTM reinforced (ours) | 30.3   |\n", "text": "\nWe also evaluate our Joint-BiLSTM structure by comparing with several other state-of-the-art baseline approaches, which exploit either local or global temporal structure. As shown in Table TABREF20 , our Joint-BiLSTM reinforced model outperforms all of the baseline methods.\n"}
{"q_uid": "6ea63327ffbab2fc734dd5c2414e59d3acc56ea5", "table": "\nTable 1: Predictive loglikelihood comparison on the text data sets (sorted by validation set performance).\n| Data          | Method        | Parameters | LSTM dims | HMM states | Validation | Training |\n|---------------|---------------|------------|-----------|------------|------------|----------|\n| Shakespeare   | Continuous HMM| 1300       |     |20        | -2.74      | -2.75     |\n|               | Discrete HMM  | 650        |     |10        | -2.69      | -2.68     |\n|               | Discrete HMM  | 1300       |      |20        | -2.5       | -2.49     |\n|               | LSTM          | 865        | 5    |     | -2.41      | -2.35     |\n|               | Hybrid        | 1515       | 5 | 10    | -2.3       | -2.26     |\n|               | Hybrid        | 2165       | 5   | 20      | -2.26      | -2.18     |\n|               | LSTM          | 2130       | 10   |     | -2.23      | -2.12     |\n|               | Joint hybrid  | 1515       | 5   | 10      | -2.21      | -2.18     |\n|               | Hybrid        | 2780       | 10  |  10      | -2.19      | -2.08     |\n|               | Hybrid        | 3430       | 10  |  20      | -2.16      | -2.04     |\n|               | Hybrid        | 4445       | 15  |  10      | -2.13      | -1.95     |\n|               | Joint hybrid  | 3430       | 10  |  10      | -2.12      | -2.07     |\n|               | LSTM          | 3795       | 15        | -2.1       | -1.95     |\n|               | Hybrid        | 5095       | 15  |  20      | -2.07      | -1.92     |\n|               | Hybrid        | 6015       | 20  | 10      | -2.03      | -1.87     |\n|               | Joint hybrid  | 4445       | 15  |  10      | -2.03      | -1.97     |\n|               | LSTM          | 5860       | 20        | -2.03      | -1.83     |\n|               | Hybrid        | 7160       | 20  |  20      | -2.02      | -1.85     |\n|               | Joint hybrid  | 7160       | 20  |  10    | -1.97      | -1.88     |\n|---------------|---------------|------------|-----------|------------|------------|----------|\n| Linux Kernel  | Discrete HMM  | 1000       |    | 10        | -2.76      | -2.77     |\n|               | Discrete HMM  | 2000       |    | 20        | -2.55      | -2.5     |\n|               | Hybrid        | 1215       | 5   |      | -2.54      | -2.48      |\n|               | LSTM          | 2215       | 5  |  10    | -2.35      | -2.26     |\n|               | Hybrid        | 2215       | 5  | 10      | -2.33      | -2.26     |\n|               | Hybrid        | 3215       | 5   |  20      | -2.25      | -2.16     |\n|               | Joint hybrid  | 4830       | 10   | 10     | -2.18      | -2.08     |\n|               | LSTM          | 2830       | 10  |      | -2.17      | -2.07     |\n|               | Hybrid        | 3830       | 10  | 10      | -2.14      | -2.05     |\n|               | Hybrid        | 4830       | 10   | 20     | -2.07      | -1.97     |\n|               | LSTM          | 4845       | 15  |      | -2.03      | -1.89     |\n|               | Joint hybrid  | 5845       | 15   |  10   | -2.00      | -1.88     |\n|               | Hybrid        | 5845       | 15   | 10     | -1.96      | -1.84     |\n|               | Hybrid        | 6845       | 15   | 20     | -1.96      | -1.83     |\n|               | Joint hybrid  | 9260       | 20   | 10     | -1.90      | -1.76     |\n|               | LSTM          | 7260       | 20        | -1.88      | -1.73     |\n|               | Hybrid        | 8260       | 20   | 10     | -1.87      | -1.73     |\n|               | Hybrid        | 9260       | 20   | 20      | -1.85      | -1.71     |\n|---------------|---------------|------------|-----------|------------|------------|----------|\n| Penn Tree Bank| Continuous HMM| 1000       | 100 | 20        | -2.58      | -2.58     |\n|               | Discrete HMM  | 500        |   | 10        | -2.43      | -2.43     |\n|               | Discrete HMM  | 1000       |   | 20        | -2.28      | -2.28     |\n|               | LSTM          | 715        | 5   |      | -2.22      | -2.22     |\n|               | Hybrid        | 1215       | 5   |  10     | -2.14      | -2.15     |\n|               | Joint hybrid  | 1215       | 5   |  10     | -2.08      | -2.08     |\n|               | Hybrid        | 1715       | 5   | 20      | -2.06      | -2.07     |\n|               | LSTM          | 1830       | 10  |      | -1.99      | -1.99     |\n|               | Hybrid        | 2330       | 10  | 10      | -1.94      | -1.95     |\n|               | Joint hybrid  | 2830       | 10  | 10      | -1.94      | -1.95     |\n|               | Hybrid        | 3345       | 15  | 20      | -1.93      | -1.94     |\n|               | LSTM          | 3345       | 15   |     | -1.82      | -1.82     |\n|               | Hybrid        | 3845       | 15   | 10     | -1.81      | -1.82     |\n|               | Hybrid        | 4345       | 15   | 20     | -1.8       | -1.81     |\n|               | Joint hybrid  | 6260       | 20   | 10     | -1.73      | -1.74     |\n|               | LSTM          | 5260       | 20    |    | -1.72      | -1.73     |\n|               | Hybrid        | 5760       | 20   | 10     | -1.72      | -1.72     |\n|               | Hybrid        | 6260       | 20   | 20    | -1.71      | -1.71     |\n", "text": "\n"}
{"q_uid": "c2b8ee872b99f698b3d2082d57f9408a91e1b4c1", "table": "\nTable 2: Comparison of annotators trained for common English news texts (micro-averaged scores on match per annotation span). The table shows micro-precision, recall and NER-style F1 for CoNLL2003, KORE50, ACE2004 and MSNBC datasets.\n| Common Test Sets | CoNLL2003 | CoNLL2003 | CoNLL2003 | KORE50 | KORE50 | KORE50 |  ACE2004 | ACE2004 | ACE2004 | MSNBC | MSNBC | MSNBC |\n|------------------|-----------|--------|---------|-------|-----------|--------|---------|-------|-----------|--------|---------|-------|\n| corpus           | RCV-1     | RCV-1  |  RCV-1     | RCV-1  |  RCV-1     | RCV-1  | newswire| newswire| newswire| MSNBC news |  MSNBC news |  MSNBC news |\n| topic            | news (en) | news (en) | news (en) | news (en) | news (en) | news (en) | news (en) | news (en) | news (en) | news (en) | news (en) | news (en) |\n| annotation guideline | named entities | named entities | named entities | named entities | named entities | named entities | all mentions | all mentions | all mentions | wikifcation | wikifcation | wikifcation |\n\n| Annotator        | Method    | Prec | Rec | F1 | Prec | Rec | F1 | Prec | Rec | F1 | Prec | Rec | F1 |\n| Babelfy          | POS+DICT  | 53.8 | 70.4 | 61.0 | 72.1 | 73.6 | 72.9 | 12.1 | 42.2 | 18.8 | 43.3 | 77.8 | 55.6 |\n| DBpedia Spotlight| DICT      | 74.7 | 66.4 | 70.3 | n/a | n/a | n/a | 13.0 | 74.8 | 22.2 | 56.2 | 49.0 | 52.4 |\n| Entityclassifier.eu | Noun phrase | 81.2 | 83.0 | 82.1 | 93.8 | 94.4 | 94.1 | 13.3 | 90.5 | 23.2 | 76.2 | 93.9 | 84.1 |\n| FOX              | Ensemble  | 99.1 | 75.2 | 85.5 | 94.6 | 73.6 | 82.8 | 12.4 | 59.5 | 20.6 | 38.3 | 31.3 | 34.4 |\n| LingPipe MUC-7   | LM+HMM    | 91.5 | 66.6 | 77.1 | 93.9 | 86.1 | 89.9 | 16.3 | 88.9 | 27.5 | 73.3 | 78.7 | 75.9 |\n| NERD-ML          | Ensemble  | 59.9 | 72.0 | 65.4 | 70.4 | 82.6 | 76.0 | 19.6 | 47.4 | 27.7 | 69.7 | 57.2 | 62.8 |\n| Stanford NER     | CRF+Dist  | 99.5 | 76.1 | 86.2 | 94.8 | 76.4 | 84.6 | 18.2 | 90.9 | 30.3 | 95.2 | 84.1 | 89.3 |\n| TagMe2           | DICT      | 68.3 | 47.7 | 56.2 | 66.5 | 88.2 | 75.8 | 23.2 | 71.6 | 35.0 | 55.6 | 38.0 | 45.2 |\n| DATEXIS-NER      | BLSTM     | 87.8 | 94.6 | 91.0 | 92.6 | 95.1 | 93.8 | 13.3 | 98.0 | 23.4 | 73.3 | 98.2 | 83.9 |\n", "text": "\n"}
{"q_uid": "8b3d3953454c88bde88181897a7a2c0c8dd87e23", "table": "\nTable 2: Threshold Correlation with vector-res\n|    |           | UMNSRS |  UMNSRS | MiniMayoSRS| MiniMayoSRS|\n| T | # bigrams | sim    | rel    |  MD    | coder |\n|---|-----------|---------|-------------|-----------|----------|\n\n| 0 | 850,959   | 0.58        | 0.41        | 0.58                | 0.65 |\n| 1 | 166,003   | 0.56        | 0.39        | 0.60                | 0.67 |\n| 2 | 655,502   | 0.64        | 0.47        | 0.56                | 0.62 |\n| 3 | 27,744    | 0.60        | 0.46        | 0.62                | 0.71 |\n| 4 | 10,091    | 0.56        | 0.43        | 0.75                | 0.76 |\n| 5 | 3,305     | 0.26        | 0.16        | 0.36                | 0.36 |\n", "text": "\nchiu2016how evaluated both the the Skip-gram and CBOW models over the PMC corpus and PubMed.\n"}
{"q_uid": "6412e97373e8e9ae3aa20aa17abef8326dc05450", "table": "\nTable 5: Performance of human evaluators and our classifiers (trained on all features), for Dataset-H as the test set\n| | A (%) | NP (%) | PP (%) | NR (%) | PR (%)|\n| Annotators| 68.8 | 71.7 | 61.7 | 83.9 | 43.5 |\n| Training Dataset |   Our classifiers |  Our classifiers | Our classifiers | Our classifiers | Our classifiers |\n| Dataset 1 | 47.3 | 70 | 40 | 26 | 81 |\n| Dataset 2 | 64 | 70 | 53 | 72 | 50 |\n", "text": "\n"}
{"q_uid": "957bda6b421ef7d2839c3cec083404ac77721f14", "table": "\nTable 1: Our Feature Set for Drunk-texting Prediction\n| Feature                        | Description                                                                 |\n|-------------------------------|-----------------------------------------------------------------------------|\n|          **N-gram Features**      | **N-gram Features**                                                         |\n| Unigram & Bigram (Presence)   | Boolean features indicating unigrams and bigrams                             |\n| Unigram & Bigram (Count)      | Real-valued features indicating unigrams and bigrams                         |\n|             **Stylistic Features**    | **Stylistic Features**                                                      |\n| LDA unigrams (Presence/Count) | Boolean & real-valued features indicating unigrams from LDA                 |\n| POS Ratio                     | Ratios of nouns, adjectives, adverbs in the tweet                           |\n| #Named Entity Mentions        | Number of named entity mentions                                             |\n| #Discourse Connectors         | Number of discourse connectors                                              |\n| Spelling errors               | Boolean feature indicating presence of spelling mistakes                    |\n| Repeated characters           | Boolean feature indicating whether a character is repeated three times consecutively |\n| Capitalisation                | Number of capital letters in the tweet                                      |\n| Length                        | Number of words                                                             |\n| Emoticon (Presence/Count)     | Boolean & real-valued features indicating unigrams                          |\n| Sentiment Ratio               | Positive and negative word ratios                                           |\n", "text": "\n"}
{"q_uid": "06eb9f2320451df83e27362c22eb02f4a426a018", "table": "\nTable 1: Statistics computed at the different levels of document preprocessing on the training set.\n|       | Lvl 1 | Lvl 2 | Lvl 3 |\n|-------|-------|-------|-------|\n| Avg. sentences | 399   | 347   | 101   |\n| Avg. words     | 9 772 | 7 874 | 1 922 |\n| Max. recall    | 83.9% | 81.8% | 70.9% |\n", "text": "\nThree incremental levels of document preprocessing are experimented with: raw text, text cleaning through document logical structure detection, and removal of keyphrase sparse sections of the document. In doing so, we present the first consistent comparison of different keyphrase extraction models and study their robustness over noisy text.\nIn this study, we concentrate our effort on re-assessing keyphrase extraction performance on three increasingly sophisticated levels of document preprocessing described below.\nTable shows the average number of sentences and words along with the maximum possible recall for each level of preprocessing. The maximum recall is obtained by computing the fraction of the reference keyphrases that occur in the documents. We observe that the level 2 preprocessing succeeds in eliminating irrelevant text by significantly reducing the number of words (-19%) while maintaining a high maximum recall (-2%). Level 3 preprocessing drastically reduce the number of words to less than a quarter of the original amount while interestingly still preserving high recall.\n"}
{"q_uid": "30803eefd7cdeb721f47c9ca72a5b1d750b8e03b", "table": "\nTable 2. Performance comparison of the Intelligent Voice speaker recognition system with various analysis on the development protocol of NIST SRE 2016.\n| Acoustic Features | Unequalized | Unequalized | Unequalized | Equalized | Equalized | Equalized |\n|-------------------|-------------|-----------|-----------|-----------|-----------|-----------|\n|                   | EER | C_min det | C_det | EER | C_min det | C_det |\n| Primary           |     |           |       |     |           |       |\n| MFCC              | 16.49 | 0.6633 | 0.6754 | 15.83 | 0.6650 | 0.6749 |\n| PLP               | 17.87 | 0.6857 | 0.6977 | 16.84 | 0.6914 | 0.6982 |\n| Fusion            | 16.04 | 0.6012 | 0.6107 | 14.93 | 0.6011 | 0.6267 |\n| Scenario A        |     |           |       |     |           |       |\n| MFCC              | 16.82 | 0.6658 | 0.6794 | 16.42 | 0.6890 | 0.7021 |\n| PLP               | 16.98 | 0.6691 | 0.6881 | 16.28 | 0.6903 | 0.7092 |\n| Fusion            | 15.73 | 0.6153 | 0.6369 | 15.12 | 0.6587 | 0.6964 |\n| Scenario B        |     |           |       |     |           |       |\n| MFCC              | 16.55 | 0.6735 | 0.6880 | 16.10 | 0.6755 | 0.6945 |\n| PLP               | 18.27 | 0.6938 | 0.7141 | 16.97 | 0.7018 | 0.7299 |\n| Fusion            | 16.31 | 0.6075 | 0.6299 | 14.70 | 0.6259 | 0.6482 |\n| Scenario C        |     |           |       |     |           |       |\n| MFCC              | 17.08 | 0.6767 | 0.6889 | 16.77 | 0.6677 | 0.6927 |\n| PLP               | 17.98 | 0.6857 | 0.6968 | 17.21 | 0.7001 | 0.7192 |\n| Fusion            | 16.59 | 0.6176 | 0.6264 | 15.70 | 0.6363 | 0.6680 |\n| Scenario D        |     |           |       |     |           |       |\n| MFCC              | 17.42 | 0.6694 | 0.6833 | 16.54 | 0.6693 | 0.6820 |\n| PLP               | 18.49 | 0.6851 | 0.7062 | 17.46 | 0.6852 | 0.7085 |\n| Fusion            | 17.03 | 0.6171 | 0.6315 | 15.73 | 0.6243 | 0.6410 |\n| Scenario E        |     |           |       |     |           |       |\n| MFCC              | 16.65 | 0.6976 | 0.7124 | 16.24 | 0.6972 | 0.7122 |\n| PLP               | 18.48 | 0.7182 | 0.7324 | 17.49 | 0.7263 | 0.7480 |\n", "text": "\nIn this section we present the results obtained on the protocol provided by NIST on the development set which is supposed to mirror that of evaluation set. The results are shown in Table TABREF26 .\n"}
{"q_uid": "b6b5f92a1d9fa623b25c70c1ac67d59d84d9eec8", "table": "\nTable 1: Final test set results in terms of average precision (AP). Dimensionalities marked with * refer to dimensionality per frame for DTW-based approaches. For CNN and LSTM models, results are given as means over several training runs (5 and 10, respectively) along with their standard deviations.\n| Model                        | Dim | AP           |\n|------------------------------|-----|--------------|\n| MFCCs + DTW [14]             | 39* | 0.214        |\n| Corr. autoencoder + DTW [22] | 100*| 0.469        |\n| Classifier CNN [14]          | 1061| 0.532 \u00b1 0.014|\n| Siamese CNN [14]             | 1024| 0.549 \u00b1 0.011|\n| Classifier LSTM              | 1061| 0.616 \u00b1 0.009|\n| Siamese LSTM                 | 1024| 0.671 \u00b1 0.011|\n", "text": "\n"}
{"q_uid": "9555aa8de322396a16a07a5423e6a79dcd76816a", "table": "\nTable 2: Rouge-N limited-length recall on DUC2004. Size denotes the size of decoder vocabulary in a model.\n| Models                                      | Size | Rouge-1 | Rouge-2 | Rouge-L |\n|---------------------------------------------|------|---------|---------|---------|\n| ZOPIARY (Zajic et al. (2004))               | -    | 25.12   | 6.46    | 20.12   |\n| ABS (Rush et al. (2015))                    | 69K  | 26.55   | 7.06    | 23.49   |\n| ABS+ (Rush et al. (2015))                   | 69K  | 28.18   | 8.49    | 23.81   |\n| RAS-LSTM (Chopra et al. (2016))             | 69K  | 27.41   | 7.69    | 23.06   |\n| RAS-Elman (Chopra et al. (2016))            | 69K  | 28.97   | 8.26    | 24.06   |\n| big-words-lvt2k-1sent (Nallapati et al. (2016)) | 69K  | 28.35   | 9.46    | 24.59   |\n| big-words-lvt5k-1sent (Nallapati et al. (2016)) | 200K | 28.61   | 9.42    | 25.24   |\n| Ours-GRU (C)                                | 15K  | 29.08   | 9.20    | 25.25   |\n| Ours-LSTM (C)                               | 15K  | 29.89   | 9.37    | 25.93   |\n| Ours-Opt-2 (C)                              | 15K  | 29.74   | 9.44    | 25.94   |\n", "text": "\nAs shown in Table TABREF35 , our method outperforms all previous methods on Rouge-1 and Rouge-L, and is comparable on Rouge-2.\n"}
{"q_uid": "b13d0e463d5eb6028cdaa0c36ac7de3b76b5e933", "table": "\nTable 1: The knowledge base completion (link prediction) results on WN18, FB15k, and FB15k-237\n| Model                              | Aux. Info.               | WN18                | FB15k               | FB15k-237           |\n|------------------------------------|--------------------------|---------------------|---------------------|---------------------|\n|                                    |                          | Hits@10 | MR        | Hits@10 | MR        | Hits@10 | MR        |\n| TransE (Bordes et al. 2013)        | NO                       | 89.2    | 251       | 47.1    | 125       | -       | -         |\n| NTN (Socher et al. 2013)           | NO                       | 66.1    | -         | 41.4    | -         | -       | -         |\n| TransH (Wang et al. 2014)          | NO                       | 86.7    | 303       | 64.4    | 87        | -       | -         |\n| TransR (Lin et al. 2015b)          | NO                       | 92.0    | 225       | 68.7    | 77        | -       | -         |\n| CTransR (Lin et al. 2015b)         | NO                       | 92.3    | 218       | 70.2    | 75        | -       | -         |\n| KG2E (He et al. 2015)              | NO                       | 93.2    | 348       | 74.0    | 59        | -       | -         |\n| TransD (Ji et al. 2015)            | NO                       | 92.2    | 212       | 77.3    | 91        | -       | -         |\n| TATEC (Garc\u00eda-Dur\u00e1n et al. 2015b)  | NO                       | -       | -         | 76.7    | 58        | -       | -         |\n| DISTMULT (Yang et al. 2015)        | NO                       | 94.2    | -         | 57.7    | -         | 41.9    | 254       |\n| STransE (Nguyen et al. 2016)       | NO                       | 93.4    | 206       | 79.7    | 69        | -       | -         |\n| HOLE (Nickel et al. 2016)          | NO                       | 94.9    | -         | 73.9    | -         | -       | -         |\n| ComplEx (Trouillon et al. 2016)    | NO                       | 94.7    | -         | 84.0    | -         | -       | -         |\n| TransG (Xiao et al. 2016)          | NO                       | 94.9    | 345       | 88.2    | 50        | -       | -         |\n| ConvE (Dettmers et al. 2017)       | NO                       | 95.5    | 504       | 87.3    | 64        | 45.8    | 330       |\n| ProjE (Shi and Weninger 2017)      | NO                       | -       | -         | 88.4    | 34        | -       | -         |\n| RTransE (Garc\u00eda-Dur\u00e1n et al. 2015a)| Path                     | -       | -         | 76.2    | 50        | -       | -         |\n| PTransE (Lin et al. 2015a)         | Path                     | -       | -         | 84.6    | 58        | -       | -         |\n| NLFeat (Toutanova et al. 2015)     | Node + Link Features     | 94.3    | -         | 87.0    | -         | -       | -         |\n| Random Walk (Wei et al. 2016)      | Path                     | 94.8    | -         | 74.7    | -         | -       | -         |\n| EKGN                                  | NO                 | 95.3   | 249       | 92.7    | 38       | 46.4      | 211        |\n", "text": "\n"}
{"q_uid": "897ba53ef44f658c128125edd26abf605060fb13", "table": "\nTable 1: Results of the English\u2192German systems in an under-resourced scenario.\n| System                            | tst2013  | tst2013     | tst2014   | tst2014      |\n|                                   | BLEU  | \u0394BLEU     | BLEU  | \u0394BLEU     |\n|-----------------------------------|---------|---------|-------|-----------|\n| Baseline (En\u2192De)                  | 24.35 | \u2014         | 20.62 | \u2014         |\n| Mix-source (En,De\u2192De,De)          | 26.99 | +2.64     | 22.71 | +2.09     |\n| Mix-multi-source (En,Fr\u2192De,De)    | 26.64 | +2.21     | 22.21 | +1.59     |\n", "text": "\n"}
{"q_uid": "cebf3e07057339047326cb2f8863ee633a62f49f", "table": "\nTable 5: Performance on multilingual sentiment analysis (not challenges). B4MSA was restricted to use only the multilingual set of parameters.\n| Language   |                           |   F1    | (F1^pos + F1^neg) / 2 | acc  |\n|------------|---------------------------|---------|--------------|------|\n| Arabic     |                           |         |              |      |\n|            | Salameh et al. [25]       | -       |  -            | 0.787|\n|            | Saif et al. [18]          | -       |    -           | 0.794|\n|            | B4MSA (100%)              | 0.642                 | 0.781| 0.799|\n| German     |                           |                       |      |      |\n|            | Mozeti\u010d et al. [19]       | -                     | 0.536| 0.610|\n|            | B4MSA (89%)               | 0.621                 | 0.559| 0.668|\n| Portuguese |                           |                       |      |      |\n|            | Mozeti\u010d et al. [19]       | -                     | 0.553| 0.507|\n|            | B4MSA (58%)               | 0.550                 | 0.591| 0.555|\n| Russian    |                           |                       |      |      |\n|            | Mozeti\u010d et al. [19]       | -                     | 0.615| 0.603|\n|            | B4MSA (69%)               | 0.754                 | 0.768| 0.750|\n| Swedish    |                           |                       |      |      |\n|            | Mozeti\u010d et al. [19]       | -                     | 0.657| 0.616|\n|            | B4MSA (93%)               | 0.680                 | 0.717| 0.691|\n", "text": "\nTable TABREF24 supports the idea of choosing B4MSA as a bootstrapping sentiment classifier because, in the overall, B4MSA reaches superior performances regardless of the language.\n"}
{"q_uid": "9e04730907ad728d62049f49ac828acb4e0a1a2a", "table": "\nTable 6: Comparison of ACC of our proposed methods and some other non-biased models on three datasets. For LPI, we project the text under the best dimension as described in Section 4.3. For both bi-LSTM and bi-GRU based clustering methods, the binary codes generated from LPI are used to guide the learning of bi-LSTM/bi-GRU models.\n| Method           | SearchSnippets ACC (%) | StackOverflow ACC (%) | Biomedical ACC (%) |\n|------------------|------------------------|-----------------------|--------------------|\n| bi-LSTM (last)   | 64.50\u00b13.18             | 46.83\u00b11.79            | 36.50\u00b11.08         |\n| bi-LSTM (mean)   | 65.85\u00b14.18             | 44.93\u00b11.83            | 35.60\u00b11.21         |\n| bi-LSTM (max)    | 61.70\u00b15.10             | 38.74\u00b11.62            | 32.83\u00b10.73         |\n| bi-GRU (last)    | 70.18\u00b12.62             | 43.36\u00b11.46            | 35.19\u00b10.78         |\n| bi-GRU (mean)    | 70.29\u00b12.61             | 44.53\u00b11.81            | 36.75\u00b11.21         |\n| bi-GRU (max)     | 65.69\u00b11.02             | 54.40\u00b12.07            | 37.23\u00b11.19         |\n| LPI (best)       | 47.11\u00b12.91             | 38.04\u00b11.72            | 37.15\u00b11.16         |\n| STC\u00b2-LPI         | 77.01\u00b14.13             | 51.14\u00b12.92            | 43.00\u00b11.25         |\n", "text": "\n"}
{"q_uid": "94e0cf44345800ef46a8c7d52902f074a1139e1a", "table": "\nTable 1: Corpora genres and number of NEs of different classes\n| Corpus           | Genre                   | N     | PER   | LOC   | ORG   |\n|------------------|-------------------------|-------|-------|-------|-------|\n| MUC 7 Train      | Newswire (NW)           | 552   | 98    | 172   | 282   |\n| MUC 7 Dev        | Newswire (NW)           | 572   | 93    | 193   | 286   |\n| MUC 7 Test       | Newswire (NW)           | 863   | 145   | 244   | 474   |\n| CoNLL Train      | Newswire (NW)           | 20061 | 6600  | 7140  | 6321  |\n| CoNLL TestA      | Newswire (NW)           | 4229  | 1641  | 1434  | 1154  |\n| CoNLL TestB      | Newswire (NW)           | 4946  | 1617  | 1668  | 1661  |\n| ACE NW           | Newswire (NW)           | 3835  | 894   | 2238  | 703   |\n| ACE BN           | Broadcast News (BN)     | 2067  | 830   | 885   | 352   |\n| ACE BC           | Broadcast Conversation (BC) | 1746  | 662   | 795   | 289   |\n| ACE WL           | Weblog (WEB)            | 1716  | 756   | 411   | 549   |\n| ACE CTS          | Conversational Telephone Speech (CTS) | 2667  | 2256  | 347   | 64    |\n| ACE UN           | Usenet Newsgroups (UN)  | 668   | 277   | 243   | 148   |\n| OntoNotes NW     | Newswire (NW)           | 52055 | 16460 | 16966 | 22419 |\n| OntoNotes BN     | Broadcast News (BN)     | 14213 | 5259  | 5919  | 3035  |\n| OntoNotes BC     | Broadcast Conversation (BC) | 7676  | 3224  | 2940  | 1512  |\n| OntoNotes WB     | Weblog (WEB)            | 6080  | 2591  | 2319  | 1170  |\n| OntoNotes TC     | Telephone Conversations (TC) | 1430  | 745   | 569   | 116   |\n| OntoNotes MZ     | Magazine (MZ)           | 8150  | 2895  | 3569  | 1686  |\n| MSM 2013 Train   | Twitter (TWI)           | 2815  | 1660  | 575   | 580   |\n| MSM 2013 Test    | Twitter (TWI)           | 1432  | 1110  | 98    | 224   |\n| Ritter           | Twitter (TWI)           | 1221  | 454   | 380   | 387   |\n| UMBC             | Twitter (TWI)           | 510   | 172   | 168   | 170   |\n", "text": "\nSince the goal of this study is to compare NER performance on corpora from diverse domains and genres, seven benchmark NER corpora are included, spanning newswire, broadcast conversation, Web content, and social media (see Table 1 for details).\n"}
{"q_uid": "a978a1ee73547ff3a80c66e6db3e6c3d3b6512f4", "table": "\nTable 1: Domain adaptation results (BLEU-4 scores) for IWSLT-CE using NTCIR-CE.\n|                                    |          | IWSLT-CE | IWSLT-CE | IWSLT-CE | IWSLT-CE | IWSLT-CE | \n| System                             | NTCIR-CE | test 2010 | test 2011 | test 2012 | test 2013 | average |\n|------------------------------------|----------|-----------|-----------|-----------|-----------|---------|\n| IWSLT-CE SMT                       | -        | 12.73     | 16.27     | 14.01     | 14.67     | 14.31   |\n| IWSLT-CE NMT                       | -        | 6.75      | 9.08      | 9.05      | 7.29      | 7.87    |\n| NTCIR-CE SMT                       | 29.54    | 3.57      | 4.70      | 4.21      | 4.74      | 4.33    |\n| NTCIR-CE NMT                       | 37.11    | 2.23      | 2.83      | 2.55      | 2.85      | 2.60    |\n| Fine tuning                        | 17.37     | 13.93     | 18.99     | 16.12     | 17.12   | 16.41   |\n| Multi domain                       | 36.40     | 13.42     | 19.07     | 16.56     | 17.54   | 16.34   |\n| Multi domain w/o tags              | 37.32     | 12.57     | 17.40     | 15.02     | 15.96   | 14.97   |\n| Multi domain + Fine tuning         |  14.47     | 13.18     | 18.03     | 16.41     | 16.80   | 15.82   |\n| Mixed fine tuning                  |  37.01     | 15.04     | 20.96     | 18.77     | 18.63   | 18.01   |\n| Mixed fine tuning w/o tags         |  39.67     | 14.47     | 20.53     | 18.10     | 17.97   | 17.43   |\n| Mixed fine tuning + Fine tuning    | 32.03     | 14.40     | 19.53     | 17.65     | 17.94   | 17.11   |\n\n", "text": "\n"}
{"q_uid": "bbb77f2d6685c9257763ca38afaaef29044b4018", "table": "\nTable 3: Classification results for different feature combinations. P\u2192 Precision, R\u2192 Recall, F\u2192 F\u2019 score, Kappa\u2192 Kappa statistics show agreement with the gold labels. Subscripts 1 and -1 correspond to sarcasm and non-sarcasm classes respectively.\n| Features                   | P(1) | P(-1) | P(avg) | R(1) | R(-1) | R(avg) | F(1) | F(-1) | F(avg) | Kappa |\n|----------------------------|------|-------|--------|------|-------|--------|------|-------|--------|-------|\n| **Multi Layered Neural Network** |\n| Unigram                    | 53.1 | 74.1  | 66.9   | 51.7 | 75.2  | 66.6   | 52.4 | 74.6  | 66.8   | 0.27  |\n| Sarcasm (Joshi et. al.)    | 59.2 | 75.4  | 69.7   | 51.7 | 80.6  | 70.4   | 55.2 | 77.9  | 69.9   | 0.33  |\n| Gaze                       | 62.4 | 76.7  | 71.7   | 54   | 82.3  | 72.3   | 57.9 | 79.4  | 71.8   | 0.37  |\n| Gaze+Sarcasm               | 63.4 | 75    | 70.9   | 48   | 84.9  | 71.9   | 54.6 | 79.7  | 70.9   | 0.34  |\n| **Na\u00efve Bayes**            |\n| Unigram                    | 45.6 | 82.4  | 69.4   | 81.4 | 47.2  | 59.3   | 58.5 | 60    | 59.5   | 0.24  |\n| Sarcasm (Joshi et. al.)    | 46.1 | 81.6  | 69.1   | 79.4 | 49.5  | 60.1   | 58.3 | 61.6  | 60.5   | 0.25  |\n| Gaze                       | 57.3 | 82.7  | 73.8   | 72.9 | 70.5  | 71.3   | 64.2 | 76.1  | 71.9   | 0.41  |\n| Gaze+Sarcasm               | 46.7 | 82.1  | 69.6   | 79.7 | 50.5  | 60.8   | 58.9 | 62.5  | 61.2   | 0.26  |\n| **Original system by Riloff et. al. : Rule Based with implicit incongruity** |\n| Ordered                    | 60   | 30    | 40     | 50   | 39    | 46     | 54   | 34    | 47     | 0.10  |\n| Unordered                  | 56   | 28    | 46     | 40   | 42    | 41     | 46   | 33    | 42     | 0.16  |\n| **Original system by Joshi et. al. : SVM with RBF Kernel** |\n| Sarcasm (Joshi et. al.)    | 73.1 | 69.4  | 70.7   | 22.6 | 95.5  | 69.8   | 34.5  | 80.4   | 64.2 |  0.21  |\n| **SVM Linear with default parameters** |\n| Unigram                    | 56.5 | 77    | 69.8   | 58.6 | 75.5  | 69.5   | 57.5  |  76.2 | 69.6  | 0.34   |\n| Sarcasm (Joshi et. al.)    | 59.9 | 78.7  | 72.1   | 61.4 | 77.6  | 71.9   | 60.6 | 78.2  | 72     | 0.39  |\n| Gaze                       | 65.9 | 75.9  | 72.4   | 49.7 | 86    | 73.2   | 56.7 | 80.6  | 72.2   | 0.38  |\n| Gaze+Sarcasm               | 63.7 | 79.5  | 74   | 61.7 | 80.9  | 74.1   | 62.7 | 80.2  | 74     | 0.43  |\n| **Multi Instance Logistic Regression: Best Performing Classifier** |\n| Gaze                       | 65.3 | 77.2  | 73     | 53   | 84.9  | 73.8   | 58.5 | 80.8  | 73.1   | 0.41  |\n| Gaze+Sarcasm               | 62.5 | 84    | 76.5   | 72.6 | 76.7  | 75.3   | 67.2 | 80.2  | 75.7   | 0.47  |\n", "text": "\n"}
{"q_uid": "e8fcfb1412c3b30da6cbc0766152b6e11e17196c", "table": "\nTable 1: Summary of high-capacity MoE-augmented models with varying computational budgets, vs. best previously published results [Jozefowicz et al., 2016]. Details in Appendix C\n|                         | Test Perplexity 10 epochs | Test Perplexity 100 epochs | #Parameters excluding embedding and softmax layers | ops/timestep | Training Time 10 epochs | TFLOPS/GPU |\n|-------------------------|---------------------------|----------------------------|---------------------------------------------------|--------------|------------------------|-------------|\n| Best Published Results  | 34.7                      | 30.6                       | 151 million                                        | 151 million  | 50 hours, 32 k40s      | 1.09        |\n| Low-Budget MoE Model    | 34.1                      | -                          | 303 million                                        | 8.9 million  | 15 hours, 16 k40s      | 0.74        |\n| Medium-Budget MoE Model | 31.3                      | -                          | 4313 million                                       | 33.8 million | 17 hours, 32 k40s      | 1.22        |\n|High-Budget MoE Model    | 28.0                    |   -                | 4371 million                                        | 142.7 million  | 47 hours, 32 k40s      | 1.56        |\n", "text": "\nThe two models achieved test perplexity of INLINEFORM0 and INLINEFORM1 respectively, showing that even in the presence of a large MoE, more computation is still useful. Results are reported at the bottom of Table TABREF76 . The larger of the two models has a similar computational budget to the best published model from the literature, and training times are similar. Comparing after 10 epochs, our model has a lower test perplexity by INLINEFORM2 .\n Table TABREF33 compares the results of these models to the best previously-published result on this dataset . Even the fastest of these models beats the best published result (when controlling for the number of training epochs), despite requiring only 6% of the computation.\n"}
{"q_uid": "20ec88c45c1d633adfd7bff7bbf3336d01fb6f37", "table": "\nTable 5: CoNLL 2003 English results.\n| Features                                      | Development | Development | Development |     Test     | Test   |  Test    |\n|-----------------------------------------------|----------------------------|---------------------------|\n|                                               | P      | R      | F1    | P      | R      | F1    |\n| Local (L)                                     | 93.02  | 87.75  | 90.31 | 87.27  | 81.32  | 84.19 |\n| L + Brown reuters (BR)                        | 92.83  | 89.33  | 91.05 | 90.28  | 86.79  | 88.50 |\n| L + Clark wiki 600 (CW600)                    | 93.98  | 90.58  | 92.24 | 90.85  | 87.16  | 88.97 |\n| L + Word2vec giga 200 (W2VG200)             | 93.16  | 89.90  | 91.45 | 89.64  | 85.06  | 87.29 |\n| L + Word2vec wiki 400 (W2VW400)               | 93.22  | 90.02  | 91.59 | 88.98  | 85.09  | 86.99 |\n| L + BR + CW600 + W2VW400 (light)              | 94.16  | 91.96  | 93.04 | 91.20  | 89.36  | 90.27 |\n| light + CR600 + W2VG200 (comp)                | 94.32  | 92.22  | 93.26 | 91.75  | 89.64  | 90.69 |\n| comp + BW (best cluster)                      | 94.21  | 92.23  | 93.26 | 91.67  | 89.98  | 90.82 |\n| comp + dict                                   | 94.60  | 92.78  | 93.68 | 91.86  | 90.53  | 91.19 |\n| BR+CR600-CW600+W2VG200+dict                   | 94.58  | 92.53  | 93.54 | 92.20  | 90.19  | 91.18 |\n| charngram 1-6 + en-91-18                      | 94.56  | 92.81  | 93.68 | 92.16  | 90.56  | 91.36 |\n| Stanford NER (distsim-conll03)                | 93.64  | 92.27  | 92.95 | 89.37  | 87.95  | 88.65 |\n| Illinois NER                                  | -      | -      | 93.50    | n/a  | n/a   | 90.57 |\n| Turian et al. (2010)                          | 94.11  | 93.81  | 93.95 | 90.10  | 90.61  | 90.36 |\n| Passos et al. (2014)                          | -      | -      |  94.46   | -   | -      | 90.90 |\n\n", "text": "\n"}
{"q_uid": "a4fe5d182ddee24e5bbf222d6d6996b3925060c8", "table": "\nTable 1: Datasets used for training, development and evaluation. MUC7: only three classes (LOC, ORG, PER) of the formal run are used for out-of-domain evaluation. As there are not standard partitions of SONAR-1 and Ancora 2.0, the full corpus was used for training and later evaluated in out-of-domain settings.\n| Corpus       | Source                          | Number of Tokens and Named Entities                                                                 |\n|--------------|---------------------------------|-----------------------------------------------------------------------------------------------------|\n|              |                                 | train           |      train    | dev            | dev              | test         |   test           |\n|              |                                 | tok             | ne             | tok             | ne             | tok             | ne             |\n|--------------|---------------------------------|-----------------|-----------------|-----------------|-----------------|-----------------|-----------------|\n| **In-domain datasets**                                          |\n| en           | CoNLL 2003                      | Reuters RCV1    | 203621          | 23499           | 51362           | 5942            | 46435           | 5648            |\n| de           | CoNLL 2003                      | Frankfurter Rundschau 1992 | 206931 | 11851 | 51414 | 4483 | 51943 | 3673 |\n| de           | GermEval 2014                   | Wikipedia/LCC news | 452853 | 31545 | 41653 | 2886 | 96499 | 6893 |\n| es           | CoNLL 2002                      | EFE 2000        | 264715          | 18798           | 52923           | 4352            | 51533           | 3558            |\n| nl           | CoNLL 2002                      | De Morgen 2000  | 199069          | 13434           | 36908          | 2616            | 67473           | 3941            |\n| eu           | Egunkaria                       | Egunkaria 1999-2003 | 44408 | 3817 | - | - | 15531 | 931 |\n| **Out-of-domain datasets**                                      |\n| en           | MUC7                            | newswire        | -               | -               | -               | -               | 53479           | 3514            |\n| en           | Wikigold                        | Wikipedia 2008  | -               | -               | -               | -               | 39007           | 3558            |\n| en           | MEANTIME                        | Wikinews 2013   | -               | -               | -               | -               | 13957           | 1432            |\n| nl           | SONAR-1                         | various genres  | -               | -               | -               | -               | 1000000         | 62505           |\n| nl           | MEANTIME                        | Wikinews 2013   | -               | -               | -               | -               | 13425           | 1545            |\n| es           | Ancora 2.0                      | newswire        | 547198          | 36938           | -               | -               |  -          |    -          |\n| es          | MEANTIME                        | Wikinews 2013   | 15853               | 1706              | -               | -               | -          | -            |\n", "text": "\n"}
{"q_uid": "1b1b0c71f1a4b37c6562d444f75c92eb2c727d9b", "table": "\nTable 8: Evaluation results of dataless document classification of coarse-grained classes measured in micro-averaged F1 along with # of dimensions (concepts) at which corresponding performance is achieved.\n| Method       | Sport x Politics | Sport x Religion |\n|--------------|------------------|------------------|\n| ESA          | 90.63 @425       | 94.39 @450       |\n| CCX (equal)  | 92.04 @2         | 95.11 @6         |\n| CRX (equal)  | 90.99 @2         | 94.81 @5         |\n|              |                  |                  |\n| WE_max       | 91.89 @425       | 93.99 @425       |\n| WE_hung      | 90.89 @275       | 94.16 @450       |\n|              |                  |                  |\n| CCX (best)   | 92.89 @4         | 95.86 @60        |\n| +bootstrap   | 93.20 @10        | 95.13 @225       |\n|              |                  |                  |\n| CRX (best)   | 93.12 @13        | 95.91 @95        |\n| +bootstrap   | 92.96 @13        | 95.53 @70        |\n", "text": "\n"}
{"q_uid": "0c7823b27326b3f5dff51f32f45fc69c91a4e06d", "table": "\nTable 4. Evaluation results on VQA dataset [1].\n| Task Type | Open-Ended |  Open-Ended |  Open-Ended |  Open-Ended |  Open-Ended |\n|-----------|-------------|------------|------------|------------|------------|\n| Test Set  | dev  |  dev  | dev  | dev  | std     |\n| Method    | Num | Y/N | Other | All |\n| LSTM Q+I [1] | 36.8 | 80.5 | 43.0 | 57.8 | 58.2 |\n| BOWIMG [1] | 33.7 | 75.8 | 37.4 | 52.6 | - |\n| iBOWIMG [35] | 35.0 | 76.6 | 42.6 | 55.7 | 55.9 |\n| DPPnet [21] | 37.2 | 80.7 | 41.7 | 57.2 | 57.4 |\n| FDA [18] | 36.2 | 81.1 | 45.8 | 59.2 | 59.5 |\n| SAN [33] | 36.6 | 79.3 | 46.1 | 58.7 | 58.9 |\n| SMem [31] | 37.3 | 80.9 | 43.1 | 58.0 | 58.2 |\n| DMN+[30] | 36.8 | 80.5 | 48.3 | 60.3 | 60.4 |\n| Refined-Neurons [19] | 36.4 | 78.4 | 46.3 | 58.4 | 58.4 |\n| QRU [12] | 37.0 | 82.3 | 47.7 | 60.7 | 60.8 |\n| CoAtt-VGG [14] | 38.4 | 79.6 | 49.1 | 60.5 | - |\n| CoAtt-ResNet [14] | 38.7 | 79.7 | 51.7 | 61.8 | 62.1 |\n| Ours+VGG [2] | 38.2 | 79.7 | 47.0 | 59.5 | - |\n| Ours+VGG(2) | 38.4 | 79.7 | 49.1 | 60.5 | 60.3 |\n", "text": "\n"}
{"q_uid": "1bb7eb5c3d029d95d1abf9f2892c1ec7b6eef306", "table": "\nTable 2. The CER of 6 to 9-layers models trained by regular Xavier Initialization, layer-wise training with CE criterion and CE + sMBR criteria. The teacher of 9-layer model is 8-layers sMBR model, while the others\u2019 teacher is CE model.\n\n| Layer | Xavier Init CE | Layer-wise CE | CE+sMBR |\n|-------|----------------|---------------|---------|\n| 6     | 3.72           | -             |  2.85   |\n| 7     | 3.93           | 3.68          | 2.81    |\n| 8     | 3.81           | 3.60          | 2.77    |\n| 9     | 3.87           | 2.82          | 2.49    |\n\n\nTable 3. The CER and RTF of 9-layers, 2-layers regular-trained and 2-layers distilled LSTM.\n\n| Models                       | CER (%) | RTF  |\n|------------------------------|---------|------|\n| 9-layers LSTM                | 2.49    | 0.74 |\n| 2-layers regular-trained LSTM| 3.06    | 0.36 |\n| 2-layers distilled LSTM      | 2.63    | 0.35 |\n\n\nTable 4. The CER of different 2-layers models, which are Shenma distilled model, Amap model further trained with Amap dataset, and Shenma model trained with sMBR on Amap dataset.\n\n| Training methods             | CER (%) |\n|------------------------------|---------|\n| Shenma model                 | 7.87    |\n| Amap CE + sMBR               | 6.81    |\n| Shenma model + Amap sMBR     | 6.26    |\n\n", "text": "\n"}
{"q_uid": "c0af8b7bf52dc15e0b33704822c4a34077e09cd1", "table": "\nTable 3. The CER and RTF of 9-layers, 2-layers regular-trained and 2-layers distilled LSTM.\n\n| Models                        | CER (%) | RTF  |\n|-------------------------------|---------|------|\n| 9-layers LSTM                 | 2.49    | 0.74 |\n| 2-layers regular-trained LSTM | 3.06    | 0.36 |\n| 2-layers distilled LSTM       | 2.63    | 0.35 |\n\n", "text": "\nUnidirectional LSTM network is applied, rather than bidirectional one, because it is well suited to real-time streaming speech recognition.\n"}
{"q_uid": "e2e31ab279d3092418159dfd24760f0f0566e9d3", "table": "\nTable 1: Cross-validation results\n| Algorithm | mean\u00b1std | \n| --- | --- |\n| Full | 0.701 \u00b10.023 |\n| No embeddings | 0.586 \u00b10.017 |\n| No pre-processing | 0.648 \u00b10.022 |\n\nTable 2: Final results\n| Algorithm | Test scores |\n| --- | --- |\n| Full | 0.745 |\n| No embeddings | 0.660 |\n| No pre-processing | 0.678 |\n", "text": "\nIn Table TABREF17 we show model's performances on the challenge training data, in a 5-fold cross-validation setting.\n\nFurther, the final performances obtained with our approach on the challenge test set are reported in Table TABREF18 . Consistently with the cross-validation performances shown earlier, we observe the beneficial impact of word-representations and basic pre-processing.\n"}
{"q_uid": "eb95af36347ed0e0808e19963fe4d058e2ce3c9f", "table": "\nTable 2: TUPLEINF is significantly better at structured reasoning than TABLEILP\n| Solvers          | 4th Grade | 8th Grade |\n|------------------|-----------|-----------|\n| TABLEILP(C)      | 39.9      | 34.1      |\n| TUPLEINF(C+T)    | 51.7      | 51.6      |\n| TABLEILP(C+T)    | 42.1      | 37.9      |\n| TUPLEINF(C+T)    | 47.5      | 48.0      |\n\n", "text": "\n"}
{"q_uid": "bde6fa2057fa21b38a91eeb2bb6a3ae7fb3a2c62", "table": "\nTable 1: Accuracies on the Stanford Sentiment Treebank 5-class classification task; except for the MVN, all results are drawn from (Lei et al., 2015).\n| Model                              | Accuracy (%) |\n|------------------------------------|--------------|\n| MVN (with convolutional features)  | 51.5         |\n| MVN                                | 49.6         |\n| high-order CNN                     | 51.2         |\n| tree-LSTM                          | 51.0         |\n| DRNN                               | 49.8         |\n| DCNN                               | 48.5         |\n| CNN-MC                             | 47.4         |\n| NBoW                               | 44.5         |\n| SVM                                | 38.3         |\n", "text": "\n"}
{"q_uid": "9ee07edc371e014df686ced4fb0c3a7b9ce3d5dc", "table": "\nTable 3: KBQA results on SimpleQuestions (SQ) and WebQSP (WQ) test sets. The numbers in green color are directly comparable to our results since we start with the same entity linking results.\n| System                              | Accuracy (SQ) | Accuracy (WQ) |\n|-------------------------------------|---------------|---------------|\n| STAGG                               | 72.8          | 63.9          |\n| AMPCNN (Yin et al., 2016)            | 76.4          | -             |\n|                                      |          |              |\n| Baseline: Our Method w/ baseline relation detector | 75.1          | 60.0          |\n|                                      |          |              |\n| Our Method                           |  77.0          | 63.0       |\n| w/o entity re-ranking                | 74.9          | 60.6         |\n| w/o constraints                      | -             | 58.0          |\n|                                      |          |      |\n| Our Method (multi-detectors)         | 78.7          | 63.9          |\n", "text": "\nAs shown on the last row of Table 3 , this gives a significant performance boost, resulting in a new state-of-the-art result on SimpleQuestions and a result comparable to the state-of-the-art on WebQSP\n"}
{"q_uid": "25e4dbc7e211a1ebe02ee8dff675b846fb18fdc5", "table": "\nTable 3: Statistics of external data.\n|              | Source  | #Chars  | #Words  | #Sents |\n|--------------|---------|---------|--------|--------|\n| Raw data     | Gigaword| 116.5m  | -      |  -    |\n| Auto seg     | Gigaword| 398.2m  | 238.6m | 12.04m |\n| Hete.        | People's Daily | 10.14m | 6.17m | 104k |\n| POS          | People's Daily | 10.14m | 6.17m | 104k |\n", "text": "\nWe consider four types of commonly explored external data to this end, all of which have been studied for statistical word segmentation, but not for neural network segmentors.\nRaw Text.\nAutomatically Segmented Text. \nHeterogenous Training Data.\nPOS Data.\n"}
{"q_uid": "a5b67470a1c4779877f0d8b7724879bbb0a3b313", "table": "\nTable 1: Test set F1 comparison on CoNLL 2003 NER task, using only CoNLL 2003 data and unlabeled text.\n| Model                         | F1 \u00b1 std       |\n|------------------------------|---------------|\n| Chiu and Nichols (2016)      | 90.91 \u00b1 0.20  |\n| Lample et al. (2016)         | 90.94         |\n| Ma and Hovy (2016)           | 91.37         |\n| Our baseline without LM      | 90.87 \u00b1 0.13  |\n| TagLM                        | 91.93 \u00b1 0.19  |\n", "text": "\nWe report the official evaluation metric (micro-averaged INLINEFORM0 ). \n"}
{"q_uid": "4640793d82aa7db30ad7b88c0bf0a1030e636558", "table": "\nTable 1: Test set F1 comparison on CoNLL 2003 NER task, using only CoNLL 2003 data and unlabeled text.\n| Model                         | F1 \u00b1 std       |\n|------------------------------|---------------|\n| Chiu and Nichols (2016)      | 90.91 \u00b1 0.20  |\n| Lample et al. (2016)         | 90.94         |\n| Ma and Hovy (2016)           | 91.37         |\n| Our baseline without LM      | 90.87 \u00b1 0.13  |\n| TagLM                        | 91.93 \u00b1 0.19  |\n", "text": "\nTables TABREF15 and TABREF16 compare results from TagLM with previously published state of the art results without additional labeled data or task specific gazetteers. Tables TABREF17 and TABREF18 compare results of TagLM to other systems that include additional labeled data or gazetteers. \n"}
{"q_uid": "cc608df2884e1e82679f663ed9d9d67a4b6c03f3", "table": "\nTable 15: Evaluation of different classifiers in the first version of the training set\n|                    | Precision | Recall | F1   | Accuracy |\n|--------------------|-----------|--------|------|----------|\n| 1NN                | 0.85      | 0.84   | 0.84 | 0.84     |\n| Logistic Regression| 0.66      | 0.68   | 0.64 | 0.68     |\n| Na\u00efve Bayes        | 0.80      | 0.79   | 0.78 | 0.79     |\n| SVM                | 0.97      | 0.96   | 0.96 | 0.96     |\n", "text": "\nThe system testers provide a set of utterances and their corresponding expected responses, and the framework automatically simulates users interacting with the bots and collect metrics, such as time taken to answer an utterance and other resource consumption metrics (e.g., memory, CPU, network bandwidth). \n"}
{"q_uid": "a1c5b95e407127c6bb2f9a19b7d9b1f1bcd4a7a5", "table": "\nTable 3: Accuracy scores for chunking on in-domain and out-of-domain test sets with POS as auxiliary task. Out-of-domain results for each target domain are averages across the 6 remaining source domains. Error reduction vs. single task: 12.8% (in-domain), 8.9% (out-of-domain); vs. hard parameter sharing: 14.8% (in-domain).\n### In-domain results\n| System            | bc    | bn    | mz    | nw    | pt    | tc    | wb    | Avg   |\n|-------------------|-------|-------|-------|-------|-------|-------|-------|-------|\n| Single task       | 90.80 | 92.20 | 91.97 | 92.76 | 97.13 | 89.84 | 92.95 | 92.52 |\n| Hard sharing      | 90.31 | 91.73 | 92.33 | 92.22 | 96.40 | 90.59 | 92.84 | 92.35 |\n| Low supervision   | 90.95 | 91.70 | 92.37 | 93.40 | 96.87 | 90.93 | 93.82 | 92.86 |\n| Cross-stitch nets | 91.40 | 92.49 | 92.59 | 93.52 | 96.99 | 91.47 | 94.00 | 93.21 |\n| Ours              | 91.72 | 92.90 | 92.90 | 94.25 | 97.17 | 90.99 | 94.40 | 93.48 |\n\n### Out-of-domain results\n| System            | bc    | bn    | mz    | nw    | pt    | tc    | wb    | Avg   |\n|-------------------|-------|-------|-------|-------|-------|-------|-------|-------|\n| Single task       | 85.95 | 87.73 | 86.81 | 84.29 | 90.91 | 84.55 | 73.36 | 84.80 |\n| Hard sharing      | 86.31 | 87.73 | 86.96 | 84.99 | 90.76 | 84.48 | 73.56 | 84.97 |\n| Low supervision   | 86.53 | 88.39 | 87.15 | 85.02 | 90.19 | 84.48 | 73.24 | 85.00 |\n| Cross-stitch nets | 87.13 | 88.40 | 87.67 | 85.37 | 91.65 | 85.51 | 73.97 | 85.67 |\n| Ours              | 87.95 | 88.95 | 88.22 | 86.23 | 91.87 | 85.32 | 74.48 | 86.15 |\n", "text": "\n"}
{"q_uid": "37edc25e39515ffc2d92115d2fcd9e6ceb18898b", "table": "\nTable 3\nThe scores on MAE for the systems. The best (lowest) score is shown in bold and is achieved in the multitask setting with the biLSTM architecture of Figure 1.\n|        | nbow       | nbow+      |\n|--------|------------|------------|\n| SVMovr | 0.840      | 0.714      |\n| SVMcs | 0.946      | 0.723      |\n| LRovr   | 0.836      | 0.712      |\n| MaxEnt | 0.842      | 0.715      |\n| (Balikas & Amini, 2016) | -     | 0.719      |\n| biLSTM (single task) | 0.827\u00b10.017 | 0.694\u00b10.04 |\n| biLSTM+Multitask | 0.786\u00b10.025 | 0.685\u00b10.024 |\n\n", "text": "\nExperimental results Table TABREF9 illustrates the performance of the models for the different data representations. The upper part of the Table summarizes the performance of the baselines. The entry \u201cBalikas et al.\u201d stands for the winning system of the 2016 edition of the challenge BIBREF2 , which to the best of our knowledge holds the state-of-the-art.\no evaluate the multitask learning approach, we compared it with several other models. Support Vector Machines (SVMs) are maximum margin classification algorithms that have been shown to achieve competitive performance in several text classification problems BIBREF16 . SVM INLINEFORM0 stands for an SVM with linear kernel and an one-vs-rest approach for the multi-class problem. Also, SVM INLINEFORM1 is an SVM with linear kernel that employs the crammer-singer strategy BIBREF18 for the multi-class problem. Logistic regression (LR) is another type of linear classification method, with probabilistic motivation. Again, we use two types of Logistic Regression depending on the multi-class strategy: LR INLINEFORM2 that uses an one-vs-rest approach and multinomial Logistic Regression also known as the MaxEnt classifier that uses a multinomial criterion.\nFor multitask learning we use the architecture shown in Figure FIGREF2 , which we implemented with Keras BIBREF20 . The embeddings are initialized with the 50-dimensional GloVe embeddings while the output of the biLSTM network is set to dimension 50. \n"}
{"q_uid": "e431661f17347607c3d3d9764928385a8f3d9650", "table": "\nTable 3\nThe scores on MAE for the systems. The best (lowest) score is shown in bold and is achieved in the multitask setting with the biLSTM architecture of Figure 1.\n|        | nbow       | nbow+      |\n|--------|------------|------------|\n| SVMovr | 0.840      | 0.714      |\n| SVMcs | 0.946      | 0.723      |\n| LRovr   | 0.836      | 0.712      |\n| MaxEnt | 0.842      | 0.715      |\n| (Balikas & Amini, 2016) | -     | 0.719      |\n| biLSTM (single task) | 0.827\u00b10.017 | 0.694\u00b10.04 |\n| biLSTM+Multitask | 0.786\u00b10.025 | 0.685\u00b10.024 |\n\n", "text": "\nIn this work, we use an extended version of LSTM called bidirectional LSTM (biLSTM)\nThe models To evaluate the multitask learning approach, we compared it with several other models. Support Vector Machines (SVMs) are maximum margin classification algorithms that have been shown to achieve competitive performance in several text classification problems BIBREF16\nAlso, SVM INLINEFORM1 is an SVM with linear kernel that employs the crammer-singer strategy BIBREF18 for the multi-class problem. Logistic regression (LR) is another type of linear classification method, with probabilistic motivation.\nExperimental results Table TABREF9 illustrates the performance of the models for the different data representations. The upper part of the Table summarizes the performance of the baselines. The entry \u201cBalikas et al.\u201d stands for the winning system of the 2016 edition of the challenge BIBREF2 , which to the best of our knowledge holds the state-of-the-art. \nTo reproduce the setting of the SemEval challenges BIBREF16 , we optimize our systems using as primary measure the macro-averaged Mean Absolute Error ( INLINEFORM0 ) given by: INLINEFORM1\n"}
{"q_uid": "157b9f6f8fb5d370fa23df31de24ae7efb75d6f3", "table": "\nTable 8. Results (accuracy) on the test set for variety, gender and their joint prediction.\n| Task   | System | Arabic | English | Portuguese | Spanish | Average | + 2nd   |\n|--------|--------|--------|---------|------------|---------|---------|---------|\n| Variety| N-GrAM | 0.8313 | 0.8988  | 0.9813     | 0.9621  | 0.9184  | 0.0013  |\n|        | LDR    | 0.8250 | 0.8996  | 0.9875     | 0.9625  | 0.9187  |         |\n| Gender | N-GrAM | 0.8006 | 0.8233  | 0.8450     | 0.8321  | 0.8253  | 0.0029  |\n|        | LDR    | 0.7044 | 0.7220  | 0.7863     | 0.7171  | 0.7325  |         |\n| Joint  | N-GrAM | 0.6831 | 0.7429  | 0.8288     | 0.8036  | 0.7646  | 0.0101  |\n|        | LDR    | 0.5888 | 0.6357  | 0.7763     | 0.6943  | 0.6738  |         |\n", "text": "\nFor the final evaluation we submitted our system, N-GrAM, as described in Section 2. Overall, N-GrAM came first in the shared task, with a score of 0.8253 for gender 0.9184 for variety, a joint score of 0.8361 and an average score of 0.8599 (final rankings were taken from this average score BIBREF0 ). \nWe present finer-grained scores showing the breakdown per language in Table TABREF24 .\nThe final column, + 2nd shows the difference between N-GrAM and that achieved by the second-highest ranked system (excluding the baseline).\n"}
{"q_uid": "ab9b0bde6113ffef8eb1c39919d21e5913a05081", "table": "\nTable 2: Error detection performance when combining manually annotated and artificial training data.\n|                     |  FCE  |  FCE  |  FCE  | CoNLL-14 TEST1 | CoNLL-14 TEST1 | CoNLL-14 TEST1 | CoNLL-14 TEST2 | CoNLL-14 TEST2 | CoNLL-14 TEST2 | \n|---------------------|-------|-------|-------|-------|-------|-------|-------|-------|-------|\n|                     | P     | R     | F0.5  | P     | R     | F0.5  | P     | R     | F0.5  |\n| R&Y (2016)          | 46.10 | 28.50 | 41.10 | 15.40 | 22.80 | 16.40 | 23.60 | 25.10 | 23.90 |\n| Annotation          | 53.91 | 26.88 | 44.84 | 16.12 | 18.42 | 16.52 | 25.72 | 20.92 | 24.57 |\n| Ann+FY14            | 58.77 | 25.55 | 46.54 | 20.48 | 14.41 | 18.88 | 33.25 | 16.67 | 27.72 |\n| Ann+PAT             | 62.47 | 24.70 | 47.81 | 21.07 | 15.02 | 19.47 | 34.04 | 17.32 | 28.49 |\n| Ann+MT              | 58.38 | 28.84 | 48.37 | 19.52 | 20.79 | 19.73 | 30.24 | 22.96 | 28.39 |\n| Ann+PAT+MT          | 60.67 | 28.08 | 49.11 | 23.28 | 18.01 | 21.87 | 35.28 | 19.42 | 30.13 |\n", "text": "\nThe error detection results can be seen in Table TABREF4 . We use INLINEFORM0 as the main evaluation measure, which was established as the preferred measure for error correction and detection by the CoNLL-14 shared task BIBREF3 .\nThe results show that error detection performance is substantially improved by making use of artificially generated data, created by any of the described methods. When comparing the error generation system by Felice2014a (FY14) with our pattern-based (PAT) and machine translation (MT) approaches, we see that the latter methods covering all error types consistently improve performance. While the added error types tend to be less frequent and more complicated to capture, the added coverage is indeed beneficial for error detection. Combining the pattern-based approach with the machine translation system (Ann+PAT+MT) gave the best overall performance on all datasets. The two frameworks learn to generate different types of errors, and taking advantage of both leads to substantial improvements in error detection.\n"}
{"q_uid": "1a8b7d3d126935c09306cacca7ddb4b953ef68ab", "table": "\nTable 3: Test Results on the NALCS (English) and LMS (Traditional Chinese) datasets.\n| Method       | Data         | NALCS | LMS  |\n|--------------|--------------|-------|------|\n| L-Char-LSTM  | chat         | 43.2  | 39.7 |\n| V-CNN-LSTM   | video        | 72.2  | 69.2 |\n| lv-LSTM      | chat+video   | 74.7  | 70.0 |\n", "text": "\n"}
{"q_uid": "907b3af3cfaf68fe188de9467ed1260e52ec6cf1", "table": "\nTable 1: For each one of the selected features, the table shows the difference between the set of tweets containing fake news and those non containing them, and the associated p-value (applying a KolmogorovSmirnov test). The null hypothesis is that both distributions are equal (two sided). Results are ordered by decreasing p-value.\n| Kolmogorov-Smirnov test |\n| feature   | difference | p-value |\n|-----------|-------------|---------|\n| Followers | 0.2357      | 2.6E-6  |\n| Friends   | 0.1747      | 0.0012  |\n| URLs      | 0.1285      | 0.0358  |\n| Favourites| 0.1218      | 0.0535  |\n| Mentions  | 0.1135      | 0.0862  |\n| Media     | 0.0948      | 0.2231  |\n| Retweets  | 0.0609      | 0.7560  |\n| hashtags  | 0.0350      | 0.9983  |\n", "text": "\n Table TABREF23 reports the actual differences (together with their associated p-values) of the distributions of viral tweets containing fake news and viral tweets not containing them for every variable considered.\n"}
{"q_uid": "ef4dba073d24042f24886580ae77add5326f2130", "table": "\nTable 2: Main results on the DL-PS data.\n| Model      | P     | R     | F1    |\n|------------|-------|-------|-------|\n| CRF        | 89.48 | 70.38 | 78.79 |\n| CRF-VT     | 85.16 | 65.07 | 73.97 |\n| CRF-MA     | 72.83 | 90.79 | 80.82 |\n| LSTM-CRF   | 90.50 | 79.97 | 84.91 |\n| LSTM-CRF-VT| 88.68 | 75.51 | 81.57 |\n| LSTM-Crowd | 86.40 | 83.43 | 84.89 |\n| ALCrowd    | 89.56 | 82.70 | 85.99 |\n", "text": "\n"}
{"q_uid": "9776156fc93daa36f4613df591e2b49827d25ad2", "table": "\nTable 1: Effect of Character Embedding\n| MODEL (BiDAF)            | F1     | EM     |\n|--------------------------|--------|--------|\n| Char Embedding Disabled  | 45.94% | 36.91% |\n| Char Embedding Enable    | 47.48% | 38.62% |\n", "text": "\n"}
{"q_uid": "e2f269997f5a01949733c2ec8169f126dabd7571", "table": "\nTable 1: An approximate number of sentence pairs for each task.\n| Task                              | Sentence Pairs |\n|-----------------------------------|----------------|\n| En-Fr (WMT14)                     | 40M            |\n| En-De (WMT15)                     | 5M             |\n| Skipthought (BookCorpus)          | 74M            |\n| AINLI (SNLI + MultiNLI)           | 1M             |\n| Parsing (PTB + 1-billion word)    | 4M             |\n| Total                             | 124M           |\n", "text": "\n"}
{"q_uid": "f17ca24b135f9fe6bb25dc5084b13e1637ec7744", "table": "\nTable 3: Multi-class Classification Results on PDTB. We report accuracy (Acc) and macro-average F1- scores for both explicit and implicit discourse relation predictions. We also report class-wise F1 scores.\n|                             | Implicit | Implicit | Implicit | Implicit | Implicit | Implicit | Explicit | Explicit |\n| Model                       | Macro | Acc   | Comp     | Cont     | Exp      | Temp     | Macro | Acc   |\n|-----------------------------|-------|-------|----------|----------|---------|---------|---------|-------|\n| (Rutherford and Xue, 2015)  | 40.50 | 57.10 | -        | -        | -        | -        | -     | -     |\n| (Liu et al., 2016)          | 44.98 | 57.27 | -        | -        | -        | -        | -     | -     |\n| (Liu and Li, 2016)          | 46.29 | 57.57 | -        | -        | -        | -        | -     | -     |\n| (Lei et al., 2017)          | 46.46 | -     | -        | -        | -        | -        | -     | -     |\n| (Lan et al., 2017)          | 47.80 | 57.39 | -        | -        | -        | -        | -     | -     |\n|                             |       |      |          |          |          |          |        |       |\n|    DU-pair level Discourse Relation Recognition (Our Own Baselines)         |\n| Bi-LSTM                     | 40.01 | 53.50 | 30.52    | 42.06    | 65.52    | 21.96    | -     | -     |\n| + tensors                   | 45.36 | 57.18 | 36.88    | 44.85    | 68.70    | 30.74    | -     | -     |\n|                             |       |      |          |          |          |          |        |       |\n|  Paragraph level Discourse Relation Recognition                  |\n| Basic System Variant (\u03b1 = 0)| 47.56 | 56.88 | 37.12    | 46.47    | 67.72    | 38.92    | -     | -     |\n| Basic System (\u03b1 = 1)        | 48.10 | 57.52 | 37.33    | 47.89    | 68.39    | 38.80    | 91.93 | 92.89 |\n| + Untie Parameters          | 48.69 | 58.20 | 37.68    | 49.19    | 68.86    | 39.04    | 93.70 | 94.46 |\n| + the CRF Layer             | 48.82 | 57.44 | 37.72    | 49.39    | 67.45    | 40.70    | 93.21 | 93.98 |\n", "text": "\nthe basic model yields good performance for recognizing explicit discourse relations as well, which is comparable with previous best result (92.05% macro F1-score and 93.09 percent accuracy as reported in BIBREF11 ).\nAfter untying parameters in the softmax prediction layer, implicit discourse relation classification performance was improved across all four relations, meanwhile, the explicit discourse relation classification performance was also improved.\nThen we also created ensemble models by applying majority voting to combine results of ten runs. From table 5 , each ensemble model obtains performance improvements compared with single model. The full model achieves performance boosting of (51.84 - 48.82 = 3.02) and (94.17 - 93.21 = 0.96) in macro F1-scores for predicting implicit and explicit discourse relations respectively. \nIn this work, we focus on the top-level discourse relation senses which are consist of four major semantic classes: Comparison (Comp), Contingency (Cont), Expansion (Exp) and Temporal (Temp).\nHowever, the performance on the three small classes (Comp, Cont and Temp) remains low.\n"}
{"q_uid": "5a9f94ae296dda06c8aec0fb389ce2f68940ea88", "table": "\nTable 2: Experimental results.\n|         | CER [%] | CER [%] | CER [%] | \n|         | Task 1 | Task 2 | Task 3 |\n|---------|--------|--------|--------|\n| Dot     | 12.7   | 9.8    | 10.7   |\n| Add     | 11.1   | 8.4    | 9.0    |\n| Loc     | 11.7   | 8.8    | 10.2   |\n| MHA-Dot | 11.6   | 8.5    | 9.3    |\n| MHA-Add | 10.7   | 8.2    | 9.1    |\n| MHA-Loc | 11.5   | 8.6    | 9.0    |\n| MHD-Loc | 11.0   | 8.4    | 9.5    |\n| HMHD (Dot+Add+Loc+Cov) | 11.0 | 8.3 | 9.0 |\n| HMHD (2xLoc+2xCov) | 10.4 | 7.7 | 8.9 |\n", "text": "\n"}
{"q_uid": "85912b87b16b45cde79039447a70bd1f6f1f8361", "table": "\nTable 1: Experimental conditions.\n| # training               | 445,068 utterances (581 hours)       |\n| # evaluation (task 1)    | 1,288 utterances (1.9 hours)         |\n| # evaluation (task 2)    | 1,305 utterances (2.0 hours)         |\n| # evaluation (task 3)    | 1,389 utterances (1.3 hours)         |\n|                          |                                      |\n| Sampling rate            | 16,000 Hz                            |\n| Window size              | 25 ms                                |\n| Shift size               | 10 ms                                |\n|                          |                                      |\n| Encoder type             | BLSTMP                               |\n| # encoder layers         | 6                                    |\n| # encoder units          | 320                                  |\n| # projection units       | 320                                  |\n| Decoder type             | LSTM                                 |\n| # decoder layers         | 1                                    |\n| # decoder units          | 320                                  |\n| # heads in MHA           | 4                                    |\n| # filter in location att. | 10                                   |\n| Filter size in location att. | 100                              |\n|                          |                                      |\n| Learning rate            | 1.0                                  |\n| Initialization           | Uniform [-0.1, 0.1]                  |\n| Gradient clipping norm   | 5                                    |\n| Batch size               | 30                                   |\n| Maximum epoch            | 15                                   |\n| Optimization method      | AdaDelta [20]                        |\n| AdaDelta \u03c1               | 0.95                                 |\n| AdaDelta \u03b5               | 10^-8                                |\n| AdaDelta decay rate      | 10^-2                                |\n|                          |                                      |\n| Beam size                | 20                                   |\n| Maximum length           | 0.5                                  |\n| Minimum length           | 0.1                                  |\n\n", "text": "\n"}
{"q_uid": "2ceced87af4c8fdebf2dc959aa700a5c95bd518f", "table": "\nTable 2: Distribution by L1s and source corpora.\n|          | COPLE2 | PEAPL2 | LEIRIA | TOTAL |\n|----------|--------|--------|--------|-------|\n| Arabic   | 13     | 1      | 0      | 14    |\n| Chinese  | 323    | 32     | 0      | 355   |\n| Dutch    | 17     | 26     | 0      | 43    |\n| English  | 142    | 62     | 31     | 235   |\n| French   | 59     | 38     | 7      | 104   |\n| German   | 86     | 88     | 40     | 214   |\n| Italian  | 49     | 83     | 83     | 215   |\n| Japanese | 52     | 15     | 0      | 67    |\n| Korean   | 9      | 9      | 48     | 66    |\n| Polish   | 31     | 28     | 12     | 71    |\n| Romanian | 12     | 16     | 51     | 79    |\n| Russian  | 80     | 11     | 1      | 92    |\n| Spanish  | 147    | 68     | 56     | 271   |\n| Swedish  | 16     | 2      | 1      | 19    |\n| Tetum    | 22     | 1      | 0      | 23    |\n| Total    | 1,058  | 480    | 330    | 1,868 |\n", "text": "\n"}
{"q_uid": "9c44df7503720709eac933a15569e5761b378046", "table": "\nTable 2: We generate vectors for OOV using subword information and search for the nearest (cosine distance) words in the embedding space. The LV-M segmentation for each word is: {(hell, o, o, o)}, {(marvel, i, cious)}, {(louis, ana)}, {(re, re, read)}, {(tu, z, read)}. We omit the LV-N and FT n-grams as they are trivial and too numerous to list.\n| Word        | Model | 5 Nearest Neighbors                                                                 |\n|-------------|-------|--------------------------------------------------------------------------------------|\n| \u201chellooo\u201d   | LV-N  | hellogoodbye, hello, helloworld, helloween, helluva                                   |\n|  \u201chellooo\u201d  | LV-M  | kitsos, finos, neros, nonno, theodoroi                                                |\n| \u201chellooo\u201d   | FT    | hello, helloworld, hellogoodbye, helloween, joegazz                                   |\n| \u201cmarvelicious\u201d | LV-N  | delicious, marveled, marveling, licious, marvellous                                 |\n| \u201cmarvelicious\u201d| LV-M  | marveling, marvelously, marveled, marvelled, loquacious                               |\n|  \u201cmarvelicious\u201d| FT    | delicious, deliciously, marveling, licious, marvelman                                 |\n| \u201clouisana\u201d  | LV-N  | luisana, pisana, belisana, chiisana, rosana                                           |\n| \u201clouisana\u201d  | LV-M  | louisy, louises, louison, louisville, louisiade                                       |\n|  \u201clouisana\u201d   | FT    | luisana, louisa, belisana, anabella, rosana                                           |\n| \u201crereerad\u201d  | LV-N  | reread, rereading, read, writeread, rerecord                                          |\n| \u201crereerad\u201d  | LV-M  | alread, carreer, whiteread, unremarked, oread                                         |\n| \u201crereerad\u201d  | FT    | reread, rereading, read, reiterate, writeread                                         |\n| \u201ctuzread\u201d   | LV-N  | tuzi, tuz, tuzla, prizren, momchilgrad, studenica                                      |\n|  \u201ctuzread\u201d    | LV-M  | tuzluca, paczk, goldsztajn, belzberg, yizkor                                          |\n|  \u201ctuzread\u201d   | FT    | pazaryeri, tufanbeyli, yenipazar, leskovac, berovo                                    |\n\n", "text": "\n"}
{"q_uid": "bd99aba3309da96e96eab3e0f4c4c8c70b51980a", "table": "\nTable 3. Comparisons with the existing models in terms of ROUGE metrics.\n| Methods                              | ROUGE-1 | ROUGE-2 | ROUGE-L |\n|--------------------------------------|---------|---------|---------|\n| RNN-context [Hu et al. 2015]         | 29.9    | 17.4    | 27.2    |\n| SRB [Ma et al 2017]                  | 33.3    | 20.0    | 30.1    |\n| CopyNet [Gu et al. 2016]             | 35.0    | 22.3    | 32.0    |\n| RNN-distract [Chen et al. 2016]      | 35.2    | 22.6    | 32.5    |\n| DRGD [Li et al. 2017]                | 37.0    | 24.1    | 34.2    |\n| Baseline (Ours)                      | 35.3    | 23.4    | 33.0    |\n| Self-Train (Ours)                    | 35.3    | 23.3    | 32.6    |\n| Dual-Train (Ours)                    | 36.2    | 24.3    | 33.8    |\n", "text": "\n"}
{"q_uid": "8bf7f1f93d0a2816234d36395ab40c481be9a0e0", "table": "\nTable 1: Summary of representative neural models for sentence pair modeling. The upper half contains sentence encoding models, and the lower half contains sentence pair interaction models\n| Models | Sentence Encoder | Interaction and Attention | Aggregation and Classification |\n|--------|------------------|--------------------------|-------------------------------|\n| [Shen et al., 2017b] | Directional self-attention network | - | MLP |\n| [Choi et al., 2017] | Gumbel Tree-LSTM | - | MLP |\n| [Wieting and Gimpel, 2017] | Gated recurrent average network | - | MLP |\n| SSE [Nie and Bansal, 2017] | Shortcut-stacked BiLSTM | - | MLP |\n| [He et al., 2015] | CNN | multi-perspective matching | pooling + MLP |\n| [Rockt\u00e4schel et al., 2016] | LSTM | word-by-word neural attention | MLP |\n| [Liu et al., 2016] | LSTM | coupled LSTMs | dynamic pooling + MLP |\n| [Yin et al., 2016] | CNN | attention matrix | logistic regression |\n| DecAtt [Parikh et al., 2016] | - | dot product + soft alignment | summation + MLP |\n| PWIM [He and Lin, 2016] | BiLSTM | cosine, Euclidean, dot product + hard alignment | CNN + MLP |\n| [Wang and Jiang, 2017] | LSTM encodes both context and attention | word-by-word neural attention | MLP |\n| ESIM [Chen et al., 2017] | BiLSTM (Tree-LSTM) before and after attention | dot product + soft alignment | average and max pooling + MLP |\n| [Wang et al., 2017] | BiLSTM | multi-perspective matching | BiLSTM + MLP |\n| [Shen et al., 2017a] | BiLSTM + intra-attention | soft alignment + orthogonal decomposition | MLP |\n| [Ghaeini et al., 2018] | dependent reading BiLSTM | dot product + soft alignment | average and max pooling + MLP |\n", "text": "\n"}
{"q_uid": "e79a5b6b6680bd2f63e9f4adbaae1d7795d81e38", "table": "\nTable 3: Accuracy results (%) for RNN-based approach compared with majority baseline and lexicon-based baseline.\n| Dataset | Majority Baseline | Lexicon-based Baseline | RNN   |\n|---------|-------------------|------------------------|-------|\n| s_r     | 72.71             | 70.98                  | 84.21 |\n| t_r     | 56.97             | 61.59                  | 74.36 |\n| d_r     | 59.63             | 70.52                  | 81.77 |\n| r_r     | 79.60             | 67.81                  | 85.61 |\n", "text": "\nConsidering the improvements over the majority baseline achieved by the RNN model for both non-English (on the average 22.76% relative improvement; 15.82% relative improvement on Spanish, 72.71% vs. 84.21%, 30.53% relative improvement on Turkish, 56.97% vs. 74.36%, 37.13% relative improvement on Dutch, 59.63% vs. 81.77%, and 7.55% relative improvement on Russian, 79.60% vs. 85.62%) and English test sets (27.34% relative improvement), we can draw the conclusion that our model is robust to handle multiple languages.\n"}
{"q_uid": "0c09a0e8f9c5bdb678563be49f912ab6e3f97619", "table": "\nTable 2: Most common syntactic patterns for each semantic role.\n| Role                | Most common syntactic patterns                                                                 |\n|---------------------|------------------------------------------------------------------------------------------------|\n| Supertype           | innermost and leftmost NP containing at least one NN                                           |\n| Differentia quality | leftovers in the innermost and leftmost NP; PP beginning with \u201cof\u201d                             |\n| Differentia event   | SBAR; VP                                                                                        |\n| Event location      | PP inside a SBAR or VP, possibly having a location named entity                                 |\n| Event time          | PP inside a SBAR or VP, possibly having a time interval named entity                            |\n| Origin location     | PP not inside a SBAR or VP, possibly having a location named entity                             |\n| Quality modifier    | NN, JJ or RB referring to an element inside a differentia quality                               |\n| Purpose             | VP beginning with TO; PP beginning with \u201cfor\u201d with a VP right after                             |\n| Associated fact     | SBAR; PP not beginning with \u201cfor\u201d with a VP right after                                         |\n| Accessory determiner| whole expression before supertype; common accessory expression                                  |\n| Accessory quality   | JJ, presence of a differentia quality, common accessory word                                    |\n| [Role] particle     | PRT                                                                                             |\n", "text": "\n"}
{"q_uid": "f513e27db363c28d19a29e01f758437d7477eb24", "table": "\nTable 2: Accuracy on CMRC-2017 dataset. Results marked with \u2020 are from the latest official CMRC-2017 Leaderboard. The best results are in bold face.\n| Model                | CMRC-2017 | CMRC-2017 | \n|                      | Valid     | Test     |\n| Random Guess \u2020       | 1.65      | 1.67     |\n| Top Frequency \u2020      | 14.85     | 14.07    |\n| AS Reader \u2020          | 69.75     | 71.23    |\n| GA Reader            | 72.90     | 74.10    |\n| SJTU BCMI-NLP \u2020      | 76.15     | 77.73    |\n| 6ESTATES PTE LTD \u2020   | 75.85     | 74.73    |\n| Xinktech \u2020           | 77.15     | 77.53    |\n| Ludong University \u2020  | 74.75     | 75.07    |\n| ECNU \u2020               | 77.95     | 77.40    |\n| WHU \u2020                | 78.20     | 76.53    |\n| SAW Reader           | 78.95     | 78.80    |                                                                                           |\n", "text": "\nTable TABREF17 shows our results on CMRC-2017 dataset, which shows that our SAW Reader (mul) outperforms all other single models on the test set, with 7.57% improvements compared with Attention Sum Reader (AS Reader) baseline\n"}
{"q_uid": "701571680724c05ca70c11bc267fb1160ea1460a", "table": "\nTable 2: GAN Model, Keywords = [parking], Varying Gamma Parameter\n| \u03b3       | Sample Output                                                                 |\n|---------|-------------------------------------------------------------------------------|\n| 0.0002  | our lovely unit now perfect on time before first same wonderful parking across orchard booking |\n| 0.00045 | travelers apartment laundry street to block for the entire parking on laundry on accommodation street |\n| 0.0007  | and parking parking parking parking parking parking parking parking convenient parking parking parking |                                                                                    |\n", "text": "\nThat said, these results, though they do show a marginal increase in dev accuracy and a decrease in CE loss, suggest that perhaps listing description is not too predictive of occupancy rate given our parameterizations. \nHowever, should a strong relationship actually exist and there be instead a problem with our method, there are a few possibilities of what went wrong.\n"}
{"q_uid": "600b097475b30480407ce1de81c28c54a0b3b2f8", "table": "\nTable 1: Results of RNN/LSTM\n| Model                                             | Test Accuracy |\n|---------------------------------------------------|---------------|\n| GloVe Vectors trained on Airbnb Data (Ensembling) | 0.403         |\n| GloVe Vectors trained on Airbnb Data (No Ensembling) | 0.397         |\n| GloVe Vectors trained on Wikipedia Corpus (Ensembling) | 0.394         |\n", "text": "\n"}
{"q_uid": "01edeca7b902ae3fd66264366bf548acea1db364", "table": "\nTable 1: Comparison of different models.\n|          | TF-IDF | RNN  | CNN  | LSTM | BiLSTM | Multi-View | SMN  | Our model |\n|----------|--------|------|------|------|--------|------------|------|-----------|\n| R10@1    | 0.159  | 0.325| 0.328| 0.365| 0.355  | 0.421      | 0.453| 0.476     |\n| R10@2    | 0.256  | 0.463| 0.515| 0.536| 0.525  | 0.601      | 0.654| 0.672     |\n| R10@5    | 0.477  | 0.775| 0.792| 0.828| 0.825  | 0.861      | 0.886| 0.893     |\n", "text": "\n"}
{"q_uid": "6aaf12505add25dd133c7b0dafe8f4fe966d1f1d", "table": "\nTable 1: Comparison of single model word-level perplexity of our model with state-of-the-art on vali- dation and test sets of Penn Treebank and Wikitext-2 dataset. For evaluation, we select the model with minimum validation loss. Lower perplexity value represents better performance.\n### WT-2\n| Model | Params | Val | Test |\n|-------|--------|-----|------|\n| Variational LSTM (Gal and Ghahramani, 2016) | - | - | - |\n| CharCNN (Kim et al., 2016) | - | - | - |\n| Pointer Sentinel-LSTM (Merity et al., 2017) | - | - | - |\n| RHN (Zilly et al., 2016) | - | - | - |\n| NAS Cell (Zoph and Le, 2017) | - | - | - |\n| Variational LSTM - (Inan et al., 2017) | 28 M | 91.5 | 87 |\n| SRU - 6 layers (Lei et al., 2018) | - | - | - |\n| QRNN (Bradbury et al., 2017) | - | - | - |\n| RAN (Lee et al., 2017) | - | - | - |\n| 4-layer skip-connection LSTM (Melis et al., 2018) | - | - | - |\n| AWD-LSTM (Merity et al., 2018) | 33 M | 69.1 | 66 |\n| AWD-LSTM - (Merity et al., 2018)-finetuned | 33 M | 68.6 | 65.8 |\n\n### PTB \n| Model | Params | Val | Test |\n|-------|--------|-----|------|\n| Variational LSTM (Gal and Ghahramani, 2016) | 20 M | - | 78.6 |\n| CharCNN (Kim et al., 2016) | - | - | 78.9 |\n| Pointer Sentinel-LSTM (Merity et al., 2017) | 19 M | 72.4 | 70.9 |\n| RHN (Zilly et al., 2016) | 23 M | 67.9 | 65.4 |\n| NAS Cell (Zoph and Le, 2017) | 25 M | - | 64.0 |\n| Variational LSTM - (Inan et al., 2017) | 24 M | 75.7 | 73.2 |\n| SRU - 6 layers (Lei et al., 2018) | 24 M | 63.4 | 60.3 |\n| QRNN (Bradbury et al., 2017) | 18 M | 82.1 | 78.3 |\n| RAN (Lee et al., 2017) | 22 M | - | 78.5 |\n| 4-layer skip-connection LSTM (Melis et al., 2018) | 24 M | 60.9 | 58.3 |\n| AWD-LSTM (Merity et al., 2018) | 24 M | 60.7 | 58.8 |\n| AWD-LSTM - (Merity et al., 2018)-finetuned | 24 M | 60 | 57.3 |\n\n### WT-2 With standard dropout \n| Model | Params | Val | Test |\n|-------|--------|-----|------|\n| LSTM (M = 1000) | 29 M | 78.93 | 75.08 |\n| LSTM (M = 1200) | 35 M | 77.93 | 74.48 |\n| LSTM (M = 1400) | 42 M | 77.55 | 74.44 |\n| Ours -PRU (g = 1, K = 2, M = 1000) | 28 M | 79.15 | 76.59 |\n| Ours -PRU (g = 2, K = 2, M = 1200) | 28 M | 76.62 | 73.79 |\n| Ours -PRU (g = 4, K = 2, M = 1400) | 28 M | 75.46 | 72.77 |\n\n### PTB With standard dropout\n| Model | Params | Val | Test |\n|-------|--------|-----|------|\n| LSTM (M = 1000) | 20 M | 68.57 | 66.29 |\n| LSTM (M = 1200) | 26 M | 69.17 | 67.16 |\n| LSTM (M = 1400) | 33 M | 70.88 | 68.55 |\n| Ours -PRU (g = 1, K = 2, M = 1000) | 19 M | 69.8 | 67.78 |\n| Ours -PRU (g = 2, K = 2, M = 1200) | 19 M | 67.17 | 64.92 |\n| Ours -PRU (g = 4, K = 2, M = 1400) | 19 M | 64.76 | 62.42 |\n\n### WT-2 With advanced dropouts\n\n| Model | Params | Val | Test |\n|-------|--------|-----|------|\n| Ours - AWD-PRU (g = 1, K = 2, M = 1000) | 28 M | 71.84 | 68.6 |\n| Ours - AWD-PRU (g = 2, K = 2, M = 1200) | 28 M | 68.57 | 65.7 |\n| Ours - AWD-PRU (g = 4, K = 2, M = 1400) | 28 M | 68.17 | 65.3 |\n| Ours - AWD-PRU (g = 4, K = 2, M = 1400)-finetuned | 28 M | 67.19 | 64.53 |\n\n### PTB  With advanced dropouts\n| Model | Params | Val | Test |\n|-------|--------|-----|------|\n| Ours - AWD-PRU (g = 1, K = 2, M = 1000) | 19 M | 61.72 | 59.54 |\n| Ours - AWD-PRU (g = 2, K = 2, M = 1200) | 19 M | 60.81 | 58.65 |\n| Ours - AWD-PRU (g = 4, K = 2, M = 1400) | 19 M | 60.62 | 58.33 |\n| Ours - AWD-PRU (g = 4, K = 2, M = 1400)-finetuned | 19 M | 58.46 | 56.56 |\n", "text": "\nTable TABREF23 compares the performance of the PRU with state-of-the-art methods. \n"}
{"q_uid": "63403ffc0232ff041f3da8fa6c30827cfd6404b7", "table": "\nTable 2: Accuracy of different models on WIKIHop closed test set and public validation set. Our Entity-GCN outperforms recent prior work without learning any language model to process the input but relying on a pre-trained one (ELMo - without fine-tuning it) and applying R-GCN to reason among entities in the text. * with coreference for unmasked dataset and without coreference for the masked one.\n| Model | Unmasked | Unmasked | Masked | Masked | \n|-------|----------|--------|\n|       | Test     | Dev    | Test | Dev |\n| Human (Welbl et al., 2018) | 74.1 | - | - | - |\n| FastQA (Welbl et al., 2018) | 25.7 | - | 35.8 | - |\n| BiDAF (Welbl et al., 2018) | 42.9 | - | 54.5 | - |\n| Coref-GRU (Dhingra et al., 2018) | 59.3 | 56.0 | - | - |\n| MHPGM (Bauer et al., 2018) | - | 58.2 | - | - |\n| Weaver / Jenga (Raison et al., 2018) | 65.3 | 64.1 | - | - |\n| MHQA-GRN (Song et al., 2018) | 65.4 | 62.8 | - | - |\n| Entity-GCN without coreference (single model) | 67.6 | 64.8 | - | 70.5 |\n| Entity-GCN with coreference (single model) | 66.4 | 65.3 | - | - |\n| Entity-GCN* (ensemble 5 models) | 71.2 | 68.5 | - | 71.6 |\n", "text": "\n"}
{"q_uid": "a25c1883f0a99d2b6471fed48c5121baccbbae82", "table": "\nTable 2: Accuracy of different models on WIKIHop closed test set and public validation set. Our Entity-GCN outperforms recent prior work without learning any language model to process the input but relying on a pre-trained one (ELMo - without fine-tuning it) and applying R-GCN to reason among entities in the text. * with coreference for unmasked dataset and without coreference for the masked one.\n| Model | Unmasked | Unmasked | Masked | Masked | \n|-------|----------|--------|\n|       | Test     | Dev    | Test | Dev |\n| Human (Welbl et al., 2018) | 74.1 | - | - | - |\n| FastQA (Welbl et al., 2018) | 25.7 | - | 35.8 | - |\n| BiDAF (Welbl et al., 2018) | 42.9 | - | 54.5 | - |\n| Coref-GRU (Dhingra et al., 2018) | 59.3 | 56.0 | - | - |\n| MHPGM (Bauer et al., 2018) | - | 58.2 | - | - |\n| Weaver / Jenga (Raison et al., 2018) | 65.3 | 64.1 | - | - |\n| MHQA-GRN (Song et al., 2018) | 65.4 | 62.8 | - | - |\n| Entity-GCN without coreference (single model) | 67.6 | 64.8 | - | 70.5 |\n| Entity-GCN with coreference (single model) | 66.4 | 65.3 | - | - |\n| Entity-GCN* (ensemble 5 models) | 71.2 | 68.5 | - | 71.6 |\n", "text": "\n"}
{"q_uid": "6aa2a1e2e3666f2b2a1f282d4cbdd1ca325eb9de", "table": "\nTable 1: Summary of datasets.\n(a) Small-scale datasets\n| Domain     |       | #Pos | #Neg | #Neu | Total |\n|------------|------|------|------|-------|-------|\n| Book  | Set 1     | 2000 | 2000 | 2000 | 6000  |\n| Book  | Set 2     | 4824 | 513 | 663  | 6000  |\n| Electronics | Set 1    | 2000 | 2000 | 2000 | 6000  |\n| Electronics | Set 2    | 4817 | 694 | 489  | 6000  |\n| Beauty      | Set 1    | 2000 | 2000 | 2000 | 6000  |\n| Beauty      | Set 2    | 4709 | 616 | 675  | 6000  |\n| Music  | Set 1    | 2000 | 2000 | 2000 | 6000  |\n| Music  | Set 2      | 4441 | 785 | 774  | 6000  |\n\n(b) Large-scale datasets\n| Domain     | #Pos    | #Neg    | #Neu   | Total   |\n|------------|---------|---------|--------|---------|\n| IMDB       | 55,242  | 11,735  | 17,942 | 84,919  |\n| Yelp       | 155,625 | 29,597  | 45,941 | 231,163 |\n| Cell Phone | 148,657 | 24,343  | 21,439 | 194,939 |\n| Baby       | 126,525 | 17,012  | 17,255 | 160,792 |\n", "text": "\nLarge-scale datasets: We further conduct experiments on four much larger datasets: IMDB (I), Yelp2014 (Y), Cell Phone (C), and Baby (B). IMDB and Yelp2014 were previously used in BIBREF25 , BIBREF26 . Cell phone and Baby are from the large-scale Amazon dataset BIBREF24 , BIBREF27 . Detailed statistics are summarized in Table TABREF9 .\nSmall-scale datasets: Our new dataset was derived from the large-scale Amazon datasets released by McAuley et al. ( BIBREF24 ). It contains four domains: Book (BK), Electronics (E), Beauty (BT), and Music (M). Each domain contains two datasets. Set 1 contains 6000 instances with exactly balanced class labels, and set 2 contains 6000 instances that are randomly sampled from the large dataset, preserving the original label distribution, which we believe better reflects the label distribution in real life. The examples in these two sets do not overlap. Detailed statistics of the generated datasets are given in Table TABREF9 .\n"}
{"q_uid": "9176d2ba1c638cdec334971c4c7f1bb959495a8e", "table": "\nTable 1: Summary of datasets.\n(a) Small-scale datasets\n| Domain     |       | #Pos | #Neg | #Neu | Total |\n|------------|------|------|------|-------|-------|\n| Book  | Set 1     | 2000 | 2000 | 2000 | 6000  |\n| Book  | Set 2     | 4824 | 513 | 663  | 6000  |\n| Electronics | Set 1    | 2000 | 2000 | 2000 | 6000  |\n| Electronics | Set 2    | 4817 | 694 | 489  | 6000  |\n| Beauty      | Set 1    | 2000 | 2000 | 2000 | 6000  |\n| Beauty      | Set 2    | 4709 | 616 | 675  | 6000  |\n| Music  | Set 1    | 2000 | 2000 | 2000 | 6000  |\n| Music  | Set 2      | 4441 | 785 | 774  | 6000  |\n\n(b) Large-scale datasets\n| Domain     | #Pos    | #Neg    | #Neu   | Total   |\n|------------|---------|---------|--------|---------|\n| IMDB       | 55,242  | 11,735  | 17,942 | 84,919  |\n| Yelp       | 155,625 | 29,597  | 45,941 | 231,163 |\n| Cell Phone | 148,657 | 24,343  | 21,439 | 194,939 |\n| Baby       | 126,525 | 17,012  | 17,255 | 160,792 |\n", "text": "\nThe dataset contains 4 different domains: Book (B), DVDs (D), Electronics (E), and Kitchen (K). \n"}
{"q_uid": "a99fdd34422f4231442c220c97eafc26c76508dd", "table": "\nTable 2: Clustering results on the labeled dataset. We compare our algorithm (with and without timestamps) with the online micro-clustering routine of Aggarwal and Yu (2006) (denoted by CluStream). The F1 values are for the precision (P) and recall (R) in the following columns. See Table 3 for a legend of the different models. Best result for each language is in bold.\n|        | algorithm                        | F1   | P    | R    |\n|--------|----------------------------------|------|------|------|\n| English | CluStream                        | 79.0 | 98.6 | 65.9 |\n| English | TOKENS+LEMMAS+ENTS               | 92.7 | 92.9 | 92.5 |\n| English | +TS                              | 94.1 | 98.2 | 90.3 |\n|        |                                   |      |      |      |\n| German | CluStream                        | 89.7 | 99.9 | 81.3 |\n| German | TOKENS+LEMMAS+ENTS               | 90.7 | 99.7 | 83.2 |\n| German | +TS                              | 97.1 | 99.9 | 94.5 |\n|        |                                   |      |      |      |\n| Spanish | CluStream                        | 78.1 | 73.4 | 83.5 |\n| Spanish | TOKENS+LEMMAS+ENTS               | 88.8 | 95.9 | 82.7 |\n| Spanish | +TS                              | 94.2 | 97.0 | 91.6 |\n", "text": "\n"}
{"q_uid": "d604f5fb114169f75f9a38fab18c1e866c5ac28b", "table": "\nTable 2: Clustering results on the labeled dataset. We compare our algorithm (with and without timestamps) with the online micro-clustering routine of Aggarwal and Yu (2006) (denoted by CluStream). The F1 values are for the precision (P) and recall (R) in the following columns. See Table 3 for a legend of the different models. Best result for each language is in bold.\n|        | algorithm                        | F1   | P    | R    |\n|--------|----------------------------------|------|------|------|\n| English | CluStream                        | 79.0 | 98.6 | 65.9 |\n| English | TOKENS+LEMMAS+ENTS               | 92.7 | 92.9 | 92.5 |\n| English | +TS                              | 94.1 | 98.2 | 90.3 |\n|        |                                   |      |      |      |\n| German | CluStream                        | 89.7 | 99.9 | 81.3 |\n| German | TOKENS+LEMMAS+ENTS               | 90.7 | 99.7 | 83.2 |\n| German | +TS                              | 97.1 | 99.9 | 94.5 |\n|        |                                   |      |      |      |\n| Spanish | CluStream                        | 78.1 | 73.4 | 83.5 |\n| Spanish | TOKENS+LEMMAS+ENTS               | 88.8 | 95.9 | 82.7 |\n| Spanish | +TS                              | 94.2 | 97.0 | 91.6 |\n\nTable 3: Accuracy of the SVM ranker on the English training set. TOKENS are the word token features, LEMMAS are the lemma features for title and body, ENTS are named entity features and TS are timestamp features. All features are described in detail in \u00a74, and are listed for both the title and the body.\n| feature                    | accuracy |\n|----------------------------|----------|\n| TOKENS                     | 85.5     |\n| TOKENS+LEMMAS              | 85.9     |\n| TOKENS+LEMMAS+ENTS         | 86.5     |\n| TOKENS+LEMMAS+ENTS+TS      | 96.9     |\n", "text": "\nTo investigate the importance of each feature, we now consider in Table TABREF37 the accuracy of the SVM ranker for English as described in \u00a7 SECREF19 . \nTable TABREF35 gives the final monolingual results on the three datasets.\n"}
{"q_uid": "4cbe5a36b492b99f9f9fea8081fe4ba10a7a0e94", "table": "\nTable 5: Causal explanation identification performance. Bold indicates significant improvement over next best model (p < .05)\n| Model                        | Prec  | Rec   | F1    |\n|------------------------------|-------|-------|-------|\n| Linear SVM                   | 0.773 | 0.727 | 0.743 |\n| RBF SVM                      | 0.739 | 0.771 | 0.749 |\n| Random Forest                | 0.747 | 0.790 | 0.746 |\n| LSTM                | 0.851 | 0.858 | 0.853 |\n", "text": "\nWe first use state-of-the-art PDTB taggers for our baseline BIBREF13 , BIBREF12 for the evaluation of the causality prediction of our models ( BIBREF12 requires sentences extracted from the text as its input, so we used our parser to extract sentences from the message).\n"}
{"q_uid": "32a3c248b928d4066ce00bbb0053534ee62596e7", "table": "\nTable 1: Example input sentence. Context MSD tags and lemmas, marked in gray, are only available in Track 1. The cyan square marks the main objective of predicting the word form made. The magenta square marks the auxiliary objective of predicting the MSD tag V;PST;V.PTCP;PASS.\n| WORD FORMS | We  | were | \u25a1 | to | feel | very | welcome | . |   |\n| LEMMAS     | we  | be   | \u25a1 | make | to   | feel | welcome | . |    |\n| MSD TAGS   | PRO;NOM;PL;1 | AUX;IND;PST;FIN | \u25a1 | PART | V;NFIN | ADV | ADJ | PUNCT |\n", "text": "\n"}
{"q_uid": "c35806cf68220b2b9bb082b62f493393b9bdff86", "table": "\nTable 3: Results of the models on the Quora Question Pairs dataset.\n| Model                                      | Acc. (%) |\n|--------------------------------------------|----------|\n| CNN (Wang et al., 2017)                    | 79.6     |\n| LSTM (Wang et al., 2017)                   | 82.6     |\n| Multi-Perspective LSTM (Wang et al., 2017) | 83.2     |\n| LSTM + ElBiS (Choi et al., 2018a)          | 87.3     |\n| REGMAPR (BASE+REG) (Brahma, 2018)          | 88.0     |\n| CAS-LSTM (ours)                            | 88.4     |\n| Bi-CAS-LSTM (ours)                         | 88.6     |\n", "text": "\nIn SNLI, our best model achieves the new state-of-the-art accuracy of 87.0% with relatively fewer parameters.\nThe results on the Quora Question Pairs dataset are summarized in Table TABREF34 . Again we can see that our models outperform other models by large margin, achieving the new state of the art.\n"}
{"q_uid": "0ad4359e3e7e5e5f261c2668fe84c12bc762b3b8", "table": "\nTable 1: The comparison of various models on different sentence classification tasks. We report the test accuracy of each model in percentage. Our SATA Tree-LSTM shows superior or competitive performance on all tasks, compared to previous tree-structured models as well as other sophisticated models. *: Latent tree-structured models. \u2020: Models which are pre-trained with large external corpora.\n| Models | SST-2 | SST-5 | MR | SUBJ | TREC |\n|--------|-------|-------|----|------|------|\n| Tree-structured models | | | | | |\n| RNTN (Socher et al. 2013) | 85.4 | 45.7 | - | - | - |\n| AdaMC-RNTN (Dong et al. 2014) | 88.5 | 46.7 | - | - | - |\n| TE-RNTN (Qian et al. 2015) | 87.7 | 49.8 | - | - | - |\n| TBCNN (Mou et al. 2015) | 87.9 | 51.4 | - | - | 96.0 |\n| Tree-LSTM (Tai, Socher, and Manning 2015) | 88.0 | 51.0 | - | - | - |\n| AdaHT-LSTM (Liu, Qiu, and Huang 2017a) | 87.8 | 50.2 | 81.9 | 94.1 | - |\n| DC-TreeLSTM (Liu, Qiu, and Huang 2017b) | 87.8 | -   | 81.7 | 93.7 | 93.8 |\n| TE-LSTM (Huang, Qian, and Zhu 2017) | 89.6 | 52.6 | 82.2 | - | - |\n| BiCoTree (Jiang and Zhang 2017) | 90.3 | 53.5 | - | - | 94.8 |\n| Gumbel TreeLSTM (Choi, Yoo, and Lee 2018) | 90.7 | 53.7 | - | - | - |\n| TreeNet (Cheng et al. 2018) | - | - | 83.6 | 95.9 | 96.1 |\n| SATA Tree-LSTM (Ours) | 91.3 | 54.4 | 83.8 | 95.4 | 96.2 |\n| Other neural models | | | | | |\n| CNN (Kim 2014) | 88.1 | 48.0 | 81.5 | 93.4 | 93.6 |\n| AdaSent (Zhao, Lu, and Poupart 2015) | - | - | 83.1 | 95.5 | 92.4 |\n| LSTM-CNN (Zhou et al. 2016) | 89.5 | 52.4 | 82.3 | 94.0 | 96.1 |\n| byte-mLSTM (Radford, Jozefowicz, and Sutskever 2017) | 91.8 | 52.9 | 86.9 | 94.6 | - |\n| BCN + Char + CoVe (McCann et al. 2017) | 90.3 | 53.7 | - | - | 95.8 |\n| BCN + Char + ELMo (Peters et al. 2018) | - | 54.7\u00b10.5 | - | - | - |\n", "text": "\nOur experimental results on the SNLI dataset are shown in table 2 . In this table, we report the test accuracy and number of trainable parameters for each model. Our SATA-LSTM again demonstrates its decent performance compared against the neural models built on both syntactic trees and latent trees, as well as the non-tree models. (Latent Syntax Tree-LSTM: BIBREF10 ( BIBREF10 ), Tree-based CNN: BIBREF35 ( BIBREF35 ), Gumbel Tree-LSTM: BIBREF11 ( BIBREF11 ), NSE: BIBREF36 ( BIBREF36 ), Reinforced Self-Attention Network: BIBREF4 ( BIBREF4 ), Residual stacked encoders: BIBREF37 ( BIBREF37 ), BiLSTM with generalized pooling: BIBREF38 ( BIBREF38 ).)\nOur experimental results on the SNLI dataset are shown in table 2 . In this table, we report the test accuracy and number of trainable parameters for each model. Our SATA-LSTM again demonstrates its decent performance compared against the neural models built on both syntactic trees and latent trees, as well as the non-tree models. (Latent Syntax Tree-LSTM: BIBREF10 ( BIBREF10 ), Tree-based CNN: BIBREF35 ( BIBREF35 ), Gumbel Tree-LSTM: BIBREF11 ( BIBREF11 ), NSE: BIBREF36 ( BIBREF36 ), Reinforced Self-Attention Network: BIBREF4 ( BIBREF4 ), Residual stacked encoders: BIBREF37 ( BIBREF37 ), BiLSTM with generalized pooling: BIBREF38 ( BIBREF38 ).)\nOur SATA-LSTM again demonstrates its decent performance compared against the neural models built on both syntactic trees and latent trees, as well as the non-tree models. (Latent Syntax Tree-LSTM: BIBREF10 ( BIBREF10 ), Tree-based CNN: BIBREF35 ( BIBREF35 ), Gumbel Tree-LSTM: BIBREF11 ( BIBREF11 ), NSE: BIBREF36 ( BIBREF36 ), Reinforced Self-Attention Network: BIBREF4 ( BIBREF4 ), Residual stacked encoders: BIBREF37 ( BIBREF37 ), BiLSTM with generalized pooling: BIBREF38 ( BIBREF38 ).)\n"}
{"q_uid": "e3c9e4bc7bb93461856e1f4354f33010bc7d28d5", "table": "\nTable 1: Experimental results(%). P/R/F1 are reported for positive samples and calculated as the mean score over 10-time experiments. Acc is defined as the proportion of test samples classified correctly, equal to micro-precision. MaxFreq refers to always predicting the most frequent label, i.e. support in our dataset. * indicates methods proposed in previous works.\n| Models          | P    | R    | F1   | Acc. |\n|-----------------|------|------|------|------|\n| MaxFreq         | 52.2 | 100  | 68.6 | 52.2 |\n| SVM*            | 57.8 | 53.5 | 55.6 | 55.5 |\n| CNN             | 76.1 | 81.9 | 79.0 | 77.6 |\n| CNN+law         | 74.4 | 79.4 | 77.0 | 76.0 |\n| GRU             | 79.2 | 72.9 | 76.1 | 76.6 |\n| GRU+law         | 78.2 | 68.2 | 72.8 | 74.4 |\n| GRU+Attention*  | 79.1 | 80.7 | 80.0 | 79.1 |\n| AoA             | 79.3 | 78.9 | 79.2 | 78.3 |\n| AoA+law         | 79.0 | 79.2 | 79.1 | 78.3 |\n| r-net           | 79.5 | 78.7 | 79.2 | 78.4 |\n| r-net+law       | 79.3 | 78.8 | 79.0 | 78.3 |\n| AutoJudge       | 80.4 | 86.6 | 83.4 | 82.2 |\n", "text": "\nFor comparison, we adopt and re-implement three kinds of baselines as follows:\n\nWe implement an SVM with lexical features in accordance with previous works BIBREF16 , BIBREF17 , BIBREF1 , BIBREF15 , BIBREF4 and select the best feature set on the development set.\n\nWe implement and fine-tune a series of neural text classifiers, including attention-based method BIBREF3 and other methods we deem important. CNN BIBREF18 and GRU BIBREF27 , BIBREF21 take as input the concatenation of fact description and plea. Similarly, CNN/GRU+law refers to using the concatenation of fact description, plea and law articles as inputs.\n\nWe implement and train some off-the-shelf RC models, including r-net BIBREF5 and AoA BIBREF6 , which are the leading models on SQuAD leaderboard. In our initial experiments, these models take fact description as passage and plea as query. Further, Law articles are added to the fact description as a part of the reading materials, which is a simple way to consider them as well.\n"}
{"q_uid": "dafa760e1466e9eaa73ad8cb39b229abd5babbda", "table": "\nTable 3: Number of run-on (RO) and non-run-on (Non-RO) sentences in our datasets.\n|               | Dataset         | RO         | Non-RO      | Total   |\n|---------------|-----------------|------------|-------------|---------|\n| Train         | FakeGiga-Train  | 2.76M (61%)| 1.75M (39%) | 4.51M   |\n| Test          | FakeGiga        | 28,232 (11%)| 218,076 (89%)| 246,308 |\n| Test          | RealESL         | 542 (1%)   | 58,987 (99%)| 59,529  |\n| Test          | FakeESL         | 5,000 (9%) | 56,350 (91%)| 61,950  |\n| Test          | FakeESL-1%      | 560 (1%)   | 56,350 (99%)| 56,910  |\n", "text": "\n"}
{"q_uid": "d3dbb5c22ef204d85707d2d24284cc77fa816b6c", "table": "\nTable 2: Comparison with published results in literature.\n\n** SQuAD 2.0 development dataset **\n|                               | EM   | F1    |\n|-------------------------------|------|-------|\n| BNA\u00b9                          | 59.8 | 62.6  |\n| DocQA\u00b9                        | 61.9 | 64.8  |\n| R.M-Reader\u00b2                   | 66.9 | 69.1  |\n| R.M-Reader + Verifier\u00b2        | 68.5 | 71.5  |\n| Joint SAN                     | 69.3 | 72.2  |\n\n** SQuAD 2.0 development dataset + ELMo **\n|                                      | EM   | F1    |\n|--------------------------------------|------|-------|\n| DocQA\u00b9                               | 65.1 | 67.6  |\n| R.M-Reader + Verifier\u00b2               | 72.3 | 74.8  |\n\n** SQuAD 2.0 test dataset  **\n|                        | EM   | F1    |\n|------------------------|------|-------|\n| BNA\u00b9                   | 59.2 | 62.1  |\n| DocQA\u00b9                 | 59.3 | 62.3  |\n| DocQA + ELMo\u00b9          | 63.4 | 66.3  |\n| R.M-Reader\u00b2*           | 71.7 | 74.2  |\n| Joint SAN#             | 68.7 | 71.4  |\n", "text": "\nTable TABREF21 reports comparison results in literature published .\nThe results in terms of EM and F1 is summarized in Table TABREF20 . We observe that Joint SAN outperforms the SAN baseline with a large margin, e.g., 67.89 vs 69.27 (+1.38) and 70.68 vs 72.20 (+1.52) in terms of EM and F1 scores respectively, so it demonstrates the effectiveness of the joint optimization.\nTable TABREF21 reports comparison results in literature published .\nThe results in terms of EM and F1 is summarized in Table TABREF20 . We observe that Joint SAN outperforms the SAN baseline with a large margin, e.g., 67.89 vs 69.27 (+1.38) and 70.68 vs 72.20 (+1.52) in terms of EM and F1 scores respectively, so it demonstrates the effectiveness of the joint optimization.\nTable TABREF21 reports comparison results in literature published .\n"}
{"q_uid": "46570c8faaeefecc8232cfc2faab0005faaba35f", "table": "\nTable 1: Benchmark datasets: Tweets, Reddit posts and online debates for sarcasm and irony detection.\n| Reference               | Dataset      | Train  | Valid | Test  | Total  | Source    |\n|-------------------------|--------------|--------|-------|-------|--------|-----------|\n| Van Hee et al., 2018    | SemEval-2018 | 3,067  | 306   | 784   | 3,834  | Twitter   |\n| Pt\u00e1\u010dek et al., 2014     | Pt\u00e1\u010dek       | 48,007 | 6,858 | 13,717| 68,582 | Twitter   |\n| Riloff et al., 2013     | Riloff       | 1,327  | 189   | 381   | 1,897  | Twitter   |\n| Khodak et al., 2017     | SARC 2.0     | 205,665| 51,417| 64,666| 321,748| Reddit    |\n| Khodak et al., 2017     | SARC 2.0 pol | 10,934 | 2,734 | 3,406 | 17,074 | Reddit    |\n| Oraby et al., 2016      | SC-V1        | 1,396  | 199   | 400   | 1,995  | Dialogues |\n| Oraby et al., 2016      | SC-V2        | 3,284  | 469   | 939   | 4,692  | Dialogues |\n", "text": "\nTwitter: We use the Twitter dataset provided for the SemEval 2018 Task 3, Irony Detection in English Tweets BIBREF18 . The dataset was manually annotated using binary labels. We also use the dataset by BIBREF4 , which is manually annotated for sarcasm. Finally, we use the dataset by BIBREF20 , who collected a user self-annotated corpus of tweets with the #sarcasm hashtag.\nReddit: BIBREF21 collected SARC, a corpus comprising of 600.000 sarcastic comments on Reddit. We use main subset, SARC 2.0, and the political subset, SARC 2.0 pol.\nOnline Dialogues: We utilize the Sarcasm Corpus V1 (SC-V1) and the Sarcasm Corpus V2 (SC-V2), which are subsets of the Internet Argument Corpus (IAC).\n"}
{"q_uid": "a3f108f60143d13fe38d911b1cc3b17bdffde3bd", "table": "\nTable 2: F1 Results\n| Method                          | SR  | HATE | HAR  |\n|--------------------------------|-----|------|------|\n| LR(Char-gram + Unigram)        | 0.79| 0.85 | 0.68 |\n| LR(Waseem and Hovy, 2016)      | 0.74| -    | -    |\n| LR (Davidson et al., 2017)     | -   | 0.90 | -    |\n| CNN (Park and Fung, 2017)      | 0.83| -    | -    |\n| GRU Text (Founta et al., 2018) | 0.83| 0.89 | -    |\n| GRU Text + Metadata            | 0.87| 0.89 | -    |\n| TWEM (Ours)                    | 0.86| 0.924| 0.71 |\n", "text": "\nTable TABREF10 illustrates the robustness of our method, which often outperform previous results, measured by weighted F1.\n"}
{"q_uid": "3aee5c856e0ee608a7664289ffdd11455d153234", "table": "\nTable 3: Performance of different models on the test datasets. EM and GM report percentages, and ED corresponds to average edit distance. The symbol \u2191 indicates that higher results are better in the corresponding column; \u2193 indicates that lower is better.\n| Model                                      | Test-Repeated Set | Test-Repeated Set |  Test-Repeated Set |  Test-Repeated Set | Test-New Set | Test-New Set | Test-New Set | Test-New Set |\n|--------------------------------------------|-------|------|------|------|-------|------|-------|-----|\n|                                            | EM \u2191  | F1 \u2191 | ED \u2193 | GM \u2191 | EM \u2191  | F1 \u2191 | ED \u2193 | GM \u2191 |\n| Baseline                                   | 25.30 | 79.83 | 2.53 | 26.28 | 25.44 | 81.38 | 2.39 | 25.44 |\n| Ablation                                   | 36.36 | 90.28 | 1.36 | 36.36 | 24.82 | 88.65 | 1.71 | 24.92 |\n| Ablation with Mask                         | 45.95 | 90.08 | 1.20 | 46.05 | 36.45 | 88.31 | 1.45 | 36.56 |\n| Ours without Mask                          | 52.47 | 91.74 | 0.95 | 53.95 | 21.94 | 87.50 | 1.78 | 22.65 |\n| Ours with Mask                             | 57.31 | 91.91 | 0.91 | 57.31 | 38.52 | 88.98 | 1.32 | 38.52 |\n| Ours without Mask and with Ordered Triplets| 57.21 | 93.37 | 0.79 | 57.71 | 33.56 | 91.02 | 1.37 | 33.78 |\n| Ours with Mask and Ordered Triplets        | 61.17 | 93.54 | 0.75 | 61.36 | 41.71 | 90.22 | 1.22 | 41.81 |\n", "text": "\n"}
{"q_uid": "2fa0b9d0cb26e1be8eae7e782ada6820bc2c037f", "table": "\nTable 1: Lemmatization accuracy using WikiNews testset\n| System       | Accuracy |\n|--------------|----------|\n| Our System   | 97.32%   |\n| MADAMIRA     | 96.61%   |\n", "text": "\n"}
{"q_uid": "11dde2be9a69a025f2fc29ce647201fb5a4df580", "table": "\nTable 2: Accuracy comparison of state-of-the-art transition-based dependency parsers on PT-SD. The \u201cType\u201d column shows the type of parser: gs is a greedy parser trained with a static oracle, gd a greedy parser trained with a dynamic oracle, b/n a beam search parser with beam size n, dp a parser that employs global training with dynamic programming, and c a constituent parser with conversion to dependencies.\n| Parser                                | Type | UAS  | LAS  |\n|---------------------------------------|------|------|------|\n| (Chen and Manning, 2014)              | gs   | 91.8 | 89.6 |\n| (Dyer et al., 2015)                   | gs   | 93.1 | 90.9 |\n| (Weiss et al., 2015) greedy           | gs   | 93.2 | 91.2 |\n| (Ballesteros et al., 2016)            | gd   | 93.5 | 91.4 |\n| (Kiperwasser and Goldberg, 2016)      | gd   | 93.9 | 91.9 |\n| (Qi and Manning, 2017)                | gs   | 94.3 | 92.2 |\n| This work                             | gs   | 94.5 | 92.4 |\n| (Weiss et al., 2015) beam             | b(8) | 94.0 | 92.1 |\n| (Alberti et al., 2015)                | b(32)| 94.2 | 92.4 |\n| (Andor et al., 2016)                  | b(32)| 94.6 | 92.8 |\n| (Shi et al., 2017)                    | dp   | 94.5 | -    |\n| (Kuncoro et al., 2017) (constit.)     | c  | 95.8 | 94.6   |\n", "text": "\nTable TABREF12 compares our novel system with other state-of-the-art transition-based dependency parsers on the PT-SD. Greedy parsers are in the first block, beam-search and dynamic programming parsers in the second block. The third block shows the best result on this benchmark, obtained with constituent parsing with generative re-ranking and conversion to dependencies. Despite being the only non-projective parser tested on a practically projective dataset, our parser achieves the highest score among greedy transition-based models (even above those trained with a dynamic oracle).\n\nWe even slightly outperform the arc-swift system of Qi2017, with the same model architecture, implementation and training setup, but based on the projective arc-eager transition-based parser instead.\n"}
{"q_uid": "1f63ccc379f01ecdccaa02ed0912970610c84b72", "table": "\nTable 2: Ablation study on the development set of SQuAD.\n| Model                       | EM    | \u0394EM  | F1    | \u0394F1  |\n|-----------------------------|-------|------|-------|------|\n| DCN+ (ours)                 | 74.5% | -    | 83.1% | -    |\n| - Deep residual coattention | 73.1% | -1.4%| 81.5% | -1.6%|\n| - Mixed objective           | 73.8% | -0.7%| 82.1% | -1.0%|\n| - Mixture of experts        | 74.0% | -0.5%| 82.4% | -0.7%|\n| DCN w/ CoVe (baseline)      | 71.3% | -3.2%| 79.9% | -3.2%|\n", "text": "\nThe contributions of each part of our model are shown in Table 2 .\n"}
{"q_uid": "3070d6d6a52aa070f0c0a7b4de8abddd3da4f056", "table": "\nTable 1: BPC on the Penn Treebank test set\n| Model                                 | BPC  |\n|---------------------------------------|------|\n| Norm-stabilized RNN (Krueger & Memisevic 2015) | 1.48 |\n| CW-RNN (Koutnik et al. 2014)          | 1.46 |\n| HF-MRNN (Mikolov et al. 2012)         | 1.41 |\n| MI-RNN (Wu et al. 2016)               | 1.39 |\n| ME n-gram (Mikolov et al. 2012)       | 1.37 |\n| BatchNorm LSTM (Cooijmans et al. 2016)| 1.32 |\n| Zoneout RNN (Krueger et al. 2016)     | 1.27 |\n| HyperNetworks (Ha et al. 2016)        | 1.27 |\n| LayerNorm HM-LSTM (Chung et al. 2016) | 1.24 |\n| LayerNorm HyperNetworks (Ha et al. 2016) | 1.23 |\n| PRPN                                  | 1.202|\n", "text": "\nIn Table TABREF40 , we show the value of test perplexity for different variants of PRPN, each variant remove part of the model. \nWord-level Language Model\n"}
{"q_uid": "c1c611409b5659a1fd4a870b6cc41f042e2e9889", "table": "\nTable 1: Experiment results on the NIST Chinese-English translation tasks. [+Cd] is the proposed model with the dynamic cache. [+Cd, Ct] is the proposed model with both the dynamic and topic cache. Avg means the average BLEU score on all test sets.\n| Model         | NIST02 | NIST04 | NIST05 | NIST06 | Avg   |\n|---------------|--------|--------|--------|--------|-------|\n| Moses         | 31.52  | 32.73  | 29.52  | 29.57  | 30.69 |\n| RNNsearch*    | 36.18  | 36.36  | 32.56  | 30.57  | 33.92 |\n| + Cd          | 36.86  | 37.16  | 33.10  | 32.60  | 34.93 |\n| + Cd, Ct      | 38.04  | 38.89  | 33.31  | 31.85  | 35.52 |\n", "text": "\n"}
{"q_uid": "1adbdb5f08d67d8b05328ccc86d297ac01bf076c", "table": "\nTable 1: Details of the BABEL data used for performing the multilingual experiments**\n| Usage  | Language   | Train    |   Train   |  Eval   |  Eval     |       |\n|--------|------------|----------------------|---------------------|-----------------|\n|        |            | # splrs. | # hours | # splrs. | # hours | # of characters |\n| Train  | Cantonese  | 952      | 126.73  | 120      | 17.71  | 3302            |\n| Train  | Bengali    | 720      | 55.18   | 121      | 9.79   | 66              |\n| Train  | Pashto     | 959      | 70.26   | 121      | 9.95   | 49              |\n| Train  | Turkish    | 963      | 68.98   | 121      | 9.76   | 66              |\n| Train  | Vietnamese | 964      | 78.62   | 120      | 10.9   | 131             |\n| Train  | Haitian    | 724      | 60.11   | 120      | 10.63  | 60              |\n| Train  | Tamil      | 724      | 62.11   | 121      | 11.61  | 49              |\n| Train  | Kurdish    | 502      | 37.69   | 120      | 10.21  | 64              |\n| Train  | Tokpisin   | 482      | 35.32   | 120      | 9.88   | 55              |\n| Train  | Georgian   | 490      | 45.35   | 120      | 12.30  | 35              |\n| Target | Assamese   | 720      | 54.35   | 120      | 9.58   | 66              |\n| Target | Tagalog    | 966      | 44.0    | 120      | 10.60  | 56              |\n| Target | Swahili    | 491      | 40.0    | 120      | 10.58  | 56              |\n| Target | Lao        | 733      | 58.79   | 119      | 10.50  | 54              |\n", "text": "\nTable TABREF14 presents the details of the languages used in this work for training and evaluation.\n"}
{"q_uid": "a3efe43a72b76b8f5e5111b54393d00e6a5c97ab", "table": "\nTable 1: Statistics of various datasets. Mean and Var indicate the mean and variance of target phrase numbers, %Pre denotes percentage of present keyphrases.\n| Dataset  | #Train | #Valid | #Test | Mean | Var  | %Pre |\n|----------|--------|--------|-------|------|------|------|\n| KP20K    | \u2248514k  | \u224820k   | \u224820k  | 5.3  | 14.2 | 63.3%|\n| INSPEC   | -      | 1500   | 500   | 9.6  | 22.4 | 78.5%|\n| Krapivin | -      | 1844   | 460   | 5.2  | 6.6  | 56.5%|\n| NUS      | -      | -      | 211   | 11.5 | 64.6  | 51.3%|\n| SemEval  | -      | 144    | 100   | 15.7 | 15.1 | 44.5%|\n| STACKEX  | \u2248298k  | \u224816k   | \u224816k  | 2.7  | 1.4  | 57.5%|\n", "text": "\n"}
{"q_uid": "74fb77a624ea9f1821f58935a52cca3086bb0981", "table": "\nTable 3: Distribution of label combinations in OLID.\n| A   | B   | C   | Training | Test | Total |\n|-----|-----|-----|----------|------|-------|\n| OFF | TIN | IND | 2,407    | 100  | 2,507 |\n| OFF | TIN | OTH | 395      | 35   | 430   |\n| OFF | TIN | GRP | 1,074    | 78   | 1,152 |\n| OFF | UNT | \u2014   | 524      | 27   | 551   |\n| NOT | \u2014   | \u2014   | 8,840    | 620  | 9,460 |\n| All |     |     | 13,240   | 860  | 14,100 |\n", "text": "\nThe breakdown of the data into training and testing for the labels from each level is shown in Table TABREF15 .\n"}
{"q_uid": "1b72aa2ec3ce02131e60626639f0cf2056ec23ca", "table": "\nTable 3: Distribution of label combinations in OLID.\n| A   | B   | C   | Training | Test | Total |\n|-----|-----|-----|----------|------|-------|\n| OFF | TIN | IND | 2,407    | 100  | 2,507 |\n| OFF | TIN | OTH | 395      | 35   | 430   |\n| OFF | TIN | GRP | 1,074    | 78   | 1,152 |\n| OFF | UNT | \u2014   | 524      | 27   | 551   |\n| NOT | \u2014   | \u2014   | 8,840    | 620  | 9,460 |\n| All |     |     | 13,240   | 860  | 14,100 |\n", "text": "\nThe breakdown of the data into training and testing for the labels from each level is shown in Table TABREF15 .\n"}
{"q_uid": "44104668796a6ca10e2ea3ecf706541da1cec2cf", "table": "\nTable 1: Test results for all the methods used. The loss measure is cross-entropy.\n| Method          | Accuracy | Perplexity | Loss (train) | Loss (test) |\n|-----------------|----------|------------|--------------|-------------|\n| Edit distance   | 0.3453   | -          | -            | -           |\n| Diacritic swapping | 0.2279 | -          | -            | -           |\n| Vector distance | 0.3945   | -          | -            | -           |\n| LSTM-1 net      | 0.4183   | 907        | 0.3          | 0.41        |\n| LSTM-2 net      | 0.6634   | 11182      | 0.1          | 0.37        |\n| LSTM-ELMo net   | 0.6818   | 706166     | 0.07         | 0.38        |\n", "text": "\nThe experimental results are presented in Table TABREF4 .\n"}
{"q_uid": "4547818a3bbb727c4bb4a76554b5a5a7b5c5fedb", "table": "\nTable 1: Training corpus size and subword vocabulary size for different subsets of IWSLT14 DE\u2192EN data, and for KO\u2192EN data.\n|           |            | subword vocabulary |  subword vocabulary |\n| sentences | words (EN) | DE/KO  | EN       |\n|-----------|------------|--------|----------|\n| DE\u2192EN     |            |        |          |\n| 159 000   | 3 220 000  | 18 870 | 13 830   |\n| 80 000    | 1 610 000  | 9850   | 7740     |\n| 40 000    | 810 000    | 7470   | 5950     |\n| 20 000    | 400 000    | 5640   | 4530     |\n| 10 000    | 200 000    | 3760   | 3110     |\n| 5000      | 100 000    | 2380   | 1990     |\n| KO\u2192EN     |            |       |           |\n| 94 000    | 2 300 000  | 32 082 | 16 006   |\n", "text": "\nWe use the TED data from the IWSLT 2014 German INLINEFORM0 English shared translation task BIBREF38 . We use the same data cleanup and train/dev split as BIBREF39 , resulting in 159000 parallel sentences of training data, and 7584 for development.\nTable TABREF14 shows statistics for each subcorpus, including the subword vocabulary.\nWe use the TED data from the IWSLT 2014 German INLINEFORM0 English shared translation task BIBREF38 . We use the same data cleanup and train/dev split as BIBREF39 , resulting in 159000 parallel sentences of training data, and 7584 for development.\nTable TABREF14 shows statistics for each subcorpus, including the subword vocabulary.\nTable TABREF18 shows the effect of adding different methods to the baseline NMT system, on the ultra-low data condition (100k words of training data) and the full IWSLT 14 training corpus (3.2M words). Our \"mainstream improvements\" add around 6-7 BLEU in both data conditions.\n"}
{"q_uid": "281cd4e78b27a62713ec43249df5000812522a89", "table": "\nTable 2: A summary of PERSPECTRUM statistics\n| Category     | Statistic                          | Value  |\n|--------------|-----------------------------------|--------|\n| Claims       | # of claims (step 1)              | 907    |\n|  Claims      | avg. claim length (tokens)        | 8.9    |\n|  Claims     | median claim length (tokens)      | 8      |\n|  Claims     | max claim length (tokens)         | 30     |\n|  Claims     | min claim length (tokens)         | 3      |\n| Perspectives | # of perspectives                 | 11,164  |\n| Perspectives | Debate websites (step 1)          | 4,230  |\n| Perspectives | Perspective paraphrase (step 2b)  | 4,507  |\n| Perspectives | Web (step 2c)                     | 2,427  |\n| Perspectives | # of perspectives with stances    | 5,095  |\n| Perspectives | # of \"support\" perspectives       | 2,627  |\n| Perspectives | # of \"opposing\" perspectives      | 2,468  |\n| Perspectives | avg size of perspective clusters  | 2.3    |\n| Perspectives | avg length of perspectives (tokens)| 11.9  |\n| Evidences    | # of total evidences (step 1)     | 8,092  |\n| Evidences   | avg length of evidences (tokens)  | 168    |\n", "text": "\nThe dataset contains about INLINEFORM0 claims with a significant length diversity (Table TABREF19 ).\n"}
{"q_uid": "d7d611f622552142723e064f330d071f985e805c", "table": "\nTable 3: Spoken data collected during different evaluation campaigns. Column \u201c#Q\u201d indicates the total number of different (written) questions presented to the pupils.\n| Year | Lang | #Pupils | #Utterances | Duration   | #Q  |\n|------|------|---------|-------------|------------|-----|\n| 2016 | ENG  | 2748    | 17462       | 69:03:37   | 85  |\n| 2016 | GER  | 2542    | 15866       | 60:03:01   | 101 |\n| 2017 | ENG  | 511     | 4112        | 16:25:45   | 24  |\n| 2017 | GER  | 478     | 3739        | 15:33:06   | 23  |\n| 2018 | ENG  | 2332    | 15770       | 39:14:53   | 24  |\n| 2018 | GER  | 2072    | 13658       | 95:54:56   | 23  |\n", "text": "\nTable reports some statistics extracted from the acquired spoken data.\n"}
{"q_uid": "5e5460ea955d8bce89526647dd7c4f19b173ab34", "table": "\nTable 7: Statistics from the spoken data sets (2017) used for ASR.\n| id          | # of utt. | duration total | duration avg | tokens total | tokens avg |\n|-------------|-----------|----------------|--------------|--------------|------------|\n| Ger Train All | 1448    | 04:47:45       | 11.92        | 9878         | 6.82       |\n| Ger Train Clean | 589   | 01:37:59       | 9.98         | 2317         | 3.93       |\n| Eng Train All | 2301    | 09:03:30       | 14.17        | 26090        | 11.34      |\n| Eng Train Clean | 916   | 02:45:42       | 10.85        | 6249         | 6.82       |\n| Ger Test All | 671      | 02:19:10       | 12.44        | 5334         | 7.95       |\n| Ger Test Clean | 260    | 00:43:25       | 10.02        | 1163         | 4.47       |\n| Eng Test All | 1142     | 04:29:43       | 14.17        | 13244        | 11.60      |\n| Eng Test Clean | 423    | 01:17:02       | 10.93        | 3404       | 8.05      |\n", "text": "\nSpeakers were assigned either to training or evaluation sets, with proportions of $\frac{2}{3}$ and $\frac{1}{3}$, respectively; then training and evaluation lists were built, accordingly. Table reports statistics from the spoken data set. The id All identifies the whole data set, while Clean defines the subset in which sentences containing background voices, incomprehensible speech and word fragments were excluded.\n"}
{"q_uid": "58f50397a075f128b45c6b824edb7a955ee8cba1", "table": "\nTable 1: Optimal hyperparameters used for final training on the ADE and CoNLL04 datasets.\n| Hyperparameter             | ADE | CoNLL04 |\n|----------------------------|-----|---------|\n| BiRNN Type                 | GRU | GRU     |\n| Character BiRNN Size       | 32  | 32      |\n| Non-Character BiRNN Size   | 128 | 256     |\n| # Shared BiRNN Layers      | 1   | 1       |\n| # NER-Specific BiRNN Layers| 1   | 1       |\n| # RE-Specific BiRNN Layers | 1   | 2       |\n| FFNN^(e1) Activation       | ReLU| tanh    |\n| FFNN^(e1) Output Size      | 64  | 64      |\n| FFNN^(r1) Activation       | ReLU| ReLU    |\n| FFNN^(r1) Output Size      | 128 | 128     |\n| Label Embedding Size       | 25  | 25      |\n| Pre-BiRNN Dropout          | 0.5 | 0.35    |\n| Pre-RE Scoring Dropout     | 0.5 | 0.5     |\n| RE Threshold \u03b8^r           | 0.9 | 0.9     |\n", "text": "\n"}
{"q_uid": "13d92cbc2c77134626e26166c64ca5c00aec0bf5", "table": "\nTable 1: Results of systems on HOTPOTQA.\n| Method | Ans |  Ans | Sup | Sup | Joint | Joint |\n|--------|-----|-----|-----|-----|-----|-----|\n|        | EM  | F1  | EM  | F1  | EM  | F1  |\n| Yang (2018) | 24.7 | 34.4 | 5.3 | 41.0 | 2.5 | 17.7 |\n| Ding (2019) | 37.6 | 49.4 | 23.1 | 58.5 | 12.2 | 35.3 |\n| whole pip. | 46.5 | 58.8 | 39.9 | 71.5 | 26.6 | 49.2 |\n| Dev set |     |     |     |     |     |     |\n| Yang (2018) | 24.0 | 32.9 | 3.9 | 37.7 | 1.9 | 16.2 |\n| MUPPET | 30.6 | 40.3 | 16.7 | 47.3 | 10.9 | 27.0 |\n| Ding (2019) | 37.1 | 48.9 | 22.8 | 57.7 | 12.4 | 34.9 |\n| whole pip. | 45.3 | 57.3 | 38.7 | 70.8 | 25.1 | 47.6 |\n| Test set |     |     |     |     |     |     |\n", "text": "\nWe chose the best system based on the dev set, and used that for submitting private test predictions on both FEVER and HotpotQA .\nAs can be seen in Table TABREF8, with the proposed hierarchical system design, the whole pipeline system achieves new start-of-the-art on HotpotQA with large-margin improvements on all the metrics. \nSimilarly for FEVER, we showed F1 for evidence, the Label Accuracy, and the FEVER Score (same as benchmark evaluation) for models in Table TABREF9.\n"}
{"q_uid": "8e2b125426d1220691cceaeaf1875f76a6049cbd", "table": "\nTable 4: Average perplexity and BLEU score (reported in percentages) for the top 10 generations under each inference dimension of Event2Mind. The best result for each dimension is emboldened.\n| Metric | Methods | xIntent | xReact | oReact |\n|--------|---------|---------|--------|--------|\n| PPL | RNN-based Seq2Seq | 44.12 | 29.18 | 14.08 |\n| PPL  | Variational Seq2Seq | 42.06 | 28.22 | 12.62 |\n| PPL  | VRNMT | 33.45 | 25.54 | 11.93 |\n| PPL  | CWVAE-Unpretrained | 31.32 | 24.07 | 11.37 |\n| PPL  | CWVAE | 29.23 | 23.17 | 11.04 |\n| BLEU | RNN-based Seq2Seq | 2.75 | 2.11 | 5.18 |\n| BLEU | Variational Seq2Seq | 2.84 | 2.43 | 2.08 |\n| BLEU | VRNMT | 4.81 | 3.94 | 6.61 |\n| BLEU | CWVAE-Unpretrained | 7.36 | 5.52 | 5.33 |\n| BLEU | CWVAE | 12.98 | 5.65 | 6.97 |\n", "text": "\nFurther, we employ BLEU score to evaluate the accuracy of generations BIBREF21, and the number of distinct n-gram to evaluate the diversity of generations BIBREF6. \n"}
