{
    "2001.03131": [
        {
            "question": "Do they perform error analysis?",
            "answers": [
                {
                    "answer": "No",
                    "type": "boolean"
                },
                {
                    "answer": "No",
                    "type": "boolean"
                }
            ],
            "q_uid": "133eb4aa4394758be5f41744c60c99901b2bc01c",
            "evidence": [
                {
                    "raw_evidence": [],
                    "highlighted_evidence": []
                },
                {
                    "raw_evidence": [],
                    "highlighted_evidence": []
                }
            ]
        },
        {
            "question": "What is the Random Kitchen Sink approach?",
            "answers": [
                {
                    "answer": "Random Kitchen Sink method uses a kernel function to map data vectors to a space where linear separation is possible.",
                    "type": "abstractive"
                },
                {
                    "answer": "explicitly maps data vectors to a space where linear separation is possible, RKS method provides an approximate kernel function via explicit mapping",
                    "type": "extractive"
                }
            ],
            "q_uid": "a778b8204a415b295f73b93623d09599f242f202",
            "evidence": [
                {
                    "raw_evidence": [
                        "RKS approach proposed in BIBREF21, BIBREF22, explicitly maps data vectors to a space where linear separation is possible. It has been explored for natural language processing tasks BIBREF23, BIBREF24. The RKS method provides an approximate kernel function via explicit mapping."
                    ],
                    "highlighted_evidence": [
                        "RKS approach proposed in BIBREF21, BIBREF22, explicitly maps data vectors to a space where linear separation is possible.",
                        "The RKS method provides an approximate kernel function via explicit mapping."
                    ]
                },
                {
                    "raw_evidence": [
                        "RKS approach proposed in BIBREF21, BIBREF22, explicitly maps data vectors to a space where linear separation is possible. It has been explored for natural language processing tasks BIBREF23, BIBREF24. The RKS method provides an approximate kernel function via explicit mapping.",
                        "Here, $\\phi (.)$ denotes the implicit mapping function (used to compute kernel matrix), $Z(.)$ denotes the explicit mapping function using RKS and ${\\Omega _k}$ denotes random variable ."
                    ],
                    "highlighted_evidence": [
                        "RKS approach proposed in BIBREF21, BIBREF22, explicitly maps data vectors to a space where linear separation is possible. It has been explored for natural language processing tasks BIBREF23, BIBREF24. The RKS method provides an approximate kernel function via explicit mapping.\n\nHere, $\\phi (.)$ denotes the implicit mapping function (used to compute kernel matrix), $Z(.)$ denotes the explicit mapping function using RKS and ${\\Omega _k}$ denotes random variable ."
                    ]
                }
            ]
        }
    ],
    "1705.07830": [
        {
            "question": "how are multiple answers from multiple reformulated questions aggregated?",
            "answers": [
                {
                    "answer": "The selection model selects the best answer from the set $\\lbrace a_i\\rbrace _{i=1}^N$ observed during the interaction by predicting the difference of the F1 score to the average F1 of all variants.",
                    "type": "extractive"
                }
            ],
            "q_uid": "33d2919f3400cd3c6fbb6960d74187ec80b41cd6",
            "evidence": [
                {
                    "raw_evidence": [
                        "During training, we have access to the reward for the answer returned for each reformulation $q_i$ . However, at test time we must predict the best answer $a^*$ . The selection model selects the best answer from the set $\\lbrace a_i\\rbrace _{i=1}^N$ observed during the interaction by predicting the difference of the F1 score to the average F1 of all variants. We use pre-trained embeddings for the tokens of query, rewrite, and answer. For each, we add a 1-dimensional CNN followed by max-pooling. The three resulting vectors are then concatenated and passed through a feed-forward network which produces the output.",
                        "Unlike the reformulation policy, we train the answer with either beam search or sampling. We can produce many rewrites of a single question from our reformulation system. We issue each rewrite to the QA environment, yielding a set of (query, rewrite, answer) tuples from which we need to pick the best instance. We train another neural network to pick the best answer from the candidates. We frame the task as binary classification, distinguishing between above and below average performance. In training, we compute the F1 score of the answer for every instance. If the rewrite produces an answer with an F1 score greater than the average score of the other rewrites the instance is assigned a positive label. We ignore questions where all rewrites yield equally good/bad answers. We evaluated FFNNs, LSTMs, and CNNs and found that the performance of all systems was comparable. We choose a CNN which offers good computational efficiency and accuracy (cf. \"Training\" )."
                    ],
                    "highlighted_evidence": [
                        " The selection model selects the best answer from the set $\\lbrace a_i\\rbrace _{i=1}^N$ observed during the interaction by predicting the difference of the F1 score to the average F1 of all variants.",
                        "We train another neural network to pick the best answer from the candidates. We frame the task as binary classification, distinguishing between above and below average performance. In training, we compute the F1 score of the answer for every instance. If the rewrite produces an answer with an F1 score greater than the average score of the other rewrites the instance is assigned a positive label."
                    ]
                }
            ]
        }
    ],
    "1909.11189": [
        {
            "question": "What is the algorithm used for the classification tasks?",
            "answers": [
                {
                    "answer": "Random Forest Ensemble classifiers",
                    "type": "extractive"
                }
            ],
            "q_uid": "bfa3776c30cb30e0088e185a5908e5172df79236",
            "evidence": [
                {
                    "raw_evidence": [
                        "To test whether topic models can be used for dating poetry or attributing authorship, we perform supervised classification experiments with Random Forest Ensemble classifiers. We find that we obtain better results by training and testing on stanzas instead of full poems, as we have more data available. Also, we use 50 year slots (instead of 25) to ease the task."
                    ],
                    "highlighted_evidence": [
                        "To test whether topic models can be used for dating poetry or attributing authorship, we perform supervised classification experiments with Random Forest Ensemble classifiers. "
                    ]
                }
            ]
        },
        {
            "question": "Is the outcome of the LDA analysis evaluated in any way?",
            "answers": [
                {
                    "answer": "Yes",
                    "type": "boolean"
                }
            ],
            "q_uid": "a2a66726a5dca53af58aafd8494c4de833a06f14",
            "evidence": [
                {
                    "raw_evidence": [
                        "The Style baseline achieves an Accuracy of 83%, LDA features 89% and a combination of the two gets 90%. However, training on full poems reduces this to 42\u201452%."
                    ],
                    "highlighted_evidence": [
                        "The Style baseline achieves an Accuracy of 83%, LDA features 89% and a combination of the two gets 90%. However, training on full poems reduces this to 42\u201452%."
                    ]
                }
            ]
        },
        {
            "question": "What is the corpus used in the study?",
            "answers": [
                {
                    "answer": "TextGrid Repository",
                    "type": "extractive"
                },
                {
                    "answer": "The Digital Library in the TextGrid Repository",
                    "type": "extractive"
                }
            ],
            "q_uid": "ee87608419e4807b9b566681631a8cd72197a71a",
            "evidence": [
                {
                    "raw_evidence": [
                        "The Digital Library in the TextGrid Repository represents an extensive collection of German texts in digital form BIBREF3. It was mined from http://zeno.org and covers a time period from the mid 16th century up to the first decades of the 20th century. It contains many important texts that can be considered as part of the literary canon, even though it is far from complete (e.g. it contains only half of Rilke\u2019s work). We find that around 51k texts are annotated with the label \u2019verse\u2019 (TGRID-V), not distinguishing between \u2019lyric verse\u2019 and \u2019epic verse\u2019. However, the average length of these texts is around 150 token, dismissing most epic verse tales. Also, the poems are distributed over 229 authors, where the average author contributed 240 poems (median 131 poems). A drawback of TGRID-V is the circumstance that it contains a noticeable amount of French, Dutch and Latin (over 400 texts). To constrain our dataset to German, we filter foreign language material with a stopword list, as training a dedicated language identification classifier is far beyond the scope of this work."
                    ],
                    "highlighted_evidence": [
                        "The Digital Library in the TextGrid Repository represents an extensive collection of German texts in digital form BIBREF3."
                    ]
                },
                {
                    "raw_evidence": [
                        "The Digital Library in the TextGrid Repository represents an extensive collection of German texts in digital form BIBREF3. It was mined from http://zeno.org and covers a time period from the mid 16th century up to the first decades of the 20th century. It contains many important texts that can be considered as part of the literary canon, even though it is far from complete (e.g. it contains only half of Rilke\u2019s work). We find that around 51k texts are annotated with the label \u2019verse\u2019 (TGRID-V), not distinguishing between \u2019lyric verse\u2019 and \u2019epic verse\u2019. However, the average length of these texts is around 150 token, dismissing most epic verse tales. Also, the poems are distributed over 229 authors, where the average author contributed 240 poems (median 131 poems). A drawback of TGRID-V is the circumstance that it contains a noticeable amount of French, Dutch and Latin (over 400 texts). To constrain our dataset to German, we filter foreign language material with a stopword list, as training a dedicated language identification classifier is far beyond the scope of this work."
                    ],
                    "highlighted_evidence": [
                        "The Digital Library in the TextGrid Repository represents an extensive collection of German texts in digital form BIBREF3. It was mined from http://zeno.org and covers a time period from the mid 16th century up to the first decades of the 20th century. It contains many important texts that can be considered as part of the literary canon, even though it is far from complete (e.g. it contains only half of Rilke\u2019s work)."
                    ]
                }
            ]
        }
    ],
    "1811.02906": [
        {
            "question": "What baseline is used?",
            "answers": [
                {
                    "answer": "SVM",
                    "type": "abstractive"
                }
            ],
            "q_uid": "38a5cc790f66a7362f91d338f2f1d78f48c1e252",
            "evidence": [
                {
                    "raw_evidence": [
                        "The baseline classifier uses a linear Support Vector Machine BIBREF7 , which is suited for a high number of features. We use a text classification framework for German BIBREF8 that has been used successfully for sentiment analysis before."
                    ],
                    "highlighted_evidence": [
                        "The baseline classifier uses a linear Support Vector Machine BIBREF7 , which is suited for a high number of features. "
                    ]
                }
            ]
        },
        {
            "question": "What topic clusters are identified by LDA?",
            "answers": [
                {
                    "answer": "Clusters of Twitter user ids from accounts of American or German political actors, musicians, media websites or sports club",
                    "type": "abstractive"
                }
            ],
            "q_uid": "0da6cfbc8cb134dc3d247e91262f5050a2200664",
            "evidence": [
                {
                    "raw_evidence": [
                        "For offensive language detection in Twitter, users addressed in tweets might be an additional relevant signal. We assume it is more likely that politicians or news agencies are addressees of offensive language than, for instance, musicians or athletes. To make use of such information, we obtain a clustering of user ids from our Twitter background corpus. From all tweets in our stream from 2016 or 2017, we extract those tweets that have at least two @-mentions and all of the @-mentions have been seen at least five times in the background corpus. Based on the resulting 1.8 million lists of about 169,000 distinct user ids, we compute a topic model with INLINEFORM0 topics using Latent Dirichlet Allocation BIBREF3 . For each of the user ids, we extract the most probable topic from the inferred user id-topic distribution as cluster id. This results in a thematic cluster id for most of the user ids in our background corpus grouping together accounts such as American or German political actors, musicians, media websites or sports clubs (see Table TABREF17 ). For our final classification approach, cluster ids for users mentioned in tweets are fed as a second input in addition to (sub-)word embeddings to the penultimate dense layer of the neural network model."
                    ],
                    "highlighted_evidence": [
                        "Based on the resulting 1.8 million lists of about 169,000 distinct user ids, we compute a topic model with INLINEFORM0 topics using Latent Dirichlet Allocation BIBREF3 . For each of the user ids, we extract the most probable topic from the inferred user id-topic distribution as cluster id. This results in a thematic cluster id for most of the user ids in our background corpus grouping together accounts such as American or German political actors, musicians, media websites or sports clubs (see Table TABREF17 ). "
                    ]
                }
            ]
        },
        {
            "question": "What are the near-offensive language categories?",
            "answers": [
                {
                    "answer": "inappropriate, discriminating",
                    "type": "extractive"
                }
            ],
            "q_uid": "9003c7041d3d2addabc2c112fa2c7efe5fab493c",
            "evidence": [
                {
                    "raw_evidence": [
                        "As introduced above, the `One Million Post' corpus provides annotation labels for more than 11,000 user comments. Although there is no directly comparable category capturing `offensive language' as defined in the shared task, there are two closely related categories. From the resource, we extract all those comments in which a majority of the annotators agree that they contain either `inappropriate' or `discriminating' content, or none of the aforementioned. We treat the first two cases as examples of `offense' and the latter case as examples of `other'. This results in 3,599 training examples (519 offense, 3080 other) from on the `One Million Post' corpus. We conduct pre-training of the neural model as a binary classification task (similar to the Task 1 of GermEval 2018)"
                    ],
                    "highlighted_evidence": [
                        "As introduced above, the `One Million Post' corpus provides annotation labels for more than 11,000 user comments. Although there is no directly comparable category capturing `offensive language' as defined in the shared task, there are two closely related categories. From the resource, we extract all those comments in which a majority of the annotators agree that they contain either `inappropriate' or `discriminating' content, or none of the aforementioned. We treat the first two cases as examples of `offense' and the latter case as examples of `other'."
                    ]
                }
            ]
        }
    ],
    "1605.07333": [
        {
            "question": "Which dataset do they train their models on?",
            "answers": [
                {
                    "answer": "relation classification dataset of the SemEval 2010 task 8",
                    "type": "extractive"
                },
                {
                    "answer": "SemEval 2010 task 8 BIBREF8",
                    "type": "extractive"
                }
            ],
            "q_uid": "30eacb4595014c9c0e5ee9669103d003cfdfe1e5",
            "evidence": [
                {
                    "raw_evidence": [
                        "We used the relation classification dataset of the SemEval 2010 task 8 BIBREF8 . It consists of sentences which have been manually labeled with 19 relations (9 directed relations and one artificial class Other). 8,000 sentences have been distributed as training set and 2,717 sentences served as test set. For evaluation, we applied the official scoring script and report the macro F1 score which also served as the official result of the shared task."
                    ],
                    "highlighted_evidence": [
                        "We used the relation classification dataset of the SemEval 2010 task 8 BIBREF8 . It consists of sentences which have been manually labeled with 19 relations (9 directed relations and one artificial class Other). 8,000 sentences have been distributed as training set and 2,717 sentences served as test set."
                    ]
                },
                {
                    "raw_evidence": [
                        "We used the relation classification dataset of the SemEval 2010 task 8 BIBREF8 . It consists of sentences which have been manually labeled with 19 relations (9 directed relations and one artificial class Other). 8,000 sentences have been distributed as training set and 2,717 sentences served as test set. For evaluation, we applied the official scoring script and report the macro F1 score which also served as the official result of the shared task."
                    ],
                    "highlighted_evidence": [
                        "We used the relation classification dataset of the SemEval 2010 task 8 BIBREF8 ."
                    ]
                }
            ]
        },
        {
            "question": "How does their simple voting scheme work?",
            "answers": [
                {
                    "answer": "we apply several CNN and RNN models presented in Tables TABREF12 and TABREF14 and predict the class with the most votes, In case of a tie, we pick one of the most frequent classes randomly",
                    "type": "extractive"
                },
                {
                    "answer": "Among all the classes predicted by several models, for each test sentence, class with most votes are picked. In case of a tie, one of the most frequent classes are picked randomly.",
                    "type": "abstractive"
                }
            ],
            "q_uid": "0f7867f888109b9e000ef68965df4dde2511a55f",
            "evidence": [
                {
                    "raw_evidence": [
                        "Finally, we combine our CNN and RNN models using a voting process. For each sentence in the test set, we apply several CNN and RNN models presented in Tables TABREF12 and TABREF14 and predict the class with the most votes. In case of a tie, we pick one of the most frequent classes randomly. The combination achieves an F1 score of 84.9 which is better than the performance of the two NN types alone. It, thus, confirms our assumption that the networks provide complementary information: while the RNN computes a weighted combination of all words in the sentence, the CNN extracts the most informative n-grams for the relation and only considers their resulting activations."
                    ],
                    "highlighted_evidence": [
                        "Finally, we combine our CNN and RNN models using a voting process. For each sentence in the test set, we apply several CNN and RNN models presented in Tables TABREF12 and TABREF14 and predict the class with the most votes. In case of a tie, we pick one of the most frequent classes randomly."
                    ]
                },
                {
                    "raw_evidence": [
                        "Finally, we combine our CNN and RNN models using a voting process. For each sentence in the test set, we apply several CNN and RNN models presented in Tables TABREF12 and TABREF14 and predict the class with the most votes. In case of a tie, we pick one of the most frequent classes randomly. The combination achieves an F1 score of 84.9 which is better than the performance of the two NN types alone. It, thus, confirms our assumption that the networks provide complementary information: while the RNN computes a weighted combination of all words in the sentence, the CNN extracts the most informative n-grams for the relation and only considers their resulting activations."
                    ],
                    "highlighted_evidence": [
                        "Finally, we combine our CNN and RNN models using a voting process. For each sentence in the test set, we apply several CNN and RNN models presented in Tables TABREF12 and TABREF14 and predict the class with the most votes. In case of a tie, we pick one of the most frequent classes randomly."
                    ]
                }
            ]
        },
        {
            "question": "Which variant of the recurrent neural network do they use?",
            "answers": [
                {
                    "answer": "uni-directional RNN",
                    "type": "extractive"
                }
            ],
            "q_uid": "e2e977d7222654ee8d983fd8ba63b930e9a5a691",
            "evidence": [
                {
                    "raw_evidence": [
                        "As a baseline for the RNN models, we apply a uni-directional RNN which predicts the relation after processing the whole sentence. With this model, we achieve an F1 score of 61.2 on the SemEval test set."
                    ],
                    "highlighted_evidence": [
                        "As a baseline for the RNN models, we apply a uni-directional RNN which predicts the relation after processing the whole sentence."
                    ]
                }
            ]
        },
        {
            "question": "How do they obtain the new context represetation?",
            "answers": [
                {
                    "answer": "They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation.",
                    "type": "abstractive"
                }
            ],
            "q_uid": "0cfe0e33fbb100751fc0916001a5a19498ae8cb5",
            "evidence": [
                {
                    "raw_evidence": [
                        "(1) We propose extended middle context, a new context representation for CNNs for relation classification. The extended middle context uses all parts of the sentence (the relation arguments, left of the relation arguments, between the arguments, right of the arguments) and pays special attention to the middle part.",
                        "One of our contributions is a new input representation especially designed for relation classification. The contexts are split into three disjoint regions based on the two relation arguments: the left context, the middle context and the right context. Since in most cases the middle context contains the most relevant information for the relation, we want to focus on it but not ignore the other regions completely. Hence, we propose to use two contexts: (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. Due to the repetition of the middle context, we force the network to pay special attention to it. The two contexts are processed by two independent convolutional and max-pooling layers. After pooling, the results are concatenated to form the sentence representation. Figure FIGREF3 depicts this procedure. It shows an examplary sentence: \u201cHe had chest pain and <e1>headaches</e1> from <e2>mold</e2> in the bedroom.\u201d If we only considered the middle context \u201cfrom\u201d, the network might be tempted to predict a relation like Entity-Origin(e1,e2). However, by also taking the left and right context into account, the model can detect the relation Cause-Effect(e2,e1). While this could also be achieved by integrating the whole context into the model, using the whole context can have disadvantages for longer sentences: The max pooling step can easily choose a value from a part of the sentence which is far away from the mention of the relation. With splitting the context into two parts, we reduce this danger. Repeating the middle context increases the chance for the max pooling step to pick a value from the middle context."
                    ],
                    "highlighted_evidence": [
                        "We propose extended middle context, a new context representation for CNNs for relation classification.",
                        "The contexts are split into three disjoint regions based on the two relation arguments: the left context, the middle context and the right context. ",
                        "Hence, we propose to use two contexts: (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context.",
                        "The two contexts are processed by two independent convolutional and max-pooling layers. After pooling, the results are concatenated to form the sentence representation."
                    ]
                }
            ]
        }
    ],
    "1710.09753": [
        {
            "question": "What is the task of slot filling?",
            "answers": [
                {
                    "answer": "The task aims at extracting information about persons, organizations or geo-political entities from a large collection of news, web and discussion forum documents.",
                    "type": "extractive"
                }
            ],
            "q_uid": "b8ffb81e74c1c1ad552051aca8741b0141ae6e97",
            "evidence": [
                {
                    "raw_evidence": [
                        "Slot Filling is an information extraction task which has become popular in the last years BIBREF3 . It is a shared task organized by the Text Analysis Conference (TAC). The task aims at extracting information about persons, organizations or geo-political entities from a large collection of news, web and discussion forum documents. An example is \u201cSteve Jobs\u201d for the slot \u201cX founded Apple\u201d. Thinking of a text passage like \u201cSteve Jobs was an American businessman. In 1976, he co-founded Apple\u201d, it is clear that coreference resolution can play an important role for finding the correct slot filler value."
                    ],
                    "highlighted_evidence": [
                        "Slot Filling is an information extraction task which has become popular in the last years BIBREF3 .",
                        "The task aims at extracting information about persons, organizations or geo-political entities from a large collection of news, web and discussion forum documents."
                    ]
                }
            ]
        }
    ],
    "2004.03788": [
        {
            "question": "How large is the dataset?",
            "answers": [
                {
                    "answer": "8757 news records",
                    "type": "extractive"
                }
            ],
            "q_uid": "018ef092ffc356a2c0e970ae64ad3c2cf8443288",
            "evidence": [
                {
                    "raw_evidence": [
                        "There are 8757 news records in our preprocessed data set. We use Jenks natural breaks BIBREF24 to discretize continuous variables $S_{N\\!P}$ and $S_{Q\\!P}$ both into five categories denoted by nominal values from 0 to 4, where larger values still fall into bins with larger nominal value. Let $D_{N\\!P}$ and $D_{Q\\!P}$ denote the discretized variables $S_{N\\!P}$ and $S_{Q\\!P}$, respectively. We derived the information table that only contains discrete features from our original dataset. A fraction of the information table is shown in Table TABREF23."
                    ],
                    "highlighted_evidence": [
                        "There are 8757 news records in our preprocessed data set. "
                    ]
                }
            ]
        },
        {
            "question": "What features do they extract?",
            "answers": [
                {
                    "answer": "Inconsistency in Noun Phrase Structures,  Inconsistency Between Clauses, Inconsistency Between Named Entities and Noun Phrases, Word Level Feature Using TF-IDF",
                    "type": "extractive"
                }
            ],
            "q_uid": "de4e180f49ff187abc519d01eff14ebcd8149cad",
            "evidence": [
                {
                    "raw_evidence": [
                        "Satirical news is not based on or does not aim to state the fact. Rather, it uses parody or humor to make statement, criticisms, or just amusements. In order to achieve such effect, contradictions are greatly utilized. Therefore, inconsistencies significantly exist in different parts of a satirical news tweet. In addition, there is a lack of entity or inconsistency between entities in news satire. We extracted these features at semantic level from different sub-structures of the news tweet. Different structural parts of the sentence are derived by part-of-speech tagging and named entity recognition by Flair. The inconsistencies in different structures are measured by cosine similarity of word phrases where words are represented by Glove word vectors. We explored three different aspects of inconsistency and designed metrics for their measurements. A word level feature using tf-idf BIBREF22 is added for robustness."
                    ],
                    "highlighted_evidence": [
                        "We explored three different aspects of inconsistency and designed metrics for their measurements. ",
                        "A word level feature using tf-idf BIBREF22 is added for robustness."
                    ]
                }
            ]
        }
    ],
    "1909.09587": [
        {
            "question": "What model is used as a baseline?  ",
            "answers": [
                {
                    "answer": "pre-trained multi-BERT",
                    "type": "extractive"
                },
                {
                    "answer": "QANet , BIBREF14,  fine-tuned a BERT model",
                    "type": "extractive"
                }
            ],
            "q_uid": "009ce6f2bea67e7df911b3f93443b23467c9f4a1",
            "evidence": [
                {
                    "raw_evidence": [
                        "Multi-BERT has showcased its ability to enable cross-lingual zero-shot learning on the natural language understanding tasks including XNLI BIBREF19, NER, POS, Dependency Parsing, and so on. We now seek to know if a pre-trained multi-BERT has ability to solve RC tasks in the zero-shot setting."
                    ],
                    "highlighted_evidence": [
                        "We now seek to know if a pre-trained multi-BERT has ability to solve RC tasks in the zero-shot setting."
                    ]
                },
                {
                    "raw_evidence": [
                        "Table TABREF6 shows the result of different models trained on either Chinese or English and tested on Chinese. In row (f), multi-BERT is fine-tuned on English but tested on Chinese, which achieves competitive performance compared with QANet trained on Chinese. We also find that multi-BERT trained on English has relatively lower EM compared with the model with comparable F1 scores. This shows that the model learned with zero-shot can roughly identify the answer spans in context but less accurate. In row (c), we fine-tuned a BERT model pre-trained on English monolingual corpus (English BERT) on Chinese RC training data directly by appending fastText-initialized Chinese word embeddings to the original word embeddings of English-BERT. Its F1 score is even lower than that of zero-shot transferring multi-BERT (rows (c) v.s. (e)). The result implies multi-BERT does acquire better cross-lingual capability through pre-training on multilingual corpus. Table TABREF8 shows the results of multi-BERT fine-tuned on different languages and then tested on English , Chinese and Korean. The top half of the table shows the results of training data without translation. It is not surprising that when the training and testing sets are in the same language, the best results are achieved, and multi-BERT shows transfer capability when training and testing sets are in different languages, especially between Chinese and Korean.",
                        "Reading Comprehension (RC) has become a central task in natural language processing, with great practical value in various industries. In recent years, many large-scale RC datasets in English BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5, BIBREF6 have nourished the development of numerous powerful and diverse RC models BIBREF7, BIBREF8, BIBREF9, BIBREF10, BIBREF11. The state-of-the-art model BIBREF12 on SQuAD, one of the most widely used RC benchmarks, even surpasses human-level performance. Nonetheless, RC on languages other than English has been limited due to the absence of sufficient training data. Although some efforts have been made to create RC datasets for Chinese BIBREF13, BIBREF14 and Korean BIBREF15, it is not feasible to collect RC datasets for every language since annotation efforts to collect a new RC dataset are often far from trivial. Therefore, the setup of transfer learning, especially zero-shot learning, is of extraordinary importance."
                    ],
                    "highlighted_evidence": [
                        "Table TABREF6 shows the result of different models trained on either Chinese or English and tested on Chinese. In row (f), multi-BERT is fine-tuned on English but tested on Chinese, which achieves competitive performance compared with QANet trained on Chinese.",
                        "BIBREF14",
                        " In row (c), we fine-tuned a BERT model pre-trained on English monolingual corpus (English BERT) on Chinese RC training data directly by appending fastText-initialized Chinese word embeddings to the original word embeddings of English-BERT. "
                    ]
                }
            ]
        },
        {
            "question": "what does the model learn in zero-shot setting?",
            "answers": [
                {
                    "answer": "we simply adopted the official training script of BERT, with default hyperparameters, to fine-tune each model until training loss converged",
                    "type": "extractive"
                }
            ],
            "q_uid": "55569d0a4586d20c01268a80a7e31a17a18198e2",
            "evidence": [
                {
                    "raw_evidence": [
                        "We have training and testing sets in three different languages: English, Chinese and Korean. The English dataset is SQuAD BIBREF2. The Chinese dataset is DRCD BIBREF14, a Chinese RC dataset with 30,000+ examples in the training set and 10,000+ examples in the development set. The Korean dataset is KorQuAD BIBREF15, a Korean RC dataset with 60,000+ examples in the training set and 10,000+ examples in the development set, created in exactly the same procedure as SQuAD. We always use the development sets of SQuAD, DRCD and KorQuAD for testing since the testing sets of the corpora have not been released yet.",
                        "The pre-trained multi-BERT is the official released one. This multi-lingual version of BERT were pre-trained on corpus in 104 languages. Data in different languages were simply mixed in batches while pre-training, without additional effort to align between languages. When fine-tuning, we simply adopted the official training script of BERT, with default hyperparameters, to fine-tune each model until training loss converged."
                    ],
                    "highlighted_evidence": [
                        "We have training and testing sets in three different languages: English, Chinese and Korean.",
                        "When fine-tuning, we simply adopted the official training script of BERT, with default hyperparameters, to fine-tune each model until training loss converged."
                    ]
                }
            ]
        }
    ],
    "1608.06378": [
        {
            "question": "What approach does this work propose for the new task?",
            "answers": [
                {
                    "answer": "We propose a listening comprehension model for the task defined above, the Attention-based Multi-hop Recurrent Neural Network (AMRNN) framework, and show that this model is able to perform reasonably well for the task. In the proposed approach, the audio of the stories is first transcribed into text by ASR, and the proposed model is developed to process the transcriptions for selecting the correct answer out of 4 choices given the question. ",
                    "type": "extractive"
                }
            ],
            "q_uid": "a53683d1a0647c80a4398ff8f4a03e11c0929be2",
            "evidence": [
                {
                    "raw_evidence": [
                        "We propose a listening comprehension model for the task defined above, the Attention-based Multi-hop Recurrent Neural Network (AMRNN) framework, and show that this model is able to perform reasonably well for the task. In the proposed approach, the audio of the stories is first transcribed into text by ASR, and the proposed model is developed to process the transcriptions for selecting the correct answer out of 4 choices given the question. The initial experiments showed that the proposed model achieves encouraging scores on the TOEFL listening comprehension test. The attention-mechanism proposed in this paper can be applied on either word or sentence levels. We found that sentence-level attention achieved better results on the manual transcriptions without ASR errors, but word-level attention outperformed the sentence-level on ASR transcriptions with errors."
                    ],
                    "highlighted_evidence": [
                        "We propose a listening comprehension model for the task defined above, the Attention-based Multi-hop Recurrent Neural Network (AMRNN) framework, and show that this model is able to perform reasonably well for the task. In the proposed approach, the audio of the stories is first transcribed into text by ASR, and the proposed model is developed to process the transcriptions for selecting the correct answer out of 4 choices given the question. "
                    ]
                }
            ]
        },
        {
            "question": "What is the new task proposed in this work?",
            "answers": [
                {
                    "answer": " listening comprehension task ",
                    "type": "extractive"
                }
            ],
            "q_uid": "0fd7d12711dfe0e35467a7ee6525127378a1bacb",
            "evidence": [
                {
                    "raw_evidence": [
                        "With the popularity of shared videos, social networks, online course, etc, the quantity of multimedia or spoken content is growing much faster beyond what human beings can view or listen to. Accessing large collections of multimedia or spoken content is difficult and time-consuming for humans, even if these materials are more attractive for humans than plain text information. Hence, it will be great if the machine can automatically listen to and understand the spoken content, and even visualize the key information for humans. This paper presents an initial attempt towards the above goal: machine comprehension of spoken content. In an initial task, we wish the machine can listen to and understand an audio story, and answer the questions related to that audio content. TOEFL listening comprehension test is for human English learners whose native language is not English. This paper reports how today's machine can perform with such a test.",
                        "The listening comprehension task considered here is highly related to Spoken Question Answering (SQA) BIBREF0 , BIBREF1 . In SQA, when the users enter questions in either text or spoken form, the machine needs to find the answer from some audio files. SQA usually worked with ASR transcripts of the spoken content, and used information retrieval (IR) techniques BIBREF2 or relied on knowledge bases BIBREF3 to find the proper answer. Sibyl BIBREF4 , a factoid SQA system, used some IR techniques and utilized several levels of linguistic information to deal with the task. Question Answering in Speech Transcripts (QAST) BIBREF5 , BIBREF6 , BIBREF7 has been a well-known evaluation program of SQA for years. However, most previous works on SQA mainly focused on factoid questions like \u201cWhat is name of the highest mountain in Taiwan?\u201d. Sometimes this kind of questions may be correctly answered by simply extracting the key terms from a properly chosen utterance without understanding the given spoken content. More difficult questions that cannot be answered without understanding the whole spoken content seemed rarely dealt with previously."
                    ],
                    "highlighted_evidence": [
                        "TOEFL listening comprehension test is for human English learners whose native language is not English. This paper reports how today's machine can perform with such a test.\n\nThe listening comprehension task considered here is highly related to Spoken Question Answering (SQA) BIBREF0 , BIBREF1 . "
                    ]
                }
            ]
        }
    ],
    "1909.07512": [
        {
            "question": "What news dataset was used?",
            "answers": [
                {
                    "answer": "collection of headlines published by HuffPost BIBREF12 between 2012 and 2018",
                    "type": "extractive"
                }
            ],
            "q_uid": "2e3265d83d2a595293ed458152d3ee76ad19e244",
            "evidence": [
                {
                    "raw_evidence": [
                        "The News Category Dataset BIBREF11 is a collection of headlines published by HuffPost BIBREF12 between 2012 and 2018, and was obtained online from Kaggle BIBREF13. The full dataset contains 200k news headlines with category labels, publication dates, and short text descriptions. For this analysis, a sample of roughly 33k headlines spanning 23 categories was used. Further analysis can be found in table SECREF12 in the appendix."
                    ],
                    "highlighted_evidence": [
                        "The News Category Dataset BIBREF11 is a collection of headlines published by HuffPost BIBREF12 between 2012 and 2018, and was obtained online from Kaggle BIBREF13. The full dataset contains 200k news headlines with category labels, publication dates, and short text descriptions."
                    ]
                }
            ]
        },
        {
            "question": "How do they determine similarity between predicted word and topics?",
            "answers": [
                {
                    "answer": "number of relevant output words as a function of the headline\u2019s category label",
                    "type": "extractive"
                }
            ],
            "q_uid": "c2432884287dca4af355698a543bc0db67a8c091",
            "evidence": [
                {
                    "raw_evidence": [
                        "To test the proposed methods ability to generate unsupervised words, it was necessary to devise a method of measuring word relevance. Topic modeling was used based on the assumption that words found in the same topic are more relevant to one another then words from different topics BIBREF14. The complete 200k headline dataset BIBREF11 was modeled using a Na\u00efve Bayes Algorithm BIBREF15 to create a word-category co-occurrence model. The top 200 most relevant words were then found for each category and used to create the topic table SECREF12. It was assumed that each category represented its own unique topic.",
                        "The number of relevant output words as a function of the headline\u2019s category label were measured, and can be found in figure SECREF4. The results demonstrate that the proposed method could correctly identify new words relevant to the input topic at a signal to noise ratio of 4 to 1."
                    ],
                    "highlighted_evidence": [
                        "To test the proposed methods ability to generate unsupervised words, it was necessary to devise a method of measuring word relevance. Topic modeling was used based on the assumption that words found in the same topic are more relevant to one another then words from different topics BIBREF14. The complete 200k headline dataset BIBREF11 was modeled using a Na\u00efve Bayes Algorithm BIBREF15 to create a word-category co-occurrence model. The top 200 most relevant words were then found for each category and used to create the topic table SECREF12. It was assumed that each category represented its own unique topic.\n\nThe number of relevant output words as a function of the headline\u2019s category label were measured, and can be found in figure SECREF4. The results demonstrate that the proposed method could correctly identify new words relevant to the input topic at a signal to noise ratio of 4 to 1."
                    ]
                }
            ]
        },
        {
            "question": "What is the language model pre-trained on?",
            "answers": [
                {
                    "answer": "Wikipedea Corpus and BooksCorpus",
                    "type": "extractive"
                }
            ],
            "q_uid": "226ae469a65611f041de3ae545be0e386dba7d19",
            "evidence": [
                {
                    "raw_evidence": [
                        "This method is unique since it avoids needing a prior dataset by using the information found within the weights of a general language model. Word embedding models, and BERT in particular, contain vast amounts of information collected through the course of their training. BERT Base for instance, has 110 Million parameters and was trained on both Wikipedea Corpus and BooksCorpus BIBREF0, a combined collection of over 3 Billion words. The full potential of such vastly trained general language models is still unfolding. This paper demonstrates that by carefully prompting and analysing these models, it is possible to extract new information from them, and extend short-text analysis beyond the limitations posed by word count."
                    ],
                    "highlighted_evidence": [
                        "Word embedding models, and BERT in particular, contain vast amounts of information collected through the course of their training. BERT Base for instance, has 110 Million parameters and was trained on both Wikipedea Corpus and BooksCorpus BIBREF0, a combined collection of over 3 Billion words."
                    ]
                }
            ]
        }
    ],
    "2004.03762": [
        {
            "question": "What metrics are used for evaluation?",
            "answers": [
                {
                    "answer": "ROUGE BIBREF29 and METEOR BIBREF30",
                    "type": "extractive"
                }
            ],
            "q_uid": "41bff17f7d7e899c03b051e20ef01f0ebc5c8bb1",
            "evidence": [
                {
                    "raw_evidence": [
                        "We automatically evaluate on four different types of interpolations (where different combinations of sentences are removed and the model is forced to regenerate them), We evaluate the generations with the ROUGE BIBREF29 and METEOR BIBREF30 metrics using the true sentences as targets. Table TABREF33 shows the automatic evaluation results from interpolations using our proposed models and baselines. The #Sent(s) column indicates which sentence(s) were removed, and then regenerated by the model. We gave the baselines a slight edge over SLDS because they pick the best out of 1000 samples while SLDS is only out of 50. The SLDS models see their largest gain over the baseline models when at least the first sentence is given as an input. The baseline models do better when the first and second sentence need to be imputed. This is likely due to the fact that having access to the earlier sentences allows a better initialization for the Gibbs sampler. Surprisingly, the semi-supervised variants of the SLDS models achieve higher scores. The reasons for this is discussed below in the Perplexity section."
                    ],
                    "highlighted_evidence": [
                        "We evaluate the generations with the ROUGE BIBREF29 and METEOR BIBREF30 metrics using the true sentences as targets."
                    ]
                }
            ]
        },
        {
            "question": "What baselines are used?",
            "answers": [
                {
                    "answer": "a two layer recurrent neural language model with GRU cells of hidden size 512, a two layer neural sequence to sequence model equipped with bi-linear attention function with GRU cells of hidden size 512, a linear dynamical system, semi-supervised SLDS models with varying amount of labelled sentiment tags",
                    "type": "extractive"
                }
            ],
            "q_uid": "b03e8e9a0cd2a44a215082773c7338f2f3be412a",
            "evidence": [
                {
                    "raw_evidence": [
                        "Language Model (LM): We train a two layer recurrent neural language model with GRU cells of hidden size 512.",
                        "Sequence-to-Sequence Attention Model (S2S): We train a two layer neural sequence to sequence model equipped with bi-linear attention function with GRU cells of hidden size 512. Sentiments tags for a narrative (1 for each sentence) are given as input to the model and the corresponding sentences are concatenated together as the output with only one <eos> tag at the end. This model is trained with a 0.1 dropout. This model is comparable to the static model of BIBREF7, and other recent works employing a notion of scaffolding into neural generation (albeit adapted for our setting).",
                        "Linear Dynamical System (LDS): We also train a linear dynamical system as discussed in Section SECREF1 as one of our baselines for fair comparisons. Apart from having just a single transition matrix this model has the same architectural details as SLDS.",
                        "Semi-Supervised SLDS (SLDS-X%): To gauge the usability of semi-supervision, we also train semi-supervised SLDS models with varying amount of labelled sentiment tags unlike the original model which uses 100% tagged data. We refer to these as SLDS-X%, where X is the % labelled data used for training: 1%, 10%, 25%, and 50%."
                    ],
                    "highlighted_evidence": [
                        "Language Model (LM): We train a two layer recurrent neural language model with GRU cells of hidden size 512.\n\n",
                        "Sequence-to-Sequence Attention Model (S2S): We train a two layer neural sequence to sequence model equipped with bi-linear attention function with GRU cells of hidden size 512. ",
                        "Linear Dynamical System (LDS): We also train a linear dynamical system as discussed in Section SECREF1 as one of our baselines for fair comparisons.",
                        "Semi-Supervised SLDS (SLDS-X%): To gauge the usability of semi-supervision, we also train semi-supervised SLDS models with varying amount of labelled sentiment tags unlike the original model which uses 100% tagged data. "
                    ]
                }
            ]
        }
    ],
    "1612.09113": [
        {
            "question": "What is the baseline?",
            "answers": [
                {
                    "answer": "The baseline is a multi-task architecture inspired by another paper.",
                    "type": "abstractive"
                }
            ],
            "q_uid": "85e45b37408bb353c6068ba62c18e516d4f67fe9",
            "evidence": [
                {
                    "raw_evidence": [
                        "We compare the results of our model to a baseline multi-task architecture inspired by yang2016multi. In our baseline model there are no explicit connections between tasks - the only shared parameters are in the hidden layer."
                    ],
                    "highlighted_evidence": [
                        "We compare the results of our model to a baseline multi-task architecture inspired by yang2016multi. In our baseline model there are no explicit connections between tasks - the only shared parameters are in the hidden layer."
                    ]
                }
            ]
        },
        {
            "question": "What is the unsupervised task in the final layer?",
            "answers": [
                {
                    "answer": "Language Modeling",
                    "type": "extractive"
                }
            ],
            "q_uid": "f4e1d2276d3fc781b686d2bb44eead73e06fbf3f",
            "evidence": [
                {
                    "raw_evidence": [
                        "In our model we represent linguistically motivated hierarchies in a multi-task Bi-Directional Recurrent Neural Network where junior tasks in the hierarchy are supervised at lower layers.This architecture builds upon sogaard2016deep, but is adapted in two ways: first, we add an unsupervised sequence labeling task (Language Modeling), second, we add a low-dimensional embedding layer between tasks in the hierarchy to learn dense representations of label tags. In addition to sogaard2016deep."
                    ],
                    "highlighted_evidence": [
                        "This architecture builds upon sogaard2016deep, but is adapted in two ways: first, we add an unsupervised sequence labeling task (Language Modeling), second, we add a low-dimensional embedding layer between tasks in the hierarchy to learn dense representations of label tags."
                    ]
                }
            ]
        }
    ],
    "1911.08673": [
        {
            "question": "What are performance compared to former models?",
            "answers": [
                {
                    "answer": "model overall still gives 1.0% higher average UAS and LAS than the previous best parser, BIAF, our model reports more than 1.0% higher average UAS than STACKPTR and 0.3% higher than BIAF",
                    "type": "extractive"
                }
            ],
            "q_uid": "24014a040447013a8cf0c0f196274667320db79f",
            "evidence": [
                {
                    "raw_evidence": [
                        "Table TABREF11 presents the results on 14 treebanks from the CoNLL shared tasks. Our model yields the best results on both UAS and LAS metrics of all languages except the Japanese. As for Japanese, our model gives unsatisfactory results because the original treebank was written in Roman phonetic characters instead of hiragana, which is used by both common Japanese writing and our pre-trained embeddings. Despite this, our model overall still gives 1.0% higher average UAS and LAS than the previous best parser, BIAF.",
                        "Following BIBREF23, we report results on the test sets of 12 different languages from the UD treebanks along with the current state-of-the-art: BIAF and STACKPTR. Although both BIAF and STACKPTR parsers have achieved relatively high parsing accuracies on the 12 languages and have all UAS higher than 90%, our model achieves state-of-the-art results in all languages for both UAS and LAS. Overall, our model reports more than 1.0% higher average UAS than STACKPTR and 0.3% higher than BIAF."
                    ],
                    "highlighted_evidence": [
                        "Our model yields the best results on both UAS and LAS metrics of all languages except the Japanese. As for Japanese, our model gives unsatisfactory results because the original treebank was written in Roman phonetic characters instead of hiragana, which is used by both common Japanese writing and our pre-trained embeddings. Despite this, our model overall still gives 1.0% higher average UAS and LAS than the previous best parser, BIAF.",
                        "Although both BIAF and STACKPTR parsers have achieved relatively high parsing accuracies on the 12 languages and have all UAS higher than 90%, our model achieves state-of-the-art results in all languages for both UAS and LAS. Overall, our model reports more than 1.0% higher average UAS than STACKPTR and 0.3% higher than BIAF."
                    ]
                }
            ]
        }
    ],
    "2003.08370": [
        {
            "question": "How much labeled data is available for these two languages?",
            "answers": [
                {
                    "answer": "10k training and 1k test, 1,101 sentences (26k tokens)",
                    "type": "extractive"
                }
            ],
            "q_uid": "06202ab8b28dcf3991523cf163b8844b42b9fc99",
            "evidence": [
                {
                    "raw_evidence": [
                        "The Hausa data used in this paper is part of the LORELEI language pack. It consists of Broad Operational Language Translation (BOLT) data gathered from news sites, forums, weblogs, Wikipedia articles and twitter messages. We use a split of 10k training and 1k test instances. Due to the Hausa data not being publicly available at the time of writing, we could only perform a limited set of experiments on it.",
                        "The Yor\u00f9b\u00e1 NER data used in this work is the annotated corpus of Global Voices news articles recently released by BIBREF22. The dataset consists of 1,101 sentences (26k tokens) divided into 709 training sentences, 113 validation sentences and 279 test sentences based on 65%/10%/25% split ratio. The named entities in the dataset are personal names (PER), organization (ORG), location (LOC) and date & time (DATE). All other tokens are assigned a tag of \"O\"."
                    ],
                    "highlighted_evidence": [
                        "The Hausa data used in this paper is part of the LORELEI language pack. It consists of Broad Operational Language Translation (BOLT) data gathered from news sites, forums, weblogs, Wikipedia articles and twitter messages. We use a split of 10k training and 1k test instances.",
                        "The Yor\u00f9b\u00e1 NER data used in this work is the annotated corpus of Global Voices news articles recently released by BIBREF22. The dataset consists of 1,101 sentences (26k tokens) divided into 709 training sentences, 113 validation sentences and 279 test sentences based on 65%/10%/25% split ratio."
                    ]
                }
            ]
        },
        {
            "question": "What classifiers were used in experiments?",
            "answers": [
                {
                    "answer": "Bi-LSTM, BERT",
                    "type": "extractive"
                }
            ],
            "q_uid": "288613077787159e512e46b79190c91cd4e5b04d",
            "evidence": [
                {
                    "raw_evidence": [
                        "The Bi-LSTM model consists of a Bi-LSTM layer followed by a linear layer to extract input features. The Bi-LSTM layer has a 300-dimensional hidden state for each direction. For the final classification, an additional linear layer is added to output predicted class distributions. For noise handling, we experiment with the Confusion Matrix model by BIBREF38 and the Cleaning model by BIBREF39. We repeat all the Bi-LSTM experiments 20 times and report the average F1-score (following the approach by BIBREF41) and the standard error.",
                        "The BERT model is obtained by fine-tuning the pre-trained BERT embeddings on NER data with an additional untrained CRF classifier. We fine-tuned all the parameters of BERT including that of the CRF end-to-end. This has been shown to give better performance than using word features extracted from BERT to train a classifier BIBREF19. The evaluation result is obtained as an average of 5 runs, we report the F1-score and the standard error in the result section."
                    ],
                    "highlighted_evidence": [
                        "The Bi-LSTM model consists of a Bi-LSTM layer followed by a linear layer to extract input features. The Bi-LSTM layer has a 300-dimensional hidden state for each direction. For the final classification, an additional linear layer is added to output predicted class distributions.",
                        "The BERT model is obtained by fine-tuning the pre-trained BERT embeddings on NER data with an additional untrained CRF classifier. We fine-tuned all the parameters of BERT including that of the CRF end-to-end."
                    ]
                }
            ]
        },
        {
            "question": "In which countries are Hausa and Yor\\`ub\\'a spoken?",
            "answers": [
                {
                    "answer": "Nigeria, Benin, Ghana, Cameroon, Togo, C\u00f4te d'Ivoire, Chad, Burkina Faso, and Sudan, Republic of Togo, Ghana, C\u00f4te d'Ivoire, Sierra Leone, Cuba and Brazil",
                    "type": "extractive"
                }
            ],
            "q_uid": "cf74ff49dfcdda2cd67a896b4b982a1c3ee51531",
            "evidence": [
                {
                    "raw_evidence": [
                        "Hausa language is the second most spoken indigenous language in Africa with over 40 million native speakers BIBREF20, and one of the three major languages in Nigeria, along with Igbo and Yor\u00f9b\u00e1. The language is native to the Northern part of Nigeria and the southern part of Niger, and it is widely spoken in West and Central Africa as a trade language in eight other countries: Benin, Ghana, Cameroon, Togo, C\u00f4te d'Ivoire, Chad, Burkina Faso, and Sudan. Hausa has several dialects but the one regarded as standard Hausa is the Kananci spoken in the ancient city of Kano in Nigeria. Kananci is the dialect popularly used in many local (e.g VON news) and international news media such as BBC, VOA, DW and Radio France Internationale. Hausa is a tone language but the tones are often ignored in writings, the language is written in a modified Latin alphabet. Despite the popularity of Hausa as an important regional language in Africa and it's popularity in news media, it has very little or no labelled data for common NLP tasks such as text classification, named entity recognition and question answering.",
                        "Yor\u00f9b\u00e1 language is the third most spoken indigenous language in Africa after Swahilli and Hausa with over 35 million native speakers BIBREF20. The language is native to the South-western part of Nigeria and the Southern part of Benin, and it is also spoken in other countries like Republic of Togo, Ghana, C\u00f4te d'Ivoire, Sierra Leone, Cuba and Brazil. Yor\u00f9b\u00e1 has several dialects but the written language has been standardized by the 1974 Joint Consultative Committee on Education BIBREF21, it has 25 letters without the Latin characters (c, q, v, x and z) and with additional characters (\u1eb9, gb, \u1e63 , \u1ecd). Yor\u00f9b\u00e1 is a tone language and the tones are represented as diacritics in written text, there are three tones in Yor\u00f9b\u00e1 namely low ( \\), mid (\u201c$-$\u201d) and high ($/$). The mid tone is usually ignored in writings. Often time articles written online including news articles like BBC and VON ignore diacritics. Ignoring diacritics makes it difficult to identify or pronounce words except they are in a context. For example, ow\u00f3 (money), \u1ecdw (broom), \u00f2w\u00f2 (business), w (honour), \u1ecdw (hand), and w (group) will be mapped to owo without diacritics. Similar to the Hausa language, there are few or no labelled datasets for NLP tasks."
                    ],
                    "highlighted_evidence": [
                        "Hausa language is the second most spoken indigenous language in Africa with over 40 million native speakers BIBREF20, and one of the three major languages in Nigeria, along with Igbo and Yor\u00f9b\u00e1. The language is native to the Northern part of Nigeria and the southern part of Niger, and it is widely spoken in West and Central Africa as a trade language in eight other countries: Benin, Ghana, Cameroon, Togo, C\u00f4te d'Ivoire, Chad, Burkina Faso, and Sudan.",
                        "Yor\u00f9b\u00e1 language is the third most spoken indigenous language in Africa after Swahilli and Hausa with over 35 million native speakers BIBREF20. The language is native to the South-western part of Nigeria and the Southern part of Benin, and it is also spoken in other countries like Republic of Togo, Ghana, C\u00f4te d'Ivoire, Sierra Leone, Cuba and Brazil."
                    ]
                }
            ]
        }
    ],
    "1912.01673": [
        {
            "question": "How many sentence transformations on average are available per unique sentence in dataset?",
            "answers": [
                {
                    "answer": "27.41 transformation on average of single seed sentence is available in dataset.",
                    "type": "abstractive"
                }
            ],
            "q_uid": "a3d83c2a1b98060d609e7ff63e00112d36ce2607",
            "evidence": [
                {
                    "raw_evidence": [
                        "In the second round, we collected 293 annotations from 12 annotators. After Korektor, there are 4262 unique sentences (including 150 seed sentences) that form the COSTRA 1.0 dataset. Statistics of individual annotators are available in tab:statistics."
                    ],
                    "highlighted_evidence": [
                        "After Korektor, there are 4262 unique sentences (including 150 seed sentences) that form the COSTRA 1.0 dataset."
                    ]
                }
            ]
        },
        {
            "question": "What annotations are available in the dataset?",
            "answers": [
                {
                    "answer": "For each source sentence, transformation sentences that are transformed according to some criteria (paraphrase, minimal change etc.)",
                    "type": "abstractive"
                }
            ],
            "q_uid": "aeda22ae760de7f5c0212dad048e4984cd613162",
            "evidence": [
                {
                    "raw_evidence": [
                        "We asked for two distinct paraphrases of each sentence because we believe that a good sentence embedding should put paraphrases close together in vector space.",
                        "Several modification types were specifically selected to constitute a thorough test of embeddings. In different meaning, the annotators should create a sentence with some other meaning using the same words as the original sentence. Other transformations which should be difficult for embeddings include minimal change, in which the sentence meaning should be significantly changed by using only very small modification, or nonsense, in which words of the source sentence should be shuffled so that it is grammatically correct, but without any sense."
                    ],
                    "highlighted_evidence": [
                        "We asked for two distinct paraphrases of each sentence because we believe that a good sentence embedding should put paraphrases close together in vector space.\n\nSeveral modification types were specifically selected to constitute a thorough test of embeddings. In different meaning, the annotators should create a sentence with some other meaning using the same words as the original sentence. Other transformations which should be difficult for embeddings include minimal change, in which the sentence meaning should be significantly changed by using only very small modification, or nonsense, in which words of the source sentence should be shuffled so that it is grammatically correct, but without any sense."
                    ]
                }
            ]
        },
        {
            "question": "How are possible sentence transformations represented in dataset, as new sentences?",
            "answers": [
                {
                    "answer": "Yes, as new sentences.",
                    "type": "abstractive"
                }
            ],
            "q_uid": "d5fa26a2b7506733f3fa0973e2fe3fc1bbd1a12d",
            "evidence": [
                {
                    "raw_evidence": [
                        "In the second round, we collected 293 annotations from 12 annotators. After Korektor, there are 4262 unique sentences (including 150 seed sentences) that form the COSTRA 1.0 dataset. Statistics of individual annotators are available in tab:statistics."
                    ],
                    "highlighted_evidence": [
                        "In the second round, we collected 293 annotations from 12 annotators. After Korektor, there are 4262 unique sentences (including 150 seed sentences) that form the COSTRA 1.0 dataset."
                    ]
                }
            ]
        },
        {
            "question": "Is this dataset publicly available?",
            "answers": [
                {
                    "answer": "Yes",
                    "type": "boolean"
                }
            ],
            "q_uid": "18482658e0756d69e39a77f8fcb5912545a72b9b",
            "evidence": [
                {
                    "raw_evidence": [
                        "The corpus is freely available at the following link:",
                        "http://hdl.handle.net/11234/1-3123"
                    ],
                    "highlighted_evidence": [
                        "The corpus is freely available at the following link:\n\nhttp://hdl.handle.net/11234/1-3123"
                    ]
                }
            ]
        },
        {
            "question": "Are some baseline models trained on this dataset?",
            "answers": [
                {
                    "answer": "Yes",
                    "type": "boolean"
                }
            ],
            "q_uid": "9d336c4c725e390b6eba8bb8fe148997135ee981",
            "evidence": [
                {
                    "raw_evidence": [
                        "We embedded COSTRA sentences with LASER BIBREF15, the method that performed very well in revealing linear relations in BaBo2019. Having browsed a number of 2D visualizations (PCA and t-SNE) of the space, we have to conclude that visually, LASER space does not seem to exhibit any of the desired topological properties discussed above, see fig:pca for one example."
                    ],
                    "highlighted_evidence": [
                        "We embedded COSTRA sentences with LASER BIBREF15, the method that performed very well in revealing linear relations in BaBo2019."
                    ]
                }
            ]
        },
        {
            "question": "Do they do any analysis of of how the modifications changed the starting set of sentences?",
            "answers": [
                {
                    "answer": "Yes",
                    "type": "boolean"
                }
            ],
            "q_uid": "016b59daa84269a93ce821070f4f5c1a71752a8a",
            "evidence": [
                {
                    "raw_evidence": [
                        "The lack of semantic relations in the LASER space is also reflected in vector similarities, summarized in similarities. The minimal change operation substantially changed the meaning of the sentence, and yet the embedding of the transformation lies very closely to the original sentence (average similarity of 0.930). Tense changes and some form of negation or banning also keep the vectors very similar."
                    ],
                    "highlighted_evidence": [
                        "The minimal change operation substantially changed the meaning of the sentence, and yet the embedding of the transformation lies very closely to the original sentence (average similarity of 0.930)."
                    ]
                }
            ]
        },
        {
            "question": "How do they introduce language variation?",
            "answers": [
                {
                    "answer": " we were looking for original and uncommon sentence change suggestions",
                    "type": "extractive"
                }
            ],
            "q_uid": "771b373d09e6eb50a74fffbf72d059ad44e73ab0",
            "evidence": [
                {
                    "raw_evidence": [
                        "We acquired the data in two rounds of annotation. In the first one, we were looking for original and uncommon sentence change suggestions. In the second one, we collected sentence alternations using ideas from the first round. The first and second rounds of annotation could be broadly called as collecting ideas and collecting data, respectively."
                    ],
                    "highlighted_evidence": [
                        "We acquired the data in two rounds of annotation. In the first one, we were looking for original and uncommon sentence change suggestions."
                    ]
                }
            ]
        },
        {
            "question": "Do they use external resources to make modifications to sentences?",
            "answers": [
                {
                    "answer": "No",
                    "type": "boolean"
                }
            ],
            "q_uid": "efb52bda7366d2b96545cf927f38de27de3b5b77",
            "evidence": [
                {
                    "raw_evidence": [],
                    "highlighted_evidence": []
                }
            ]
        }
    ],
    "1905.12801": [
        {
            "question": "which existing strategies are compared?",
            "answers": [
                {
                    "answer": "CDA, REG",
                    "type": "extractive"
                }
            ],
            "q_uid": "c59078efa7249acfb9043717237c96ae762c0a8c",
            "evidence": [
                {
                    "raw_evidence": [
                        "Initially, we measure the co-occurrence bias in the training data. After training the baseline model, we implement our loss function and tune for the INLINEFORM0 hyperparameter. We test the existing debiasing approaches, CDA and REG, as well but since BIBREF5 reported that results fluctuate substantially with different REG regularization coefficients, we perform hyperparameter tuning and report the best results in Table TABREF12 . Additionally, we implement a combination of our loss function and CDA and tune for INLINEFORM1 . Finally, bias evaluation is performed for all the trained models. Causal occupation bias is measured directly from the models using template datasets discussed above and co-occurrence bias is measured from the model-generated texts, which consist of 10,000 documents of 500 words each."
                    ],
                    "highlighted_evidence": [
                        "We test the existing debiasing approaches, CDA and REG, as well but since BIBREF5 reported that results fluctuate substantially with different REG regularization coefficients, we perform hyperparameter tuning and report the best results in Table TABREF12 ."
                    ]
                }
            ]
        },
        {
            "question": "what dataset was used?",
            "answers": [
                {
                    "answer": "Daily Mail news articles released by BIBREF9 ",
                    "type": "extractive"
                },
                {
                    "answer": "Daily Mail news articles",
                    "type": "extractive"
                }
            ],
            "q_uid": "73bddaaf601a4f944a3182ca0f4de85a19cdc1d2",
            "evidence": [
                {
                    "raw_evidence": [
                        "For the training data, we use Daily Mail news articles released by BIBREF9 . This dataset is composed of 219,506 articles covering a diverse range of topics including business, sports, travel, etc., and is claimed to be biased and sensational BIBREF5 . For manageability, we randomly subsample 5% of the text. The subsample has around 8.25 million tokens in total."
                    ],
                    "highlighted_evidence": [
                        "For the training data, we use Daily Mail news articles released by BIBREF9 . "
                    ]
                },
                {
                    "raw_evidence": [
                        "For the training data, we use Daily Mail news articles released by BIBREF9 . This dataset is composed of 219,506 articles covering a diverse range of topics including business, sports, travel, etc., and is claimed to be biased and sensational BIBREF5 . For manageability, we randomly subsample 5% of the text. The subsample has around 8.25 million tokens in total."
                    ],
                    "highlighted_evidence": [
                        "For the training data, we use Daily Mail news articles released by BIBREF9 ."
                    ]
                }
            ]
        },
        {
            "question": "what kinds of male and female words are looked at?",
            "answers": [
                {
                    "answer": "gendered word pairs like he and she",
                    "type": "extractive"
                }
            ],
            "q_uid": "d4e5e3f37679ff68914b55334e822ea18e60a6cf",
            "evidence": [
                {
                    "raw_evidence": [
                        "Language modelling is a pivotal task in NLP with important downstream applications such as text generation BIBREF4 . Recent studies by BIBREF0 and BIBREF5 have shown that this task is vulnerable to gender bias in the training corpus. Two prior works focused on reducing bias in language modelling by data preprocessing BIBREF0 and word embedding debiasing BIBREF5 . In this study, we investigate the efficacy of bias reduction during training by introducing a new loss function which encourages the language model to equalize the probabilities of predicting gendered word pairs like he and she. Although we recognize that gender is non-binary, for the purpose of this study, we focus on female and male words."
                    ],
                    "highlighted_evidence": [
                        "In this study, we investigate the efficacy of bias reduction during training by introducing a new loss function which encourages the language model to equalize the probabilities of predicting gendered word pairs like he and she. "
                    ]
                }
            ]
        },
        {
            "question": "how is mitigation of gender bias evaluated?",
            "answers": [
                {
                    "answer": "Using INLINEFORM0 and INLINEFORM1",
                    "type": "abstractive"
                }
            ],
            "q_uid": "5f60defb546f35d25a094ff34781cddd4119e400",
            "evidence": [
                {
                    "raw_evidence": [
                        "Results for the experiments are listed in Table TABREF12 . It is interesting to observe that the baseline model amplifies the bias in the training data set as measured by INLINEFORM0 and INLINEFORM1 . From measurements using the described bias metrics, our method effectively mitigates bias in language modelling without a significant increase in perplexity. At INLINEFORM2 value of 1, it reduces INLINEFORM3 by 58.95%, INLINEFORM4 by 45.74%, INLINEFORM5 by 100%, INLINEFORM6 by 98.52% and INLINEFORM7 by 98.98%. Compared to the results of CDA and REG, it achieves the best results in both occupation biases, INLINEFORM8 and INLINEFORM9 , and INLINEFORM10 . We notice that all methods result in INLINEFORM11 around 1, indicating that there are near equal amounts of female and male words in the generated texts. In our experiments we note that with increasing INLINEFORM12 , the bias steadily decreases and perplexity tends to slightly increase. This indicates that there is a trade-off between bias and perplexity."
                    ],
                    "highlighted_evidence": [
                        "It is interesting to observe that the baseline model amplifies the bias in the training data set as measured by INLINEFORM0 and INLINEFORM1 . From measurements using the described bias metrics, our method effectively mitigates bias in language modelling without a significant increase in perplexity."
                    ]
                }
            ]
        },
        {
            "question": "what bias evaluation metrics are used?",
            "answers": [
                {
                    "answer": "gender bias, normalized version of INLINEFORM0, ratio of occurrence of male and female words in the model generated text, Causal occupation bias conditioned on occupation, causal occupation bias conditioned on gender, INLINEFORM1",
                    "type": "extractive"
                }
            ],
            "q_uid": "90d946ccc3abf494890e147dd85bd489b8f3f0e8",
            "evidence": [
                {
                    "raw_evidence": [
                        "Co-occurrence bias is computed from the model-generated texts by comparing the occurrences of all gender-neutral words with female and male words. A word is considered to be biased towards a certain gender if it occurs more frequently with words of that gender. This definition was first used by BIBREF7 and later adapted by BIBREF5 . Using the definition of gender bias similar to the one used by BIBREF5 , we define gender bias as INLINEFORM0",
                        "where INLINEFORM0 is a set of gender-neutral words, and INLINEFORM1 is the occurrences of a word INLINEFORM2 with words of gender INLINEFORM3 in the same window. This score is designed to capture unequal co-occurrences of neutral words with male and female words. Co-occurrences are computed using a sliding window of size 10 extending equally in both directions. Furthermore, we only consider words that occur more than 20 times with gendered words to exclude random effects.",
                        "We also evaluate a normalized version of INLINEFORM0 which we denote by conditional co-occurrence bias, INLINEFORM1 . This is defined as INLINEFORM2",
                        "INLINEFORM0 is less affected by the disparity in the general distribution of male and female words in the text. The disparity between the occurrences of the two genders means that text is more inclined to mention one over the other, so it can also be considered a form of bias. We report the ratio of occurrence of male and female words in the model generated text, INLINEFORM1 , as INLINEFORM2",
                        "Causal occupation bias conditioned on occupation is represented as INLINEFORM0",
                        "Here, the vertical bar separates the seed sequence that is fed into the language models from the target occupation, for which we observe the output softmax probability. We measure causal occupation bias conditioned on gender as INLINEFORM0",
                        "where INLINEFORM0 is a set of gender-neutral occupations and INLINEFORM1 is the size of the gender pairs set. For example, INLINEFORM2 is the softmax probability of the word INLINEFORM3 where the seed sequence is He is a. The second set of templates like below, aims to capture how the probabilities of gendered words depend on the occupation words in the seed. INLINEFORM4",
                        "Our debiasing approach does not explicitly address the bias in the embedding layer. Therefore, we use gender-neutral occupations to measure the embedding bias to observe if debiasing the output layer also decreases the bias in the embedding. We define the embedding bias, INLINEFORM0 , as the difference between the Euclidean distance of an occupation word to male words and the distance of the occupation word to the female counterparts. This definition is equivalent to bias by projection described by BIBREF6 . We define INLINEFORM1 as INLINEFORM2",
                        "where INLINEFORM0 is a set of gender-neutral occupations, INLINEFORM1 is the size of the gender pairs set and INLINEFORM2 is the word-to-vector dictionary."
                    ],
                    "highlighted_evidence": [
                        "Using the definition of gender bias similar to the one used by BIBREF5 , we define gender bias as INLINEFORM0\n\nwhere INLINEFORM0 is a set of gender-neutral words, and INLINEFORM1 is the occurrences of a word INLINEFORM2 with words of gender INLINEFORM3 in the same window. ",
                        "We also evaluate a normalized version of INLINEFORM0 which we denote by conditional co-occurrence bias, INLINEFORM1 . ",
                        "We report the ratio of occurrence of male and female words in the model generated text, INLINEFORM1 , as INLINEFORM2",
                        "Causal occupation bias conditioned on occupation is represented as INLINEFORM0",
                        "We measure causal occupation bias conditioned on gender as INLINEFORM0\n\nwhere INLINEFORM0 is a set of gender-neutral occupations and INLINEFORM1 is the size of the gender pairs set.",
                        "We define INLINEFORM1 as INLINEFORM2\n\nwhere INLINEFORM0 is a set of gender-neutral occupations, INLINEFORM1 is the size of the gender pairs set and INLINEFORM2 is the word-to-vector dictionary."
                    ]
                }
            ]
        }
    ],
    "1705.05437": [
        {
            "question": "What is NER?",
            "answers": [
                {
                    "answer": "Named Entity Recognition",
                    "type": "extractive"
                },
                {
                    "answer": "Named Entity Recognition, including entities such as proteins, genes, diseases, treatments, drugs, etc. in the biomedical domain",
                    "type": "abstractive"
                }
            ],
            "q_uid": "cf874cd9023d901e10aa8664b813d32501e7e4d2",
            "evidence": [
                {
                    "raw_evidence": [
                        "Named Entity Recognition (NER) in the Biomedical domain usually includes recognition of entities such as proteins, genes, diseases, treatments, drugs, etc. Fact extraction involves extraction of Named Entities from a corpus, usually given a certain ontology. When compared to NER in the domain of general text, the biomedical domain has some characteristic challenges:"
                    ],
                    "highlighted_evidence": [
                        "Named Entity Recognition (NER) in the Biomedical domain usually includes recognition of entities such as proteins, genes, diseases, treatments, drugs, etc. "
                    ]
                },
                {
                    "raw_evidence": [
                        "Named Entity Recognition (NER) in the Biomedical domain usually includes recognition of entities such as proteins, genes, diseases, treatments, drugs, etc. Fact extraction involves extraction of Named Entities from a corpus, usually given a certain ontology. When compared to NER in the domain of general text, the biomedical domain has some characteristic challenges:"
                    ],
                    "highlighted_evidence": [
                        "Named Entity Recognition (NER) in the Biomedical domain usually includes recognition of entities such as proteins, genes, diseases, treatments, drugs, etc."
                    ]
                }
            ]
        },
        {
            "question": "Does the paper explore extraction from electronic health records?",
            "answers": [
                {
                    "answer": "Yes",
                    "type": "boolean"
                }
            ],
            "q_uid": "42084c41343e5a6ae58a22e5bfc5ce987b5173de",
            "evidence": [
                {
                    "raw_evidence": [
                        "BIBREF5 adopt a machine learning approach for NER. Their NER system extracts medical problems, tests and treatments from discharge summaries and progress notes. They use a semi-Conditional Random Field (semi-CRF) BIBREF6 to output labels over all tokens in the sentence. They use a variety of token, context and sentence level features. They also use some concept mapping features using existing annotation tools, as well as Brown clustering to form 128 clusters over the unlabelled data. The dataset used is the i2b2 2010 challenge dataset. Their system achieves an F-Score of 0.85. BIBREF7 is an incremental paper on NER taggers. It uses 3 types of word-representation techniques (Brown clustering, distributional clustering, word vectors) to improve performance of the NER Conditional Random Field tagger, and achieves marginal F-Score improvements."
                    ],
                    "highlighted_evidence": [
                        "BIBREF5 adopt a machine learning approach for NER. Their NER system extracts medical problems, tests and treatments from discharge summaries and progress notes.",
                        "The dataset used is the i2b2 2010 challenge dataset. "
                    ]
                }
            ]
        }
    ],
    "1706.00139": [
        {
            "question": "What is the difference of the proposed model with a standard RNN encoder-decoder?",
            "answers": [
                {
                    "answer": "Introduce a \"Refinement Adjustment LSTM-based component\" to the decoder",
                    "type": "abstractive"
                }
            ],
            "q_uid": "981fd79dd69581659cb1d4e2b29178e82681eb4d",
            "evidence": [
                {
                    "raw_evidence": [
                        "While the RNN-based generators with DA gating-vector can prevent the undesirable semantic repetitions, the ARED-based generators show signs of better adapting to a new domain. However, none of the models show significant advantage from out-of-domain data. To better analyze model generalization to an unseen, new domain as well as model leveraging the out-of-domain sources, we propose a new architecture which is an extension of the ARED model. In order to better select, aggregate and control the semantic information, a Refinement Adjustment LSTM-based component (RALSTM) is introduced to the decoder side. The proposed model can learn from unaligned data by jointly training the sentence planning and surface realization to produce natural language sentences. We conducted experiments on four different NLG domains and found that the proposed methods significantly outperformed the state-of-the-art methods regarding BLEU BIBREF15 and slot error rate ERR scores BIBREF4 . The results also showed that our generators could scale to new domains by leveraging the out-of-domain data. To sum up, we make three key contributions in this paper:"
                    ],
                    "highlighted_evidence": [
                        "To better analyze model generalization to an unseen, new domain as well as model leveraging the out-of-domain sources, we propose a new architecture which is an extension of the ARED model. In order to better select, aggregate and control the semantic information, a Refinement Adjustment LSTM-based component (RALSTM) is introduced to the decoder side. The proposed model can learn from unaligned data by jointly training the sentence planning and surface realization to produce natural language sentences. "
                    ]
                }
            ]
        },
        {
            "question": "Does the model evaluated on NLG datasets or dialog datasets?",
            "answers": [
                {
                    "answer": "NLG datasets",
                    "type": "abstractive"
                },
                {
                    "answer": "NLG datasets",
                    "type": "abstractive"
                }
            ],
            "q_uid": "03e9ac1a2d90152cd041342a11293a1ebd33bcc3",
            "evidence": [
                {
                    "raw_evidence": [
                        "We assessed the proposed models on four different NLG domains: finding a restaurant, finding a hotel, buying a laptop, and buying a television. The Restaurant and Hotel were collected in BIBREF4 , while the Laptop and TV datasets have been released by BIBREF22 with a much larger input space but only one training example for each DA so that the system must learn partial realization of concepts and be able to recombine and apply them to unseen DAs. This makes the NLG tasks for the Laptop and TV domains become much harder. The dataset statistics are shown in Table 1 ."
                    ],
                    "highlighted_evidence": [
                        "We assessed the proposed models on four different NLG domains: finding a restaurant, finding a hotel, buying a laptop, and buying a television. The Restaurant and Hotel were collected in BIBREF4 , while the Laptop and TV datasets have been released by BIBREF22 with a much larger input space but only one training example for each DA so that the system must learn partial realization of concepts and be able to recombine and apply them to unseen DAs. "
                    ]
                },
                {
                    "raw_evidence": [
                        "We assessed the proposed models on four different NLG domains: finding a restaurant, finding a hotel, buying a laptop, and buying a television. The Restaurant and Hotel were collected in BIBREF4 , while the Laptop and TV datasets have been released by BIBREF22 with a much larger input space but only one training example for each DA so that the system must learn partial realization of concepts and be able to recombine and apply them to unseen DAs. This makes the NLG tasks for the Laptop and TV domains become much harder. The dataset statistics are shown in Table 1 ."
                    ],
                    "highlighted_evidence": [
                        "We assessed the proposed models on four different NLG domains: finding a restaurant, finding a hotel, buying a laptop, and buying a television. "
                    ]
                }
            ]
        }
    ],
    "1606.01433": [
        {
            "question": "How do they obtain distant supervision rules for predicting relations?",
            "answers": [
                {
                    "answer": "dominant temporal associations can be learned from training data",
                    "type": "extractive"
                }
            ],
            "q_uid": "4519afe91b1042876d7c021487d98e2d72a09861",
            "evidence": [
                {
                    "raw_evidence": [
                        "Phase 2: In order to predict the relationship between an event and the creation time of its parent document, we assign a DocRelTime random variable to every Timex3 and Event mention. For Events, these values are provided by the training data, for Timex3s we have to compute class labels. Around 42% of Timex3 mentions are simple dates (\u201c12/29/08\", \u201cOctober 16\", etc.) and can be naively canonicalized to a universal timestamp. This is done using regular expressions to identify common date patterns and heuristics to deal with incomplete dates. The missing year in \u201cOctober 16\", for example, can be filled in using the nearest preceding date mention; if that isn't available we use the document creation year. These mentions are then assigned a class using the parent document's DocTime value and any revision timestamps. Other Timex3 mentions are more ambiguous so we use a distant supervision approach. Phrases like \u201ccurrently\" and \u201ctoday's\" tend to occur near Events that overlap the current document creation time, while \u201cago\" or \u201c INLINEFORM0 -years\" indicate past events. These dominant temporal associations can be learned from training data and then used to label Timex3s. Finally, we define a logistic regression rule to predict entity DocRelTime values as well as specify a linear skip-chain factor over Event mentions and their nearest Timex3 neighbor, encoding the baseline system heuristic directly as an inference rule."
                    ],
                    "highlighted_evidence": [
                        "Other Timex3 mentions are more ambiguous so we use a distant supervision approach. Phrases like \u201ccurrently\" and \u201ctoday's\" tend to occur near Events that overlap the current document creation time, while \u201cago\" or \u201c INLINEFORM0 -years\" indicate past events. These dominant temporal associations can be learned from training data and then used to label Timex3s. Finally, we define a logistic regression rule to predict entity DocRelTime values as well as specify a linear skip-chain factor over Event mentions and their nearest Timex3 neighbor, encoding the baseline system heuristic directly as an inference rule."
                    ]
                }
            ]
        },
        {
            "question": "Which structured prediction approach do they adopt for temporal entity extraction?",
            "answers": [
                {
                    "answer": "DeepDive BIBREF1",
                    "type": "extractive"
                }
            ],
            "q_uid": "0cfaca6f3f33ebdb338c5f991f6a7a33ff33844d",
            "evidence": [
                {
                    "raw_evidence": [
                        "We examine a deep-learning approach to sequence labeling using a vanilla recurrent neural network (RNN) with word embeddings, as well as a joint inference, structured prediction approach using Stanford's knowledge base construction framework DeepDive BIBREF1 . Our DeepDive application outperformed the RNN and scored similarly to 2015's best-in-class extraction systems, even though it only used a small set of context window and dictionary features. Extraction performance, however lagged this year's best system submission. For document creation time relations, we again use DeepDive. Our system examined a simple temporal distant supervision rule for labeling time expressions and linking them to nearby event mentions via inference rules. Overall system performance was better than this year's median submission, but again fell short of the best system."
                    ],
                    "highlighted_evidence": [
                        "We examine a deep-learning approach to sequence labeling using a vanilla recurrent neural network (RNN) with word embeddings, as well as a joint inference, structured prediction approach using Stanford's knowledge base construction framework DeepDive BIBREF1 ."
                    ]
                }
            ]
        }
    ],
    "1905.11901": [
        {
            "question": "what were their experimental results in the low-resource dataset?",
            "answers": [
                {
                    "answer": "10.37 BLEU",
                    "type": "extractive"
                }
            ],
            "q_uid": "07d7652ad4a0ec92e6b44847a17c378b0d9f57f5",
            "evidence": [
                {
                    "raw_evidence": [
                        "Table TABREF21 shows results for Korean INLINEFORM0 English, using the same configurations (1, 2 and 8) as for German\u2013English. Our results confirm that the techniques we apply are successful across datasets, and result in stronger systems than previously reported on this dataset, achieving 10.37 BLEU as compared to 5.97 BLEU reported by gu-EtAl:2018:EMNLP1."
                    ],
                    "highlighted_evidence": [
                        "Table TABREF21 shows results for Korean INLINEFORM0 English, using the same configurations (1, 2 and 8) as for German\u2013English. Our results confirm that the techniques we apply are successful across datasets, and result in stronger systems than previously reported on this dataset, achieving 10.37 BLEU as compared to 5.97 BLEU reported by gu-EtAl:2018:EMNLP1."
                    ]
                }
            ]
        },
        {
            "question": "what are the methods they compare with in the korean-english dataset?",
            "answers": [
                {
                    "answer": "gu-EtAl:2018:EMNLP1",
                    "type": "extractive"
                }
            ],
            "q_uid": "9f3444c9fb2e144465d63abf58520cddd4165a01",
            "evidence": [
                {
                    "raw_evidence": [
                        "Table TABREF21 shows results for Korean INLINEFORM0 English, using the same configurations (1, 2 and 8) as for German\u2013English. Our results confirm that the techniques we apply are successful across datasets, and result in stronger systems than previously reported on this dataset, achieving 10.37 BLEU as compared to 5.97 BLEU reported by gu-EtAl:2018:EMNLP1."
                    ],
                    "highlighted_evidence": [
                        "Table TABREF21 shows results for Korean INLINEFORM0 English, using the same configurations (1, 2 and 8) as for German\u2013English. Our results confirm that the techniques we apply are successful across datasets, and result in stronger systems than previously reported on this dataset, achieving 10.37 BLEU as compared to 5.97 BLEU reported by gu-EtAl:2018:EMNLP1."
                    ]
                }
            ]
        },
        {
            "question": "what pitfalls are mentioned in the paper?",
            "answers": [
                {
                    "answer": "highly data-inefficient, underperform phrase-based statistical machine translation",
                    "type": "extractive"
                }
            ],
            "q_uid": "2348d68e065443f701d8052018c18daa4ecc120e",
            "evidence": [
                {
                    "raw_evidence": [
                        "While neural machine translation (NMT) has achieved impressive performance in high-resource data conditions, becoming dominant in the field BIBREF0 , BIBREF1 , BIBREF2 , recent research has argued that these models are highly data-inefficient, and underperform phrase-based statistical machine translation (PBSMT) or unsupervised methods in low-data conditions BIBREF3 , BIBREF4 . In this paper, we re-assess the validity of these results, arguing that they are the result of lack of system adaptation to low-resource settings. Our main contributions are as follows:"
                    ],
                    "highlighted_evidence": [
                        "While neural machine translation (NMT) has achieved impressive performance in high-resource data conditions, becoming dominant in the field BIBREF0 , BIBREF1 , BIBREF2 , recent research has argued that these models are highly data-inefficient, and underperform phrase-based statistical machine translation (PBSMT) or unsupervised methods in low-data conditions BIBREF3 , BIBREF4 . "
                    ]
                }
            ]
        }
    ],
    "2002.01320": [
        {
            "question": "What domains are covered in the corpus?",
            "answers": [
                {
                    "answer": "No specific domain is covered in the corpus.",
                    "type": "abstractive"
                }
            ],
            "q_uid": "6a219d7c58451842aa5d6819a7cdf51c55e9fc0f",
            "evidence": [
                {
                    "raw_evidence": [
                        "Common Voice BIBREF10 is a crowdsourcing speech recognition corpus with an open CC0 license. Contributors record voice clips by reading from a bank of donated sentences. Each voice clip was validated by at least two other users. Most of the sentences are covered by multiple speakers, with potentially different genders, age groups or accents."
                    ],
                    "highlighted_evidence": [
                        "Contributors record voice clips by reading from a bank of donated sentences."
                    ]
                }
            ]
        },
        {
            "question": "What is the architecture of their model?",
            "answers": [
                {
                    "answer": "follow the architecture in berard2018end, but have 3 decoder layers like that in pino2019harnessing",
                    "type": "extractive"
                }
            ],
            "q_uid": "cee8cfaf26e49d98e7d34fa1b414f8f31d6502ad",
            "evidence": [
                {
                    "raw_evidence": [
                        "Our ASR and ST models follow the architecture in berard2018end, but have 3 decoder layers like that in pino2019harnessing. For MT, we use a Transformer base architecture BIBREF15, but with 3 encoder layers, 3 decoder layers and 0.3 dropout. We use a batch size of 10,000 frames for ASR and ST, and a batch size of 4,000 tokens for MT. We train all models using Fairseq BIBREF20 for up to 200,000 updates. We use SpecAugment BIBREF21 for ASR and ST to alleviate overfitting."
                    ],
                    "highlighted_evidence": [
                        "Our ASR and ST models follow the architecture in berard2018end, but have 3 decoder layers like that in pino2019harnessing."
                    ]
                }
            ]
        },
        {
            "question": "How was the dataset collected?",
            "answers": [
                {
                    "answer": "Contributors record voice clips by reading from a bank of donated sentences.",
                    "type": "extractive"
                },
                {
                    "answer": "crowdsourcing",
                    "type": "extractive"
                }
            ],
            "q_uid": "f8f4e4a50d2b3fbd193327e79ea32d8d057e1414",
            "evidence": [
                {
                    "raw_evidence": [
                        "Raw CoVo data contains samples that passed validation as well as those that did not. To build CoVoST, we only use the former one and reuse the official train-development-test partition of the validated data. As of January 2020, the latest CoVo 2019-06-12 release includes 29 languages. CoVoST is currently built on that release and covers the following 11 languages: French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian and Chinese.",
                        "Common Voice BIBREF10 is a crowdsourcing speech recognition corpus with an open CC0 license. Contributors record voice clips by reading from a bank of donated sentences. Each voice clip was validated by at least two other users. Most of the sentences are covered by multiple speakers, with potentially different genders, age groups or accents."
                    ],
                    "highlighted_evidence": [
                        "As of January 2020, the latest CoVo 2019-06-12 release includes 29 languages. CoVoST is currently built on that release and covers the following 11 languages: French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian and Chinese.",
                        "Common Voice BIBREF10 is a crowdsourcing speech recognition corpus with an open CC0 license. Contributors record voice clips by reading from a bank of donated sentences"
                    ]
                },
                {
                    "raw_evidence": [
                        "Common Voice BIBREF10 is a crowdsourcing speech recognition corpus with an open CC0 license. Contributors record voice clips by reading from a bank of donated sentences. Each voice clip was validated by at least two other users. Most of the sentences are covered by multiple speakers, with potentially different genders, age groups or accents.",
                        "Tatoeba (TT) is a community built language learning corpus having sentences aligned across multiple languages with the corresponding speech partially available. Its sentences are on average shorter than those in CoVoST (see also Table TABREF2) given the original purpose of language learning. Sentences in TT are licensed under CC BY 2.0 FR and part of the speeches are available under various CC licenses."
                    ],
                    "highlighted_evidence": [
                        "Common Voice BIBREF10 is a crowdsourcing speech recognition corpus with an open CC0 license. ",
                        "Tatoeba (TT) is a community built language learning corpus having sentences aligned across multiple languages with the corresponding speech partially available."
                    ]
                }
            ]
        },
        {
            "question": "Which languages are part of the corpus?",
            "answers": [
                {
                    "answer": "French (Fr), German (De), Dutch (Nl), Russian (Ru), Spanish (Es), Italian (It), Turkish (Tr), Persian (Fa), Swedish (Sv), Mongolian (Mn) and Chinese (Zh)",
                    "type": "extractive"
                },
                {
                    "answer": "French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian and Chinese",
                    "type": "extractive"
                }
            ],
            "q_uid": "bc84c5a58c57038910f7720d7a784560054d3e1a",
            "evidence": [
                {
                    "raw_evidence": [
                        "In this paper, we introduce CoVoST, a multilingual ST corpus based on Common Voice BIBREF10 for 11 languages into English, diversified with over 11,000 speakers and over 60 accents. It includes a total 708 hours of French (Fr), German (De), Dutch (Nl), Russian (Ru), Spanish (Es), Italian (It), Turkish (Tr), Persian (Fa), Swedish (Sv), Mongolian (Mn) and Chinese (Zh) speeches, with French and German ones having the largest durations among existing public corpora. We also collect an additional evaluation corpus from Tatoeba for French, German, Dutch, Russian and Spanish, resulting in a total of 9.3 hours of speech. Both corpora are created at the sentence level and do not require additional alignments or segmentation. Using the official Common Voice train-development-test split, we also provide baseline models, including, to our knowledge, the first end-to-end many-to-one multilingual ST models. CoVoST is released under CC0 license and free to use. The Tatoeba evaluation samples are also available under friendly CC licenses. All the data can be acquired at https://github.com/facebookresearch/covost."
                    ],
                    "highlighted_evidence": [
                        "It includes a total 708 hours of French (Fr), German (De), Dutch (Nl), Russian (Ru), Spanish (Es), Italian (It), Turkish (Tr), Persian (Fa), Swedish (Sv), Mongolian (Mn) and Chinese (Zh) speeches, with French and German ones having the largest durations among existing public corpora."
                    ]
                },
                {
                    "raw_evidence": [
                        "Raw CoVo data contains samples that passed validation as well as those that did not. To build CoVoST, we only use the former one and reuse the official train-development-test partition of the validated data. As of January 2020, the latest CoVo 2019-06-12 release includes 29 languages. CoVoST is currently built on that release and covers the following 11 languages: French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian and Chinese.",
                        "We construct an evaluation set from TT (for French, German, Dutch, Russian and Spanish) as a complement to CoVoST development and test sets. We collect (speech, transcript, English translation) triplets for the 5 languages and do not include those whose speech has a broken URL or is not CC licensed. We further filter these samples by sentence lengths (minimum 4 words including punctuations) to reduce the portion of short sentences. This makes the resulting evaluation set closer to real-world scenarios and more challenging."
                    ],
                    "highlighted_evidence": [
                        "CoVoST is currently built on that release and covers the following 11 languages: French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian and Chinese.",
                        "We construct an evaluation set from TT (for French, German, Dutch, Russian and Spanish) as a complement to CoVoST development and test sets."
                    ]
                }
            ]
        },
        {
            "question": "How is the quality of the data empirically evaluated? ",
            "answers": [
                {
                    "answer": "Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets",
                    "type": "extractive"
                },
                {
                    "answer": "computed sentence-level BLEU, We manually inspected examples where the source transcript was identical to the translation, measured the perplexity of the translations, computed the ratio of English characters in the translations, calculate similarity scores between transcripts and translations",
                    "type": "extractive"
                }
            ],
            "q_uid": "29923a824c98b3ba85ced964a0e6a2af35758abe",
            "evidence": [
                {
                    "raw_evidence": [
                        "Validated transcripts were sent to professional translators. Note that the translators had access to the transcripts but not the corresponding voice clips since clips would not carry additional information. Since transcripts were duplicated due to multiple speakers, we deduplicated the transcripts before sending them to translators. As a result, different voice clips of the same content (transcript) will have identical translations in CoVoST for train, development and test splits.",
                        "In order to control the quality of the professional translations, we applied various sanity checks to the translations BIBREF11. 1) For German-English, French-English and Russian-English translations, we computed sentence-level BLEU BIBREF12 with the NLTK BIBREF13 implementation between the human translations and the automatic translations produced by a state-of-the-art system BIBREF14 (the French-English system was a Transformer big BIBREF15 separately trained on WMT14). We applied this method to these three language pairs only as we are confident about the quality of the corresponding systems. Translations with a score that was too low were manually inspected and sent back to the translators when needed. 2) We manually inspected examples where the source transcript was identical to the translation. 3) We measured the perplexity of the translations using a language model trained on a large amount of clean monolingual data BIBREF14. We manually inspected examples where the translation had a high perplexity and sent them back to translators accordingly. 4) We computed the ratio of English characters in the translations. We manually inspected examples with a low ratio and sent them back to translators accordingly. 5) Finally, we used VizSeq BIBREF16 to calculate similarity scores between transcripts and translations based on LASER cross-lingual sentence embeddings BIBREF17. Samples with low scores were manually inspected and sent back for translation when needed.",
                        "We also sanity check the overlaps of train, development and test sets in terms of transcripts and voice clips (via MD5 file hashing), and confirm they are totally disjoint."
                    ],
                    "highlighted_evidence": [
                        "Validated transcripts were sent to professional translators.",
                        "In order to control the quality of the professional translations, we applied various sanity checks to the translations BIBREF11.",
                        "We also sanity check the overlaps of train, development and test sets in terms of transcripts and voice clips (via MD5 file hashing), and confirm they are totally disjoint."
                    ]
                },
                {
                    "raw_evidence": [
                        "In order to control the quality of the professional translations, we applied various sanity checks to the translations BIBREF11. 1) For German-English, French-English and Russian-English translations, we computed sentence-level BLEU BIBREF12 with the NLTK BIBREF13 implementation between the human translations and the automatic translations produced by a state-of-the-art system BIBREF14 (the French-English system was a Transformer big BIBREF15 separately trained on WMT14). We applied this method to these three language pairs only as we are confident about the quality of the corresponding systems. Translations with a score that was too low were manually inspected and sent back to the translators when needed. 2) We manually inspected examples where the source transcript was identical to the translation. 3) We measured the perplexity of the translations using a language model trained on a large amount of clean monolingual data BIBREF14. We manually inspected examples where the translation had a high perplexity and sent them back to translators accordingly. 4) We computed the ratio of English characters in the translations. We manually inspected examples with a low ratio and sent them back to translators accordingly. 5) Finally, we used VizSeq BIBREF16 to calculate similarity scores between transcripts and translations based on LASER cross-lingual sentence embeddings BIBREF17. Samples with low scores were manually inspected and sent back for translation when needed."
                    ],
                    "highlighted_evidence": [
                        "In order to control the quality of the professional translations, we applied various sanity checks to the translations BIBREF11. 1) For German-English, French-English and Russian-English translations, we computed sentence-level BLEU BIBREF1",
                        "In order to control the quality of the professional translations, we applied various sanity checks to the translations BIBREF11. 1) For German-English, French-English and Russian-English translations, we computed sentence-level BLEU BIBREF12 with the NLTK BIBREF13 implementation between the human translations and the automatic translations produced by a state-of-the-art system BIBREF14 (the French-English system was a Transformer big BIBREF15 separately trained on WMT14).",
                        "2) We manually inspected examples where the source transcript was identical to the translation",
                        "3) We measured the perplexity of the translations using a language model trained on a large amount of clean monolingual data BIBREF14.",
                        "4) We computed the ratio of English characters in the translations.",
                        "5) Finally, we used VizSeq BIBREF16 to calculate similarity scores between transcripts and translations based on LASER cross-lingual sentence embeddings BIBREF17."
                    ]
                }
            ]
        },
        {
            "question": "Is the data in CoVoST annotated for dialect?",
            "answers": [
                {
                    "answer": "No",
                    "type": "boolean"
                }
            ],
            "q_uid": "559c68802ee2bb8b11e2188127418ca3a6155ba7",
            "evidence": [
                {
                    "raw_evidence": [],
                    "highlighted_evidence": []
                }
            ]
        },
        {
            "question": "Is Arabic one of the 11 languages in CoVost?",
            "answers": [
                {
                    "answer": "No",
                    "type": "boolean"
                },
                {
                    "answer": "No",
                    "type": "boolean"
                }
            ],
            "q_uid": "8dc707a0daf7bff61a97d9d854283e65c0c85064",
            "evidence": [
                {
                    "raw_evidence": [
                        "Raw CoVo data contains samples that passed validation as well as those that did not. To build CoVoST, we only use the former one and reuse the official train-development-test partition of the validated data. As of January 2020, the latest CoVo 2019-06-12 release includes 29 languages. CoVoST is currently built on that release and covers the following 11 languages: French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian and Chinese."
                    ],
                    "highlighted_evidence": [
                        "CoVoST is currently built on that release and covers the following 11 languages: French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian and Chinese."
                    ]
                },
                {
                    "raw_evidence": [
                        "Raw CoVo data contains samples that passed validation as well as those that did not. To build CoVoST, we only use the former one and reuse the official train-development-test partition of the validated data. As of January 2020, the latest CoVo 2019-06-12 release includes 29 languages. CoVoST is currently built on that release and covers the following 11 languages: French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian and Chinese."
                    ],
                    "highlighted_evidence": [
                        "CoVoST is currently built on that release and covers the following 11 languages: French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian and Chinese."
                    ]
                }
            ]
        }
    ],
    "1611.01400": [
        {
            "question": "what were the baselines?",
            "answers": [
                {
                    "answer": "Rank by the number of times a citation is mentioned in the document,  Rank by the number of times the citation is cited in the literature (citation impact). , Rank using Google Scholar Related Articles., Rank by the TF*IDF weighted cosine similarity. , ank using a learning-to-rank model trained on text similarity rankings",
                    "type": "extractive"
                },
                {
                    "answer": "(1) Rank by the number of times a citation is mentioned in the document., (2) Rank by the number of times the citation is cited in the literature (citation impact)., (3) Rank using Google Scholar Related Articles., (4) Rank by the TF*IDF weighted cosine similarity., (5) Rank using a learning-to-rank model trained on text similarity rankings.",
                    "type": "extractive"
                }
            ],
            "q_uid": "d32b6ac003cfe6277f8c2eebc7540605a60a3904",
            "evidence": [
                {
                    "raw_evidence": [
                        "We compare our system to a variety of baselines. (1) Rank by the number of times a citation is mentioned in the document. (2) Rank by the number of times the citation is cited in the literature (citation impact). (3) Rank using Google Scholar Related Articles. (4) Rank by the TF*IDF weighted cosine similarity. (5) Rank using a learning-to-rank model trained on text similarity rankings. The first two baseline systems are models where the values are ordered from highest to lowest to generate the ranking. The idea behind them is that the number of times a citation is mentioned in an article, or the citation impact may already be good indicators of their closeness. The text similarity model is trained using the same features and methods used by the annotation model, but trained using text similarity rankings instead of the author's judgments."
                    ],
                    "highlighted_evidence": [
                        "We compare our system to a variety of baselines. (1) Rank by the number of times a citation is mentioned in the document. (2) Rank by the number of times the citation is cited in the literature (citation impact). (3) Rank using Google Scholar Related Articles. (4) Rank by the TF*IDF weighted cosine similarity. (5) Rank using a learning-to-rank model trained on text similarity rankings. The first two baseline systems are models where the values are ordered from highest to lowest to generate the ranking. The idea behind them is that the number of times a citation is mentioned in an article, or the citation impact may already be good indicators of their closeness. The text similarity model is trained using the same features and methods used by the annotation model, but trained using text similarity rankings instead of the author's judgments."
                    ]
                },
                {
                    "raw_evidence": [
                        "We compare our system to a variety of baselines. (1) Rank by the number of times a citation is mentioned in the document. (2) Rank by the number of times the citation is cited in the literature (citation impact). (3) Rank using Google Scholar Related Articles. (4) Rank by the TF*IDF weighted cosine similarity. (5) Rank using a learning-to-rank model trained on text similarity rankings. The first two baseline systems are models where the values are ordered from highest to lowest to generate the ranking. The idea behind them is that the number of times a citation is mentioned in an article, or the citation impact may already be good indicators of their closeness. The text similarity model is trained using the same features and methods used by the annotation model, but trained using text similarity rankings instead of the author's judgments."
                    ],
                    "highlighted_evidence": [
                        "We compare our system to a variety of baselines. (1) Rank by the number of times a citation is mentioned in the document. (2) Rank by the number of times the citation is cited in the literature (citation impact). (3) Rank using Google Scholar Related Articles. (4) Rank by the TF*IDF weighted cosine similarity. (5) Rank using a learning-to-rank model trained on text similarity rankings."
                    ]
                }
            ]
        },
        {
            "question": "what is the supervised model they developed?",
            "answers": [
                {
                    "answer": "SVMRank",
                    "type": "extractive"
                }
            ],
            "q_uid": "c10f38ee97ed80484c1a70b8ebba9b1fb149bc91",
            "evidence": [
                {
                    "raw_evidence": [
                        "Support Vector Machine (SVM) ( BIBREF25 ) is a commonly used supervised classification algorithm that has shown good performance over a range of tasks. SVM can be thought of as a binary linear classifier where the goal is to maximize the size of the gap between the class-separating line and the points on either side of the line. This helps avoid over-fitting on the training data. SVMRank is a modification to SVM that assigns scores to each data point and allows the results to be ranked ( BIBREF26 ). We use SVMRank in the experiments below. SVMRank has previously been used in the task of document retrieval in ( BIBREF27 ) for a more traditional short query task and has been shown to be a top-performing system for ranking."
                    ],
                    "highlighted_evidence": [
                        "SVMRank is a modification to SVM that assigns scores to each data point and allows the results to be ranked ( BIBREF26 ). We use SVMRank in the experiments below. "
                    ]
                }
            ]
        },
        {
            "question": "what is the size of this built corpus?",
            "answers": [
                {
                    "answer": "90 annotated documents with 5 citations each ranked 1 to 5, where 1 is least relevant and 5 is most relevant for a total of 450 annotated citations",
                    "type": "extractive"
                }
            ],
            "q_uid": "340501f23ddc0abe344a239193abbaaab938cc3a",
            "evidence": [
                {
                    "raw_evidence": [
                        "We asked authors to rank documents by how \u201cclose to your work\u201d they were. The definition of closeness was left to the discretion of the author. The dataset is composed of 90 annotated documents with 5 citations each ranked 1 to 5, where 1 is least relevant and 5 is most relevant for a total of 450 annotated citations."
                    ],
                    "highlighted_evidence": [
                        "The dataset is composed of 90 annotated documents with 5 citations each ranked 1 to 5, where 1 is least relevant and 5 is most relevant for a total of 450 annotated citations."
                    ]
                }
            ]
        },
        {
            "question": "what crowdsourcing platform is used?",
            "answers": [
                {
                    "answer": "asked the authors to rank by closeness five citations we selected from their paper",
                    "type": "extractive"
                }
            ],
            "q_uid": "fbb85cbd41de6d2818e77e8f8d4b91e431931faa",
            "evidence": [
                {
                    "raw_evidence": [
                        "Given the full text of a scientific publication, we want to rank its citations according to the author's judgments. We collected recent publications from the open-access PLoS journals and asked the authors to rank by closeness five citations we selected from their paper. PLoS articles were selected because its journals cover a wide array of topics and the full text articles are available in XML format. We selected the most recent publications as previous work in crowd-sourcing annotation shows that authors' willingness to participate in an unpaid annotation task declines with the age of publication ( BIBREF23 ). We then extracted the abstract, citations, full text, authors, and corresponding author email address from each document. The titles and abstracts of the citations were retrieved from PubMed, and the cosine similarity between the PLoS abstract and the citation's abstract was calculated. We selected the top five most similar abstracts using TF*IDF weighted cosine similarity, shuffled their order, and emailed them to the corresponding author for annotation. We believe that ranking five articles (rather than the entire collection of the references) is a more manageable task for an author compared to asking them to rank all references. Because the documents to be annotated were selected based on text similarity, they also represent a challenging baseline for models based on text-similarity features. In total 416 authors were contacted, and 92 responded (22% response rate). Two responses were removed from the dataset for incomplete annotation."
                    ],
                    "highlighted_evidence": [
                        "Given the full text of a scientific publication, we want to rank its citations according to the author's judgments. We collected recent publications from the open-access PLoS journals and asked the authors to rank by closeness five citations we selected from their paper."
                    ]
                }
            ]
        }
    ],
    "1912.06927": [
        {
            "question": "Do the tweets come from a specific region?",
            "answers": [
                {
                    "answer": "No",
                    "type": "boolean"
                }
            ],
            "q_uid": "a4422019d19f9c3d95ce8dc1d529bf3da5edcfb1",
            "evidence": [
                {
                    "raw_evidence": [
                        "Table TABREF14 presents the distribution of the tweets by country before and after the filtering process. A large portion of the samples is from India because the MeToo movement has peaked towards the end of 2018 in India. There are very few samples from Russia likely because of content moderation and regulations on social media usage in the country. Figure FIGREF15 gives a geographical distribution of the curated dataset."
                    ],
                    "highlighted_evidence": [
                        "Table TABREF14 presents the distribution of the tweets by country before and after the filtering process. A large portion of the samples is from India because the MeToo movement has peaked towards the end of 2018 in India. There are very few samples from Russia likely because of content moderation and regulations on social media usage in the country."
                    ]
                }
            ]
        }
    ],
    "1909.08090": [
        {
            "question": "Do they compare their algorithm to voting without weights?",
            "answers": [
                {
                    "answer": "No",
                    "type": "boolean"
                }
            ],
            "q_uid": "10045d7dac063013a8447b5a4bc3a3c2f18f9e82",
            "evidence": [
                {
                    "raw_evidence": [],
                    "highlighted_evidence": []
                }
            ]
        }
    ],
    "1709.10367": [
        {
            "question": "What experiments are used to demonstrate the benefits of this approach?",
            "answers": [
                {
                    "answer": "On each dataset, we compare four approaches based on sefe with two efe BIBREF10 baselines. All are fit using sgd BIBREF34 . In particular, we compare the following methods:",
                    "type": "extractive"
                },
                {
                    "answer": "Calculate test log-likelihood on the three considered datasets",
                    "type": "abstractive"
                }
            ],
            "q_uid": "777217e025132ddc173cf33747ee590628a8f62f",
            "evidence": [
                {
                    "raw_evidence": [
                        "In this section, we describe the experimental study. We fit the sefe model on three datasets and compare it against the efe BIBREF10 . Our quantitative results show that sharing the context vectors provides better results, and that amortization and hierarchical structure give further improvements.",
                        "Data. We apply the sefe on three datasets: ArXiv papers, U.S. Senate speeches, and purchases on supermarket grocery shopping data. We describe these datasets below, and we provide a summary of the datasets in Table TABREF17 .",
                        "ArXiv papers: This dataset contains the abstracts of papers published on the ArXiv under the 19 different tags between April 2007 and June 2015. We treat each tag as a group and fit sefe with the goal of uncovering which words have the strongest shift in usage. We split the abstracts into training, validation, and test sets, with proportions of INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 , respectively.",
                        "Senate speeches: This dataset contains U.S. Senate speeches from 1994 to mid 2009. In contrast to the ArXiv collection, it is a transcript of spoken language. We group the data into state of origin of the speaker and his or her party affiliation. Only affiliations with the Republican and Democratic Party are considered. As a result, there are 83 groups (Republicans from Alabama, Democrats from Alabama, Republicans from Arkansas, etc.). Some of the state/party combinations are not available in the data, as some of the 50 states have only had Senators with the same party affiliation. We split the speeches into training ( INLINEFORM0 ), validation ( INLINEFORM1 ), and testing ( INLINEFORM2 ).",
                        "Grocery shopping data: This dataset contains the purchases of INLINEFORM0 customers. The data covers a period of 97 weeks. After removing low-frequency items, the data contains INLINEFORM1 unique items at the 1.10upc (Universal Product Code) level. We split the data into a training, test, and validation sets, with proportions of INLINEFORM2 , INLINEFORM3 , and INLINEFORM4 , respectively. The training data contains INLINEFORM5 shopping trips and INLINEFORM6 purchases in total.",
                        "For the text corpora, we fix the vocabulary to the 15k most frequent terms and remove all words that are not in the vocabulary. Following BIBREF2 , we additionally remove each word with probability INLINEFORM0 , where INLINEFORM1 is the word frequency. This downsamples especially the frequent words and speeds up training. (Sizes reported in Table TABREF17 are the number of words remaining after preprocessing.)",
                        "Models. Our goal is to fit the sefe model on these datasets. For the text data, we use the Bernoulli distribution as the conditional exponential family, while for the shopping data we use the Poisson distribution, which is more appropriate for count data.",
                        "On each dataset, we compare four approaches based on sefe with two efe BIBREF10 baselines. All are fit using sgd BIBREF34 . In particular, we compare the following methods:"
                    ],
                    "highlighted_evidence": [
                        "In this section, we describe the experimental study. We fit the sefe model on three datasets and compare it against the efe BIBREF10 . Our quantitative results show that sharing the context vectors provides better results, and that amortization and hierarchical structure give further improvements.",
                        "Data. We apply the sefe on three datasets: ArXiv papers, U.S. Senate speeches, and purchases on supermarket grocery shopping data. We describe these datasets below, and we provide a summary of the datasets in Table TABREF17 .",
                        "ArXiv papers: This dataset contains the abstracts of papers published on the ArXiv under the 19 different tags between April 2007 and June 2015. We treat each tag as a group and fit sefe with the goal of uncovering which words have the strongest shift in usage. We split the abstracts into training, validation, and test sets, with proportions of INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 , respectively.\n\nSenate speeches: This dataset contains U.S. Senate speeches from 1994 to mid 2009. In contrast to the ArXiv collection, it is a transcript of spoken language. We group the data into state of origin of the speaker and his or her party affiliation. Only affiliations with the Republican and Democratic Party are considered. As a result, there are 83 groups (Republicans from Alabama, Democrats from Alabama, Republicans from Arkansas, etc.). Some of the state/party combinations are not available in the data, as some of the 50 states have only had Senators with the same party affiliation. We split the speeches into training ( INLINEFORM0 ), validation ( INLINEFORM1 ), and testing ( INLINEFORM2 ).\n\nGrocery shopping data: This dataset contains the purchases of INLINEFORM0 customers. The data covers a period of 97 weeks. After removing low-frequency items, the data contains INLINEFORM1 unique items at the 1.10upc (Universal Product Code) level. We split the data into a training, test, and validation sets, with proportions of INLINEFORM2 , INLINEFORM3 , and INLINEFORM4 , respectively. The training data contains INLINEFORM5 shopping trips and INLINEFORM6 purchases in total.",
                        "For the text corpora, we fix the vocabulary to the 15k most frequent terms and remove all words that are not in the vocabulary. Following BIBREF2 , we additionally remove each word with probability INLINEFORM0 , where INLINEFORM1 is the word frequency. This downsamples especially the frequent words and speeds up training. (Sizes reported in Table TABREF17 are the number of words remaining after preprocessing.)",
                        "Models. Our goal is to fit the sefe model on these datasets. For the text data, we use the Bernoulli distribution as the conditional exponential family, while for the shopping data we use the Poisson distribution, which is more appropriate for count data.",
                        "On each dataset, we compare four approaches based on sefe with two efe BIBREF10 baselines. All are fit using sgd BIBREF34 . In particular, we compare the following methods:"
                    ]
                },
                {
                    "raw_evidence": [
                        "Our contributions are thus as follows. We introduce the sefe model, extending efe to grouped data. We present two techniques to share statistical strength among the embedding vectors, one based on hierarchical modeling and one based on amortization. We carry out a thorough experimental study on two text databases, ArXiv papers by section and U.S. Congressional speeches by home state and political party. Using Poisson embeddings, we study market basket data from a large grocery store, grouped by season. On all three data sets, sefe outperforms efe in terms of held-out log-likelihood. Qualitatively, we demonstrate how sefe discovers which words are used most differently across U.S. states and political parties, and show how word usage changes in different ArXiv disciplines."
                    ],
                    "highlighted_evidence": [
                        "We carry out a thorough experimental study on two text databases, ArXiv papers by section and U.S. Congressional speeches by home state and political party. Using Poisson embeddings, we study market basket data from a large grocery store, grouped by season. On all three data sets, sefe outperforms efe in terms of held-out log-likelihood."
                    ]
                }
            ]
        },
        {
            "question": "What hierarchical modelling approach is used?",
            "answers": [
                {
                    "answer": "the group-specific embedding representations are tied through a global embedding",
                    "type": "extractive"
                }
            ],
            "q_uid": "2dbf6fe095cd879a9bf40f110b7b72c8bdde9475",
            "evidence": [
                {
                    "raw_evidence": [
                        "We propose two methods to share statistical strength among the embedding vectors. The first approach is based on hierarchical modeling BIBREF13 , which assumes that the group-specific embedding representations are tied through a global embedding. The second approach is based on amortization BIBREF14 , BIBREF15 , which considers that the individual embeddings are the output of a deterministic function of a global embedding representation. We use stochastic optimization to fit large data sets."
                    ],
                    "highlighted_evidence": [
                        "The first approach is based on hierarchical modeling BIBREF13 , which assumes that the group-specific embedding representations are tied through a global embedding."
                    ]
                }
            ]
        },
        {
            "question": "Which words are used differently across ArXiv?",
            "answers": [
                {
                    "answer": "intelligence",
                    "type": "extractive"
                }
            ],
            "q_uid": "de830c534c23f103288c198eb19174c76bfd38a1",
            "evidence": [
                {
                    "raw_evidence": [
                        "Figure FIGREF1 illustrates the kind of variation that we can capture. We fit an sefe to ArXiv abstracts grouped into different sections, such as computer science (cs), quantitative finance (q-fin), and nonlinear sciences (nlin). sefe results in a per-section embedding of each term in the vocabulary. Using the fitted embeddings, we illustrate similar words to the word 1.10intelligence. We can see that how 1.10intelligence is used varies by field: in computer science the most similar words include 1.10artificial and 1.10ai; in finance, similar words include 1.10abilities and 1.10consciousness."
                    ],
                    "highlighted_evidence": [
                        "We can see that how 1.10intelligence is used varies by field: in computer science the most similar words include 1.10artificial and 1.10ai; in finance, similar words include 1.10abilities and 1.10consciousness."
                    ]
                }
            ]
        }
    ],
    "1607.00410": [
        {
            "question": "How many examples are there in the source domain?",
            "answers": [
                {
                    "answer": "78,976",
                    "type": "extractive"
                }
            ],
            "q_uid": "2af66730a85b29ff28dbfa58342e0ae6265d2963",
            "evidence": [
                {
                    "raw_evidence": [
                        "To simulate the scenario, we split the Microsoft COCO dataset into food and non-food domain datasets. The MS COCO dataset contains approximately 80K images for training and 40K images for validation; each image has 5 captions BIBREF12 . The dataset contains images of diverse categories, including animals, indoor scenes, sports, and foods. We selected the \u201cfood category\u201d data by scoring the captions according to how much those are related to the food category. The score is computed based on wordnet similarities BIBREF13 . The training and validation datasets are split by the score with the same threshold. Consequently, the food dataset has 3,806 images for training and 1,775 for validation. The non-food dataset has 78,976 images for training and 38,749 for validation.",
                        "Adaptation to food domain captioning"
                    ],
                    "highlighted_evidence": [
                        "To simulate the scenario, we split the Microsoft COCO dataset into food and non-food domain datasets.",
                        "the food dataset has 3,806 images for training and 1,775 for validation. The non-food dataset has 78,976 images for training and 38,749 for validation.",
                        "Adaptation to food domain captioning"
                    ]
                }
            ]
        },
        {
            "question": "How many examples are there in the target domain?",
            "answers": [
                {
                    "answer": "the food dataset has 3,806 images for training ",
                    "type": "extractive"
                }
            ],
            "q_uid": "146fe3e97d8080f04222ed20903dd0d5fd2f551c",
            "evidence": [
                {
                    "raw_evidence": [
                        "Adaptation to food domain captioning",
                        "To simulate the scenario, we split the Microsoft COCO dataset into food and non-food domain datasets. The MS COCO dataset contains approximately 80K images for training and 40K images for validation; each image has 5 captions BIBREF12 . The dataset contains images of diverse categories, including animals, indoor scenes, sports, and foods. We selected the \u201cfood category\u201d data by scoring the captions according to how much those are related to the food category. The score is computed based on wordnet similarities BIBREF13 . The training and validation datasets are split by the score with the same threshold. Consequently, the food dataset has 3,806 images for training and 1,775 for validation. The non-food dataset has 78,976 images for training and 38,749 for validation."
                    ],
                    "highlighted_evidence": [
                        "Adaptation to food domain captioning",
                        "Consequently, the food dataset has 3,806 images for training and 1,775 for validation. The non-food dataset has 78,976 images for training and 38,749 for validation."
                    ]
                }
            ]
        },
        {
            "question": "Did they only experiment with captioning task?",
            "answers": [
                {
                    "answer": "Yes",
                    "type": "boolean"
                }
            ],
            "q_uid": "0fc17e51a17efce17577e2db89a24abd6607bb2b",
            "evidence": [
                {
                    "raw_evidence": [
                        "In the latter part of this paper, we apply our domain adaptation method to a neural captioning model and show performance improvement over other standard methods on several datasets and metrics. In the datasets, the source and target have different word distributions, and thus adaptation of output parameters is important. We augment the output parameters to facilitate adaptation. Although we use captioning models in the experiments, our method can be applied to any neural networks trained with a cross-entropy loss."
                    ],
                    "highlighted_evidence": [
                        "we apply our domain adaptation method to a neural captioning model and show performance improvement over other standard methods on several datasets and metrics. "
                    ]
                }
            ]
        }
    ],
    "1906.05963": [
        {
            "question": "What are the common captioning metrics?",
            "answers": [
                {
                    "answer": "the CIDEr-D BIBREF22 , SPICE BIBREF23 , BLEU BIBREF24 , METEOR BIBREF25 , and ROUGE-L BIBREF26 metrics",
                    "type": "extractive"
                }
            ],
            "q_uid": "9a5d02062fa7eec7097f1dc1c38b5e6d5c82acdf",
            "evidence": [
                {
                    "raw_evidence": [
                        "We trained and evaluated our algorithm on the Microsoft COCO (MS-COCO) 2014 Captions dataset BIBREF21 . We report results on the Karpathy validation and test splits BIBREF8 , which are commonly used in other image captioning publications. The dataset contains 113K training images with 5 human annotated captions for each image. The Karpathy test and validation sets contain 5K images each. We evaluate our models using the CIDEr-D BIBREF22 , SPICE BIBREF23 , BLEU BIBREF24 , METEOR BIBREF25 , and ROUGE-L BIBREF26 metrics. While it has been shown experimentally that BLEU and ROUGE have lower correlation with human judgments than the other metrics BIBREF23 , BIBREF22 , the common practice in the image captioning literature is to report all the mentioned metrics."
                    ],
                    "highlighted_evidence": [
                        "We trained and evaluated our algorithm on the Microsoft COCO (MS-COCO) 2014 Captions dataset BIBREF21 . We report results on the Karpathy validation and test splits BIBREF8 , which are commonly used in other image captioning publications. The dataset contains 113K training images with 5 human annotated captions for each image. The Karpathy test and validation sets contain 5K images each. We evaluate our models using the CIDEr-D BIBREF22 , SPICE BIBREF23 , BLEU BIBREF24 , METEOR BIBREF25 , and ROUGE-L BIBREF26 metrics. While it has been shown experimentally that BLEU and ROUGE have lower correlation with human judgments than the other metrics BIBREF23 , BIBREF22 , the common practice in the image captioning literature is to report all the mentioned metrics."
                    ]
                }
            ]
        }
    ],
    "1910.07481": [
        {
            "question": "Which datasets were used in the experiment?",
            "answers": [
                {
                    "answer": "WMT 2019 parallel dataset, a restricted dataset containing the full TED corpus from MUST-C BIBREF10, sampled sentences from WMT 2019 dataset",
                    "type": "extractive"
                }
            ],
            "q_uid": "749a307c3736c5b06d7b605dc228d80de36cbabe",
            "evidence": [
                {
                    "raw_evidence": [
                        "Contribution: We propose a preliminary study of a generic approach allowing any model to benefit from document-level information while translating sentence pairs. The core idea is to augment source data by adding document information to each sentence of a source corpus. This document information corresponds to the belonging document of a sentence and is computed prior to training, it takes every document word into account. Our approach focuses on pre-processing and consider whole documents as long as they have defined boundaries. We conduct experiments using the Transformer base model BIBREF1. For the English-German language pair we use the full WMT 2019 parallel dataset. For the English-French language pair we use a restricted dataset containing the full TED corpus from MUST-C BIBREF10 and sampled sentences from WMT 2019 dataset. We obtain important improvements over the baseline and present evidences that this approach helps to resolve cross-sentence ambiguities."
                    ],
                    "highlighted_evidence": [
                        "For the English-German language pair we use the full WMT 2019 parallel dataset. For the English-French language pair we use a restricted dataset containing the full TED corpus from MUST-C BIBREF10 and sampled sentences from WMT 2019 dataset. "
                    ]
                }
            ]
        },
        {
            "question": "What evaluation metrics did they use?",
            "answers": [
                {
                    "answer": "BLEU and TER scores",
                    "type": "extractive"
                }
            ],
            "q_uid": "102de97c123bb1e247efec0f1d958f8a3a86e2f6",
            "evidence": [
                {
                    "raw_evidence": [
                        "We consider two different models for each language pair: the Baseline and the Document model. We evaluate them on 3 test sets and report BLEU and TER scores. All experiments are run 8 times with different seeds, we report averaged results and p-values for each experiment."
                    ],
                    "highlighted_evidence": [
                        "We consider two different models for each language pair: the Baseline and the Document model. We evaluate them on 3 test sets and report BLEU and TER scores. All experiments are run 8 times with different seeds, we report averaged results and p-values for each experiment."
                    ]
                }
            ]
        }
    ],
    "1911.03385": [
        {
            "question": "Is this style generator compared to some baseline?",
            "answers": [
                {
                    "answer": "Yes",
                    "type": "boolean"
                }
            ],
            "q_uid": "9213159f874b3bdd9b4de956a88c703aac988411",
            "evidence": [
                {
                    "raw_evidence": [
                        "We compare the above model to a similar model, where rather than explicitly represent $K$ features as input, we have $K$ features in the form of a genre embedding, i.e. we learn a genre specific embedding for each of the gothic, scifi, and philosophy genres, as studied in BIBREF8 and BIBREF7. To generate in a specific style, we simply set the appropriate embedding. We use genre embeddings of size 850 which is equivalent to the total size of the $K$ feature embeddings in the StyleEQ model."
                    ],
                    "highlighted_evidence": [
                        "We compare the above model to a similar model, where rather than explicitly represent $K$ features as input, we have $K$ features in the form of a genre embedding, i.e. we learn a genre specific embedding for each of the gothic, scifi, and philosophy genres, as studied in BIBREF8 and BIBREF7."
                    ]
                }
            ]
        },
        {
            "question": "How they perform manual evaluation, what is criteria?",
            "answers": [
                {
                    "answer": "accuracy",
                    "type": "extractive"
                }
            ],
            "q_uid": "5f4e6ce4a811c4b3ab07335d89db2fd2a8d8d8b2",
            "evidence": [
                {
                    "raw_evidence": [
                        "Each annotator annotated 90 reference sentences (i.e. from the training corpus) with which style they thought the sentence was from. The accuracy on this baseline task for annotators A1, A2, and A3 was 80%, 88%, and 80% respectively, giving us an upper expected bound on the human evaluation."
                    ],
                    "highlighted_evidence": [
                        "Each annotator annotated 90 reference sentences (i.e. from the training corpus) with which style they thought the sentence was from. The accuracy on this baseline task for annotators A1, A2, and A3 was 80%, 88%, and 80% respectively, giving us an upper expected bound on the human evaluation."
                    ]
                }
            ]
        },
        {
            "question": "What metrics are used for automatic evaluation?",
            "answers": [
                {
                    "answer": "classification accuracy, BLEU scores, model perplexities of the reconstruction",
                    "type": "extractive"
                }
            ],
            "q_uid": "a234bcbf2e41429422adda37d9e926b49ef66150",
            "evidence": [
                {
                    "raw_evidence": [
                        "In tab:blueperpl we report BLEU scores for the reconstruction of test set sentences from their content and feature representations, as well as the model perplexities of the reconstruction. For both models, we use beam decoding with a beam size of eight. Beam candidates are ranked according to their length normalized log-likelihood. On these automatic measures we see that StyleEQ is better able to reconstruct the original sentences. In some sense this evaluation is mostly a sanity check, as the feature controls contain more locally specific information than the genre embeddings, which say very little about how many specific function words one should expect to see in the output.",
                        "In table:fasttext-results we see the results. Note that for both models, the all and top classification accuracy tends to be quite similar, though for the Baseline they are often almost exactly the same when the Baseline has little to no diversity in the outputs."
                    ],
                    "highlighted_evidence": [
                        "In tab:blueperpl we report BLEU scores for the reconstruction of test set sentences from their content and feature representations, as well as the model perplexities of the reconstruction. For both models, we use beam decoding with a beam size of eight.",
                        "Note that for both models, the all and top classification accuracy tends to be quite similar, though for the Baseline they are often almost exactly the same when the Baseline has little to no diversity in the outputs."
                    ]
                }
            ]
        },
        {
            "question": "How they know what are content words?",
            "answers": [
                {
                    "answer": " words found in the control word lists are then removed, The remaining words, which represent the content",
                    "type": "extractive"
                }
            ],
            "q_uid": "c383fa9170ae00a4a24a8e39358c38395c5f034b",
            "evidence": [
                {
                    "raw_evidence": [
                        "fig:sentenceinput illustrates the process. Controls are calculated heuristically. All words found in the control word lists are then removed from the reference sentence. The remaining words, which represent the content, are used as input into the model, along with their POS tags and lemmas.",
                        "In this way we encourage models to construct a sentence using content and style independently. This will allow us to vary the stylistic controls while keeping the content constant, and successfully perform style transfer. When generating a new sentence, the controls correspond to the counts of the corresponding syntactic features that we expect to be realized in the output."
                    ],
                    "highlighted_evidence": [
                        "Controls are calculated heuristically. All words found in the control word lists are then removed from the reference sentence. The remaining words, which represent the content, are used as input into the model, along with their POS tags and lemmas.\n\nIn this way we encourage models to construct a sentence using content and style independently."
                    ]
                }
            ]
        },
        {
            "question": "How they model style as a suite of low-level linguistic controls, such as frequency of pronouns, prepositions, and subordinate clause constructions?",
            "answers": [
                {
                    "answer": "style of a sentence is represented as a vector of counts of closed word classes (like personal pronouns) as well as counts of syntactic features like the number of SBAR non-terminals in its constituency parse, since clause structure has been shown to be indicative of style",
                    "type": "extractive"
                }
            ],
            "q_uid": "83251fd4a641cea8b180b49027e74920bca2699a",
            "evidence": [
                {
                    "raw_evidence": [
                        "Given that non-content words are distinctive enough for a classifier to determine style, we propose a suite of low-level linguistic feature counts (henceforth, controls) as our formal, content-blind definition of style. The style of a sentence is represented as a vector of counts of closed word classes (like personal pronouns) as well as counts of syntactic features like the number of SBAR non-terminals in its constituency parse, since clause structure has been shown to be indicative of style BIBREF20. Controls are extracted heuristically, and almost all rely on counts of pre-defined word lists. For constituency parses we use the Stanford Parser BIBREF21. table:controlexamples lists all the controls along with examples."
                    ],
                    "highlighted_evidence": [
                        "Given that non-content words are distinctive enough for a classifier to determine style, we propose a suite of low-level linguistic feature counts (henceforth, controls) as our formal, content-blind definition of style. The style of a sentence is represented as a vector of counts of closed word classes (like personal pronouns) as well as counts of syntactic features like the number of SBAR non-terminals in its constituency parse, since clause structure has been shown to be indicative of style BIBREF20. Controls are extracted heuristically, and almost all rely on counts of pre-defined word lists. For constituency parses we use the Stanford Parser BIBREF21. table:controlexamples lists all the controls along with examples."
                    ]
                }
            ]
        }
    ],
    "1801.07804": [
        {
            "question": "Do they consider relations other than binary relations?",
            "answers": [
                {
                    "answer": "Yes",
                    "type": "boolean"
                }
            ],
            "q_uid": "2a0f14740ee14224d116d4f51dacde6863bcdc1e",
            "evidence": [
                {
                    "raw_evidence": [],
                    "highlighted_evidence": []
                }
            ]
        },
        {
            "question": "Are the grammar clauses manually created?",
            "answers": [
                {
                    "answer": "Yes",
                    "type": "boolean"
                }
            ],
            "q_uid": "82c7d9e92c7d7b784de2cae87fb7293034c551f4",
            "evidence": [
                {
                    "raw_evidence": [],
                    "highlighted_evidence": []
                }
            ]
        },
        {
            "question": "Do they use an NER system in their pipeline?",
            "answers": [
                {
                    "answer": "No",
                    "type": "boolean"
                }
            ],
            "q_uid": "6d623c96cd3898c3758338e337e9157565f34185",
            "evidence": [
                {
                    "raw_evidence": [],
                    "highlighted_evidence": []
                }
            ]
        }
    ],
    "1503.00841": [
        {
            "question": "What background knowledge do they leverage?",
            "answers": [
                {
                    "answer": "labeled features",
                    "type": "extractive"
                },
                {
                    "answer": "labelled features, which are words whose presence strongly indicates a specific class or topic",
                    "type": "abstractive"
                }
            ],
            "q_uid": "50be4a737dc0951b35d139f51075011095d77f2a",
            "evidence": [
                {
                    "raw_evidence": [
                        "We address the robustness problem on top of GE-FL BIBREF0 , a GE method which leverages labeled features as prior knowledge. A labeled feature is a strong indicator of a specific class and is manually provided to the classifier. For example, words like amazing, exciting can be labeled features for class positive in sentiment classification."
                    ],
                    "highlighted_evidence": [
                        "We address the robustness problem on top of GE-FL BIBREF0 , a GE method which leverages labeled features as prior knowledge."
                    ]
                },
                {
                    "raw_evidence": [
                        "We address the robustness problem on top of GE-FL BIBREF0 , a GE method which leverages labeled features as prior knowledge. A labeled feature is a strong indicator of a specific class and is manually provided to the classifier. For example, words like amazing, exciting can be labeled features for class positive in sentiment classification.",
                        "As described in BIBREF0 , there are two ways to obtain labeled features. The first way is to use information gain. We first calculate the mutual information of all features according to the labels of the documents and select the top 20 as labeled features for each class as a feature pool. Note that using information gain requires the document label, but this is only to simulate how we human provide prior knowledge to the model. The second way is to use LDA BIBREF9 to select features. We use the same selection process as BIBREF0 , where they first train a LDA on the dataset, and then select the most probable features of each topic (sorted by $P(w_i|t_j)$ , the probability of word $w_i$ given topic $t_j$ ).",
                        "We evaluate our methods on several commonly used datasets whose themes range from sentiment, web-page, science to medical and healthcare. We use bag-of-words feature and remove stopwords in the preprocess stage. Though we have labels of all documents, we do not use them during the learning process, instead, we use the label of features."
                    ],
                    "highlighted_evidence": [
                        "We address the robustness problem on top of GE-FL BIBREF0 , a GE method which leverages labeled features as prior knowledge. A labeled feature is a strong indicator of a specific class and is manually provided to the classifier.",
                        "As described in BIBREF0 , there are two ways to obtain labeled features.",
                        "We use bag-of-words feature and remove stopwords in the preprocess stage. ",
                        "We first calculate the mutual information of all features according to the labels of the documents and select the top 20 as labeled features for each class as a feature pool.",
                        "The second way is to use LDA BIBREF9 to select features. We use the same selection process as BIBREF0 , where they first train a LDA on the dataset, and then select the most probable features of each topic (sorted by $P(w_i|t_j)$ , the probability of word $w_i$ given topic $t_j$ )."
                    ]
                }
            ]
        },
        {
            "question": "What are the three regularization terms?",
            "answers": [
                {
                    "answer": "a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, the KL divergence between reference and predicted class distribution",
                    "type": "extractive"
                },
                {
                    "answer": "a regularization term associated with neutral features,  the maximum entropy of class distribution, KL divergence between reference and predicted class distribution",
                    "type": "extractive"
                }
            ],
            "q_uid": "6becff2967fe7c5256fe0b00231765be5b9db9f1",
            "evidence": [
                {
                    "raw_evidence": [
                        "More specifically, we explore three regularization terms to address the problem: (1) a regularization term associated with neutral features; (2) the maximum entropy of class distribution regularization term; and (3) the KL divergence between reference and predicted class distribution. For the first manner, we simply use the most common features as neutral features and assume the neutral features are distributed uniformly over class labels. For the second and third one, we assume we have some knowledge about the class distribution which will be detailed soon later."
                    ],
                    "highlighted_evidence": [
                        "More specifically, we explore three regularization terms to address the problem: (1) a regularization term associated with neutral features; (2) the maximum entropy of class distribution regularization term; and (3) the KL divergence between reference and predicted class distribution."
                    ]
                },
                {
                    "raw_evidence": [
                        "More specifically, we explore three regularization terms to address the problem: (1) a regularization term associated with neutral features; (2) the maximum entropy of class distribution regularization term; and (3) the KL divergence between reference and predicted class distribution. For the first manner, we simply use the most common features as neutral features and assume the neutral features are distributed uniformly over class labels. For the second and third one, we assume we have some knowledge about the class distribution which will be detailed soon later."
                    ],
                    "highlighted_evidence": [
                        "More specifically, we explore three regularization terms to address the problem: (1) a regularization term associated with neutral features; (2) the maximum entropy of class distribution regularization term; and (3) the KL divergence between reference and predicted class distribution."
                    ]
                }
            ]
        },
        {
            "question": "What NLP tasks do they consider?",
            "answers": [
                {
                    "answer": "text classification for themes including sentiment, web-page, science, medical and healthcare",
                    "type": "abstractive"
                }
            ],
            "q_uid": "76121e359dfe3f16c2a352bd35f28005f2a40da3",
            "evidence": [
                {
                    "raw_evidence": [
                        "In this section, we first justify the approach when there exists unbalance in the number of labeled features or in class distribution. Then, to test the influence of $\\lambda $ , we conduct some experiments with the method which incorporates the KL divergence of class distribution. Last, we evaluate our approaches in 9 commonly used text classification datasets. We set $\\lambda = 5|K|$ by default in all experiments unless there is explicit declaration. The baseline we choose here is GE-FL BIBREF0 , a method based on generalization expectation criteria.",
                        "We evaluate our methods on several commonly used datasets whose themes range from sentiment, web-page, science to medical and healthcare. We use bag-of-words feature and remove stopwords in the preprocess stage. Though we have labels of all documents, we do not use them during the learning process, instead, we use the label of features."
                    ],
                    "highlighted_evidence": [
                        " Last, we evaluate our approaches in 9 commonly used text classification datasets.",
                        "We evaluate our methods on several commonly used datasets whose themes range from sentiment, web-page, science to medical and healthcare."
                    ]
                }
            ]
        },
        {
            "question": "How do they define robustness of a model?",
            "answers": [
                {
                    "answer": "ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced",
                    "type": "abstractive"
                },
                {
                    "answer": "Low sensitivity to bias in prior knowledge",
                    "type": "abstractive"
                }
            ],
            "q_uid": "02428a8fec9788f6dc3a86b5d5f3aa679935678d",
            "evidence": [
                {
                    "raw_evidence": [
                        "GE-FL reduces the heavy load of instance annotation and performs well when we provide prior knowledge with no bias. In our experiments, we observe that comparable numbers of labeled features for each class have to be supplied. But as mentioned before, it is often the case that we are not able to provide enough knowledge for some of the classes. For the baseball-hockey classification task, as shown before, GE-FL will predict most of the instances as baseball. In this section, we will show three terms to make the model more robust.",
                        "(a) We randomly select $t \\in [1, 20]$ features from the feature pool for one class, and only one feature for the other. The original balanced movie dataset is used (positive:negative=1:1).",
                        "Our methods are also evaluated on datasets with different unbalanced class distributions. We manually construct several movie datasets with class distributions of 1:2, 1:3, 1:4 by randomly removing 50%, 67%, 75% positive documents. The original balanced movie dataset is used as a control group. We test with both balanced and unbalanced labeled features. For the balanced case, we randomly select 10 features from the feature pool for each class, and for the unbalanced case, we select 10 features for one class, and 1 feature for the other. Results are shown in Figure 3 .",
                        "Figure 3 (b) shows that when the labeled features are unbalanced, our methods significantly outperforms GE-FL. Incorporating KL divergence is robust enough to control unbalance both in the dataset and in labeled features while the other three methods are not so competitive."
                    ],
                    "highlighted_evidence": [
                        "GE-FL reduces the heavy load of instance annotation and performs well when we provide prior knowledge with no bias. In our experiments, we observe that comparable numbers of labeled features for each class have to be supplied.",
                        "We randomly select $t \\in [1, 20]$ features from the feature pool for one class, and only one feature for the other.",
                        "Our methods are also evaluated on datasets with different unbalanced class distributions. We manually construct several movie datasets with class distributions of 1:2, 1:3, 1:4 by randomly removing 50%, 67%, 75% positive documents.",
                        "Incorporating KL divergence is robust enough to control unbalance both in the dataset and in labeled features while the other three methods are not so competitive."
                    ]
                },
                {
                    "raw_evidence": [
                        "However, a crucial problem, which has rarely been addressed, is the bias in the prior knowledge that we supply to the learning model. Would the model be robust or sensitive to the prior knowledge? Or, which kind of knowledge is appropriate for the task? Let's see an example: we may be a baseball fan but unfamiliar with hockey so that we can provide a few number of feature words of baseball, but much less of hockey for a baseball-hockey classification task. Such prior knowledge may mislead the model with heavy bias to baseball. If the model cannot handle this situation appropriately, the performance may be undesirable.",
                        "In this paper, we investigate into the problem in the framework of Generalized Expectation Criteria BIBREF7 . The study aims to reveal the factors of reducing the sensibility of the prior knowledge and therefore to make the model more robust and practical. To this end, we introduce auxiliary regularization terms in which our prior knowledge is formalized as distribution over output variables. Recall the example just mentioned, though we do not have enough knowledge to provide features for class hockey, it is easy for us to provide some neutral words, namely words that are not strong indicators of any class, like player here. As one of the factors revealed in this paper, supplying neutral feature words can boost the performance remarkably, making the model more robust."
                    ],
                    "highlighted_evidence": [
                        "However, a crucial problem, which has rarely been addressed, is the bias in the prior knowledge that we supply to the learning model. Would the model be robust or sensitive to the prior knowledge?",
                        "The study aims to reveal the factors of reducing the sensibility of the prior knowledge and therefore to make the model more robust and practical."
                    ]
                }
            ]
        }
    ],
    "1701.02962": [
        {
            "question": "What dataset do they use to evaluate their method?",
            "answers": [
                {
                    "answer": "antonym and synonym pairs, collected from WordNet BIBREF9 and Wordnik",
                    "type": "extractive"
                },
                {
                    "answer": "English Wikipedia dump from June 2016",
                    "type": "extractive"
                }
            ],
            "q_uid": "2fbb6322e485e7743ec3fb4bb02d44bf4b5ea8a6",
            "evidence": [
                {
                    "raw_evidence": [
                        "For training the models, neural networks require a large amount of training data. We use the existing large-scale antonym and synonym pairs previously used by Nguyen:16. Originally, the data pairs were collected from WordNet BIBREF9 and Wordnik."
                    ],
                    "highlighted_evidence": [
                        "We use the existing large-scale antonym and synonym pairs previously used by Nguyen:16. Originally, the data pairs were collected from WordNet BIBREF9 and Wordnik."
                    ]
                },
                {
                    "raw_evidence": [
                        "We use the English Wikipedia dump from June 2016 as the corpus resource for our methods and baselines. For parsing the corpus, we rely on spaCy. For the lemma embeddings, we rely on the word embeddings of the dLCE model BIBREF10 which is the state-of-the-art vector representation for distinguishing antonyms from synonyms. We re-implemented this cutting-edge model on Wikipedia with 100 dimensions, and then make use of the dLCE word embeddings for initialization the lemma embeddings. The embeddings of POS tags, dependency labels, distance labels, and out-of-vocabulary lemmas are initialized randomly. The number of dimensions is set to 10 for the embeddings of POS tags, dependency labels and distance labels. We use the validation sets to tune the number of dimensions for these labels. For optimization, we rely on the cross-entropy loss function and Stochastic Gradient Descent with the Adadelta update rule BIBREF11 . For training, we use the Theano framework BIBREF12 . Regularization is applied by a dropout of 0.5 on each of component's embeddings (dropout rate is tuned on the validation set). We train the models with 40 epochs and update all embeddings during training."
                    ],
                    "highlighted_evidence": [
                        "We use the English Wikipedia dump from June 2016 as the corpus resource for our methods and baselines. "
                    ]
                }
            ]
        }
    ],
    "1908.11047": [
        {
            "question": "Which syntactic features are obtained automatically on downstream task data?",
            "answers": [
                {
                    "answer": "token-level chunk label embeddings,  chunk boundary information is passed into the task model via BIOUL encoding of the labels",
                    "type": "extractive"
                }
            ],
            "q_uid": "9132d56e26844dc13b3355448d0f14b95bd2178a",
            "evidence": [
                {
                    "raw_evidence": [
                        "Our second approach incorporates shallow syntactic information in downstream tasks via token-level chunk label embeddings. Task training (and test) data is automatically chunked, and chunk boundary information is passed into the task model via BIOUL encoding of the labels. We add randomly initialized chunk label embeddings to task-specific input encoders, which are then fine-tuned for task-specific objectives. This approach does not require a shallow syntactic encoder or chunk annotations for pretraining cwrs, only a chunker. Hence, this can more directly measure the impact of shallow syntax for a given task."
                    ],
                    "highlighted_evidence": [
                        "Our second approach incorporates shallow syntactic information in downstream tasks via token-level chunk label embeddings. Task training (and test) data is automatically chunked, and chunk boundary information is passed into the task model via BIOUL encoding of the labels. We add randomly initialized chunk label embeddings to task-specific input encoders, which are then fine-tuned for task-specific objectives."
                    ]
                }
            ]
        }
    ],
    "1610.04377": [
        {
            "question": "What classifier is used for emergency categorization?",
            "answers": [
                {
                    "answer": "multi-class Naive Bayes",
                    "type": "extractive"
                }
            ],
            "q_uid": "17de58c17580350c9da9c2f3612784b432154d11",
            "evidence": [
                {
                    "raw_evidence": [
                        "We employ a multi-class Naive Bayes classifier as the second stage classification mechanism, for categorizing tweets appropriately, depending on the type of emergencies they indicate. This multi-class classifier is trained on data manually labeled with classes. We tokenize the training data using \u201cNgramTokenizer\u201d and then, apply a filter to create word vectors of strings before training. We use \u201ctrigrams\u201d as features to build a model which, later, classifies tweets into appropriate categories, in real time. We then perform cross validation using standard techniques to calculate the results, which are shown under the label \u201cStage 2\u201d, in table TABREF20 ."
                    ],
                    "highlighted_evidence": [
                        "We employ a multi-class Naive Bayes classifier as the second stage classification mechanism, for categorizing tweets appropriately, depending on the type of emergencies they indicate."
                    ]
                }
            ]
        },
        {
            "question": "What classifier is used for emergency detection?",
            "answers": [
                {
                    "answer": "SVM",
                    "type": "extractive"
                }
            ],
            "q_uid": "ff27d6e6eb77e55b4d39d343870118d1a6debd5e",
            "evidence": [
                {
                    "raw_evidence": [],
                    "highlighted_evidence": [
                        "The first classifier model acts as a filter for the second stage of classification. We use both SVM and NB to compare the results and choose SVM later for stage one classification model, owing to a better F-score. The training is performed on tweets labeled with classes , and based on unigrams as features. We create word vectors of strings in the tweet using a filter available in the WEKA API BIBREF9 , and perform cross validation using standard classification techniques."
                    ]
                }
            ]
        },
        {
            "question": "Do the tweets come from any individual?",
            "answers": [
                {
                    "answer": "Yes",
                    "type": "boolean"
                }
            ],
            "q_uid": "29772ba04886bee2d26b7320e1c6d9b156078891",
            "evidence": [
                {
                    "raw_evidence": [
                        "We collect data by using the Twitter API for saved data, available for public use. For our experiments we collect 3200 tweets filtered by keywords like \u201cfire\u201d, \u201cearthquake\u201d, \u201ctheft\u201d, \u201crobbery\u201d, \u201cdrunk driving\u201d, \u201cdrunk driving accident\u201d etc. Later, we manually label tweets with <emergency>and <non-emergency>labels for classification as stage one. Our dataset contains 1313 tweet with positive label <emergency>and 1887 tweets with a negative label <non-emergency>. We create another dataset with the positively labeled tweets and provide them with category labels like \u201cfire\u201d, \u201caccident\u201d, \u201cearthquake\u201d etc."
                    ],
                    "highlighted_evidence": [
                        "We collect data by using the Twitter API for saved data, available for public use. For our experiments we collect 3200 tweets filtered by keywords like \u201cfire\u201d, \u201cearthquake\u201d, \u201ctheft\u201d, \u201crobbery\u201d, \u201cdrunk driving\u201d, \u201cdrunk driving accident\u201d etc. "
                    ]
                }
            ]
        },
        {
            "question": "Are the tweets specific to a region?",
            "answers": [
                {
                    "answer": "No",
                    "type": "boolean"
                }
            ],
            "q_uid": "d27e3a099954e917b6491e81b2e907478d7f1233",
            "evidence": [
                {
                    "raw_evidence": [],
                    "highlighted_evidence": [
                        "We collect data by using the Twitter API for saved data, available for public use. For our experiments we collect 3200 tweets filtered by keywords like \u201cfire\u201d, \u201cearthquake\u201d, \u201ctheft\u201d, \u201crobbery\u201d, \u201cdrunk driving\u201d, \u201cdrunk driving accident\u201d etc. Later, we manually label tweets with and labels for classification as stage one. Our dataset contains 1313 tweet with positive label and 1887 tweets with a negative label . We create another dataset with the positively labeled tweets and provide them with category labels like \u201cfire\u201d, \u201caccident\u201d, \u201cearthquake\u201d etc."
                    ]
                }
            ]
        }
    ],
    "1911.07228": [
        {
            "question": "What word embeddings were used?",
            "answers": [
                {
                    "answer": "Kyubyong Park, Edouard Grave et al BIBREF11",
                    "type": "extractive"
                }
            ],
            "q_uid": "f46a907360d75ad566620e7f6bf7746497b6e4a9",
            "evidence": [
                {
                    "raw_evidence": [
                        "We use the word embeddings for Vietnamese that created by Kyubyong Park and Edouard Grave at al:",
                        "Kyubyong Park: In his project, he uses two methods including fastText and word2vec to generate word embeddings from wikipedia database backup dumps. His word embedding is the vector of 100 dimension and it has about 10k words.",
                        "Edouard Grave et al BIBREF11: They use fastText tool to generate word embeddings from Wikipedia. The format is the same at Kyubyong's, but their embedding is the vector of 300 dimension, and they have about 200k words"
                    ],
                    "highlighted_evidence": [
                        "We use the word embeddings for Vietnamese that created by Kyubyong Park and Edouard Grave at al:\n\nKyubyong Park: In his project, he uses two methods including fastText and word2vec to generate word embeddings from wikipedia database backup dumps.",
                        "Edouard Grave et al BIBREF11: They use fastText tool to generate word embeddings from Wikipedia."
                    ]
                }
            ]
        },
        {
            "question": "What type of errors were produced by the BLSTM-CNN-CRF system?",
            "answers": [
                {
                    "answer": "No extraction, No annotation, Wrong range, Wrong tag, Wrong range and tag",
                    "type": "extractive"
                }
            ],
            "q_uid": "79d999bdf8a343ce5b2739db3833661a1deab742",
            "evidence": [
                {
                    "raw_evidence": [
                        "Step 2: Based on the best results (BLSTM-CNN-CRF), error analysis is performed based on five types of errors (No extraction, No annotation, Wrong range, Wrong tag, Wrong range and tag), in a way similar to BIBREF10, but we analyze on both gold labels and predicted labels (more detail in figure 1 and 2)."
                    ],
                    "highlighted_evidence": [
                        "Based on the best results (BLSTM-CNN-CRF), error analysis is performed based on five types of errors (No extraction, No annotation, Wrong range, Wrong tag, Wrong range and tag), in a way similar to BIBREF10, but we analyze on both gold labels and predicted labels (more detail in figure 1 and 2)."
                    ]
                }
            ]
        }
    ],
    "1909.05360": [
        {
            "question": "Is this the first paper to propose a joint model for event and temporal relation extraction?",
            "answers": [
                {
                    "answer": "Yes",
                    "type": "boolean"
                }
            ],
            "q_uid": "cfb5ab893ed77f9df7eeb4940b6bacdef5acccea",
            "evidence": [
                {
                    "raw_evidence": [
                        "As far as we know, all existing systems treat this task as a pipeline of two separate subtasks, i.e., event extraction and temporal relation classification, and they also assume that gold events are given when training the relation classifier BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5. Specifically, they built end-to-end systems that extract events first and then predict temporal relations between them (Fig. FIGREF1). In these pipeline models, event extraction errors will propagate to the relation classification step and cannot be corrected afterwards. Our first contribution is the proposal of a joint model that extracts both events and temporal relations simultaneously (see Fig. FIGREF1). The motivation is that if we train the relation classifier with NONE relations between non-events, then it will potentially have the capability of correcting event extraction mistakes. For instance in Fig. FIGREF1, if the relation classifier predicts NONE for (Hutu, war) with a high confidence, then this is a strong signal that can be used by the event classifier to infer that at least one of them is not an event."
                    ],
                    "highlighted_evidence": [
                        "As far as we know, all existing systems treat this task as a pipeline of two separate subtasks, i.e., event extraction and temporal relation classification, and they also assume that gold events are given when training the relation classifier BIBREF0, BIBREF1, BIBREF2, BIBREF3, BIBREF4, BIBREF5. Specifically, they built end-to-end systems that extract events first and then predict temporal relations between them (Fig. FIGREF1). In these pipeline models, event extraction errors will propagate to the relation classification step and cannot be corrected afterwards. Our first contribution is the proposal of a joint model that extracts both events and temporal relations simultaneously (see Fig. FIGREF1)."
                    ]
                }
            ]
        },
        {
            "question": "What datasets were used for this work?",
            "answers": [
                {
                    "answer": "TB-Dense,  MATRES",
                    "type": "extractive"
                }
            ],
            "q_uid": "a5abd4dd91e6f2855e9098bd6ae1481c0fdb0d4a",
            "evidence": [
                {
                    "raw_evidence": [
                        "The TB-Dense dataset mitigates this issue by forcing annotators to examine all pairs of events within the same or neighboring sentences, and it has been widely evaluated on this task BIBREF3, BIBREF4, BIBREF19, BIBREF5. Recent data construction efforts such as MATRES BIBREF25 further enhance the data quality by using a multi-axis annotation scheme and adopting a start-point of events to improve inter-annotator agreements. We use TB-Dense and MATRES in our experiments and briefly summarize the data statistics in Table TABREF33."
                    ],
                    "highlighted_evidence": [
                        "We use TB-Dense and MATRES in our experiments and briefly summarize the data statistics in Table TABREF33."
                    ]
                }
            ]
        }
    ],
    "2002.02492": [
        {
            "question": "Is infinite-length sequence generation a result of training with maximum likelihood?",
            "answers": [
                {
                    "answer": "There are is a strong conjecture that it might be the reason but it is not proven.",
                    "type": "abstractive"
                }
            ],
            "q_uid": "61fb982b2c67541725d6db76b9c710dd169b533d",
            "evidence": [
                {
                    "raw_evidence": [
                        "We extended the notion of consistency of a recurrent language model put forward by BIBREF16 to incorporate a decoding algorithm, and used it to analyze the discrepancy between a model and the distribution induced by a decoding algorithm. We proved that incomplete decoding is inconsistent, and proposed two methods to prevent this: consistent decoding and the self-terminating recurrent language model. Using a sequence completion task, we confirmed that empirical inconsistency occurs in practice, and that each method prevents inconsistency while maintaining the quality of generated sequences. We suspect the absence of decoding in maximum likelihood estimation as a cause behind this inconsistency, and suggest investigating sequence-level learning as an alternative in the future.",
                        "Inconsistency may arise from the lack of decoding in solving this optimization problem. Maximum likelihood learning fits the model $p_{\\theta }$ using the data distribution, whereas a decoded sequence from the trained model follows the distribution $q_{\\mathcal {F}}$ induced by a decoding algorithm. Based on this discrepancy, we make a strong conjecture: we cannot be guaranteed to obtain a good consistent sequence generator using maximum likelihood learning and greedy decoding. Sequence-level learning, however, uses a decoding algorithm during training BIBREF25, BIBREF26. We hypothesize that sequence-level learning can result in a good sequence generator that is consistent with respect to incomplete decoding."
                    ],
                    "highlighted_evidence": [
                        "We suspect the absence of decoding in maximum likelihood estimation as a cause behind this inconsistency, and suggest investigating sequence-level learning as an alternative in the future.",
                        "Inconsistency may arise from the lack of decoding in solving this optimization problem. Maximum likelihood learning fits the model $p_{\\theta }$ using the data distribution, whereas a decoded sequence from the trained model follows the distribution $q_{\\mathcal {F}}$ induced by a decoding algorithm. Based on this discrepancy, we make a strong conjecture: we cannot be guaranteed to obtain a good consistent sequence generator using maximum likelihood learning and greedy decoding."
                    ]
                }
            ]
        }
    ],
    "1909.03135": [
        {
            "question": "Do the authors mention any downside of lemmatizing input before training ELMo?",
            "answers": [
                {
                    "answer": "Yes",
                    "type": "boolean"
                },
                {
                    "answer": "Yes",
                    "type": "boolean"
                }
            ],
            "q_uid": "5aa12b4063d6182a71870c98e4e1815ff3dc8a72",
            "evidence": [
                {
                    "raw_evidence": [
                        "russian\u0414\u043e\u043c\u0438\u043d\u043e is the only target noun in the RUSSE'18 that has no inflected forms, since it is a borrowed word. This leaves no room for improvement when using lemma-based ELMo models: all tokens of this word are already identical. At the same time, some information about inflected word forms in the context can be useful, but it is lost during lemmatization, and this leads to the decreased score. Arguably, this means that lemmatization brings along both advantages and disadvantages for WSD with ELMo. For inflected words (which constitute the majority of Russian vocabulary) profits outweigh the losses, but for atypical non-changeable words it can be the opposite."
                    ],
                    "highlighted_evidence": [
                        "At the same time, some information about inflected word forms in the context can be useful, but it is lost during lemmatization, and this leads to the decreased score. Arguably, this means that lemmatization brings along both advantages and disadvantages for WSD with ELMo. "
                    ]
                },
                {
                    "raw_evidence": [
                        "russian\u0414\u043e\u043c\u0438\u043d\u043e is the only target noun in the RUSSE'18 that has no inflected forms, since it is a borrowed word. This leaves no room for improvement when using lemma-based ELMo models: all tokens of this word are already identical. At the same time, some information about inflected word forms in the context can be useful, but it is lost during lemmatization, and this leads to the decreased score. Arguably, this means that lemmatization brings along both advantages and disadvantages for WSD with ELMo. For inflected words (which constitute the majority of Russian vocabulary) profits outweigh the losses, but for atypical non-changeable words it can be the opposite."
                    ],
                    "highlighted_evidence": [
                        "At the same time, some information about inflected word forms in the context can be useful, but it is lost during lemmatization, and this leads to the decreased score. Arguably, this means that lemmatization brings along both advantages and disadvantages for WSD with ELMo. "
                    ]
                }
            ]
        },
        {
            "question": "What other examples of morphologically-rich languages do the authors give?",
            "answers": [
                {
                    "answer": "Russian",
                    "type": "extractive"
                },
                {
                    "answer": "Russian",
                    "type": "extractive"
                }
            ],
            "q_uid": "22815878083ebd2f9e08bc33a5e733063dac7a0f",
            "evidence": [
                {
                    "raw_evidence": [
                        "For the Russian language, with its rich morphology, lemmatizing the training and testing data for ELMo representations yields small but consistent improvements in the WSD task. This is unlike English, where the differences are negligible."
                    ],
                    "highlighted_evidence": [
                        "For the Russian language, with its rich morphology, lemmatizing the training and testing data for ELMo representations yields small but consistent improvements in the WSD task. "
                    ]
                },
                {
                    "raw_evidence": [
                        "The RUSSE'18 dataset was created in 2018 for the shared task in Russian word sense induction. This dataset contains only nouns; the list of words with their English translations is given in Table TABREF30.",
                        "To sum up, the RUSSE'18 dataset is morphologically far more complex than the Senseval3, reflecting the properties of the respective languages. In the next section we will see that this leads to substantial differences regarding comparisons between token-based and lemma-based ELMo models."
                    ],
                    "highlighted_evidence": [
                        "The RUSSE'18 dataset was created in 2018 for the shared task in Russian word sense induction. ",
                        "To sum up, the RUSSE'18 dataset is morphologically far more complex than the Senseval3, reflecting the properties of the respective languages. "
                    ]
                }
            ]
        },
        {
            "question": "Why is lemmatization not necessary in English?",
            "answers": [
                {
                    "answer": "Advanced neural architectures and contextualized embedding models learn how to handle spelling and morphology variations.",
                    "type": "abstractive"
                }
            ],
            "q_uid": "220d11a03897d85af91ec88a9b502815c7d2b6f3",
            "evidence": [
                {
                    "raw_evidence": [
                        "A long-standing tradition if the field of applying deep learning to NLP tasks can be summarised as follows: as minimal pre-processing as possible. It is widely believed that lemmatization or other text input normalisation is not necessary. Advanced neural architectures based on character input (CNNs, BPE, etc) are supposed to be able to learn how to handle spelling and morphology variations themselves, even for languages with rich morphology: `just add more layers!'. Contextualised embedding models follow this tradition: as a rule, they are trained on raw text collections, with minimal linguistic pre-processing. Below, we show that this is not entirely true."
                    ],
                    "highlighted_evidence": [
                        "Advanced neural architectures based on character input (CNNs, BPE, etc) are supposed to be able to learn how to handle spelling and morphology variations themselves, even for languages with rich morphology: `just add more layers!'. Contextualised embedding models follow this tradition: as a rule, they are trained on raw text collections, with minimal linguistic pre-processing. Below, we show that this is not entirely true."
                    ]
                }
            ]
        }
    ],
    "1810.05241": [
        {
            "question": "How is keyphrase diversity measured?",
            "answers": [
                {
                    "answer": "average unique predictions, illustrate the difference of predictions between our proposed models, we show an example chosen from the KP20k validation set",
                    "type": "extractive"
                }
            ],
            "q_uid": "1e4dbfc556cf237accb8b370de2f164fa723687b",
            "evidence": [
                {
                    "raw_evidence": [
                        "To verify our assumption that target encoding and orthogonal regularization help to boost the diversity of generated sequences, we use two metrics, one quantitative and one qualitative, to measure diversity of generation.",
                        "First, we simply calculate the average unique predictions produced by both INLINEFORM0 and INLINEFORM1 in experiments shown in Section SECREF36 . The resulting numbers are 20.38 and 89.70 for INLINEFORM2 and INLINEFORM3 respectively. Second, from the model running on the KP20k validation set, we randomly sample 2000 decoder hidden states at INLINEFORM4 steps following a delimiter ( INLINEFORM5 ) and apply an unsupervised clustering method (t-SNE BIBREF35 ) on them. From the Figure FIGREF46 we can see that hidden states sampled from INLINEFORM6 are easier to cluster while hidden states sampled from INLINEFORM7 yield one mass of vectors with no obvious distinct clusters. Results on both metrics suggest target encoding and orthogonal regularization indeed help diversifying generation of our model.",
                        "To illustrate the difference of predictions between our proposed models, we show an example chosen from the KP20k validation set in Appendix SECREF10 . In this example there are 29 ground truth phrases. Neither of the models is able to generate all of the keyphrases, but it is obvious that the predictions from INLINEFORM0 all start with \u201ctest\u201d, while predictions from INLINEFORM1 are diverse. This to some extent verifies our assumption that without the target encoder and orthogonal regularization, decoder states following delimiters are less diverse."
                    ],
                    "highlighted_evidence": [
                        "To verify our assumption that target encoding and orthogonal regularization help to boost the diversity of generated sequences, we use two metrics, one quantitative and one qualitative, to measure diversity of generation.",
                        "First, we simply calculate the average unique predictions produced by both INLINEFORM0 and INLINEFORM1 in experiments shown in Section SECREF36 .",
                        " In this example there are 29 ground truth phrases. Neither of the models is able to generate all of the keyphrases, but it is obvious that the predictions from INLINEFORM0 all start with \u201ctest\u201d, while predictions from INLINEFORM1 are diverse."
                    ]
                }
            ]
        },
        {
            "question": "How was the StackExchange dataset collected?",
            "answers": [
                {
                    "answer": "they obtained computer science related topics by looking at titles and user-assigned tags",
                    "type": "abstractive"
                }
            ],
            "q_uid": "fff5c24dca92bc7d5435a2600e6764f039551787",
            "evidence": [
                {
                    "raw_evidence": [
                        "Inspired by the StackLite tag recommendation task on Kaggle, we build a new benchmark based on the public StackExchange data. We use questions with titles as source, and user-assigned tags as target keyphrases.",
                        "Since oftentimes the questions on StackExchange contain less information than in scientific publications, there are fewer keyphrases per data point in StackEx. Furthermore, StackExchange uses a tag recommendation system that suggests topic-relevant tags to users while submitting questions; therefore, we are more likely to see general terminology such as Linux and Java. This characteristic challenges models with respect to their ability to distill major topics of a question rather than selecting specific snippets from the text."
                    ],
                    "highlighted_evidence": [
                        "Inspired by the StackLite tag recommendation task on Kaggle, we build a new benchmark based on the public StackExchange data. We use questions with titles as source, and user-assigned tags as target keyphrases.\n\nSince oftentimes the questions on StackExchange contain less information than in scientific publications, there are fewer keyphrases per data point in StackEx. Furthermore, StackExchange uses a tag recommendation system that suggests topic-relevant tags to users while submitting questions; therefore, we are more likely to see general terminology such as Linux and Java. This characteristic challenges models with respect to their ability to distill major topics of a question rather than selecting specific snippets from the text."
                    ]
                }
            ]
        },
        {
            "question": "What two metrics are proposed?",
            "answers": [
                {
                    "answer": "average unique predictions, randomly sample 2000 decoder hidden states at INLINEFORM4 steps following a delimiter ( INLINEFORM5 ) and apply an unsupervised clustering method (t-SNE BIBREF35 )",
                    "type": "extractive"
                }
            ],
            "q_uid": "19b7312cfdddb02c3d4eaa40301a67143a72a35a",
            "evidence": [
                {
                    "raw_evidence": [
                        "First, we simply calculate the average unique predictions produced by both INLINEFORM0 and INLINEFORM1 in experiments shown in Section SECREF36 . The resulting numbers are 20.38 and 89.70 for INLINEFORM2 and INLINEFORM3 respectively. Second, from the model running on the KP20k validation set, we randomly sample 2000 decoder hidden states at INLINEFORM4 steps following a delimiter ( INLINEFORM5 ) and apply an unsupervised clustering method (t-SNE BIBREF35 ) on them. From the Figure FIGREF46 we can see that hidden states sampled from INLINEFORM6 are easier to cluster while hidden states sampled from INLINEFORM7 yield one mass of vectors with no obvious distinct clusters. Results on both metrics suggest target encoding and orthogonal regularization indeed help diversifying generation of our model."
                    ],
                    "highlighted_evidence": [
                        "First, we simply calculate the average unique predictions produced by both INLINEFORM0 and INLINEFORM1 in experiments shown in Section SECREF36 ."
                    ]
                }
            ]
        }
    ],
    "1611.02988": [
        {
            "question": "What was their performance on emotion detection?",
            "answers": [
                {
                    "answer": "Answer with content missing: (Table 3) Best author's model B-M average micro f-score is 0.409, 0.459, 0.411 on Affective, Fairy Tales and ISEAR datasets respectively. ",
                    "type": "abstractive"
                }
            ],
            "q_uid": "fa30a938b58fc05131c3854f12efe376cbad887f",
            "evidence": [
                {
                    "raw_evidence": [
                        "In Table TABREF26 we report the results of our model on the three datasets standardly used for the evaluation of emotion classification, which we have described in Section SECREF3 ."
                    ],
                    "highlighted_evidence": [
                        "In Table TABREF26 we report the results of our model on the three datasets standardly used for the evaluation of emotion classification, which we have described in Section SECREF3 ."
                    ]
                }
            ]
        },
        {
            "question": "Which existing benchmarks did they compare to?",
            "answers": [
                {
                    "answer": "Affective Text, Fairy Tales, ISEAR",
                    "type": "extractive"
                },
                {
                    "answer": " Affective Text dataset, Fairy Tales dataset, ISEAR dataset",
                    "type": "extractive"
                }
            ],
            "q_uid": "f875337f2ecd686cd7789e111174d0f14972638d",
            "evidence": [
                {
                    "raw_evidence": [
                        "Three datasets annotated with emotions are commonly used for the development and evaluation of emotion detection systems, namely the Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. In order to compare our performance to state-of-the-art results, we have used them as well. In this Section, in addition to a description of each dataset, we provide an overview of the emotions used, their distribution, and how we mapped them to those we obtained from Facebook posts in Section SECREF7 . A summary is provided in Table TABREF8 , which also shows, in the bottom row, what role each dataset has in our experiments: apart from the development portion of the Affective Text, which we used to develop our models (Section SECREF4 ), all three have been used as benchmarks for our evaluation."
                    ],
                    "highlighted_evidence": [
                        "Three datasets annotated with emotions are commonly used for the development and evaluation of emotion detection systems, namely the Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset.",
                        "A summary is provided in Table TABREF8 , which also shows, in the bottom row, what role each dataset has in our experiments: apart from the development portion of the Affective Text, which we used to develop our models (Section SECREF4 ), all three have been used as benchmarks for our evaluation."
                    ]
                },
                {
                    "raw_evidence": [
                        "Three datasets annotated with emotions are commonly used for the development and evaluation of emotion detection systems, namely the Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset. In order to compare our performance to state-of-the-art results, we have used them as well. In this Section, in addition to a description of each dataset, we provide an overview of the emotions used, their distribution, and how we mapped them to those we obtained from Facebook posts in Section SECREF7 . A summary is provided in Table TABREF8 , which also shows, in the bottom row, what role each dataset has in our experiments: apart from the development portion of the Affective Text, which we used to develop our models (Section SECREF4 ), all three have been used as benchmarks for our evaluation."
                    ],
                    "highlighted_evidence": [
                        "Three datasets annotated with emotions are commonly used for the development and evaluation of emotion detection systems, namely the Affective Text dataset, the Fairy Tales dataset, and the ISEAR dataset."
                    ]
                }
            ]
        },
        {
            "question": "Which Facebook pages did they look at?",
            "answers": [
                {
                    "answer": "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney",
                    "type": "extractive"
                },
                {
                    "answer": "FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney.",
                    "type": "extractive"
                }
            ],
            "q_uid": "de53af4eddbc30c808d90b8a11a29217d377569e",
            "evidence": [
                {
                    "raw_evidence": [
                        "We collected Facebook posts and their corresponding reactions from public pages using the Facebook API, which we accessed via the Facebook-sdk python library. We chose different pages (and therefore domains and stances), aiming at a balanced and varied dataset, but we did so mainly based on intuition (see Section SECREF4 ) and with an eye to the nature of the datasets available for evaluation (see Section SECREF5 ). The choice of which pages to select posts from is far from trivial, and we believe this is actually an interesting aspect of our approach, as by using different Facebook pages one can intrinsically tackle the domain-adaptation problem (See Section SECREF6 for further discussion on this). The final collection of Facebook pages for the experiments described in this paper is as follows: FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."
                    ],
                    "highlighted_evidence": [
                        "The final collection of Facebook pages for the experiments described in this paper is as follows: FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."
                    ]
                },
                {
                    "raw_evidence": [
                        "We collected Facebook posts and their corresponding reactions from public pages using the Facebook API, which we accessed via the Facebook-sdk python library. We chose different pages (and therefore domains and stances), aiming at a balanced and varied dataset, but we did so mainly based on intuition (see Section SECREF4 ) and with an eye to the nature of the datasets available for evaluation (see Section SECREF5 ). The choice of which pages to select posts from is far from trivial, and we believe this is actually an interesting aspect of our approach, as by using different Facebook pages one can intrinsically tackle the domain-adaptation problem (See Section SECREF6 for further discussion on this). The final collection of Facebook pages for the experiments described in this paper is as follows: FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."
                    ],
                    "highlighted_evidence": [
                        "The final collection of Facebook pages for the experiments described in this paper is as follows: FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney."
                    ]
                }
            ]
        }
    ],
    "2004.03925": [
        {
            "question": "Do the authors give examples of positive and negative sentiment with regard to the virus?",
            "answers": [
                {
                    "answer": "No",
                    "type": "boolean"
                }
            ],
            "q_uid": "d0444cbf01efdcc247b313c7487120a2f047f421",
            "evidence": [
                {
                    "raw_evidence": [],
                    "highlighted_evidence": []
                }
            ]
        },
        {
            "question": "Which word frequencies reflect on the psychology of the twitter users, according to the authors?",
            "answers": [
                {
                    "answer": "unigram, bigram and trigram",
                    "type": "extractive"
                }
            ],
            "q_uid": "1f6666c2c1d1d5f66208a6fa7da3b3442a577dbc",
            "evidence": [
                {
                    "raw_evidence": [
                        "Statistical analysis of tweets ::: Unigram, Bigram an Trigram word frequency analysis",
                        "Three forms of tokens of words have been considered for the study viz. unigram, bigram and trigram. These represent the frequencies of one word, two words together and finally three words coupled. The dataset provides the top 1000 unigrams, top 1000 bigrams and the top 1000 trigrams."
                    ],
                    "highlighted_evidence": [
                        "Statistical analysis of tweets ::: Unigram, Bigram an Trigram word frequency analysis\nThree forms of tokens of words have been considered for the study viz. unigram, bigram and trigram."
                    ]
                }
            ]
        },
        {
            "question": "Do they specify which countries they collected twitter data from?",
            "answers": [
                {
                    "answer": "No",
                    "type": "boolean"
                }
            ],
            "q_uid": "a78a6fd6ca36413586836838e38f3fa9282646ee",
            "evidence": [
                {
                    "raw_evidence": [],
                    "highlighted_evidence": []
                }
            ]
        },
        {
            "question": "Do they collect only English data?",
            "answers": [
                {
                    "answer": "No",
                    "type": "boolean"
                }
            ],
            "q_uid": "c4a0c7b6f1a00f3233a5fe16240a98d9975701c0",
            "evidence": [
                {
                    "raw_evidence": [
                        "In this section, we present the details of the analysis performed on the data obtained pertaining to Twitter messages from January 2020 upto now, that is the time since the news of the Coronavirus outbreak in China was spread across nations. The word frequency data corresponding to the twitter messages has been taken from BIBREF16. The data source indicates that during March 11th to March 30th there were over 4 million tweets a day with the surge in the awareness. Also, the data prominently captures the tweets in English, Spanish, and French languages. A total of four datasets have been used to carry out the study."
                    ],
                    "highlighted_evidence": [
                        "In this section, we present the details of the analysis performed on the data obtained pertaining to Twitter messages from January 2020 upto now, that is the time since the news of the Coronavirus outbreak in China was spread across nations.",
                        "Also, the data prominently captures the tweets in English, Spanish, and French languages."
                    ]
                }
            ]
        }
    ],
    "2002.08899": [
        {
            "question": "Does having constrained neural units imply word meanings are fixed across different context?",
            "answers": [
                {
                    "answer": "No",
                    "type": "boolean"
                }
            ],
            "q_uid": "8ec94313ea908b6462e1f5ee809a977a7b6bdf01",
            "evidence": [
                {
                    "raw_evidence": [
                        "In a natural language translation setting, suppose that an input word corresponds to a set of output tokens independently of its context. Even though this information might be useful to determine the syntax of the input utterance in the first place, the syntax does not determine this knowledge at all (by supposition). So, we can impose the constraint that our model's representation of the input's syntax cannot contain this context-invariant information. This regularization is strictly preferable to allowing all aspects of word meaning to propagate into the input's syntax representation. Without such a constraint, all inputs could, in principle, be given their own syntactic categories. This scenario is refuted by cognitive and neural theories. We incorporate the regularization with neural units that can separate representations of word meaning and arrangement."
                    ],
                    "highlighted_evidence": [
                        "So, we can impose the constraint that our model's representation of the input's syntax cannot contain this context-invariant information. This regularization is strictly preferable to allowing all aspects of word meaning to propagate into the input's syntax representation. Without such a constraint, all inputs could, in principle, be given their own syntactic categories."
                    ]
                }
            ]
        },
        {
            "question": "Do they perform a quantitative analysis of their model displaying knowledge distortions?",
            "answers": [
                {
                    "answer": "Yes",
                    "type": "boolean"
                }
            ],
            "q_uid": "f052444f3b3bf61a3f226645278b780ebd7774db",
            "evidence": [
                {
                    "raw_evidence": [
                        "Additionally, we provide evidence that the model learns knowledge of a separation between syntax and the lexicon that is similar to that of a human. Figure FIGREF6 displays the learned $\\sigma (w)$ embeddings for some input words, across the domains. To avoid cherry-picking the results, we chose the input words arbitrarily, subject to the following constraint. We considered each word to typically have a different syntactic category than the other choices from that domain. This constraint was used to present a diverse selection of words. Table TABREF5 displays the output behavior of models that we damaged to resemble the damage that causes aphasia. To avoid cherry-picking the results, we arbitrarily chose an input for each domain, subject to the following constraint. The input is not in the train set and the undamaged LLA-LSTM model produces a translation that we judge to be correct. For all inputs that we chose, damage to the analog of Broca's area (the LSTMs) results in an output that describes content only if it is described by the input. However, the output does not show understanding of the input's syntax. In the naturalistic domains, damage to the analog of Wernicke's area (the Lexicon Unit) results in an output with incorrect content that would be acceptable if the input had different words but the same syntax. These knowledge distortions are precisely those that are expected in the respective human aphasics BIBREF0. We also provide corpus-level results from the damaged models by presenting mean precision on the test sets. Because the output languages in all of our domains use tokens to represent meanings in many cases, it is expected that the analog to Wernicke's area is responsible for maintaining a high precision."
                    ],
                    "highlighted_evidence": [
                        "TABREF5 displays the output behavior of models that we damaged to resemble the damage that causes aphasia.",
                        "Because the output languages in all of our domains use tokens to represent meanings in many cases, it is expected that the analog to Wernicke's area is responsible for maintaining a high precision."
                    ]
                }
            ]
        }
    ],
    "1809.01060": [
        {
            "question": "What provisional explanation do the authors give for the impact of document context?",
            "answers": [
                {
                    "answer": "adding context causes speakers to focus on broader semantic and pragmatic issues of discourse coherence",
                    "type": "extractive"
                }
            ],
            "q_uid": "6b7d76c1e1a2490beb69609ba5652476dde8831b",
            "evidence": [
                {
                    "raw_evidence": [
                        "This effect of context on human ratings is very similar to the one reported in BIBREF5 . They find that sentences rated as ill formed out of context are improved when they are presented in their document contexts. However the mean ratings for sentences judged to be highly acceptable out of context declined when assessed in context. BIBREF5 's linear regression chart for the correlation between out-of-context and in-context acceptability judgments looks remarkably like our Fig FIGREF15 . There is, then, a striking parallel in the compression pattern that context appears to exert on human judgments for two entirely different linguistic properties.",
                        "This pattern requires an explanation. BIBREF5 suggest that adding context causes speakers to focus on broader semantic and pragmatic issues of discourse coherence, rather than simply judging syntactic well formedness (measured as naturalness) when a sentence is considered in isolation. On this view, compression of rating results from a pressure to construct a plausible interpretation for any sentence within its context."
                    ],
                    "highlighted_evidence": [
                        "This effect of context on human ratings is very similar to the one reported in BIBREF5 . They find that sentences rated as ill formed out of context are improved when they are presented in their document contexts.",
                        "BIBREF5 suggest that adding context causes speakers to focus on broader semantic and pragmatic issues of discourse coherence, rather than simply judging syntactic well formedness (measured as naturalness) when a sentence is considered in isolation."
                    ]
                }
            ]
        },
        {
            "question": "What document context was added?",
            "answers": [
                {
                    "answer": "Preceding and following sentence of each metaphor and paraphrase are added as document context",
                    "type": "abstractive"
                }
            ],
            "q_uid": "37753fbffc06ce7de6ada80c89f1bf5f190bbd88",
            "evidence": [
                {
                    "raw_evidence": [
                        "We extracted 200 sentence pairs from BIBREF3 's dataset and provided each pair with a document context consisting of a preceding and a following sentence, as in the following example."
                    ],
                    "highlighted_evidence": [
                        "We extracted 200 sentence pairs from BIBREF3 's dataset and provided each pair with a document context consisting of a preceding and a following sentence, as in the following example."
                    ]
                }
            ]
        },
        {
            "question": "What were the results of the first experiment?",
            "answers": [
                {
                    "answer": "Best performance achieved is 0.72 F1 score",
                    "type": "abstractive"
                }
            ],
            "q_uid": "7ee29d657ccb8eb9d5ec64d4afc3ca8b5f3bcc9f",
            "evidence": [
                {
                    "raw_evidence": [
                        "We also observe that the best combination seems to consist in training our model on the original out-of-context dataset and testing it on the in-context pairs. In this configuration we reach an F-score (0.72) only slightly lower than the one reported in BIBREF3 (0.74), and we record the highest Pearson correlation, 0.3 (which is still not strong, compared to BIBREF3 's best run, 0.75). This result may partly be an artifact of the the larger amount of training data provided by the out-of-context pairs."
                    ],
                    "highlighted_evidence": [
                        "We also observe that the best combination seems to consist in training our model on the original out-of-context dataset and testing it on the in-context pairs. In this configuration we reach an F-score (0.72) only slightly lower than the one reported in BIBREF3 (0.74), and we record the highest Pearson correlation, 0.3 (which is still not strong, compared to BIBREF3 's best run, 0.75). "
                    ]
                }
            ]
        }
    ],
    "1608.03902": [
        {
            "question": "what was their baseline comparison?",
            "answers": [
                {
                    "answer": "Support Vector Machine (SVM), Logistic Regression (LR), Random Forest (RF)",
                    "type": "extractive"
                }
            ],
            "q_uid": "8fcbae7c3bd85034ae074fa58a35e773936edb5b",
            "evidence": [
                {
                    "raw_evidence": [
                        "To compare our neural models with the traditional approaches, we experimented with a number of existing models including: Support Vector Machine (SVM), a discriminative max-margin model; Logistic Regression (LR), a discriminative probabilistic model; and Random Forest (RF), an ensemble model of decision trees. We use the implementation from the scikit-learn toolkit BIBREF19 . All algorithms use the default value of their parameters."
                    ],
                    "highlighted_evidence": [
                        "To compare our neural models with the traditional approaches, we experimented with a number of existing models including: Support Vector Machine (SVM), a discriminative max-margin model; Logistic Regression (LR), a discriminative probabilistic model; and Random Forest (RF), an ensemble model of decision trees."
                    ]
                }
            ]
        }
    ],
    "1811.11136": [
        {
            "question": "Was the introduced LSTM+CNN model trained on annotated data in a supervised fashion?",
            "answers": [
                {
                    "answer": "Yes",
                    "type": "boolean"
                }
            ],
            "q_uid": "1405824a6845082eae0458c94c4affd7456ad0f7",
            "evidence": [
                {
                    "raw_evidence": [
                        "We train our models on Sentiment140 and Amazon product reviews. Both of these datasets concentrates on sentiment represented by a short text. Summary description of other datasets for validation are also as below:",
                        "We have provided baseline results for the accuracy of other models against datasets (as shown in Table 1 ) For training the softmax model, we divide the text sentiment to two kinds of emotion, positive and negative. And for training the tanh model, we convert the positive and negative emotion to [-1.0, 1.0] continuous sentiment score, while 1.0 means positive and vice versa. We also test our model on various models and calculate metrics such as accuracy, precision and recall and show the results are in Table 2 . Table 3 , Table 4 , Table 5 , Table 6 and Table 7 . Table 8 are more detail information with precisions and recall of our models against other datasets."
                    ],
                    "highlighted_evidence": [
                        "We train our models on Sentiment140 and Amazon product reviews. Both of these datasets concentrates on sentiment represented by a short text. ",
                        "For training the softmax model, we divide the text sentiment to two kinds of emotion, positive and negative. And for training the tanh model, we convert the positive and negative emotion to [-1.0, 1.0] continuous sentiment score, while 1.0 means positive and vice versa. "
                    ]
                }
            ]
        }
    ],
    "1902.09314": [
        {
            "question": "Do they use multi-attention heads?",
            "answers": [
                {
                    "answer": "Yes",
                    "type": "boolean"
                }
            ],
            "q_uid": "9bffc9a9c527e938b2a95ba60c483a916dbd1f6b",
            "evidence": [
                {
                    "raw_evidence": [
                        "The attentional encoder layer is a parallelizable and interactive alternative of LSTM and is applied to compute the hidden states of the input embeddings. This layer consists of two submodules: the Multi-Head Attention (MHA) and the Point-wise Convolution Transformation (PCT)."
                    ],
                    "highlighted_evidence": [
                        "This layer consists of two submodules: the Multi-Head Attention (MHA) and the Point-wise Convolution Transformation (PCT)."
                    ]
                }
            ]
        },
        {
            "question": "How is their model different from BERT?",
            "answers": [
                {
                    "answer": "overall architecture of the proposed Attentional Encoder Network (AEN), which mainly consists of an embedding layer, an attentional encoder layer, a target-specific attention layer, and an output layer.",
                    "type": "extractive"
                }
            ],
            "q_uid": "b67420da975689e47d3ea1c12b601851018c4071",
            "evidence": [
                {
                    "raw_evidence": [
                        "Figure FIGREF9 illustrates the overall architecture of the proposed Attentional Encoder Network (AEN), which mainly consists of an embedding layer, an attentional encoder layer, a target-specific attention layer, and an output layer. Embedding layer has two types: GloVe embedding and BERT embedding. Accordingly, the models are named AEN-GloVe and AEN-BERT."
                    ],
                    "highlighted_evidence": [
                        "Figure FIGREF9 illustrates the overall architecture of the proposed Attentional Encoder Network (AEN), which mainly consists of an embedding layer, an attentional encoder layer, a target-specific attention layer, and an output layer. Embedding layer has two types: GloVe embedding and BERT embedding. Accordingly, the models are named AEN-GloVe and AEN-BERT."
                    ]
                }
            ]
        }
    ],
    "1808.09029": [
        {
            "question": "what data did they use?",
            "answers": [
                {
                    "answer": " Penn Treebank, WikiText2",
                    "type": "extractive"
                },
                {
                    "answer": "Penn Treebank (PTB) , WikiText2 (WT-2)",
                    "type": "extractive"
                }
            ],
            "q_uid": "7bd6a6ec230e1efb27d691762cc0674237dc7967",
            "evidence": [
                {
                    "raw_evidence": [
                        "Following recent works, we compare on two widely used datasets, the Penn Treebank (PTB) BIBREF28 as prepared by BIBREF29 and WikiText2 (WT-2) BIBREF20 . For both datasets, we follow the same training, validation, and test splits as in BIBREF0 ."
                    ],
                    "highlighted_evidence": [
                        "Following recent works, we compare on two widely used datasets, the Penn Treebank (PTB) BIBREF28 as prepared by BIBREF29 and WikiText2 (WT-2) BIBREF20 ."
                    ]
                },
                {
                    "raw_evidence": [
                        "Following recent works, we compare on two widely used datasets, the Penn Treebank (PTB) BIBREF28 as prepared by BIBREF29 and WikiText2 (WT-2) BIBREF20 . For both datasets, we follow the same training, validation, and test splits as in BIBREF0 ."
                    ],
                    "highlighted_evidence": [
                        "Following recent works, we compare on two widely used datasets, the Penn Treebank (PTB) BIBREF28 as prepared by BIBREF29 and WikiText2 (WT-2) BIBREF20 . "
                    ]
                }
            ]
        }
    ],
    "1912.03627": [
        {
            "question": "how was the speech collected?",
            "answers": [
                {
                    "answer": "The speech was collected from respondents using an android application.",
                    "type": "abstractive"
                },
                {
                    "answer": "Android application",
                    "type": "extractive"
                }
            ],
            "q_uid": "fde700d5134a9ae8f7579bea1f1b75f34d7c1c4c",
            "evidence": [
                {
                    "raw_evidence": [
                        "DeepMine is publicly available for everybody with a variety of licenses for different users. It was collected using crowdsourcing BIBREF4. The data collection was done using an Android application. Each respondent installed the application on his/her personal device and recorded several phrases in different sessions. The Android application did various checks on each utterance and if it passed all of them, the respondent was directed to the next phrase. For more information about data collection scenario, please refer to BIBREF4."
                    ],
                    "highlighted_evidence": [
                        "DeepMine is publicly available for everybody with a variety of licenses for different users. It was collected using crowdsourcing BIBREF4. The data collection was done using an Android application. Each respondent installed the application on his/her personal device and recorded several phrases in different sessions. "
                    ]
                },
                {
                    "raw_evidence": [
                        "DeepMine is publicly available for everybody with a variety of licenses for different users. It was collected using crowdsourcing BIBREF4. The data collection was done using an Android application. Each respondent installed the application on his/her personal device and recorded several phrases in different sessions. The Android application did various checks on each utterance and if it passed all of them, the respondent was directed to the next phrase. For more information about data collection scenario, please refer to BIBREF4."
                    ],
                    "highlighted_evidence": [
                        "It was collected using crowdsourcing BIBREF4. The data collection was done using an Android application."
                    ]
                }
            ]
        },
        {
            "question": "what evaluation protocols are provided?",
            "answers": [
                {
                    "answer": "three experimental setups with different numbers of speakers in the evaluation set, three experimental setups with different number of speaker in the evaluation set are defined,  first one, respondents with at least 17 recording sessions are included to the evaluation set, respondents with 16 sessions to the development and the rest of respondents to the background set, second setup, respondents with at least 8 sessions are included to the evaluation set, respondents with 6 or 7 sessions to the development and the rest of respondents to the background set",
                    "type": "extractive"
                }
            ],
            "q_uid": "f9edd8f9c13b54d8b1253ed30e7decc1999602da",
            "evidence": [
                {
                    "raw_evidence": [
                        "The DeepMine database consists of three parts. The first one contains fixed common phrases to perform text-dependent speaker verification. The second part consists of random sequences of words useful for text-prompted speaker verification, and the last part includes phrases with word- and phoneme-level transcription, useful for text-independent speaker verification using a random phrase (similar to Part4 of RedDots). This part can also serve for Persian ASR training. Each part is described in more details below. Table TABREF11 shows the number of unique phrases in each part of the database. For the English text-dependent part, the following phrases were selected from part1 of the RedDots database, hence the RedDots can be used as an additional training set for this part:",
                        "DeepMine Database Parts ::: Part1 - Text-dependent (TD)",
                        "This part contains a set of fixed phrases which are used to verify speakers in text-dependent mode. Each speaker utters 5 Persian phrases, and if the speaker can read English, 5 phrases selected from Part1 of the RedDots database are also recorded.",
                        "We have created three experimental setups with different numbers of speakers in the evaluation set. For each setup, speakers with more recording sessions are included in the evaluation set and the rest of the speakers are used for training in the background set (in the database, all background sets are basically training data). The rows in Table TABREF13 corresponds to the different experimental setups and shows the numbers of speakers in each set. Note that, for English, we have filtered the (Persian native) speakers by the ability to read English. Therefore, there are fewer speakers in each set for English than for Persian. There is a small \u201cdev\u201d set in each setup which can be used for parameter tuning to prevent over-tuning on the evaluation set.",
                        "Similar to the text-dependent case, three experimental setups with different number of speaker in the evaluation set are defined (corresponding to the rows in Table TABREF16). However, different strategy is used for defining trials: Depending on the enrollment condition (1- to 3-sess), trials are enrolled on utterances of all words from 1 to 3 different sessions (i.e. 3 to 9 utterances). Further, we consider two conditions for test utterances: seq test utterance with only 3 or 4 words and full test utterances with all words (i.e. same words as in enrollment but in different order). From all setups an all conditions, the 100-spk with 1-session enrolment (1-sess) is considered as the main evaluation condition for the text-prompted case. In Table TABREF16, the numbers of trials (sum for both seq and full conditions) for Persian 1-sess are shown for the different types of trials in the text-prompted SV. Again, we just create one IW trial for each IC trial.",
                        "Based on the recording sessions, we created two experimental setups for speaker verification. In the first one, respondents with at least 17 recording sessions are included to the evaluation set, respondents with 16 sessions to the development and the rest of respondents to the background set (can be used as training data). In the second setup, respondents with at least 8 sessions are included to the evaluation set, respondents with 6 or 7 sessions to the development and the rest of respondents to the background set. Table TABREF18 shows numbers of speakers in each set of the database for text-independent SV case."
                    ],
                    "highlighted_evidence": [
                        "The DeepMine database consists of three parts. The first one contains fixed common phrases to perform text-dependent speaker verification. The second part consists of random sequences of words useful for text-prompted speaker verification, and the last part includes phrases with word- and phoneme-level transcription, useful for text-independent speaker verification using a random phrase (similar to Part4 of RedDots).",
                        "DeepMine Database Parts ::: Part1 - Text-dependent (TD)\nThis part contains a set of fixed phrases which are used to verify speakers in text-dependent mode. Each speaker utters 5 Persian phrases, and if the speaker can read English, 5 phrases selected from Part1 of the RedDots database are also recorded.\n\nWe have created three experimental setups with different numbers of speakers in the evaluation set.",
                        "Similar to the text-dependent case, three experimental setups with different number of speaker in the evaluation set are defined (corresponding to the rows in Table TABREF16). However, different strategy is used for defining trials: Depending on the enrollment condition (1- to 3-sess), trials are enrolled on utterances of all words from 1 to 3 different sessions (i.e. 3 to 9 utterances). Further, we consider two conditions for test utterances: seq test utterance with only 3 or 4 words and full test utterances with all words (i.e. same words as in enrollment but in different order).",
                        "Based on the recording sessions, we created two experimental setups for speaker verification. In the first one, respondents with at least 17 recording sessions are included to the evaluation set, respondents with 16 sessions to the development and the rest of respondents to the background set (can be used as training data). In the second setup, respondents with at least 8 sessions are included to the evaluation set, respondents with 6 or 7 sessions to the development and the rest of respondents to the background set. ",
                        "two experimental setups for speaker verification"
                    ]
                }
            ]
        },
        {
            "question": "what is the source of the data?",
            "answers": [
                {
                    "answer": "Android application",
                    "type": "extractive"
                }
            ],
            "q_uid": "30af1926559079f59b0df055da76a3a34df8336f",
            "evidence": [
                {
                    "raw_evidence": [
                        "DeepMine is publicly available for everybody with a variety of licenses for different users. It was collected using crowdsourcing BIBREF4. The data collection was done using an Android application. Each respondent installed the application on his/her personal device and recorded several phrases in different sessions. The Android application did various checks on each utterance and if it passed all of them, the respondent was directed to the next phrase. For more information about data collection scenario, please refer to BIBREF4."
                    ],
                    "highlighted_evidence": [
                        "It was collected using crowdsourcing BIBREF4. The data collection was done using an Android application."
                    ]
                }
            ]
        }
    ],
    "1710.06700": [
        {
            "question": "How was speed measured?",
            "answers": [
                {
                    "answer": "how long it takes the system to lemmatize a set number of words",
                    "type": "abstractive"
                }
            ],
            "q_uid": "da845a2a930fd6a3267950bec5928205b6c6e8e8",
            "evidence": [
                {
                    "raw_evidence": [
                        "In terms of speed, our system was able to lemmatize 7.4 million words on a personal laptop in almost 2 minutes compared to 2.5 hours for MADAMIRA, i.e. 75 times faster. The code is written entirely in Java without any external dependency which makes its integration in other systems quite simple."
                    ],
                    "highlighted_evidence": [
                        "In terms of speed, our system was able to lemmatize 7.4 million words on a personal laptop in almost 2 minutes compared to 2.5 hours for MADAMIRA, i.e. 75 times faster. "
                    ]
                }
            ]
        },
        {
            "question": "What is the state of the art?",
            "answers": [
                {
                    "answer": " MADAMIRA BIBREF6 system",
                    "type": "extractive"
                }
            ],
            "q_uid": "76ce9e02d97e2d77fe28c0fa78526809e7c195c6",
            "evidence": [
                {
                    "raw_evidence": [
                        "Khoja's stemmer BIBREF4 and Buckwalter morphological analyzer BIBREF5 are other root-based analyzers and stemmers which use tables of valid combinations between prefixes and suffixes, prefixes and stems, and stems and suffixes. Recently, MADAMIRA BIBREF6 system has been evaluated using a blind testset (25K words for Modern Standard Arabic (MSA) selected from Penn Arabic Tree bank (PATB)), and the reported accuracy was 96.2% as the percentage of words where the chosen analysis (provided by SAMA morphological analyzer BIBREF7 ) has the correct lemma.",
                        "As MSA is usually written without diacritics and IR systems normally remove all diacritics from search queries and indexed data as a basic preprocessing step, so another column for undiacritized lemma is added and it's used for evaluating our lemmatizer and comparing with state-of-the-art system for lemmatization; MADAMIRA."
                    ],
                    "highlighted_evidence": [
                        "Recently, MADAMIRA BIBREF6 system has been evaluated using a blind testset (25K words for Modern Standard Arabic (MSA) selected from Penn Arabic Tree bank (PATB)), and the reported accuracy was 96.2% as the percentage of words where the chosen analysis (provided by SAMA morphological analyzer BIBREF7 ) has the correct lemma.",
                        "As MSA is usually written without diacritics and IR systems normally remove all diacritics from search queries and indexed data as a basic preprocessing step, so another column for undiacritized lemma is added and it's used for evaluating our lemmatizer and comparing with state-of-the-art system for lemmatization; MADAMIRA."
                    ]
                }
            ]
        },
        {
            "question": "How was the dataset annotated?",
            "answers": [
                {
                    "answer": "Lemmatization is done by an expert Arabic linguist where spelling corrections are marked, and lemmas are provided with full diacritization",
                    "type": "extractive"
                }
            ],
            "q_uid": "64c7545ce349265e0c97fd6c434a5f8efdc23777",
            "evidence": [
                {
                    "raw_evidence": [
                        "To make the annotated data publicly available, we selected 70 news articles from Arabic WikiNews site https://ar.wikinews.org/wiki. These articles cover recent news from year 2013 to year 2015 in multiple genres (politics, economics, health, science and technology, sports, arts, and culture.) Articles contain 18,300 words, and they are evenly distributed among these 7 genres with 10 articles per each.",
                        "Word are white-space and punctuation separated, and some spelling errors are corrected (1.33% of the total words) to have very clean test cases. Lemmatization is done by an expert Arabic linguist where spelling corrections are marked, and lemmas are provided with full diacritization as shown in Figure FIGREF2 ."
                    ],
                    "highlighted_evidence": [
                        "To make the annotated data publicly available, we selected 70 news articles from Arabic WikiNews site https://ar.wikinews.org/wiki. These articles cover recent news from year 2013 to year 2015 in multiple genres (politics, economics, health, science and technology, sports, arts, and culture.) Articles contain 18,300 words, and they are evenly distributed among these 7 genres with 10 articles per each.\n\nWord are white-space and punctuation separated, and some spelling errors are corrected (1.33% of the total words) to have very clean test cases. Lemmatization is done by an expert Arabic linguist where spelling corrections are marked, and lemmas are provided with full diacritization as shown in Figure FIGREF2 ."
                    ]
                }
            ]
        },
        {
            "question": "What is the size of the dataset?",
            "answers": [
                {
                    "answer": "Articles contain 18,300 words, and they are evenly distributed among these 7 genres with 10 articles per each",
                    "type": "extractive"
                }
            ],
            "q_uid": "47822fec590e840438a3054b7f512fec09dbd1e1",
            "evidence": [
                {
                    "raw_evidence": [
                        "To make the annotated data publicly available, we selected 70 news articles from Arabic WikiNews site https://ar.wikinews.org/wiki. These articles cover recent news from year 2013 to year 2015 in multiple genres (politics, economics, health, science and technology, sports, arts, and culture.) Articles contain 18,300 words, and they are evenly distributed among these 7 genres with 10 articles per each."
                    ],
                    "highlighted_evidence": [
                        "To make the annotated data publicly available, we selected 70 news articles from Arabic WikiNews site https://ar.wikinews.org/wiki. These articles cover recent news from year 2013 to year 2015 in multiple genres (politics, economics, health, science and technology, sports, arts, and culture.) Articles contain 18,300 words, and they are evenly distributed among these 7 genres with 10 articles per each."
                    ]
                }
            ]
        },
        {
            "question": "Where did they collect their dataset from?",
            "answers": [
                {
                    "answer": "from Arabic WikiNews site https://ar.wikinews.org/wiki",
                    "type": "extractive"
                }
            ],
            "q_uid": "989271972b3176d0a5dabd1cc0e4bdb671269c96",
            "evidence": [
                {
                    "raw_evidence": [
                        "To make the annotated data publicly available, we selected 70 news articles from Arabic WikiNews site https://ar.wikinews.org/wiki. These articles cover recent news from year 2013 to year 2015 in multiple genres (politics, economics, health, science and technology, sports, arts, and culture.) Articles contain 18,300 words, and they are evenly distributed among these 7 genres with 10 articles per each."
                    ],
                    "highlighted_evidence": [
                        "To make the annotated data publicly available, we selected 70 news articles from Arabic WikiNews site https://ar.wikinews.org/wiki. These articles cover recent news from year 2013 to year 2015 in multiple genres (politics, economics, health, science and technology, sports, arts, and culture.) Articles contain 18,300 words, and they are evenly distributed among these 7 genres with 10 articles per each."
                    ]
                }
            ]
        }
    ],
    "1908.05828": [
        {
            "question": "What is the best model?",
            "answers": [
                {
                    "answer": "BiLSTM+CNN(grapheme-level) and BiLSTM+CNN(G)+POS ",
                    "type": "extractive"
                }
            ],
            "q_uid": "567dc9bad8428ea9a2658c88203a0ed0f8da0dc3",
            "evidence": [
                {
                    "raw_evidence": [
                        "We also present a neural architecture BiLSTM+CNN(grapheme-level) which turns out to be performing on par with BiLSTM+CNN(character-level) under the same configuration. We believe this will not only help Nepali language but also other languages falling under the umbrellas of Devanagari languages. Our model BiLSTM+CNN(grapheme-level) and BiLSTM+CNN(G)+POS outperforms all other model experimented in OurNepali and ILPRL dataset respectively."
                    ],
                    "highlighted_evidence": [
                        "Our model BiLSTM+CNN(grapheme-level) and BiLSTM+CNN(G)+POS outperforms all other model experimented in OurNepali and ILPRL dataset respectively."
                    ]
                }
            ]
        },
        {
            "question": "Do the authors train a Naive Bayes classifier on their dataset?",
            "answers": [
                {
                    "answer": "No",
                    "type": "boolean"
                },
                {
                    "answer": "No",
                    "type": "boolean"
                }
            ],
            "q_uid": "d8627ba08b7342e473b8a2b560baa8cdbae3c7fd",
            "evidence": [
                {
                    "raw_evidence": [],
                    "highlighted_evidence": []
                },
                {
                    "raw_evidence": [],
                    "highlighted_evidence": []
                }
            ]
        },
        {
            "question": "Which machine learning models do they explore?",
            "answers": [
                {
                    "answer": "BiLSTM, BiLSTM-CNN, BiLSTM-CRF, BiLSTM-CNN-CRF",
                    "type": "extractive"
                },
                {
                    "answer": "BiLSTMBIBREF14, BiLSTM+CNNBIBREF20, BiLSTM+CRFBIBREF1, BiLSTM+CNN+CRFBIBREF2, CNN modelBIBREF0 and Stanford CRF modelBIBREF21",
                    "type": "extractive"
                }
            ],
            "q_uid": "8a7615fc6ff1de287d36ab21bf2c6a3b2914f73d",
            "evidence": [
                {
                    "raw_evidence": [
                        "In this section, we present the details about training our neural network. The neural network architecture are implemented using PyTorch framework BIBREF26. The training is performed on a single Nvidia Tesla P100 SXM2. We first run our experiment on BiLSTM, BiLSTM-CNN, BiLSTM-CRF BiLSTM-CNN-CRF using the hyper-parameters mentioned in Table TABREF30. The training and evaluation was done on sentence-level. The RNN variants are initialized randomly from $(-\\sqrt{k},\\sqrt{k})$ where $k=\\frac{1}{hidden\\_size}$."
                    ],
                    "highlighted_evidence": [
                        "We first run our experiment on BiLSTM, BiLSTM-CNN, BiLSTM-CRF BiLSTM-CNN-CRF using the hyper-parameters mentioned in Table TABREF30. "
                    ]
                },
                {
                    "raw_evidence": [
                        "Similar approaches has been applied to many South Asian languages like HindiBIBREF6, IndonesianBIBREF7, BengaliBIBREF19 and In this paper, we present the neural network architecture for NER task in Nepali language, which doesn't require any manual feature engineering nor any data pre-processing during training. First we are comparing BiLSTMBIBREF14, BiLSTM+CNNBIBREF20, BiLSTM+CRFBIBREF1, BiLSTM+CNN+CRFBIBREF2 models with CNN modelBIBREF0 and Stanford CRF modelBIBREF21. Secondly, we show the comparison between models trained on general word embeddings, word embedding + character-level embedding, word embedding + part-of-speech(POS) one-hot encoding and word embedding + grapheme clustered or sub-word embeddingBIBREF22. The experiments were performed on the dataset that we created and on the dataset received from ILPRL lab. Our extensive study shows that augmenting word embedding with character or grapheme-level representation and POS one-hot encoding vector yields better results compared to using general word embedding alone."
                    ],
                    "highlighted_evidence": [
                        "First we are comparing BiLSTMBIBREF14, BiLSTM+CNNBIBREF20, BiLSTM+CRFBIBREF1, BiLSTM+CNN+CRFBIBREF2 models with CNN modelBIBREF0 and Stanford CRF modelBIBREF21."
                    ]
                }
            ]
        },
        {
            "question": "What is the source of their dataset?",
            "answers": [
                {
                    "answer": "daily newspaper of the year 2015-2016",
                    "type": "extractive"
                },
                {
                    "answer": "daily newspaper of the year 2015-2016",
                    "type": "extractive"
                }
            ],
            "q_uid": "bb2de20ee5937da7e3e6230e942bec7b6e8f61ee",
            "evidence": [
                {
                    "raw_evidence": [
                        "Since, we there was no publicly available standard Nepali NER dataset and did not receive any dataset from the previous researchers, we had to create our own dataset. This dataset contains the sentences collected from daily newspaper of the year 2015-2016. This dataset has three major classes Person (PER), Location (LOC) and Organization (ORG). Pre-processing was performed on the text before creation of the dataset, for example all punctuations and numbers besides ',', '-', '|' and '.' were removed. Currently, the dataset is in standard CoNLL-2003 IO formatBIBREF25."
                    ],
                    "highlighted_evidence": [
                        "Since, we there was no publicly available standard Nepali NER dataset and did not receive any dataset from the previous researchers, we had to create our own dataset. This dataset contains the sentences collected from daily newspaper of the year 2015-2016. "
                    ]
                },
                {
                    "raw_evidence": [
                        "Since, we there was no publicly available standard Nepali NER dataset and did not receive any dataset from the previous researchers, we had to create our own dataset. This dataset contains the sentences collected from daily newspaper of the year 2015-2016. This dataset has three major classes Person (PER), Location (LOC) and Organization (ORG). Pre-processing was performed on the text before creation of the dataset, for example all punctuations and numbers besides ',', '-', '|' and '.' were removed. Currently, the dataset is in standard CoNLL-2003 IO formatBIBREF25."
                    ],
                    "highlighted_evidence": [
                        "This dataset contains the sentences collected from daily newspaper of the year 2015-2016."
                    ]
                }
            ]
        },
        {
            "question": "Do they try to use byte-pair encoding representations?",
            "answers": [
                {
                    "answer": "No",
                    "type": "boolean"
                },
                {
                    "answer": "No",
                    "type": "boolean"
                }
            ],
            "q_uid": "1170e4ee76fa202cabac9f621e8fbeb4a6c5f094",
            "evidence": [
                {
                    "raw_evidence": [],
                    "highlighted_evidence": []
                },
                {
                    "raw_evidence": [],
                    "highlighted_evidence": []
                }
            ]
        },
        {
            "question": "Which models are used to solve NER for Nepali?",
            "answers": [
                {
                    "answer": "BiLSTMBIBREF14, BiLSTM+CNNBIBREF20, BiLSTM+CRFBIBREF1, BiLSTM+CNN+CRFBIBREF2, CNN modelBIBREF0 and Stanford CRF modelBIBREF21",
                    "type": "extractive"
                },
                {
                    "answer": "BiLSTM, BiLSTM+CNN, BiLSTM+CRF, BiLSTM+CNN+CRF, CNN, Stanford CRF",
                    "type": "extractive"
                }
            ],
            "q_uid": "6d1217b3d9cfb04be7fcd2238666fa02855ce9c5",
            "evidence": [
                {
                    "raw_evidence": [
                        "Similar approaches has been applied to many South Asian languages like HindiBIBREF6, IndonesianBIBREF7, BengaliBIBREF19 and In this paper, we present the neural network architecture for NER task in Nepali language, which doesn't require any manual feature engineering nor any data pre-processing during training. First we are comparing BiLSTMBIBREF14, BiLSTM+CNNBIBREF20, BiLSTM+CRFBIBREF1, BiLSTM+CNN+CRFBIBREF2 models with CNN modelBIBREF0 and Stanford CRF modelBIBREF21. Secondly, we show the comparison between models trained on general word embeddings, word embedding + character-level embedding, word embedding + part-of-speech(POS) one-hot encoding and word embedding + grapheme clustered or sub-word embeddingBIBREF22. The experiments were performed on the dataset that we created and on the dataset received from ILPRL lab. Our extensive study shows that augmenting word embedding with character or grapheme-level representation and POS one-hot encoding vector yields better results compared to using general word embedding alone."
                    ],
                    "highlighted_evidence": [
                        "First we are comparing BiLSTMBIBREF14, BiLSTM+CNNBIBREF20, BiLSTM+CRFBIBREF1, BiLSTM+CNN+CRFBIBREF2 models with CNN modelBIBREF0 and Stanford CRF modelBIBREF21."
                    ]
                },
                {
                    "raw_evidence": [
                        "Similar approaches has been applied to many South Asian languages like HindiBIBREF6, IndonesianBIBREF7, BengaliBIBREF19 and In this paper, we present the neural network architecture for NER task in Nepali language, which doesn't require any manual feature engineering nor any data pre-processing during training. First we are comparing BiLSTMBIBREF14, BiLSTM+CNNBIBREF20, BiLSTM+CRFBIBREF1, BiLSTM+CNN+CRFBIBREF2 models with CNN modelBIBREF0 and Stanford CRF modelBIBREF21. Secondly, we show the comparison between models trained on general word embeddings, word embedding + character-level embedding, word embedding + part-of-speech(POS) one-hot encoding and word embedding + grapheme clustered or sub-word embeddingBIBREF22. The experiments were performed on the dataset that we created and on the dataset received from ILPRL lab. Our extensive study shows that augmenting word embedding with character or grapheme-level representation and POS one-hot encoding vector yields better results compared to using general word embedding alone."
                    ],
                    "highlighted_evidence": [
                        "Similar approaches has been applied to many South Asian languages like HindiBIBREF6, IndonesianBIBREF7, BengaliBIBREF19 and In this paper, we present the neural network architecture for NER task in Nepali language, which doesn't require any manual feature engineering nor any data pre-processing during training. First we are comparing BiLSTMBIBREF14, BiLSTM+CNNBIBREF20, BiLSTM+CRFBIBREF1, BiLSTM+CNN+CRFBIBREF2 models with CNN modelBIBREF0 and Stanford CRF modelBIBREF21. "
                    ]
                }
            ]
        }
    ],
    "1907.11907": [
        {
            "question": "How are the substitution rules built?",
            "answers": [
                {
                    "answer": "from the Database of Modern Icelandic Inflection (DMII) BIBREF1",
                    "type": "extractive"
                }
            ],
            "q_uid": "8c981f8b992cb583e598f71741c322f522c6d2ad",
            "evidence": [
                {
                    "raw_evidence": [
                        "In this paper, we describe and evaluate Nefnir BIBREF0 , a new open source lemmatizer for Icelandic. Nefnir uses suffix substitution rules derived (learned) from the Database of Modern Icelandic Inflection (DMII) BIBREF1 , which contains over 5.8 million inflectional forms."
                    ],
                    "highlighted_evidence": [
                        "In this paper, we describe and evaluate Nefnir BIBREF0 , a new open source lemmatizer for Icelandic. Nefnir uses suffix substitution rules derived (learned) from the Database of Modern Icelandic Inflection (DMII) BIBREF1 , which contains over 5.8 million inflectional forms."
                    ]
                }
            ]
        },
        {
            "question": "Which dataset do they use?",
            "answers": [
                {
                    "answer": "a reference corpus of 21,093 tokens and their correct lemmas",
                    "type": "extractive"
                }
            ],
            "q_uid": "16f33de90b76975a99572e0684632d5aedbd957c",
            "evidence": [
                {
                    "raw_evidence": [
                        "We have evaluated the output of Nefnir against a reference corpus of 21,093 tokens and their correct lemmas.",
                        "Samples for the reference corpus were extracted from two larger corpora, in order to obtain a diverse vocabulary:"
                    ],
                    "highlighted_evidence": [
                        "We have evaluated the output of Nefnir against a reference corpus of 21,093 tokens and their correct lemmas.\n\nSamples for the reference corpus were extracted from two larger corpora, in order to obtain a diverse vocabulary:"
                    ]
                }
            ]
        }
    ],
    "1912.06813": [
        {
            "question": "What datasets are experimented with?",
            "answers": [
                {
                    "answer": "the CMU ARCTIC database BIBREF33,  the M-AILABS speech dataset BIBREF34 ",
                    "type": "extractive"
                }
            ],
            "q_uid": "6ee27ab55b1f64783a9e72e3f83b7c9ec5cc8073",
            "evidence": [
                {
                    "raw_evidence": [
                        "We conducted our experiments on the CMU ARCTIC database BIBREF33, which contains parallel recordings of professional US English speakers sampled at 16 kHz. One female (slt) was chosen as the target speaker and one male (bdl) and one female (clb) were chosen as sources. We selected 100 utterances each for validation and evaluation, and the other 932 utterances were used as training data. For the TTS corpus, we chose a US female English speaker (judy bieber) from the M-AILABS speech dataset BIBREF34 to train a single-speaker Transformer-TTS model. With the sampling rate also at 16 kHz, the training set contained 15,200 utterances, which were roughly 32 hours long."
                    ],
                    "highlighted_evidence": [
                        "We conducted our experiments on the CMU ARCTIC database BIBREF33, which contains parallel recordings of professional US English speakers sampled at 16 kHz. One female (slt) was chosen as the target speaker and one male (bdl) and one female (clb) were chosen as sources. We selected 100 utterances each for validation and evaluation, and the other 932 utterances were used as training data. For the TTS corpus, we chose a US female English speaker (judy bieber) from the M-AILABS speech dataset BIBREF34 to train a single-speaker Transformer-TTS model. With the sampling rate also at 16 kHz, the training set contained 15,200 utterances, which were roughly 32 hours long."
                    ]
                }
            ]
        },
        {
            "question": "What is the baseline model?",
            "answers": [
                {
                    "answer": "a RNN-based seq2seq VC model called ATTS2S based on the Tacotron model",
                    "type": "abstractive"
                }
            ],
            "q_uid": "bb4de896c0fa4bf3c8c43137255a4895f52abeef",
            "evidence": [
                {
                    "raw_evidence": [
                        "Next, we compared our VTN model with an RNN-based seq2seq VC model called ATTS2S BIBREF8. This model is based on the Tacotron model BIBREF32 with the help of context preservation loss and guided attention loss to stabilize training and maintain linguistic consistency after conversion. We followed the configurations in BIBREF8 but used mel spectrograms instead of WORLD features."
                    ],
                    "highlighted_evidence": [
                        "Next, we compared our VTN model with an RNN-based seq2seq VC model called ATTS2S BIBREF8. This model is based on the Tacotron model BIBREF32 with the help of context preservation loss and guided attention loss to stabilize training and maintain linguistic consistency after conversion. We followed the configurations in BIBREF8 but used mel spectrograms instead of WORLD features."
                    ]
                }
            ]
        }
    ],
    "1909.06762": [
        {
            "question": "What were the evaluation metrics?",
            "answers": [
                {
                    "answer": "BLEU, Micro Entity F1, quality of the responses according to correctness, fluency, and humanlikeness on a scale from 1 to 5",
                    "type": "extractive"
                }
            ],
            "q_uid": "ee31c8a94e07b3207ca28caef3fbaf9a38d94964",
            "evidence": [
                {
                    "raw_evidence": [
                        "Follow the prior works BIBREF6, BIBREF7, BIBREF9, we adopt the BLEU and the Micro Entity F1 to evaluate our model performance. The experimental results are illustrated in Table TABREF30.",
                        "We provide human evaluation on our framework and the compared models. These responses are based on distinct dialogue history. We hire several human experts and ask them to judge the quality of the responses according to correctness, fluency, and humanlikeness on a scale from 1 to 5. In each judgment, the expert is presented with the dialogue history, an output of a system with the name anonymized, and the gold response."
                    ],
                    "highlighted_evidence": [
                        "Follow the prior works BIBREF6, BIBREF7, BIBREF9, we adopt the BLEU and the Micro Entity F1 to evaluate our model performance. ",
                        "We provide human evaluation on our framework and the compared models. ",
                        "We hire several human experts and ask them to judge the quality of the responses according to correctness, fluency, and humanlikeness on a scale from 1 to 5."
                    ]
                }
            ]
        },
        {
            "question": "What were the baseline systems?",
            "answers": [
                {
                    "answer": "Attn seq2seq, Ptr-UNK, KV Net, Mem2Seq, DSR",
                    "type": "extractive"
                }
            ],
            "q_uid": "66d743b735ba75589486e6af073e955b6bb9d2a4",
            "evidence": [
                {
                    "raw_evidence": [
                        "We compare our model with several baselines including:",
                        "Attn seq2seq BIBREF22: A model with simple attention over the input context at each time step during decoding.",
                        "Ptr-UNK BIBREF23: Ptr-UNK is the model which augments a sequence-to-sequence architecture with attention-based copy mechanism over the encoder context.",
                        "KV Net BIBREF6: The model adopted and argumented decoder which decodes over the concatenation of vocabulary and KB entities, which allows the model to generate entities.",
                        "Mem2Seq BIBREF7: Mem2Seq is the model that takes dialogue history and KB entities as input and uses a pointer gate to control either generating a vocabulary word or selecting an input as the output.",
                        "DSR BIBREF9: DSR leveraged dialogue state representation to retrieve the KB implicitly and applied copying mechanism to retrieve entities from knowledge base while decoding."
                    ],
                    "highlighted_evidence": [
                        "We compare our model with several baselines including:\n\nAttn seq2seq BIBREF22: A model with simple attention over the input context at each time step during decoding.\n\nPtr-UNK BIBREF23: Ptr-UNK is the model which augments a sequence-to-sequence architecture with attention-based copy mechanism over the encoder context.\n\nKV Net BIBREF6: The model adopted and argumented decoder which decodes over the concatenation of vocabulary and KB entities, which allows the model to generate entities.\n\nMem2Seq BIBREF7: Mem2Seq is the model that takes dialogue history and KB entities as input and uses a pointer gate to control either generating a vocabulary word or selecting an input as the output.\n\nDSR BIBREF9: DSR leveraged dialogue state representation to retrieve the KB implicitly and applied copying mechanism to retrieve entities from knowledge base while decoding."
                    ]
                }
            ]
        },
        {
            "question": "Which dialog datasets did they experiment with?",
            "answers": [
                {
                    "answer": "Camrest, InCar Assistant",
                    "type": "extractive"
                }
            ],
            "q_uid": "b9f852256113ef468d60e95912800fab604966f6",
            "evidence": [
                {
                    "raw_evidence": [
                        "Since dialogue dataset is not typically annotated with the retrieval results, training the KB-retriever is non-trivial. To make the training feasible, we propose two methods: 1) we use a set of heuristics to derive the training data and train the retriever in a distant supervised fashion; 2) we use Gumbel-Softmax BIBREF14 as an approximation of the non-differentiable selecting process and train the retriever along with the Seq2Seq dialogue generation model. Experiments on two publicly available datasets (Camrest BIBREF11 and InCar Assistant BIBREF6) confirm the effectiveness of the KB-retriever. Both the retrievers trained with distant-supervision and Gumbel-Softmax technique outperform the compared systems in the automatic and human evaluations. Analysis empirically verifies our assumption that more than 80% responses in the dataset can be supported by a single KB row and better retrieval results lead to better task-oriented dialogue generation performance."
                    ],
                    "highlighted_evidence": [
                        "Experiments on two publicly available datasets (Camrest BIBREF11 and InCar Assistant BIBREF6) confirm the effectiveness of the KB-retriever."
                    ]
                }
            ]
        }
    ],
    "1903.09722": [
        {
            "question": "What dataset do they use?",
            "answers": [
                {
                    "answer": "German newscrawl distributed by WMT'18 , English newscrawl data, WMT'18 English-German (en-de) news translation task , WMT'18 English-Turkish (en-tr) news task",
                    "type": "extractive"
                },
                {
                    "answer": "German newscrawl, English newscrawl, WMT'18 English-German (en-de) news, WMT'18 English-Turkish (en-tr) news task, WMT'18 English-German (en-de) news translation task and we validate our findings on the WMT'18 English-Turkish (en-tr) news task",
                    "type": "extractive"
                }
            ],
            "q_uid": "6ca938324dc7e1742a840d0a54dc13cc207394a1",
            "evidence": [
                {
                    "raw_evidence": [
                        "We train language models on two languages: One model is estimated on the German newscrawl distributed by WMT'18 comprising 260M sentences or 6B tokens. Another model is trained on the English newscrawl data comprising 193M sentences or 5B tokens. We learn a joint Byte-Pair-Encoding (BPE; Sennrich et al., 2016) vocabulary of 37K types on the German and English newscrawl and train the language models with this vocabulary.",
                        "We consider two benchmarks: Most experiments are run on the WMT'18 English-German (en-de) news translation task and we validate our findings on the WMT'18 English-Turkish (en-tr) news task. For WMT'18 English-German, the training corpus consists of all available bitext excluding the ParaCrawl corpus and we remove sentences longer than 250 tokens as well as sentence-pairs with a source/target length ratio exceeding 1.5. This results in 5.18M sentence pairs. We tokenize all data with the Moses tokenizer BIBREF8 and apply the BPE vocabulary learned on the monolingual corpora."
                    ],
                    "highlighted_evidence": [
                        "We train language models on two languages: One model is estimated on the German newscrawl distributed by WMT'18 comprising 260M sentences or 6B tokens. Another model is trained on the English newscrawl data comprising 193M sentences or 5B tokens. We learn a joint Byte-Pair-Encoding (BPE; Sennrich et al., 2016) vocabulary of 37K types on the German and English newscrawl and train the language models with this vocabulary.",
                        "We consider two benchmarks: Most experiments are run on the WMT'18 English-German (en-de) news translation task and we validate our findings on the WMT'18 English-Turkish (en-tr) news task. For WMT'18 English-German, the training corpus consists of all available bitext excluding the ParaCrawl corpus and we remove sentences longer than 250 tokens as well as sentence-pairs with a source/target length ratio exceeding 1.5. This results in 5.18M sentence pairs. We tokenize all data with the Moses tokenizer BIBREF8 and apply the BPE vocabulary learned on the monolingual corpora."
                    ]
                },
                {
                    "raw_evidence": [
                        "We train language models on two languages: One model is estimated on the German newscrawl distributed by WMT'18 comprising 260M sentences or 6B tokens. Another model is trained on the English newscrawl data comprising 193M sentences or 5B tokens. We learn a joint Byte-Pair-Encoding (BPE; Sennrich et al., 2016) vocabulary of 37K types on the German and English newscrawl and train the language models with this vocabulary.",
                        "We consider two benchmarks: Most experiments are run on the WMT'18 English-German (en-de) news translation task and we validate our findings on the WMT'18 English-Turkish (en-tr) news task. For WMT'18 English-German, the training corpus consists of all available bitext excluding the ParaCrawl corpus and we remove sentences longer than 250 tokens as well as sentence-pairs with a source/target length ratio exceeding 1.5. This results in 5.18M sentence pairs. We tokenize all data with the Moses tokenizer BIBREF8 and apply the BPE vocabulary learned on the monolingual corpora.",
                        "For WMT'18 English-Turkish, we use all of the available bitext comprising 208K sentence-pairs without any filtering. We develop on newstest2017 and test on newstest2018. For en-tr we only experiment with adding representations to the encoder and therefore apply the language model vocabulary to the source side. For the target vocabulary we learn a BPE code with 32K merge operations on the Turkish side of the bitext. Both datasets are evaluated in terms of case-sensitive de-tokenized BLEU BIBREF9 , BIBREF10 .",
                        "We consider the abstractive document summarization task comprising over 280K news articles paired with multi-sentence summaries. is a widely used dataset for abstractive text summarization. Following BIBREF11 , we report results on the non-anonymized version of rather than the entity-anonymized version BIBREF12 , BIBREF13 because the language model was trained on full text. Articles are truncated to 400 tokens BIBREF11 and we use a BPE vocabulary of 32K types BIBREF14 . We evaluate in terms of F1-Rouge, that is Rouge-1, Rouge-2 and Rouge-L BIBREF15 ."
                    ],
                    "highlighted_evidence": [
                        "One model is estimated on the German newscrawl distributed by WMT'18 comprising 260M sentences or 6B tokens. Another model is trained on the English newscrawl data comprising 193M sentences or 5B tokens.",
                        "We consider two benchmarks: Most experiments are run on the WMT'18 English-German (en-de) news translation task and we validate our findings on the WMT'18 English-Turkish (en-tr) news task. For WMT'18 English-German, the training corpus consists of all available bitext excluding the ParaCrawl corpus and we remove sentences longer than 250 tokens as well as sentence-pairs with a source/target length ratio exceeding 1.5. This results in 5.18M sentence pairs. We tokenize all data with the Moses tokenizer BIBREF8 and apply the BPE vocabulary learned on the monolingual corpora.\n\nFor WMT'18 English-Turkish, we use all of the available bitext comprising 208K sentence-pairs without any filtering. We develop on newstest2017 and test on newstest2018.",
                        "We consider the abstractive document summarization task comprising over 280K news articles paired with multi-sentence summaries. is a widely used dataset for abstractive text summarization. Following BIBREF11 , we report results on the non-anonymized version of rather than the entity-anonymized version BIBREF12 , BIBREF13 because the language model was trained on full text."
                    ]
                }
            ]
        },
        {
            "question": "What other models do they compare to?",
            "answers": [
                {
                    "answer": "BIBREF11 , BIBREF26 ",
                    "type": "extractive"
                }
            ],
            "q_uid": "4fa6fbb9df1a4c32583d4ef70d2b29ece4b3d802",
            "evidence": [
                {
                    "raw_evidence": [
                        "Following BIBREF11 , we experiment on the non-anonymized version of . When generating summaries, we follow standard practice of tuning the maximum output length and disallow repeating the same trigram BIBREF27 , BIBREF14 . For this task we train language model representations on the combination of newscrawl and the training data. Table TABREF16 shows that pre-trained embeddings can significantly improve on top of a strong baseline transformer. We also compare to BIBREF26 who use a task-specific architecture compared to our generic sequence to sequence baseline. Pre-trained representations are complementary to their method."
                    ],
                    "highlighted_evidence": [
                        "Following BIBREF11 , we experiment on the non-anonymized version of . When generating summaries, we follow standard practice of tuning the maximum output length and disallow repeating the same trigram BIBREF27 , BIBREF14 . For this task we train language model representations on the combination of newscrawl and the training data. Table TABREF16 shows that pre-trained embeddings can significantly improve on top of a strong baseline transformer. We also compare to BIBREF26 who use a task-specific architecture compared to our generic sequence to sequence baseline. Pre-trained representations are complementary to their method."
                    ]
                }
            ]
        },
        {
            "question": "What language model architectures are used?",
            "answers": [
                {
                    "answer": "uni-directional model to augment the decoder",
                    "type": "extractive"
                },
                {
                    "answer": "bi-directional language model to augment the sequence to sequence encoder ,  uni-directional model to augment the decoder",
                    "type": "extractive"
                }
            ],
            "q_uid": "4d47bef19afd70c10bbceafd1846516546641a2f",
            "evidence": [
                {
                    "raw_evidence": [
                        "We consider two types of architectures: a bi-directional language model to augment the sequence to sequence encoder and a uni-directional model to augment the decoder. Both use self-attention BIBREF16 and the uni-directional model contains INLINEFORM0 transformer blocks, followed by a word classifier to predict the next word on the right. The bi-directional model solves a cloze-style token prediction task at training time BIBREF17 . The model consists of two towers, the forward tower operates left-to-right and the tower operating right-to-left as backward tower; each tower contains INLINEFORM1 transformer blocks. The forward and backward representations are combined via a self-attention module and the output of this module is used to predict the token at position INLINEFORM2 . The model has access to the entire input surrounding the current target token. Models use the standard settings for the Big Transformer BIBREF16 . The bi-directional model contains 353M parameters and the uni-directional model 190M parameters. Both models were trained for 1M steps using Nesterov's accelerated gradient BIBREF18 with momentum INLINEFORM3 following BIBREF19 . The learning rate is linearly warmed up from INLINEFORM4 to 1 for 16K steps and then annealed using a cosine learning rate schedule with a single phase to 0.0001 BIBREF20 . We train on 32 Nvidia V100 SXM2 GPUs and use the NCCL2 library as well as the torch distributed package for inter-GPU communication. Training relies on 16-bit floating point operations BIBREF21 and it took six days for the bi-directional model and four days for the uni-directional model."
                    ],
                    "highlighted_evidence": [
                        "We consider two types of architectures: a bi-directional language model to augment the sequence to sequence encoder and a uni-directional model to augment the decoder. Both use self-attention BIBREF16 and the uni-directional model contains INLINEFORM0 transformer blocks, followed by a word classifier to predict the next word on the right.",
                        "bi-directional language model to augment the sequence to sequence encoder"
                    ]
                },
                {
                    "raw_evidence": [
                        "We consider two types of architectures: a bi-directional language model to augment the sequence to sequence encoder and a uni-directional model to augment the decoder. Both use self-attention BIBREF16 and the uni-directional model contains INLINEFORM0 transformer blocks, followed by a word classifier to predict the next word on the right. The bi-directional model solves a cloze-style token prediction task at training time BIBREF17 . The model consists of two towers, the forward tower operates left-to-right and the tower operating right-to-left as backward tower; each tower contains INLINEFORM1 transformer blocks. The forward and backward representations are combined via a self-attention module and the output of this module is used to predict the token at position INLINEFORM2 . The model has access to the entire input surrounding the current target token. Models use the standard settings for the Big Transformer BIBREF16 . The bi-directional model contains 353M parameters and the uni-directional model 190M parameters. Both models were trained for 1M steps using Nesterov's accelerated gradient BIBREF18 with momentum INLINEFORM3 following BIBREF19 . The learning rate is linearly warmed up from INLINEFORM4 to 1 for 16K steps and then annealed using a cosine learning rate schedule with a single phase to 0.0001 BIBREF20 . We train on 32 Nvidia V100 SXM2 GPUs and use the NCCL2 library as well as the torch distributed package for inter-GPU communication. Training relies on 16-bit floating point operations BIBREF21 and it took six days for the bi-directional model and four days for the uni-directional model."
                    ],
                    "highlighted_evidence": [
                        "We consider two types of architectures: a bi-directional language model to augment the sequence to sequence encoder and a uni-directional model to augment the decoder. Both use self-attention BIBREF16 and the uni-directional model contains INLINEFORM0 transformer blocks, followed by a word classifier to predict the next word on the right. "
                    ]
                }
            ]
        }
    ],
    "1806.03125": [
        {
            "question": "Which dataset has been used in this work?",
            "answers": [
                {
                    "answer": "Reuters-8 dataset without stop words",
                    "type": "extractive"
                },
                {
                    "answer": "The Reuters-8 dataset (with stop words removed)",
                    "type": "abstractive"
                }
            ],
            "q_uid": "440faf8d0af8291d324977ad0f68c8d661fe365e",
            "evidence": [
                {
                    "raw_evidence": [
                        "In this section we describe the experiments performed to demonstrate the validity of our proposed method and its extension. We used the Reuters-8 dataset without stop words from BIBREF27 aiming at single-label classification, which is a preprocessed format of the Reuters-21578. Words in the texts were considered as they appeared, without performing stemming or typo correction. This database has eight different classes with the number of samples varying from 51 to over 3000 documents, as can be seen in Table 1 ."
                    ],
                    "highlighted_evidence": [
                        "We used the Reuters-8 dataset without stop words from BIBREF27 aiming at single-label classification, which is a preprocessed format of the Reuters-21578."
                    ]
                },
                {
                    "raw_evidence": [
                        "In this section we describe the experiments performed to demonstrate the validity of our proposed method and its extension. We used the Reuters-8 dataset without stop words from BIBREF27 aiming at single-label classification, which is a preprocessed format of the Reuters-21578. Words in the texts were considered as they appeared, without performing stemming or typo correction. This database has eight different classes with the number of samples varying from 51 to over 3000 documents, as can be seen in Table 1 ."
                    ],
                    "highlighted_evidence": [
                        "We used the Reuters-8 dataset without stop words from BIBREF27 aiming at single-label classification, which is a preprocessed format of the Reuters-21578. "
                    ]
                }
            ]
        },
        {
            "question": "What can word subspace represent?",
            "answers": [
                {
                    "answer": "Word vectors, usually in the context of others within the same class",
                    "type": "abstractive"
                }
            ],
            "q_uid": "0ec56e15005a627d0b478a67fd627a9d85c3920e",
            "evidence": [
                {
                    "raw_evidence": [
                        "To tackle this problem, we introduce the novel concept of word subspace. It is mathematically defined as a low dimensional linear subspace in a word vector space with high dimensionality. Given that words from texts of the same class belong to the same context, it is possible to model word vectors of each class as word subspaces and efficiently compare them in terms of similarity by using canonical angles between the word subspaces. Through this representation, most of the variability of the class is retained. Consequently, a word subspace can effectively and compactly represent the context of the corresponding text. We achieve this framework through the mutual subspace method (MSM) BIBREF9 ."
                    ],
                    "highlighted_evidence": [
                        "Given that words from texts of the same class belong to the same context, it is possible to model word vectors of each class as word subspaces and efficiently compare them in terms of similarity by using canonical angles between the word subspaces. "
                    ]
                }
            ]
        }
    ],
    "2003.12738": [
        {
            "question": "What baselines other than standard transformers are used in experiments?",
            "answers": [
                {
                    "answer": "attention-based sequence-to-sequence model , CVAE",
                    "type": "extractive"
                }
            ],
            "q_uid": "6aed1122050b2d508dc1790c13cdbe38ff126089",
            "evidence": [
                {
                    "raw_evidence": [
                        "An attention-based sequence-to-sequence model with the emoji vector as additional input as discribed in MojiTalk BIBREF16.",
                        "An RNN-based conditional variational autoencoder for dialogue response generation BIBREF16, which uses a multivariate Gaussian latent variable to model the response and concatenate it with the last hidden state of the encoder as the initial state of the decoder. KL annealing, early stopping strategy and bag-of-word auxiliary loss are applied during the training. We use the implementation released by BIBREF16."
                    ],
                    "highlighted_evidence": [
                        "An attention-based sequence-to-sequence model with the emoji vector as additional input as discribed in MojiTalk BIBREF16.",
                        "CVAE.\nAn RNN-based conditional variational autoencoder for dialogue response generation BIBREF16, which uses a multivariate Gaussian latent variable to model the response and concatenate it with the last hidden state of the encoder as the initial state of the decoder. KL annealing, early stopping strategy and bag-of-word auxiliary loss are applied during the training. We use the implementation released by BIBREF16"
                    ]
                }
            ]
        },
        {
            "question": "What three conversational datasets are used for evaluation?",
            "answers": [
                {
                    "answer": "MojiTalk , PersonaChat , Empathetic-Dialogues",
                    "type": "extractive"
                }
            ],
            "q_uid": "8740c3000e740ac5c0bc8f329d908309f7ffeff6",
            "evidence": [
                {
                    "raw_evidence": [
                        "We evaluate the proposed models on three conversationet dataset such as MojiTalk BIBREF16, PersonaChat BIBREF11, Empathetic-Dialogues BIBREF26."
                    ],
                    "highlighted_evidence": [
                        "We evaluate the proposed models on three conversationet dataset such as MojiTalk BIBREF16, PersonaChat BIBREF11, Empathetic-Dialogues BIBREF26."
                    ]
                }
            ]
        }
    ],
    "1808.07625": [
        {
            "question": "How does the model compute the likelihood of executing to the correction semantic denotation?",
            "answers": [
                {
                    "answer": "By treating logical forms as a latent variable and training a discriminative log-linear model over logical form y given x.",
                    "type": "abstractive"
                }
            ],
            "q_uid": "ad0a7fe75db5553652cd25555c6980f497e08113",
            "evidence": [
                {
                    "raw_evidence": [
                        "Since the training data consists only of utterance-denotation pairs, the ranker is trained to maximize the log-likelihood of the correct answer $z$ by treating logical forms as a latent variable:",
                        "It is impractical to rely solely on a neural decoder to find the most likely logical form at run time in the weakly-supervised setting. One reason is that although the decoder utilizes global utterance features for generation, it cannot leverage global features of the logical form since a logical form is conditionally generated following a specific tree-traversal order. To this end, we follow previous work BIBREF21 and introduce a ranker to the system. The role of the ranker is to score the candidate logical forms generated by the parser; at test time, the logical form receiving the highest score will be used for execution. The ranker is a discriminative log-linear model over logical form $y$ given utterance $x$ :"
                    ],
                    "highlighted_evidence": [
                        "Since the training data consists only of utterance-denotation pairs, the ranker is trained to maximize the log-likelihood of the correct answer $z$ by treating logical forms as a latent variable",
                        "The role of the ranker is to score the candidate logical forms generated by the parser; at test time, the logical form receiving the highest score will be used for execution. The ranker is a discriminative log-linear model over logical form $y$ given utterance $x$ : "
                    ]
                }
            ]
        }
    ],
    "1909.08041": [
        {
            "question": "How do they train the retrieval modules?",
            "answers": [
                {
                    "answer": "We treated the neural semantic retrieval at both the paragraph and sentence level as binary classification problems with models' parameters updated by minimizing binary cross entropy loss.",
                    "type": "extractive"
                }
            ],
            "q_uid": "9df4a7bd0abb99ae81f0ebb29c488f1caa0f268f",
            "evidence": [
                {
                    "raw_evidence": [
                        "Semantic Retrieval: We treated the neural semantic retrieval at both the paragraph and sentence level as binary classification problems with models' parameters updated by minimizing binary cross entropy loss. To be specific, we fed the query and context into BERT as:",
                        "We applied an affine layer and sigmoid activation on the last layer output of the [$\\mathit {CLS}$] token which is a scalar value. The parameters were updated with the objective function:",
                        "where $\\hat{p}_i$ is the output of the model, $\\mathbf {T}^{p/s}_{pos}$ is the positive set and $\\mathbf {T}^{p/s}_{neg}$ is the negative set. As shown in Fig. FIGREF2, at sentence level, ground-truth sentences were served as positive examples while other sentences from upstream retrieved set were served as negative examples. Similarly at the paragraph-level, paragraphs having any ground-truth sentence were used as positive examples and other paragraphs from the upstream term-based retrieval processes were used as negative examples."
                    ],
                    "highlighted_evidence": [
                        "Semantic Retrieval: We treated the neural semantic retrieval at both the paragraph and sentence level as binary classification problems with models' parameters updated by minimizing binary cross entropy loss. To be specific, we fed the query and context into BERT as:\n\nWe applied an affine layer and sigmoid activation on the last layer output of the [$\\mathit {CLS}$] token which is a scalar value. The parameters were updated with the objective function:\n\nwhere $\\hat{p}_i$ is the output of the model, $\\mathbf {T}^{p/s}_{pos}$ is the positive set and $\\mathbf {T}^{p/s}_{neg}$ is the negative set. As shown in Fig. FIGREF2, at sentence level, ground-truth sentences were served as positive examples while other sentences from upstream retrieved set were served as negative examples. Similarly at the paragraph-level, paragraphs having any ground-truth sentence were used as positive examples and other paragraphs from the upstream term-based retrieval processes were used as negative examples."
                    ]
                }
            ]
        },
        {
            "question": "How do they model the neural retrieval modules?",
            "answers": [
                {
                    "answer": "BERT-Base BIBREF2 to provide the state-of-the-art contextualized modeling",
                    "type": "extractive"
                }
            ],
            "q_uid": "b7291845ccf08313e09195befd3c8030f28f6a9e",
            "evidence": [
                {
                    "raw_evidence": [
                        "Throughout all our experiments, we used BERT-Base BIBREF2 to provide the state-of-the-art contextualized modeling of the input text.",
                        "Semantic Retrieval: We treated the neural semantic retrieval at both the paragraph and sentence level as binary classification problems with models' parameters updated by minimizing binary cross entropy loss. To be specific, we fed the query and context into BERT as:"
                    ],
                    "highlighted_evidence": [
                        "Throughout all our experiments, we used BERT-Base BIBREF2 to provide the state-of-the-art contextualized modeling of the input text.\n\nSemantic Retrieval: We treated the neural semantic retrieval at both the paragraph and sentence level as binary classification problems with models' parameters updated by minimizing binary cross entropy loss."
                    ]
                }
            ]
        }
    ],
    "1709.07814": [
        {
            "question": "Which architecture do they use for the encoder and decoder?",
            "answers": [
                {
                    "answer": "we construct an encoder with several convolutional layers BIBREF14 followed by NIN layers BIBREF15 as the lower part in the encoder and integrate them with deep bidirectional long short-term memory (Bi-LSTM) BIBREF16 at the higher part, On the decoder side, we use a standard deep unidirectional LSTM with global attention BIBREF13 that is calculated by a multi-layer perceptron (MLP)",
                    "type": "extractive"
                },
                {
                    "answer": "In encoder they use convolutional, NIN and bidirectional LSTM layers and in decoder they use unidirectional LSTM ",
                    "type": "abstractive"
                }
            ],
            "q_uid": "1b23c4535a6c10eb70bbc95313c465e4a547db5e",
            "evidence": [
                {
                    "raw_evidence": [
                        "In this work, we use the raw waveform as the input representation instead of spectral-based features and a grapheme (character) sequence as the output representation. In contrast to most encoder-decoder architectures, which are purely based on recurrent neural network (RNNs) framework, we construct an encoder with several convolutional layers BIBREF14 followed by NIN layers BIBREF15 as the lower part in the encoder and integrate them with deep bidirectional long short-term memory (Bi-LSTM) BIBREF16 at the higher part. We use convolutional layers because they are suitable for extracting local information from raw speech. We use a striding mechanism to reduce the dimension from the input frames BIBREF17 , while the NIN layer represents more complex structures on the top of the convolutional layers. On the decoder side, we use a standard deep unidirectional LSTM with global attention BIBREF13 that is calculated by a multi-layer perceptron (MLP) as described in Eq. EQREF2 . For more details, we illustrate our architecture in Figure FIGREF4 ."
                    ],
                    "highlighted_evidence": [
                        " In contrast to most encoder-decoder architectures, which are purely based on recurrent neural network (RNNs) framework, we construct an encoder with several convolutional layers BIBREF14 followed by NIN layers BIBREF15 as the lower part in the encoder and integrate them with deep bidirectional long short-term memory (Bi-LSTM) BIBREF16 at the higher part.",
                        "On the decoder side, we use a standard deep unidirectional LSTM with global attention BIBREF13 that is calculated by a multi-layer perceptron (MLP) as described in Eq. EQREF2 ."
                    ]
                },
                {
                    "raw_evidence": [
                        "On the top layers of the encoder after the transferred convolutional and NIN layers, we put three bidirectional LSTMs (Bi-LSTM) with 256 hidden units (total 512 units for both directions). To reduce the computational time, we used hierarchical subsampling BIBREF21 , BIBREF22 , BIBREF10 . We applied subsampling on all the Bi-LSTM layers and reduced the length by a factor of 8.",
                        "On the decoder side, the previous input phonemes / characters were converted into real vectors by a 128-dimensional embedding matrix. We used one unidirectional LSTM with 512 hidden units and followed by a softmax layer to output the character probability. For the end-to-end training phase, we froze the parameter values from the transferred layers from epoch 0 to epoch 10, and after epoch 10 we jointly optimized all the parameters together until the end of training (a total 40 epochs). We used an Adam BIBREF23 optimizer with a learning rate of 0.0005."
                    ],
                    "highlighted_evidence": [
                        "On the top layers of the encoder after the transferred convolutional and NIN layers, we put three bidirectional LSTMs (Bi-LSTM) with 256 hidden units (total 512 units for both directions).",
                        "On the decoder side, the previous input phonemes / characters were converted into real vectors by a 128-dimensional embedding matrix. We used one unidirectional LSTM with 512 hidden units and followed by a softmax layer to output the character probability. "
                    ]
                }
            ]
        },
        {
            "question": "How does their decoder generate text?",
            "answers": [
                {
                    "answer": "decoder task, which predicts the target sequence probability at time INLINEFORM3 based on previous output and context information",
                    "type": "extractive"
                },
                {
                    "answer": "Decoder predicts the sequence of phoneme or grapheme at each time based on the previous output and context information with a beam search strategy",
                    "type": "abstractive"
                }
            ],
            "q_uid": "0a75a52450ed866df3a304077769e1725a995bb7",
            "evidence": [
                {
                    "raw_evidence": [
                        "where INLINEFORM0 , INLINEFORM1 is the number of hidden units for the encoder and INLINEFORM2 is the number of hidden units for the decoder. Finally, the decoder task, which predicts the target sequence probability at time INLINEFORM3 based on previous output and context information INLINEFORM4 , can be formulated as: DISPLAYFORM0"
                    ],
                    "highlighted_evidence": [
                        "Finally, the decoder task, which predicts the target sequence probability at time INLINEFORM3 based on previous output and context information INLINEFORM4 , can be formulated as: DISPLAYFORM0"
                    ]
                },
                {
                    "raw_evidence": [
                        "where INLINEFORM0 , INLINEFORM1 is the number of hidden units for the encoder and INLINEFORM2 is the number of hidden units for the decoder. Finally, the decoder task, which predicts the target sequence probability at time INLINEFORM3 based on previous output and context information INLINEFORM4 , can be formulated as: DISPLAYFORM0",
                        "The most common input INLINEFORM0 for speech recognition tasks is a sequence of feature vectors such as log Mel-spectral spectrogram and/or MFCC. Therefore, INLINEFORM1 where D is the number of the features and S is the total length of the utterance in frames. The output INLINEFORM2 can be either phoneme or grapheme (character) sequence.",
                        "In the decoding phase, we used a beam search strategy with beam size INLINEFORM0 and we adjusted the score by dividing with the transcription length to prevent the decoder from favoring shorter transcriptions. We did not use any language model or lexicon dictionary for decoding. All of our models were implemented on the PyTorch framework ."
                    ],
                    "highlighted_evidence": [
                        "Finally, the decoder task, which predicts the target sequence probability at time INLINEFORM3 based on previous output and context information INLINEFORM4 , can be formulated as: DISPLAYFORM0",
                        "The output INLINEFORM2 can be either phoneme or grapheme (character) sequence.",
                        "In the decoding phase, we used a beam search strategy with beam size INLINEFORM0 and we adjusted the score by dividing with the transcription length to prevent the decoder from favoring shorter transcriptions."
                    ]
                }
            ]
        },
        {
            "question": "Which dataset do they use?",
            "answers": [
                {
                    "answer": "WSJ",
                    "type": "extractive"
                },
                {
                    "answer": "WSJ-SI84, WSJ-SI284",
                    "type": "extractive"
                }
            ],
            "q_uid": "fd0a3e9c210163a55d3ed791e95ae3875184b8f8",
            "evidence": [
                {
                    "raw_evidence": [
                        "In this study, we investigate the performance of our proposed models on WSJ BIBREF5 . We used the same definitions of the training, development and test set as the Kaldi s5 recipe BIBREF18 . The raw speech waveforms were segmented into multiple frames with a 25ms window size and a 10ms step size. We normalized the raw speech waveform into the range -1 to 1. For spectral based features such as MFCC and log Mel-spectrogram, we normalized the features for each dimension into zero mean and unit variance. For WSJ, we separated into two experiments by using WSJ-SI84 only and WSJ-SI284 data. We used dev_93 for our validation set and eval_92 for our test set. We used the character sequence as our decoder target and followed the preprocessing step proposed by BIBREF19 . The text from all the utterances was mapped into a 32-character set: 26 (a-z) alphabet, apostrophe, period, dash, space, noise, and \u201ceos\"."
                    ],
                    "highlighted_evidence": [
                        "In this study, we investigate the performance of our proposed models on WSJ BIBREF5 . "
                    ]
                },
                {
                    "raw_evidence": [
                        "An example of our transfer learning results is shown in Figure FIGREF8 , and Table TABREF14 shows the speech recognition performance in CER for both the WSJ-SI84 and WSJ-SI284 datasets. We compared our method with several published models like CTC, Attention Encoder-Decoder and Joint CTC-Attention model that utilize CTC for training the encoder part. Besides, we also train our own baseline Attention Encoder-Decoder with Mel-scale spectrogram. The difference between our Attention Encoder-Decoder (\u201cAtt Enc-Dec (ours)\", \u201cAtt Enc-Dec Wav2Text\") with Attention Encoder-Decoder from BIBREF24 (\u201cAtt Enc-Dec Content\", \u201cAtt Enc-Dec Location\") is we used the current hidden states to generate the attention vector instead of the previous hidden states. Another addition is we utilized \u201cinput feedback\" method BIBREF13 by concatenating the previous context vector into the current input along with the character embedding vector. By using those modifications, we are able to improve the baseline performance."
                    ],
                    "highlighted_evidence": [
                        "An example of our transfer learning results is shown in Figure FIGREF8 , and Table TABREF14 shows the speech recognition performance in CER for both the WSJ-SI84 and WSJ-SI284 datasets."
                    ]
                }
            ]
        }
    ],
    "1909.05855": [
        {
            "question": "How did they gather the data?",
            "answers": [
                {
                    "answer": "simulation-based dialogue generation, where the user and system roles are simulated to generate a complete conversation flow, which can then be converted to natural language using crowd workers ",
                    "type": "extractive"
                }
            ],
            "q_uid": "3ee721c3531bf1b9a1356a40205d088c9a7a44fc",
            "evidence": [
                {
                    "raw_evidence": [
                        "Machine-machine Interaction A related line of work explores simulation-based dialogue generation, where the user and system roles are simulated to generate a complete conversation flow, which can then be converted to natural language using crowd workers BIBREF1. Such a framework may be cost-effective and error-resistant since the underlying crowd worker task is simpler, and semantic annotations are obtained automatically.",
                        "It is often argued that simulation-based data collection does not yield natural dialogues or sufficient coverage, when compared to other approaches such as Wizard-of-Oz. We argue that simulation-based collection is a better alternative for collecting datasets like this owing to the factors below."
                    ],
                    "highlighted_evidence": [
                        "Machine-machine Interaction A related line of work explores simulation-based dialogue generation, where the user and system roles are simulated to generate a complete conversation flow, which can then be converted to natural language using crowd workers BIBREF1. Such a framework may be cost-effective and error-resistant since the underlying crowd worker task is simpler, and semantic annotations are obtained automatically.",
                        "It is often argued that simulation-based data collection does not yield natural dialogues or sufficient coverage, when compared to other approaches such as Wizard-of-Oz. We argue that simulation-based collection is a better alternative for collecting datasets like this owing to the factors below."
                    ]
                }
            ]
        }
    ],
    "2004.02451": [
        {
            "question": "What neural language models are explored?",
            "answers": [
                {
                    "answer": "LSTM-LM ",
                    "type": "extractive"
                }
            ],
            "q_uid": "818c89b11471a6ca4f13c838713864fdf282c2ca",
            "evidence": [
                {
                    "raw_evidence": [
                        "Since our focus in this paper is an additional loss exploiting negative examples (Section method), we fix the baseline LM throughout the experiments. Our baseline is a three-layer LSTM-LM with 1,150 hidden units at internal layers trained with the standard cross-entropy loss. Word embeddings are 400-dimensional, and input and output embeddings are tied BIBREF11. Deviating from some prior work BIBREF0, BIBREF1, we train LMs at sentence level as in sequence-to-sequence models BIBREF12. This setting has been employed in some previous work BIBREF3, BIBREF6."
                    ],
                    "highlighted_evidence": [
                        " Our baseline is a three-layer LSTM-LM with 1,150 hidden units at internal layers trained with the standard cross-entropy loss. "
                    ]
                }
            ]
        },
        {
            "question": "How do they perform data augmentation?",
            "answers": [
                {
                    "answer": "They randomly sample sentences from Wikipedia that contains an object RC and add them to training data",
                    "type": "abstractive"
                }
            ],
            "q_uid": "7994b4001925798dfb381f9aa5c0545cdbd77220",
            "evidence": [
                {
                    "raw_evidence": [
                        "We first inspect the frequencies of object and subject RCs in the training data, by parsing them with the state-of-the-art Berkeley neural parser BIBREF19. In total, while subject RCs occur 373,186 times, object RCs only occur 106,558 times. We create three additional training datasets by adding sentences involving object RCs to the original Wikipedia corpus (Section lm). To this end, we randomly pick up 30 million sentences from Wikipedia (not overlapped to any sentences in the original corpus), parse by the same parser, and filter sentences containing an object RC, amounting to 680,000 sentences. Among the test cases about object RCs, we compare accuracies on subject-verb agreement, to make a comparison with subject RCs. We also evaluate on \u201canimate only\u201d subset, which has a correspondence to the test cases for subject RC with only differences in word order and inflection (like (UNKREF45) and (UNKREF46); see footnote FOOTREF47). Of particular interest to us is the accuracy on these animate cases. Since the vocabularies are exactly the same, we hypothesize that the accuracy will reach the same level as that on subject RCs with our augmentation."
                    ],
                    "highlighted_evidence": [
                        "We create three additional training datasets by adding sentences involving object RCs to the original Wikipedia corpus (Section lm). To this end, we randomly pick up 30 million sentences from Wikipedia (not overlapped to any sentences in the original corpus), parse by the same parser, and filter sentences containing an object RC, amounting to 680,000 sentences. "
                    ]
                }
            ]
        }
    ],
    "1708.06185": [
        {
            "question": "what dataset was used?",
            "answers": [
                {
                    "answer": "WASSA-2017 Shared Task on Emotion Intensity",
                    "type": "extractive"
                }
            ],
            "q_uid": "e48e750743aef36529fbea4328b8253dbe928b4d",
            "evidence": [
                {
                    "raw_evidence": [
                        "We would like to thank the organizers of the WASSA-2017 Shared Task on Emotion Intensity, for providing the data, the guidelines and timely support."
                    ],
                    "highlighted_evidence": [
                        "We would like to thank the organizers of the WASSA-2017 Shared Task on Emotion Intensity, for providing the data, the guidelines and timely support."
                    ]
                }
            ]
        },
        {
            "question": "how many total combined features were there?",
            "answers": [
                {
                    "answer": "Fourteen ",
                    "type": "extractive"
                }
            ],
            "q_uid": "c08aab979dcdc8f4fe8ec1337c3c8290ab13414e",
            "evidence": [
                {
                    "raw_evidence": [
                        "Based on these observations, the feature extraction step is implemented as a union of different independent feature extractors (featurizers) in a light-weight and easy to use Python program EmoInt . It comprises of all features available in the baseline model BIBREF2 along with additional feature extractors and bi-gram support. Fourteen such feature extractors have been implemented which can be clubbed into 3 major categories:",
                        "[noitemsep]",
                        "Lexicon Features",
                        "Word Vectors",
                        "Syntax Features"
                    ],
                    "highlighted_evidence": [
                        "Fourteen such feature extractors have been implemented which can be clubbed into 3 major categories:\n\n[noitemsep]\n\nLexicon Features\n\nWord Vectors\n\nSyntax Features"
                    ]
                }
            ]
        },
        {
            "question": "what pretrained word embeddings were used?",
            "answers": [
                {
                    "answer": "Pretrained word embeddings  were not used",
                    "type": "abstractive"
                },
                {
                    "answer": "GloVe, Edinburgh embeddings BIBREF14, Emoji embeddings BIBREF16",
                    "type": "extractive"
                }
            ],
            "q_uid": "8756b7b9ff5e87e4efdf6c2f73a0512f05b5ae3f",
            "evidence": [
                {
                    "raw_evidence": [
                        "Word Vectors: We focus primarily on the word vector representations (word embeddings) created specifically using the twitter dataset. GloVe BIBREF13 is an unsupervised learning algorithm for obtaining vector representations for words. 200-dimensional GloVe embeddings trained on 2 Billion tweets are integrated. Edinburgh embeddings BIBREF14 are obtained by training skip-gram model on Edinburgh corpus BIBREF15 . Since tweets are abundant with emojis, Emoji embeddings BIBREF16 which are learned from the emoji descriptions have been used. Embeddings for each tweet are obtained by summing up individual word vectors and then dividing by the number of tokens in the tweet."
                    ],
                    "highlighted_evidence": [
                        "We focus primarily on the word vector representations (word embeddings) created specifically using the twitter dataset. GloVe BIBREF13 is an unsupervised learning algorithm for obtaining vector representations for words. 200-dimensional GloVe embeddings trained on 2 Billion tweets are integrated. Edinburgh embeddings BIBREF14 are obtained by training skip-gram model on Edinburgh corpus BIBREF15 . Since tweets are abundant with emojis, Emoji embeddings BIBREF16 which are learned from the emoji descriptions have been used. Embeddings for each tweet are obtained by summing up individual word vectors and then dividing by the number of tokens in the tweet."
                    ]
                },
                {
                    "raw_evidence": [
                        "Word Vectors: We focus primarily on the word vector representations (word embeddings) created specifically using the twitter dataset. GloVe BIBREF13 is an unsupervised learning algorithm for obtaining vector representations for words. 200-dimensional GloVe embeddings trained on 2 Billion tweets are integrated. Edinburgh embeddings BIBREF14 are obtained by training skip-gram model on Edinburgh corpus BIBREF15 . Since tweets are abundant with emojis, Emoji embeddings BIBREF16 which are learned from the emoji descriptions have been used. Embeddings for each tweet are obtained by summing up individual word vectors and then dividing by the number of tokens in the tweet."
                    ],
                    "highlighted_evidence": [
                        "GloVe embeddings trained on 2 Billion tweets are integrated.",
                        "Edinburgh embeddings BIBREF14 are obtained by training skip-gram model on Edinburgh corpus BIBREF15 .",
                        "Emoji embeddings BIBREF16 which are learned from the emoji descriptions have been used."
                    ]
                }
            ]
        }
    ],
    "1709.02271": [
        {
            "question": "How are discourse embeddings analyzed?",
            "answers": [
                {
                    "answer": "They perform t-SNE clustering to analyze discourse embeddings",
                    "type": "abstractive"
                }
            ],
            "q_uid": "cfbccb51f0f8f8f125b40168ed66384e2a09762b",
            "evidence": [
                {
                    "raw_evidence": [
                        "To further study the information encoded in the discourse embeddings, we perform t-SNE clustering BIBREF20 on them, using the best performing model CNN2-DE (global). We examine the closest neighbors of each embedding, and observe that similar discourse relations tend to go together (e.g., explanation and interpretation; consequence and result). Some examples are given in Table TABREF29 . However, it is unclear how this pattern helps improve classification performance. We intend to investigate this question in future work."
                    ],
                    "highlighted_evidence": [
                        "To further study the information encoded in the discourse embeddings, we perform t-SNE clustering BIBREF20 on them, using the best performing model CNN2-DE (global)."
                    ]
                }
            ]
        },
        {
            "question": "What was the previous state-of-the-art?",
            "answers": [
                {
                    "answer": "character bigram CNN classifier",
                    "type": "extractive"
                }
            ],
            "q_uid": "feb4e92ff1609f3a5e22588da66532ff689f3bcc",
            "evidence": [
                {
                    "raw_evidence": [
                        "In our paper, we opt for a state-of-the-art character bigram CNN classifier BIBREF4 , and investigate various ways in which the discourse information can be featurized and integrated into the CNN. Specifically,"
                    ],
                    "highlighted_evidence": [
                        "In our paper, we opt for a state-of-the-art character bigram CNN classifier BIBREF4 , and investigate various ways in which the discourse information can be featurized and integrated into the CNN. "
                    ]
                }
            ]
        },
        {
            "question": "How are discourse features incorporated into the model?",
            "answers": [
                {
                    "answer": "They derive entity grid with grammatical relations and RST discourse relations and concatenate them with pooling vector for the char-bigrams before feeding to the resulting vector to the softmax layer.",
                    "type": "abstractive"
                }
            ],
            "q_uid": "f10325d022e3f95223f79ab00f8b42e3bb7ca040",
            "evidence": [
                {
                    "raw_evidence": [
                        "CNN2-PV. This model (Figure FIGREF10 , left+center) featurizes discourse information into a vector of relation probabilities. In order to derive the discourse features, an entity grid is constructed by feeding the document through an NLP pipeline to identify salient entities. Two flavors of discourse features are created by populating the entity grid with either (i) grammatical relations (GR) or (ii) RST discourse relations (RST). The GR features are represented as grammatical relation transitions derived from the entity grid, e.g., INLINEFORM0 . The RST features are represented as RST discourse relations with their nuclearity, e.g., INLINEFORM1 . The probability vectors are then distributions over relation types. For GR, the vector is a distribution over all the entity role transitions, i.e., INLINEFORM2 (see Table TABREF2 ). For RST, the vector is a distribution over all the RST discourse relations, i.e., INLINEFORM3 Denoting a feature as such with INLINEFORM4 , we construct the pooling vector INLINEFORM5 for the char-bigrams, and concatenate INLINEFORM6 to INLINEFORM7 before feeding the resulting vector to the softmax layer."
                    ],
                    "highlighted_evidence": [
                        "In order to derive the discourse features, an entity grid is constructed by feeding the document through an NLP pipeline to identify salient entities. Two flavors of discourse features are created by populating the entity grid with either (i) grammatical relations (GR) or (ii) RST discourse relations (RST). ",
                        "Denoting a feature as such with INLINEFORM4 , we construct the pooling vector INLINEFORM5 for the char-bigrams, and concatenate INLINEFORM6 to INLINEFORM7 before feeding the resulting vector to the softmax layer."
                    ]
                }
            ]
        },
        {
            "question": "What discourse features are used?",
            "answers": [
                {
                    "answer": "Entity grid with grammatical relations and RST discourse relations.",
                    "type": "abstractive"
                }
            ],
            "q_uid": "5e65bb0481f3f5826291c7cc3e30436ab4314c61",
            "evidence": [
                {
                    "raw_evidence": [
                        "CNN2-PV. This model (Figure FIGREF10 , left+center) featurizes discourse information into a vector of relation probabilities. In order to derive the discourse features, an entity grid is constructed by feeding the document through an NLP pipeline to identify salient entities. Two flavors of discourse features are created by populating the entity grid with either (i) grammatical relations (GR) or (ii) RST discourse relations (RST). The GR features are represented as grammatical relation transitions derived from the entity grid, e.g., INLINEFORM0 . The RST features are represented as RST discourse relations with their nuclearity, e.g., INLINEFORM1 . The probability vectors are then distributions over relation types. For GR, the vector is a distribution over all the entity role transitions, i.e., INLINEFORM2 (see Table TABREF2 ). For RST, the vector is a distribution over all the RST discourse relations, i.e., INLINEFORM3 Denoting a feature as such with INLINEFORM4 , we construct the pooling vector INLINEFORM5 for the char-bigrams, and concatenate INLINEFORM6 to INLINEFORM7 before feeding the resulting vector to the softmax layer."
                    ],
                    "highlighted_evidence": [
                        "In order to derive the discourse features, an entity grid is constructed by feeding the document through an NLP pipeline to identify salient entities. Two flavors of discourse features are created by populating the entity grid with either (i) grammatical relations (GR) or (ii) RST discourse relations (RST). "
                    ]
                }
            ]
        }
    ],
    "1905.07791": [
        {
            "question": "How much higher quality is the resulting annotated data?",
            "answers": [
                {
                    "answer": "improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added",
                    "type": "extractive"
                }
            ],
            "q_uid": "12d7055baf5bffb6e9e95e977c000ef2e77a4362",
            "evidence": [
                {
                    "raw_evidence": [
                        "The results show adding more training data with crowd annotation still improves at least 1 point F1 score in all three extraction tasks. The improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added. The model trained with re-annotating the difficult subset (D+Other) also outperforms the model with re-annotating the random subset (R+Other) by 2 points in F1. The model trained with re-annotating both of difficult and random subsets (D+R+Other), however, achieves only marginally higher F1 than the model trained with the re-annotated difficult subset (D+Other). In sum, the results clearly indicate that mixing expert and crowd annotations leads to better models than using solely crowd data, and better than using expert data alone. More importantly, there is greater gain in performance when instances are routed according to difficulty, as compared to randomly selecting the data for expert annotators. These findings align with our motivating hypothesis that annotation quality for difficult instances is important for final model performance. They also indicate that mixing annotations from expert and crowd could be an effective way to achieve acceptable model performance given a limited budget."
                    ],
                    "highlighted_evidence": [
                        "The improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added."
                    ]
                }
            ]
        },
        {
            "question": "How do they match annotators to instances?",
            "answers": [
                {
                    "answer": "Annotations from experts are used if they have already been collected.",
                    "type": "abstractive"
                }
            ],
            "q_uid": "498c0229f831c82a5eb494cdb3547452112a66a0",
            "evidence": [
                {
                    "raw_evidence": [
                        "So far a system was trained on one type of data, either labeled by crowd or experts. We now examine the performance of a system trained on data that was routed to either experts or crowd annotators depending on their predicted difficult. Given the results presented so far mixing annotators may be beneficial given their respective trade-offs of precision and recall. We use the annotations from experts for an abstract if it exists otherwise use crowd annotations. The results are presented in Table 6 ."
                    ],
                    "highlighted_evidence": [
                        "We use the annotations from experts for an abstract if it exists otherwise use crowd annotations. "
                    ]
                }
            ]
        },
        {
            "question": "How much data is needed to train the task-specific encoder?",
            "answers": [
                {
                    "answer": "57,505 sentences",
                    "type": "abstractive"
                },
                {
                    "answer": "57,505 sentences",
                    "type": "abstractive"
                }
            ],
            "q_uid": "8c48c726bb17a17d70ab29db4d65a93030dd5382",
            "evidence": [
                {
                    "raw_evidence": [
                        "Our contributions in this work are summarized as follows. We define a task difficulty prediction task and show how this is related to, but distinct from, inter-worker agreement. We introduce a new model for difficulty prediction combining learned representations induced via a pre-trained `universal' sentence encoder BIBREF6 , and a sentence encoder learned from scratch for this task. We show that predicting annotation difficulty can be used to improve the task routing and model performance for a biomedical information extraction task. Our results open up a new direction for ensuring corpus quality. We believe that item difficulty prediction will likely be useful in other, non-specialized tasks as well, and that the most effective data collection in specialized domains requires research addressing the fundamental questions we examine here.",
                        "An abstract may contain some `easy' and some `difficult' sentences. We thus perform our analysis at the sentence level. We split abstracts into sentences using spaCy. We excluded sentences that comprise fewer than two tokens, as these are likely an artifact of errors in sentence splitting. In total, this resulted in 57,505 and 2,428 sentences in the train and test set abstracts, respectively."
                    ],
                    "highlighted_evidence": [
                        "We introduce a new model for difficulty prediction combining learned representations induced via a pre-trained `universal' sentence encoder BIBREF6 , and a sentence encoder learned from scratch for this task.",
                        "In total, this resulted in 57,505 and 2,428 sentences in the train and test set abstracts, respectively."
                    ]
                },
                {
                    "raw_evidence": [
                        "An abstract may contain some `easy' and some `difficult' sentences. We thus perform our analysis at the sentence level. We split abstracts into sentences using spaCy. We excluded sentences that comprise fewer than two tokens, as these are likely an artifact of errors in sentence splitting. In total, this resulted in 57,505 and 2,428 sentences in the train and test set abstracts, respectively."
                    ],
                    "highlighted_evidence": [
                        "In total, this resulted in 57,505 and 2,428 sentences in the train and test set abstracts, respectively.",
                        "57,505"
                    ]
                }
            ]
        },
        {
            "question": "Is an instance a sentence or an IE tuple?",
            "answers": [
                {
                    "answer": "sentence",
                    "type": "extractive"
                }
            ],
            "q_uid": "06b5272774ec43ee5facfa7111033386f06cf448",
            "evidence": [
                {
                    "raw_evidence": [
                        "Table 1 shows an example of difficult and easy examples according to our definition of difficulty. The underlined text demarcates the (consensus) reference label provided by domain experts. In the difficult examples, crowd workers marked text distinct from these reference annotations; whereas in the easy cases they reproduced them with reasonable fidelity. The difficult sentences usually exhibit complicated structure and feature jargon."
                    ],
                    "highlighted_evidence": [
                        "Table 1 shows an example of difficult and easy examples according to our definition of difficulty. The underlined text demarcates the (consensus) reference label provided by domain experts. In the difficult examples, crowd workers marked text distinct from these reference annotations; whereas in the easy cases they reproduced them with reasonable fidelity. The difficult sentences usually exhibit complicated structure and feature jargon."
                    ]
                }
            ]
        }
    ],
    "1903.11437": [
        {
            "question": "why are their techniques cheaper to implement?",
            "answers": [
                {
                    "answer": "They use a slightly modified copy of the target to create the pseudo-text instead of full BT to make their technique cheaper",
                    "type": "abstractive"
                },
                {
                    "answer": "They do not require the availability of a backward translation engine.",
                    "type": "abstractive"
                }
            ],
            "q_uid": "87bb3105e03ed6ac5abfde0a7ca9b8de8985663c",
            "evidence": [
                {
                    "raw_evidence": [
                        "In this paper, we have analyzed various ways to integrate monolingual data in an NMT framework, focusing on their impact on quality and domain adaptation. While confirming the effectiveness of BT, our study also proposed significantly cheaper ways to improve the baseline performance, using a slightly modified copy of the target, instead of its full BT. When no high quality BT is available, using GANs to make the pseudo-source sentences closer to natural source sentences is an efficient solution for domain adaptation."
                    ],
                    "highlighted_evidence": [
                        "While confirming the effectiveness of BT, our study also proposed significantly cheaper ways to improve the baseline performance, using a slightly modified copy of the target, instead of its full BT. "
                    ]
                },
                {
                    "raw_evidence": [
                        "We now analyze the effect of using much simpler data generation schemes, which do not require the availability of a backward translation engine."
                    ],
                    "highlighted_evidence": [
                        "We now analyze the effect of using much simpler data generation schemes, which do not require the availability of a backward translation engine."
                    ]
                }
            ]
        },
        {
            "question": "what data simulation techniques were introduced?",
            "answers": [
                {
                    "answer": "copy, copy-marked, copy-dummies",
                    "type": "extractive"
                },
                {
                    "answer": "copy, copy-marked, copy-dummies",
                    "type": "extractive"
                }
            ],
            "q_uid": "d9980676a83295dda37c20cfd5d58e574d0a4859",
            "evidence": [
                {
                    "raw_evidence": [
                        "We use the following cheap ways to generate pseudo-source texts:",
                        "copy: in this setting, the source side is a mere copy of the target-side data. Since the source vocabulary of the NMT is fixed, copying the target sentences can cause the occurrence of OOVs. To avoid this situation, BIBREF24 decompose the target words into source-side units to make the copy look like source sentences. Each OOV found in the copy is split into smaller units until all the resulting chunks are in the source vocabulary.",
                        "copy-marked: another way to integrate copies without having to deal with OOVs is to augment the source vocabulary with a copy of the target vocabulary. In this setup, BIBREF25 ensure that both vocabularies never overlap by marking the target word copies with a special language identifier. Therefore the English word resume cannot be confused with the homographic French word, which is marked @fr@resume.",
                        "copy-dummies: instead of using actual copies, we replace each word with \u201cdummy\u201d tokens. We use this unrealistic setup to observe the training over noisy and hardly informative source sentences."
                    ],
                    "highlighted_evidence": [
                        "We use the following cheap ways to generate pseudo-source texts:\n\ncopy: in this setting, the source side is a mere copy of the target-side data.",
                        "copy-marked: another way to integrate copies without having to deal with OOVs is to augment the source vocabulary with a copy of the target vocabulary. ",
                        "copy-dummies: instead of using actual copies, we replace each word with \u201cdummy\u201d tokens. "
                    ]
                },
                {
                    "raw_evidence": [
                        "We use the following cheap ways to generate pseudo-source texts:",
                        "copy: in this setting, the source side is a mere copy of the target-side data. Since the source vocabulary of the NMT is fixed, copying the target sentences can cause the occurrence of OOVs. To avoid this situation, BIBREF24 decompose the target words into source-side units to make the copy look like source sentences. Each OOV found in the copy is split into smaller units until all the resulting chunks are in the source vocabulary.",
                        "copy-marked: another way to integrate copies without having to deal with OOVs is to augment the source vocabulary with a copy of the target vocabulary. In this setup, BIBREF25 ensure that both vocabularies never overlap by marking the target word copies with a special language identifier. Therefore the English word resume cannot be confused with the homographic French word, which is marked @fr@resume.",
                        "copy-dummies: instead of using actual copies, we replace each word with \u201cdummy\u201d tokens. We use this unrealistic setup to observe the training over noisy and hardly informative source sentences."
                    ],
                    "highlighted_evidence": [
                        "We use the following cheap ways to generate pseudo-source texts:",
                        "copy: in this setting, the source side is a mere copy of the target-side data. ",
                        "copy-marked: another way to integrate copies without having to deal with OOVs is to augment the source vocabulary with a copy of the target vocabulary. ",
                        "copy-dummies: instead of using actual copies, we replace each word with \u201cdummy\u201d tokens. "
                    ]
                }
            ]
        },
        {
            "question": "what is their explanation for the effectiveness of back-translation?",
            "answers": [
                {
                    "answer": "when using BT, cases where the source is shorter than the target are rarer; cases when they have the same length are more frequent, automatic word alignments between artificial sources tend to be more monotonic than when using natural sources",
                    "type": "extractive"
                }
            ],
            "q_uid": "9225b651e0fed28d4b6261a9f6b443b52597e401",
            "evidence": [
                {
                    "raw_evidence": [
                        "Comparing the natural and artificial sources of our parallel data wrt. several linguistic and distributional properties, we observe that (see Fig. FIGREF21 - FIGREF22 ):",
                        "artificial sources are on average shorter than natural ones: when using BT, cases where the source is shorter than the target are rarer; cases when they have the same length are more frequent.",
                        "automatic word alignments between artificial sources tend to be more monotonic than when using natural sources, as measured by the average Kendall INLINEFORM0 of source-target alignments BIBREF22 : for French-English the respective numbers are 0.048 (natural) and 0.018 (artificial); for German-English 0.068 and 0.053. Using more monotonic sentence pairs turns out to be a facilitating factor for NMT, as also noted by BIBREF20 .",
                        "The intuition is that properties (i) and (ii) should help translation as compared to natural source, while property (iv) should be detrimental. We checked (ii) by building systems with only 10M words from the natural parallel data selecting these data either randomly or based on the regularity of their word alignments. Results in Table TABREF23 show that the latter is much preferable for the overall performance. This might explain that the mostly monotonic BT from Moses are almost as good as the fluid BT from NMT and that both boost the baseline."
                    ],
                    "highlighted_evidence": [
                        "Comparing the natural and artificial sources of our parallel data wrt. several linguistic and distributional properties, we observe that (see Fig. FIGREF21 - FIGREF22 ):\n\nartificial sources are on average shorter than natural ones: when using BT, cases where the source is shorter than the target are rarer; cases when they have the same length are more frequent.\n\nautomatic word alignments between artificial sources tend to be more monotonic than when using natural sources, as measured by the average Kendall INLINEFORM0 of source-target alignments BIBREF22 : for French-English the respective numbers are 0.048 (natural) and 0.018 (artificial); for German-English 0.068 and 0.053. ",
                        "The intuition is that properties (i) and (ii) should help translation as compared to natural source, while property (iv) should be detrimental."
                    ]
                }
            ]
        },
        {
            "question": "what dataset is used?",
            "answers": [
                {
                    "answer": "Europarl corpus , WMT newstest 2014, News-Commentary-11, Wikipedia from WMT 2014, Multi-UN, EU-Bookshop, Rapid, Common-Crawl (WMT 2017)",
                    "type": "extractive"
                },
                {
                    "answer": "Europarl tests from 2006, 2007, 2008; WMT newstest 2014.",
                    "type": "abstractive"
                }
            ],
            "q_uid": "565189b672efee01d22f4fc6b73cd5287b2ee72c",
            "evidence": [
                {
                    "raw_evidence": [
                        "We are mostly interested with the following training scenario: a large out-of-domain parallel corpus, and limited monolingual in-domain data. We focus here on the Europarl domain, for which we have ample data in several languages, and use as in-domain training data the Europarl corpus BIBREF5 for two translation directions: English INLINEFORM0 German and English INLINEFORM1 French. As we study the benefits of monolingual data, most of our experiments only use the target side of this corpus. The rationale for choosing this domain is to (i) to perform large scale comparisons of synthetic and natural parallel corpora; (ii) to study the effect of BT in a well-defined domain-adaptation scenario. For both language pairs, we use the Europarl tests from 2007 and 2008 for evaluation purposes, keeping test 2006 for development. When measuring out-of-domain performance, we will use the WMT newstest 2014.",
                        "Our baseline NMT system implements the attentional encoder-decoder approach BIBREF6 , BIBREF7 as implemented in Nematus BIBREF8 on 4 million out-of-domain parallel sentences. For French we use samples from News-Commentary-11 and Wikipedia from WMT 2014 shared translation task, as well as the Multi-UN BIBREF9 and EU-Bookshop BIBREF10 corpora. For German, we use samples from News-Commentary-11, Rapid, Common-Crawl (WMT 2017) and Multi-UN (see table TABREF5 ). Bilingual BPE units BIBREF11 are learned with 50k merge operations, yielding vocabularies of about respectively 32k and 36k for English INLINEFORM0 French and 32k and 44k for English INLINEFORM1 German."
                    ],
                    "highlighted_evidence": [
                        "We focus here on the Europarl domain, for which we have ample data in several languages, and use as in-domain training data the Europarl corpus BIBREF5 for two translation directions: English INLINEFORM0 German and English INLINEFORM1 French.",
                        "When measuring out-of-domain performance, we will use the WMT newstest 2014.",
                        "or French we use samples from News-Commentary-11 and Wikipedia from WMT 2014 shared translation task, as well as the Multi-UN BIBREF9 and EU-Bookshop BIBREF10 corpora. For German, we use samples from News-Commentary-11, Rapid, Common-Crawl (WMT 2017) and Multi-UN (see table TABREF5 ). "
                    ]
                },
                {
                    "raw_evidence": [
                        "We are mostly interested with the following training scenario: a large out-of-domain parallel corpus, and limited monolingual in-domain data. We focus here on the Europarl domain, for which we have ample data in several languages, and use as in-domain training data the Europarl corpus BIBREF5 for two translation directions: English INLINEFORM0 German and English INLINEFORM1 French. As we study the benefits of monolingual data, most of our experiments only use the target side of this corpus. The rationale for choosing this domain is to (i) to perform large scale comparisons of synthetic and natural parallel corpora; (ii) to study the effect of BT in a well-defined domain-adaptation scenario. For both language pairs, we use the Europarl tests from 2007 and 2008 for evaluation purposes, keeping test 2006 for development. When measuring out-of-domain performance, we will use the WMT newstest 2014."
                    ],
                    "highlighted_evidence": [
                        "For both language pairs, we use the Europarl tests from 2007 and 2008 for evaluation purposes, keeping test 2006 for development. When measuring out-of-domain performance, we will use the WMT newstest 2014."
                    ]
                }
            ]
        },
        {
            "question": "what language pairs are explored?",
            "answers": [
                {
                    "answer": "English-German, English-French.",
                    "type": "abstractive"
                },
                {
                    "answer": "English-German, English-French",
                    "type": "abstractive"
                }
            ],
            "q_uid": "b6f7fadaa1bb828530c2d6780289f12740229d84",
            "evidence": [
                {
                    "raw_evidence": [
                        "We are mostly interested with the following training scenario: a large out-of-domain parallel corpus, and limited monolingual in-domain data. We focus here on the Europarl domain, for which we have ample data in several languages, and use as in-domain training data the Europarl corpus BIBREF5 for two translation directions: English INLINEFORM0 German and English INLINEFORM1 French. As we study the benefits of monolingual data, most of our experiments only use the target side of this corpus. The rationale for choosing this domain is to (i) to perform large scale comparisons of synthetic and natural parallel corpora; (ii) to study the effect of BT in a well-defined domain-adaptation scenario. For both language pairs, we use the Europarl tests from 2007 and 2008 for evaluation purposes, keeping test 2006 for development. When measuring out-of-domain performance, we will use the WMT newstest 2014."
                    ],
                    "highlighted_evidence": [
                        "We focus here on the Europarl domain, for which we have ample data in several languages, and use as in-domain training data the Europarl corpus BIBREF5 for two translation directions: English INLINEFORM0 German and English INLINEFORM1 French."
                    ]
                },
                {
                    "raw_evidence": [
                        "We are mostly interested with the following training scenario: a large out-of-domain parallel corpus, and limited monolingual in-domain data. We focus here on the Europarl domain, for which we have ample data in several languages, and use as in-domain training data the Europarl corpus BIBREF5 for two translation directions: English INLINEFORM0 German and English INLINEFORM1 French. As we study the benefits of monolingual data, most of our experiments only use the target side of this corpus. The rationale for choosing this domain is to (i) to perform large scale comparisons of synthetic and natural parallel corpora; (ii) to study the effect of BT in a well-defined domain-adaptation scenario. For both language pairs, we use the Europarl tests from 2007 and 2008 for evaluation purposes, keeping test 2006 for development. When measuring out-of-domain performance, we will use the WMT newstest 2014."
                    ],
                    "highlighted_evidence": [
                        "We focus here on the Europarl domain, for which we have ample data in several languages, and use as in-domain training data the Europarl corpus BIBREF5 for two translation directions: English INLINEFORM0 German and English INLINEFORM1 French. "
                    ]
                }
            ]
        },
        {
            "question": "what language is the data in?",
            "answers": [
                {
                    "answer": "English , German, French",
                    "type": "extractive"
                }
            ],
            "q_uid": "7b9ca0e67e394f1674f0bcf1c53dfc2d474f8613",
            "evidence": [
                {
                    "raw_evidence": [
                        "We are mostly interested with the following training scenario: a large out-of-domain parallel corpus, and limited monolingual in-domain data. We focus here on the Europarl domain, for which we have ample data in several languages, and use as in-domain training data the Europarl corpus BIBREF5 for two translation directions: English INLINEFORM0 German and English INLINEFORM1 French. As we study the benefits of monolingual data, most of our experiments only use the target side of this corpus. The rationale for choosing this domain is to (i) to perform large scale comparisons of synthetic and natural parallel corpora; (ii) to study the effect of BT in a well-defined domain-adaptation scenario. For both language pairs, we use the Europarl tests from 2007 and 2008 for evaluation purposes, keeping test 2006 for development. When measuring out-of-domain performance, we will use the WMT newstest 2014."
                    ],
                    "highlighted_evidence": [
                        "We focus here on the Europarl domain, for which we have ample data in several languages, and use as in-domain training data the Europarl corpus BIBREF5 for two translation directions: English INLINEFORM0 German and English INLINEFORM1 French. "
                    ]
                }
            ]
        }
    ],
    "1912.01252": [
        {
            "question": "Does the paper report the results of previous models applied to the same tasks?",
            "answers": [
                {
                    "answer": "Yes",
                    "type": "boolean"
                },
                {
                    "answer": "No",
                    "type": "boolean"
                }
            ],
            "q_uid": "5679fabeadf680e35a4f7b092d39e8638dca6b4d",
            "evidence": [
                {
                    "raw_evidence": [
                        "Technical and theoretical questions related to the proposed method and infrastructure for the exploration and facilitation of debates will be discussed in three sections. The first section concerns notions of how to define what constitutes a belief or opinion and how these can be mined from texts. To this end, an approach based on the automated extraction of semantic frames expressing causation is proposed. The observatory thus builds on the theoretical premise that expressions of causation such as `global warming causes rises in sea levels' can be revelatory for a person or group's underlying belief systems. Through a further technical description of the observatory's data-analytical components, section two of the paper deals with matters of spatially modelling the output of the semantic frame extractor and how this might be achieved without sacrificing nuances of meaning. The final section of the paper, then, discusses how insights gained from technologically observing opinion dynamics can inform conceptual modelling efforts and approaches to on-line opinion facilitation. As such, the paper brings into view and critically evaluates the fundamental conceptual leap from machine-guided observation to debate facilitation and intervention."
                    ],
                    "highlighted_evidence": [
                        "The final section of the paper, then, discusses how insights gained from technologically observing opinion dynamics can inform conceptual modelling efforts and approaches to on-line opinion facilitation. As such, the paper brings into view and critically evaluates the fundamental conceptual leap from machine-guided observation to debate facilitation and intervention."
                    ]
                },
                {
                    "raw_evidence": [],
                    "highlighted_evidence": []
                }
            ]
        },
        {
            "question": "What are the causal mapping methods employed?",
            "answers": [
                {
                    "answer": "Axelrod's causal mapping method",
                    "type": "extractive"
                }
            ],
            "q_uid": "312417675b3dc431eb7e7b16a917b7fed98d4376",
            "evidence": [
                {
                    "raw_evidence": [
                        "Axelrod's causal mapping method comprises a set of conventions to graphically represent networks of causes and effects (the nodes in a network) as well as the qualitative aspects of this relation (the network\u2019s directed edges, notably assertions of whether the causal linkage is positive or negative). These causes and effects are to be extracted from relevant sources by means of a series of heuristics and an encoding scheme (it should be noted that for this task Axelrod had human readers in mind). The graphs resulting from these efforts provide a structural overview of the relations among causal assertions (and thus beliefs):"
                    ],
                    "highlighted_evidence": [
                        "Axelrod's causal mapping method comprises a set of conventions to graphically represent networks of causes and effects (the nodes in a network) as well as the qualitative aspects of this relation (the network\u2019s directed edges, notably assertions of whether the causal linkage is positive or negative)."
                    ]
                }
            ]
        }
    ],
    "1910.12795": [
        {
            "question": "What off-the-shelf reward learning algorithm from RL for joint data manipulation learning and model training is adapted?",
            "answers": [
                {
                    "answer": "BIBREF7",
                    "type": "extractive"
                },
                {
                    "answer": " reward learning algorithm BIBREF7",
                    "type": "extractive"
                }
            ],
            "q_uid": "223dc2b9ea34addc0f502003c2e1c1141f6b36a7",
            "evidence": [
                {
                    "raw_evidence": [
                        "In this work, we propose a new approach that enables learning for different manipulation schemes with the same single algorithm. Our approach draws inspiration from the recent work BIBREF6 that shows equivalence between the data in supervised learning and the reward function in reinforcement learning. We thus adapt an off-the-shelf reward learning algorithm BIBREF7 to the supervised setting for automated data manipulation. The marriage of the two paradigms results in a simple yet general algorithm, where various manipulation schemes are reduced to different parameterization of the data reward. Free parameters of manipulation are learned jointly with the target model through efficient gradient descent on validation examples. We demonstrate instantiations of the approach for automatically fine-tuning an augmentation network and learning data weights, respectively."
                    ],
                    "highlighted_evidence": [
                        "We thus adapt an off-the-shelf reward learning algorithm BIBREF7 to the supervised setting for automated data manipulation."
                    ]
                },
                {
                    "raw_evidence": [
                        "In this work, we propose a new approach that enables learning for different manipulation schemes with the same single algorithm. Our approach draws inspiration from the recent work BIBREF6 that shows equivalence between the data in supervised learning and the reward function in reinforcement learning. We thus adapt an off-the-shelf reward learning algorithm BIBREF7 to the supervised setting for automated data manipulation. The marriage of the two paradigms results in a simple yet general algorithm, where various manipulation schemes are reduced to different parameterization of the data reward. Free parameters of manipulation are learned jointly with the target model through efficient gradient descent on validation examples. We demonstrate instantiations of the approach for automatically fine-tuning an augmentation network and learning data weights, respectively."
                    ],
                    "highlighted_evidence": [
                        "We thus adapt an off-the-shelf reward learning algorithm BIBREF7 to the supervised setting for automated data manipulation."
                    ]
                }
            ]
        }
    ],
    "1906.01512": [
        {
            "question": "What models are included in the toolkit?",
            "answers": [
                {
                    "answer": " recurrent neural network (RNN)-based sequence-to-sequence (Seq2Seq) models for NATS",
                    "type": "extractive"
                }
            ],
            "q_uid": "ae95a7d286cb7a0d5bc1a8283ecbf803e9305951",
            "evidence": [
                {
                    "raw_evidence": [
                        "Modules: Modules are the basic building blocks of different models. In LeafNATS, we provide ready-to-use modules for constructing recurrent neural network (RNN)-based sequence-to-sequence (Seq2Seq) models for NATS, e.g., pointer-generator network BIBREF1 . These modules include embedder, RNN encoder, attention BIBREF24 , temporal attention BIBREF6 , attention on decoder BIBREF2 and others. We also use these basic modules to assemble a pointer-generator decoder module and the corresponding beam search algorithms. The embedder can also be used to realize the embedding-weights sharing mechanism BIBREF2 ."
                    ],
                    "highlighted_evidence": [
                        "Modules: Modules are the basic building blocks of different models. In LeafNATS, we provide ready-to-use modules for constructing recurrent neural network (RNN)-based sequence-to-sequence (Seq2Seq) models for NATS, e.g., pointer-generator network BIBREF1 . These modules include embedder, RNN encoder, attention BIBREF24 , temporal attention BIBREF6 , attention on decoder BIBREF2 and others. We also use these basic modules to assemble a pointer-generator decoder module and the corresponding beam search algorithms. The embedder can also be used to realize the embedding-weights sharing mechanism BIBREF2 ."
                    ]
                }
            ]
        }
    ],
    "2002.06424": [
        {
            "question": "Do they repot results only on English data?",
            "answers": [
                {
                    "answer": "Yes",
                    "type": "boolean"
                }
            ],
            "q_uid": "eae13c9693ace504eab1f96c91b16a0627cd1f75",
            "evidence": [
                {
                    "raw_evidence": [
                        "We evaluate the proposed architecture on two publicly available datasets: the Adverse Drug Events (ADE) dataset BIBREF6 and the CoNLL04 dataset BIBREF7. We show that our architecture is able to outperform the current state-of-the-art (SOTA) results on both the NER and RE tasks in the case of ADE. In the case of CoNLL04, our proposed architecture achieves SOTA performance on the NER task and achieves near SOTA performance on the RE task. On both datasets, our results are SOTA when averaging performance across both tasks. Moreover, we achieve these results using an order of magnitude fewer trainable parameters than the current SOTA architecture."
                    ],
                    "highlighted_evidence": [
                        "We evaluate the proposed architecture on two publicly available datasets: the Adverse Drug Events (ADE) dataset BIBREF6 and the CoNLL04 dataset BIBREF7. We show that our architecture is able to outperform the current state-of-the-art (SOTA) results on both the NER and RE tasks in the case of ADE. In the case of CoNLL04, our proposed architecture achieves SOTA performance on the NER task and achieves near SOTA performance on the RE task. On both datasets, our results are SOTA when averaging performance across both tasks."
                    ]
                }
            ]
        },
        {
            "question": "What were the variables in the ablation study?",
            "answers": [
                {
                    "answer": "(i) zero NER-specific BiRNN layers, (ii) zero RE-specific BiRNN layers, or (iii) zero task-specific BiRNN layers of any kind",
                    "type": "extractive"
                }
            ],
            "q_uid": "bcec22a75c1f899e9fcea4996457cf177c50c4c5",
            "evidence": [
                {
                    "raw_evidence": [
                        "To further demonstrate the effectiveness of the additional task-specific BiRNN layers in our architecture, we conducted an ablation study using the CoNLL04 dataset. We trained and evaluated in the same manner described above, using the same hyperparameters, with the following exceptions:",
                        "We used either (i) zero NER-specific BiRNN layers, (ii) zero RE-specific BiRNN layers, or (iii) zero task-specific BiRNN layers of any kind.",
                        "We increased the number of shared BiRNN layers to keep the total number of model parameters consistent with the number of parameters in the baseline model.",
                        "We average the results for each set of hyperparameter across three trials with random weight initializations."
                    ],
                    "highlighted_evidence": [
                        "To further demonstrate the effectiveness of the additional task-specific BiRNN layers in our architecture, we conducted an ablation study using the CoNLL04 dataset. We trained and evaluated in the same manner described above, using the same hyperparameters, with the following exceptions:\n\nWe used either (i) zero NER-specific BiRNN layers, (ii) zero RE-specific BiRNN layers, or (iii) zero task-specific BiRNN layers of any kind.\n\nWe increased the number of shared BiRNN layers to keep the total number of model parameters consistent with the number of parameters in the baseline model.\n\nWe average the results for each set of hyperparameter across three trials with random weight initializations."
                    ]
                }
            ]
        }
    ],
    "1608.06757": [
        {
            "question": "what standard dataset were used?",
            "answers": [
                {
                    "answer": "The GENIA Corpus , CoNLL2003",
                    "type": "extractive"
                },
                {
                    "answer": "GENIA Corpus BIBREF3, CoNLL2003 BIBREF14, KORE50 BIBREF21 , ACE2004 BIBREF22 and MSNBC",
                    "type": "extractive"
                },
                {
                    "answer": "CoNLL2003-testA, GENIA",
                    "type": "extractive"
                }
            ],
            "q_uid": "8eefa116e3c3d3db751423cc4095d1c4153d3a5f",
            "evidence": [
                {
                    "raw_evidence": [
                        "Table TABREF33 gives an overview of the standard data sets we use for training. The GENIA Corpus BIBREF3 contains biomedical abstracts from the PubMed database. We use GENIA technical term annotations 3.02, which cover linguistic expressions to entities of interest in molecular biology, e.g. proteins, genes and cells. CoNLL2003 BIBREF14 is a standard NER dataset based on the Reuters RCV-1 news corpus. It covers named entities of type person, location, organization and misc.",
                        "For testing the overall annotation performance, we utilize CoNLL2003-testA and a 50 document split from GENIA. Additionally, we test on the complete KORE50 BIBREF21 , ACE2004 BIBREF22 and MSNBC data sets using the GERBIL evaluation framework BIBREF23 ."
                    ],
                    "highlighted_evidence": [
                        "The GENIA Corpus BIBREF3 contains biomedical abstracts from the PubMed database. We use GENIA technical term annotations 3.02, which cover linguistic expressions to entities of interest in molecular biology, e.g. proteins, genes and cells. CoNLL2003 BIBREF14 is a standard NER dataset based on the Reuters RCV-1 news corpus. It covers named entities of type person, location, organization and misc.\n\nFor testing the overall annotation performance, we utilize CoNLL2003-testA and a 50 document split from GENIA. "
                    ]
                },
                {
                    "raw_evidence": [
                        "Table TABREF33 gives an overview of the standard data sets we use for training. The GENIA Corpus BIBREF3 contains biomedical abstracts from the PubMed database. We use GENIA technical term annotations 3.02, which cover linguistic expressions to entities of interest in molecular biology, e.g. proteins, genes and cells. CoNLL2003 BIBREF14 is a standard NER dataset based on the Reuters RCV-1 news corpus. It covers named entities of type person, location, organization and misc.",
                        "For testing the overall annotation performance, we utilize CoNLL2003-testA and a 50 document split from GENIA. Additionally, we test on the complete KORE50 BIBREF21 , ACE2004 BIBREF22 and MSNBC data sets using the GERBIL evaluation framework BIBREF23 ."
                    ],
                    "highlighted_evidence": [
                        "The GENIA Corpus BIBREF3 contains biomedical abstracts from the PubMed database. We use GENIA technical term annotations 3.02, which cover linguistic expressions to entities of interest in molecular biology, e.g. proteins, genes and cells. CoNLL2003 BIBREF14 is a standard NER dataset based on the Reuters RCV-1 news corpus. It covers named entities of type person, location, organization and misc.\n\nFor testing the overall annotation performance, we utilize CoNLL2003-testA and a 50 document split from GENIA. Additionally, we test on the complete KORE50 BIBREF21 , ACE2004 BIBREF22 and MSNBC data sets using the GERBIL evaluation framework BIBREF23 ."
                    ]
                },
                {
                    "raw_evidence": [
                        "For testing the overall annotation performance, we utilize CoNLL2003-testA and a 50 document split from GENIA. Additionally, we test on the complete KORE50 BIBREF21 , ACE2004 BIBREF22 and MSNBC data sets using the GERBIL evaluation framework BIBREF23 ."
                    ],
                    "highlighted_evidence": [
                        "For testing the overall annotation performance, we utilize CoNLL2003-testA and a 50 document split from GENIA. "
                    ]
                }
            ]
        }
    ],
    "2001.05284": [
        {
            "question": "Which ASR system(s) is used in this work?",
            "answers": [
                {
                    "answer": "Oracle ",
                    "type": "extractive"
                }
            ],
            "q_uid": "67131c15aceeb51ae1d3b2b8241c8750a19cca8e",
            "evidence": [
                {
                    "raw_evidence": [
                        "The preliminary architecture is shown in Fig. FIGREF4. For a given transcribed utterance, it is firstly encoded with Byte Pair Encoding (BPE) BIBREF14, a compression algorithm splitting words to fundamental subword units (pairs of bytes or BPs) and reducing the embedded vocabulary size. Then we use a BiLSTM BIBREF15 encoder and the output state of the BiLSTM is regarded as a vector representation for this utterance. Finally, a fully connected Feed-forward Neural Network (FNN) followed by a softmax layer, labeled as a multilayer perceptron (MLP) module, is used to perform the domain/intent classification task based on the vector.",
                        "For convenience, we simplify the whole process in Fig.FIGREF4 as a mapping $BM$ (Baseline Mapping) from the input utterance $S$ to an estimated tag's probability $p(\\tilde{t})$, where $p(\\tilde{t}) \\leftarrow BM(S)$. The $Baseline$ is trained on transcription and evaluated on ASR 1st best hypothesis ($S=\\text{ASR}\\ 1^{st}\\ \\text{best})$. The $Oracle$ is trained on transcription and evaluated on transcription ($S = \\text{Transcription}$). We name it Oracle simply because we assume that hypotheses are noisy versions of transcription."
                    ],
                    "highlighted_evidence": [
                        "For a given transcribed utterance, it is firstly encoded with Byte Pair Encoding (BPE) BIBREF14, a compression algorithm splitting words to fundamental subword units (pairs of bytes or BPs) and reducing the embedded vocabulary size. Then we use a BiLSTM BIBREF15 encoder and the output state of the BiLSTM is regarded as a vector representation for this utterance. Finally, a fully connected Feed-forward Neural Network (FNN) followed by a softmax layer, labeled as a multilayer perceptron (MLP) module, is used to perform the domain/intent classification task based on the vector.",
                        "We name it Oracle simply because we assume that hypotheses are noisy versions of transcription."
                    ]
                }
            ]
        },
        {
            "question": "What are the series of simple models?",
            "answers": [
                {
                    "answer": "perform experiments to utilize ASR $n$-best hypotheses during evaluation",
                    "type": "extractive"
                }
            ],
            "q_uid": "579a0603ec56fc2b4aa8566810041dbb0cd7b5e7",
            "evidence": [
                {
                    "raw_evidence": [
                        "Besides the Baseline and Oracle, where only ASR 1-best hypothesis is considered, we also perform experiments to utilize ASR $n$-best hypotheses during evaluation. The models evaluating with $n$-bests and a BM (pre-trained on transcription) are called Direct Models (in Fig. FIGREF7):"
                    ],
                    "highlighted_evidence": [
                        "Besides the Baseline and Oracle, where only ASR 1-best hypothesis is considered, we also perform experiments to utilize ASR $n$-best hypotheses during evaluation."
                    ]
                }
            ]
        },
        {
            "question": "Over which datasets/corpora is this work evaluated?",
            "answers": [
                {
                    "answer": "$\\sim $ 8.7M annotated anonymised user utterances",
                    "type": "extractive"
                },
                {
                    "answer": "on $\\sim $ 8.7M annotated anonymised user utterances",
                    "type": "extractive"
                }
            ],
            "q_uid": "c9c85eee41556c6993f40e428fa607af4abe80a9",
            "evidence": [
                {
                    "raw_evidence": [
                        "We conduct our experiments on $\\sim $ 8.7M annotated anonymised user utterances. They are annotated and derived from requests across 23 domains."
                    ],
                    "highlighted_evidence": [
                        "We conduct our experiments on $\\sim $ 8.7M annotated anonymised user utterances. They are annotated and derived from requests across 23 domains."
                    ]
                },
                {
                    "raw_evidence": [
                        "We conduct our experiments on $\\sim $ 8.7M annotated anonymised user utterances. They are annotated and derived from requests across 23 domains."
                    ],
                    "highlighted_evidence": [
                        "We conduct our experiments on $\\sim $ 8.7M annotated anonymised user utterances. They are annotated and derived from requests across 23 domains."
                    ]
                }
            ]
        }
    ],
    "1909.09067": [
        {
            "question": "Which information about text structure is included in the corpus?",
            "answers": [
                {
                    "answer": "paragraphs, lines, Information on physical page segmentation (for PDFs only), paragraph segmentation, and line segmentation",
                    "type": "extractive"
                },
                {
                    "answer": "paragraph, lines, textspan element (paragraph segmentation, line segmentation, Information on physical page segmentation(for PDF only))",
                    "type": "abstractive"
                }
            ],
            "q_uid": "9eabb54c2408dac24f00f92cf1061258c7ea2e1a",
            "evidence": [
                {
                    "raw_evidence": [
                        "The paper at hand introduces a corpus developed for use in automatic readability assessment and automatic text simplification of German. The focus of this publication is on representing information that is valuable for these tasks but that hitherto has largely been ignored in machine learning approaches centering around simplified language, specifically, text structure (e.g., paragraphs, lines), typography (e.g., font type, font style), and image (content, position, and dimensions) information. The importance of considering such information has repeatedly been asserted theoretically BIBREF11, BIBREF12, BIBREF0. The remainder of this paper is structured as follows: Section SECREF2 presents previous corpora used for automatic readability assessment and text simplification. Section SECREF3 describes our corpus, introducing its novel aspects and presenting the primary data (Section SECREF7), the metadata (Section SECREF10), the secondary data (Section SECREF28), the profile (Section SECREF35), and the results of machine learning experiments carried out on the corpus (Section SECREF37).",
                        "Information on physical page segmentation (for PDFs only), paragraph segmentation, and line segmentation was added as part of a textspan element in the textstructure layer"
                    ],
                    "highlighted_evidence": [
                        "The focus of this publication is on representing information that is valuable for these tasks but that hitherto has largely been ignored in machine learning approaches centering around simplified language, specifically, text structure (e.g., paragraphs, lines), typography (e.g., font type, font style), and image (content, position, and dimensions) information.",
                        "Information on physical page segmentation (for PDFs only), paragraph segmentation, and line segmentation was added as part of a textspan element in the textstructure layer"
                    ]
                },
                {
                    "raw_evidence": [
                        "The paper at hand introduces a corpus developed for use in automatic readability assessment and automatic text simplification of German. The focus of this publication is on representing information that is valuable for these tasks but that hitherto has largely been ignored in machine learning approaches centering around simplified language, specifically, text structure (e.g., paragraphs, lines), typography (e.g., font type, font style), and image (content, position, and dimensions) information. The importance of considering such information has repeatedly been asserted theoretically BIBREF11, BIBREF12, BIBREF0. The remainder of this paper is structured as follows: Section SECREF2 presents previous corpora used for automatic readability assessment and text simplification. Section SECREF3 describes our corpus, introducing its novel aspects and presenting the primary data (Section SECREF7), the metadata (Section SECREF10), the secondary data (Section SECREF28), the profile (Section SECREF35), and the results of machine learning experiments carried out on the corpus (Section SECREF37).",
                        "Information on physical page segmentation (for PDFs only), paragraph segmentation, and line segmentation was added as part of a textspan element in the textstructure layer"
                    ],
                    "highlighted_evidence": [
                        "The focus of this publication is on representing information that is valuable for these tasks but that hitherto has largely been ignored in machine learning approaches centering around simplified language, specifically, text structure (e.g., paragraphs, lines), typography (e.g., font type, font style), and image (content, position, and dimensions) information. ",
                        "Information on physical page segmentation (for PDFs only), paragraph segmentation, and line segmentation was added as part of a textspan element in the textstructure layer"
                    ]
                }
            ]
        },
        {
            "question": "Which information about typography is included in the corpus?",
            "answers": [
                {
                    "answer": "font type, font style, Information on the font type and font style (e.g., italics, bold print) of a token and its position on the physical page",
                    "type": "extractive"
                },
                {
                    "answer": "font type and font style (e.g., italics, bold print) of a token and its position on the physical page (for PDFs only) was specified as attributes to the token elements of the tokens layer, A separate fonts layer was introduced to preserve detailed information on the font configurations referenced in the tokens layer",
                    "type": "extractive"
                }
            ],
            "q_uid": "3d013f15796ae7fed5272183a166c45f16e24e39",
            "evidence": [
                {
                    "raw_evidence": [
                        "The paper at hand introduces a corpus developed for use in automatic readability assessment and automatic text simplification of German. The focus of this publication is on representing information that is valuable for these tasks but that hitherto has largely been ignored in machine learning approaches centering around simplified language, specifically, text structure (e.g., paragraphs, lines), typography (e.g., font type, font style), and image (content, position, and dimensions) information. The importance of considering such information has repeatedly been asserted theoretically BIBREF11, BIBREF12, BIBREF0. The remainder of this paper is structured as follows: Section SECREF2 presents previous corpora used for automatic readability assessment and text simplification. Section SECREF3 describes our corpus, introducing its novel aspects and presenting the primary data (Section SECREF7), the metadata (Section SECREF10), the secondary data (Section SECREF28), the profile (Section SECREF35), and the results of machine learning experiments carried out on the corpus (Section SECREF37).",
                        "Information on the font type and font style (e.g., italics, bold print) of a token and its position on the physical page (for PDFs only) was specified as attributes to the token elements of the tokens layer (cf. Figure FIGREF34 for an example)"
                    ],
                    "highlighted_evidence": [
                        "The focus of this publication is on representing information that is valuable for these tasks but that hitherto has largely been ignored in machine learning approaches centering around simplified language, specifically, text structure (e.g., paragraphs, lines), typography (e.g., font type, font style), and image (content, position, and dimensions) information.",
                        "Information on the font type and font style (e.g., italics, bold print) of a token and its position on the physical page (for PDFs only) was specified as attributes to the token elements of the tokens layer (cf. Figure FIGREF34 for an example)"
                    ]
                },
                {
                    "raw_evidence": [
                        "Information on the font type and font style (e.g., italics, bold print) of a token and its position on the physical page (for PDFs only) was specified as attributes to the token elements of the tokens layer (cf. Figure FIGREF34 for an example)",
                        "A separate fonts layer was introduced to preserve detailed information on the font configurations referenced in the tokens layer",
                        "For the webpages, a static dump of all documents was created. Following this, the documents were manually checked to verify the language. The main content was subsequently extracted, i.e., HTML markup and boilerplate removed using the Beautiful Soup library for Python. Information on text structure (e.g., paragraphs, lines) and typography (e.g., boldface, italics) was retained. Similarly, image information (content, position, and dimensions of an image) was preserved."
                    ],
                    "highlighted_evidence": [
                        "Information on the font type and font style (e.g., italics, bold print) of a token and its position on the physical page (for PDFs only) was specified as attributes to the token elements of the tokens layer (cf. Figure FIGREF34 for an example)",
                        "A separate fonts layer was introduced to preserve detailed information on the font configurations referenced in the tokens layer",
                        "Information on text structure (e.g., paragraphs, lines) and typography (e.g., boldface, italics) was retained."
                    ]
                }
            ]
        }
    ],
    "1908.01294": [
        {
            "question": "Which deep learning architecture do they use for sentence segmentation?",
            "answers": [
                {
                    "answer": "Bi-LSTM-CRF",
                    "type": "extractive"
                },
                {
                    "answer": "Bi-LSTM-CRF",
                    "type": "extractive"
                }
            ],
            "q_uid": "f697d00a82750b14376fe20a5a2b249e98bebe9b",
            "evidence": [
                {
                    "raw_evidence": [
                        "The CRF module achieved the best result on the Thai sentence segmentation task BIBREF8 ; therefore, we adopt the Bi-LSTM-CRF model as our baseline. This paper makes the following three contributions to improve Bi-LSTM-CRF for sentence segmentation."
                    ],
                    "highlighted_evidence": [
                        "The CRF module achieved the best result on the Thai sentence segmentation task BIBREF8 ; therefore, we adopt the Bi-LSTM-CRF model as our baseline. "
                    ]
                },
                {
                    "raw_evidence": [
                        "Several deep learning approaches have been applied in various tasks of natural language processing (NLP), including the long short-term memory BIBREF10 , self-attention BIBREF11 , and other models. Huang Z. et al. BIBREF12 proposed a deep learning sequence tagging model called Bi-LSTM-CRF, which integrates a conditional random field (CRF) module to gain the benefit of both deep learning and traditional machine learning approaches. In their experiments, the Bi-LSTM-CRF model achieved an improved level of accuracy in many NLP sequence tagging tasks, such as named entity recognition, POS tagging and chunking.",
                        "The CRF module achieved the best result on the Thai sentence segmentation task BIBREF8 ; therefore, we adopt the Bi-LSTM-CRF model as our baseline. This paper makes the following three contributions to improve Bi-LSTM-CRF for sentence segmentation."
                    ],
                    "highlighted_evidence": [
                        "Several deep learning approaches have been applied in various tasks of natural language processing (NLP), including the long short-term memory BIBREF10 , self-attention BIBREF11 , and other models. Huang Z. et al. BIBREF12 proposed a deep learning sequence tagging model called Bi-LSTM-CRF, which integrates a conditional random field (CRF) module to gain the benefit of both deep learning and traditional machine learning approaches. In their experiments, the Bi-LSTM-CRF model achieved an improved level of accuracy in many NLP sequence tagging tasks, such as named entity recognition, POS tagging and chunking.\n\nThe CRF module achieved the best result on the Thai sentence segmentation task BIBREF8 ; therefore, we adopt the Bi-LSTM-CRF model as our baseline. This paper makes the following three contributions to improve Bi-LSTM-CRF for sentence segmentation."
                    ]
                }
            ]
        }
    ],
    "1709.05036": [
        {
            "question": "Do they experiment with their proposed model on any other dataset other than MovieQA?",
            "answers": [
                {
                    "answer": "Yes",
                    "type": "boolean"
                },
                {
                    "answer": "Yes",
                    "type": "boolean"
                }
            ],
            "q_uid": "50cb50657572e315fd452a89f3e0be465094b66f",
            "evidence": [
                {
                    "raw_evidence": [
                        "We also applied our model to MCTest dataset which requires machines to answer multiple-choice reading comprehension questions about fictional stories. The original paper describes that a baseline method uses a combination of a sliding window score and a distance based . They achieve 66.7% and 56.7% on MC500 and MC160 separately. Because of the restricted training set and development set, we trained our model on MovieQA training set and applied the result to test MCTest dataset. On MCTest dataset, we still outperform baseline and achieve 68.1% accuracy on MC160 and 61.5% accuracy on MC500."
                    ],
                    "highlighted_evidence": [
                        "We also applied our model to MCTest dataset which requires machines to answer multiple-choice reading comprehension questions about fictional stories. "
                    ]
                },
                {
                    "raw_evidence": [
                        "We also applied our model to MCTest dataset which requires machines to answer multiple-choice reading comprehension questions about fictional stories. The original paper describes that a baseline method uses a combination of a sliding window score and a distance based . They achieve 66.7% and 56.7% on MC500 and MC160 separately. Because of the restricted training set and development set, we trained our model on MovieQA training set and applied the result to test MCTest dataset. On MCTest dataset, we still outperform baseline and achieve 68.1% accuracy on MC160 and 61.5% accuracy on MC500."
                    ],
                    "highlighted_evidence": [
                        "We also applied our model to MCTest dataset which requires machines to answer multiple-choice reading comprehension questions about fictional stories."
                    ]
                }
            ]
        }
    ],
    "1809.00530": [
        {
            "question": "What is the architecture of the model?",
            "answers": [
                {
                    "answer": "one-layer CNN structure from previous works BIBREF22 , BIBREF4",
                    "type": "extractive"
                },
                {
                    "answer": " one-layer CNN",
                    "type": "extractive"
                }
            ],
            "q_uid": "b46c0015a122ee5fb95c2a45691cb97f80de1bb6",
            "evidence": [
                {
                    "raw_evidence": [
                        "For the proposed model, we denote INLINEFORM0 parameterized by INLINEFORM1 as a neural-based feature encoder that maps documents from both domains to a shared feature space, and INLINEFORM2 parameterized by INLINEFORM3 as a fully connected layer with softmax activation serving as the sentiment classifier. We aim to learn feature representations that are domain-invariant and at the same time discriminative on both domains, thus we simultaneously consider three factors in our objective: (1) minimize the classification error on the labeled source examples; (2) minimize the domain discrepancy; and (3) leverage unlabeled data via semi-supervised learning.",
                        "We have left the feature encoder INLINEFORM0 unspecified, for which, a few options can be considered. In our implementation, we adopt a one-layer CNN structure from previous works BIBREF22 , BIBREF4 , as it has been demonstrated to work well for sentiment classification tasks. Given a review document INLINEFORM1 consisting of INLINEFORM2 words, we begin by associating each word with a continuous word embedding BIBREF23 INLINEFORM3 from an embedding matrix INLINEFORM4 , where INLINEFORM5 is the vocabulary size and INLINEFORM6 is the embedding dimension. INLINEFORM7 is jointly updated with other network parameters during training. Given a window of dense word embeddings INLINEFORM8 , the convolution layer first concatenates these vectors to form a vector INLINEFORM9 of length INLINEFORM10 and then the output vector is computed by Equation ( EQREF11 ): DISPLAYFORM0"
                    ],
                    "highlighted_evidence": [
                        "For the proposed model, we denote INLINEFORM0 parameterized by INLINEFORM1 as a neural-based feature encoder that maps documents from both domains to a shared feature space, and INLINEFORM2 parameterized by INLINEFORM3 as a fully connected layer with softmax activation serving as the sentiment classifier.",
                        "We have left the feature encoder INLINEFORM0 unspecified, for which, a few options can be considered. In our implementation, we adopt a one-layer CNN structure from previous works BIBREF22 , BIBREF4 , as it has been demonstrated to work well for sentiment classification tasks."
                    ]
                },
                {
                    "raw_evidence": [
                        "We have left the feature encoder INLINEFORM0 unspecified, for which, a few options can be considered. In our implementation, we adopt a one-layer CNN structure from previous works BIBREF22 , BIBREF4 , as it has been demonstrated to work well for sentiment classification tasks. Given a review document INLINEFORM1 consisting of INLINEFORM2 words, we begin by associating each word with a continuous word embedding BIBREF23 INLINEFORM3 from an embedding matrix INLINEFORM4 , where INLINEFORM5 is the vocabulary size and INLINEFORM6 is the embedding dimension. INLINEFORM7 is jointly updated with other network parameters during training. Given a window of dense word embeddings INLINEFORM8 , the convolution layer first concatenates these vectors to form a vector INLINEFORM9 of length INLINEFORM10 and then the output vector is computed by Equation ( EQREF11 ): DISPLAYFORM0"
                    ],
                    "highlighted_evidence": [
                        "We have left the feature encoder INLINEFORM0 unspecified, for which, a few options can be considered. In our implementation, we adopt a one-layer CNN structure from previous works BIBREF22 , BIBREF4 , as it has been demonstrated to work well for sentiment classification tasks."
                    ]
                }
            ]
        },
        {
            "question": "What are the baseline methods?",
            "answers": [
                {
                    "answer": "(1) Naive, (2) mSDA BIBREF7, (3) NaiveNN, (4) AuxNN BIBREF4, (5) ADAN BIBREF16, (6) MMD",
                    "type": "extractive"
                },
                {
                    "answer": "non-domain-adaptive baseline with bag-of-words representations and SVM classifier, mSDA, non-domain-adaptive CNN trained on source domain, neural model that exploits auxiliary tasks, adversarial training to reduce representation difference between domains, variants of deep CNNs are used for encoding images and the MMDs of multiple layers are jointly minimized",
                    "type": "extractive"
                }
            ],
            "q_uid": "5b7a4994bfdbf8882f391adf1cd2218dbc2255a0",
            "evidence": [
                {
                    "raw_evidence": [
                        "We compare with the following baselines:",
                        "(1) Naive: A non-domain-adaptive baseline with bag-of-words representations and SVM classifier trained on the source domain.",
                        "(2) mSDA BIBREF7 : This is the state-of-the-art method based on discrete input features. Top 1000 bag-of-words features are kept as pivot features. We set the number of stacked layers to 3 and the corruption probability to 0.5.",
                        "(3) NaiveNN: This is a non-domain-adaptive CNN trained on source domain, which is a variant of our model by setting INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 to zeros.",
                        "(4) AuxNN BIBREF4 : This is a neural model that exploits auxiliary tasks, which has achieved state-of-the-art results on cross-domain sentiment classification. The sentence encoder used in this model is the same as ours.",
                        "(5) ADAN BIBREF16 : This method exploits adversarial training to reduce representation difference between domains. The original paper uses a simple feedforward network as encoder. For fair comparison, we replace it with our CNN-based encoder. We train 5 iterations on the discriminator per iteration on the encoder and sentiment classifier as suggested in their paper.",
                        "(6) MMD: MMD has been widely used for minimizing domain discrepancy on images. In those works BIBREF9 , BIBREF13 , variants of deep CNNs are used for encoding images and the MMDs of multiple layers are jointly minimized. In NLP, adding more layers of CNNs may not be very helpful and thus those models from image-related tasks can not be directly applied to our problem. To compare with MMD-based method, we train a model that jointly minimize the classification loss INLINEFORM0 on the source domain and MMD between INLINEFORM1 and INLINEFORM2 . For computing MMD, we use a Gaussian RBF which is a common choice for characteristic kernel."
                    ],
                    "highlighted_evidence": [
                        "We compare with the following baselines:\n\n(1) Naive: A non-domain-adaptive baseline with bag-of-words representations and SVM classifier trained on the source domain.\n\n(2) mSDA BIBREF7 : This is the state-of-the-art method based on discrete input features. Top 1000 bag-of-words features are kept as pivot features. We set the number of stacked layers to 3 and the corruption probability to 0.5.\n\n(3) NaiveNN: This is a non-domain-adaptive CNN trained on source domain, which is a variant of our model by setting INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 to zeros.\n\n(4) AuxNN BIBREF4 : This is a neural model that exploits auxiliary tasks, which has achieved state-of-the-art results on cross-domain sentiment classification. The sentence encoder used in this model is the same as ours.\n\n(5) ADAN BIBREF16 : This method exploits adversarial training to reduce representation difference between domains. The original paper uses a simple feedforward network as encoder. For fair comparison, we replace it with our CNN-based encoder. We train 5 iterations on the discriminator per iteration on the encoder and sentiment classifier as suggested in their paper.\n\n(6) MMD: MMD has been widely used for minimizing domain discrepancy on images. In those works BIBREF9 , BIBREF13 , variants of deep CNNs are used for encoding images and the MMDs of multiple layers are jointly minimized. In NLP, adding more layers of CNNs may not be very helpful and thus those models from image-related tasks can not be directly applied to our problem. To compare with MMD-based method, we train a model that jointly minimize the classification loss INLINEFORM0 on the source domain and MMD between INLINEFORM1 and INLINEFORM2 . For computing MMD, we use a Gaussian RBF which is a common choice for characteristic kernel."
                    ]
                },
                {
                    "raw_evidence": [
                        "We compare with the following baselines:",
                        "(1) Naive: A non-domain-adaptive baseline with bag-of-words representations and SVM classifier trained on the source domain.",
                        "(2) mSDA BIBREF7 : This is the state-of-the-art method based on discrete input features. Top 1000 bag-of-words features are kept as pivot features. We set the number of stacked layers to 3 and the corruption probability to 0.5.",
                        "(3) NaiveNN: This is a non-domain-adaptive CNN trained on source domain, which is a variant of our model by setting INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 to zeros.",
                        "(4) AuxNN BIBREF4 : This is a neural model that exploits auxiliary tasks, which has achieved state-of-the-art results on cross-domain sentiment classification. The sentence encoder used in this model is the same as ours.",
                        "(5) ADAN BIBREF16 : This method exploits adversarial training to reduce representation difference between domains. The original paper uses a simple feedforward network as encoder. For fair comparison, we replace it with our CNN-based encoder. We train 5 iterations on the discriminator per iteration on the encoder and sentiment classifier as suggested in their paper.",
                        "(6) MMD: MMD has been widely used for minimizing domain discrepancy on images. In those works BIBREF9 , BIBREF13 , variants of deep CNNs are used for encoding images and the MMDs of multiple layers are jointly minimized. In NLP, adding more layers of CNNs may not be very helpful and thus those models from image-related tasks can not be directly applied to our problem. To compare with MMD-based method, we train a model that jointly minimize the classification loss INLINEFORM0 on the source domain and MMD between INLINEFORM1 and INLINEFORM2 . For computing MMD, we use a Gaussian RBF which is a common choice for characteristic kernel."
                    ],
                    "highlighted_evidence": [
                        "We compare with the following baselines:\n\n(1) Naive: A non-domain-adaptive baseline with bag-of-words representations and SVM classifier trained on the source domain.\n\n(2) mSDA BIBREF7 : This is the state-of-the-art method based on discrete input features. Top 1000 bag-of-words features are kept as pivot features. We set the number of stacked layers to 3 and the corruption probability to 0.5.\n\n(3) NaiveNN: This is a non-domain-adaptive CNN trained on source domain, which is a variant of our model by setting INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 to zeros.\n\n(4) AuxNN BIBREF4 : This is a neural model that exploits auxiliary tasks, which has achieved state-of-the-art results on cross-domain sentiment classification. The sentence encoder used in this model is the same as ours.\n\n(5) ADAN BIBREF16 : This method exploits adversarial training to reduce representation difference between domains. The original paper uses a simple feedforward network as encoder. For fair comparison, we replace it with our CNN-based encoder. We train 5 iterations on the discriminator per iteration on the encoder and sentiment classifier as suggested in their paper.\n\n(6) MMD: MMD has been widely used for minimizing domain discrepancy on images. In those works BIBREF9 , BIBREF13 , variants of deep CNNs are used for encoding images and the MMDs of multiple layers are jointly minimized. "
                    ]
                }
            ]
        }
    ],
    "1908.07218": [
        {
            "question": "What types of word representations are they evaluating?",
            "answers": [
                {
                    "answer": "GloVE; SGNS",
                    "type": "abstractive"
                }
            ],
            "q_uid": "3b995a7358cefb271b986e8fc6efe807f25d60dc",
            "evidence": [
                {
                    "raw_evidence": [
                        "We trained word embeddings using either GloVe BIBREF11 or SGNS BIBREF12 on a small or a large corpus. The small corpus consists of the traditional Chinese part of Chinese Gigaword BIBREF13 and ASBC 4.0 BIBREF9 . The large corpus additionally includes the Chinese part of Wikipedia."
                    ],
                    "highlighted_evidence": [
                        "We trained word embeddings using either GloVe BIBREF11 or SGNS BIBREF12 on a small or a large corpus."
                    ]
                }
            ]
        }
    ],
    "1808.03738": [
        {
            "question": "what NMT models did they compare with?",
            "answers": [
                {
                    "answer": "RNN-based NMT model, Transformer-NMT",
                    "type": "extractive"
                }
            ],
            "q_uid": "27dbbd63c86d6ca82f251d4f2f030ed3e88f58fa",
            "evidence": [
                {
                    "raw_evidence": [
                        "RNN-based NMT model",
                        "We first briefly introduce the RNN based Neural Machine Translation (RNN-based NMT) model. The RNN-based NMT with attention mechanism BIBREF0 has achieved remarkable performance on many translation tasks. It consists of encoder and decoder part.",
                        "Transformer-NMT",
                        "Recently, the Transformer model BIBREF4 has made remarkable progress in machine translation. This model contains a multi-head self-attention encoder and a multi-head self-attention decoder."
                    ],
                    "highlighted_evidence": [
                        "RNN-based NMT model\nWe first briefly introduce the RNN based Neural Machine Translation (RNN-based NMT) model.",
                        "Transformer-NMT\nRecently, the Transformer model BIBREF4 has made remarkable progress in machine translation. This model contains a multi-head self-attention encoder and a multi-head self-attention decoder."
                    ]
                }
            ]
        },
        {
            "question": "Where does the ancient Chinese dataset come from?",
            "answers": [
                {
                    "answer": "ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era",
                    "type": "extractive"
                },
                {
                    "answer": "Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet ",
                    "type": "abstractive"
                }
            ],
            "q_uid": "b9d07757e2d2c4be41823dd1ea3b9c7f115b5f72",
            "evidence": [
                {
                    "raw_evidence": [
                        "Data Collection. To build the large ancient-modern Chinese dataset, we collected 1.7K bilingual ancient-modern Chinese articles from the internet. More specifically, a large part of the ancient Chinese data we used come from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era. They used plain and accurate words to express what happened at that time, and thus ensure the generality of the translated materials."
                    ],
                    "highlighted_evidence": [
                        "To build the large ancient-modern Chinese dataset, we collected 1.7K bilingual ancient-modern Chinese articles from the internet. More specifically, a large part of the ancient Chinese data we used come from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era."
                    ]
                },
                {
                    "raw_evidence": [
                        "Data Collection. To build the large ancient-modern Chinese dataset, we collected 1.7K bilingual ancient-modern Chinese articles from the internet. More specifically, a large part of the ancient Chinese data we used come from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era. They used plain and accurate words to express what happened at that time, and thus ensure the generality of the translated materials."
                    ],
                    "highlighted_evidence": [
                        "Data Collection. To build the large ancient-modern Chinese dataset, we collected 1.7K bilingual ancient-modern Chinese articles from the internet. More specifically, a large part of the ancient Chinese data we used come from ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era. "
                    ]
                }
            ]
        }
    ],
    "1804.02233": [
        {
            "question": "How many tweets were manually labelled? ",
            "answers": [
                {
                    "answer": "44,000 tweets",
                    "type": "extractive"
                }
            ],
            "q_uid": "dc28ac845602904c2522f5349374153f378c42d3",
            "evidence": [
                {
                    "raw_evidence": [
                        "Tweets related to Forex, specifically to EUR and USD, were acquired through the Twitter search API with the following query: \u201cEURUSD\u201d, \u201cUSDEUR\u201d, \u201cEUR\u201d, or \u201cUSD\u201d. In the period of three years (January 2014 to December 2016) almost 15 million tweets were collected. A subset of them (44,000 tweets) was manually labeled by knowledgeable students of finance. The label captures the leaning or stance of the Twitter user with respect to the anticipated move of one currency w.r.t. the other. The stance is represented by three values: buy (EUR vs. USD), hold, or sell. The tweets were collected, labeled and provided to us by the Sowa Labs company (http://www.sowalabs.com)."
                    ],
                    "highlighted_evidence": [
                        "Tweets related to Forex, specifically to EUR and USD, were acquired through the Twitter search API with the following query: \u201cEURUSD\u201d, \u201cUSDEUR\u201d, \u201cEUR\u201d, or \u201cUSD\u201d. In the period of three years (January 2014 to December 2016) almost 15 million tweets were collected. A subset of them (44,000 tweets) was manually labeled by knowledgeable students of finance. "
                    ]
                }
            ]
        }
    ],
    "1906.10551": [
        {
            "question": "Do they report results only on English data?",
            "answers": [
                {
                    "answer": "Yes",
                    "type": "boolean"
                }
            ],
            "q_uid": "61b0db2b5718d409b07f83f912bad6a788bfee5a",
            "evidence": [
                {
                    "raw_evidence": [
                        "A serious challenge in the field of AV is the lack of publicly available (and suitable) corpora, which are required to train and evaluate AV methods. Among the few publicly available corpora are those that were released by the organizers of the well-known PAN-AV competitions BIBREF11 , BIBREF5 , BIBREF12 . In regard to our experiments, however, we cannot use these corpora, due to the absence of relevant meta-data such as the precise time spans where the documents have been written as well as the topic category of the texts. Therefore, we decided to compile our own corpora based on English documents, which we crawled from different publicly accessible sources. In what follows, we describe our three constructed corpora, which are listed together with their statistics in Table TABREF23 . Note that all corpora are balanced such that verification cases with matching (Y) and non-matching (N) authorships are evenly distributed."
                    ],
                    "highlighted_evidence": [
                        "Therefore, we decided to compile our own corpora based on English documents, which we crawled from different publicly accessible sources."
                    ]
                }
            ]
        },
        {
            "question": "Which is the best performing method?",
            "answers": [
                {
                    "answer": "Caravel, COAV and NNCD",
                    "type": "extractive"
                }
            ],
            "q_uid": "b217d9730ba469f48426280945dbb77542b39183",
            "evidence": [
                {
                    "raw_evidence": [
                        "The top performing approaches Caravel, COAV and NNCD deserve closer attention. All three are based on character-level language models that capture low-level features similar to character INLINEFORM0 -grams, which have been shown in numerous AA and AV studies (for instance, BIBREF39 , BIBREF26 ) to be highly effective and robust. In BIBREF19 , BIBREF28 , it has been shown that Caravel and COAV were also the two top-performing approaches, where in BIBREF19 they were evaluated on the PAN-2015 AV corpus BIBREF12 , while in BIBREF28 they were applied on texts obtained from Project Gutenberg. Although both approaches perform similarly, they differ in the way how the decision criterion INLINEFORM1 is determined. While COAV requires a training corpus to learn INLINEFORM2 , Caravel assumes that the given test corpus (which provides the impostors) is balanced. Given this assumption, Caravel first computes similarity scores for all verification problems in the corpus and then sets INLINEFORM3 to the median of all similarities (cf. Figure FIGREF49 ). Thus, from a machine learning perspective, there is some undue training on the test set. Moreover, the applicability of Caravel in realistic scenarios is questionable, as a forensic case is not part of a corpus where the Y/N-distribution is known beforehand."
                    ],
                    "highlighted_evidence": [
                        "The top performing approaches Caravel, COAV and NNCD deserve closer attention."
                    ]
                }
            ]
        },
        {
            "question": "What size are the corpora?",
            "answers": [
                {
                    "answer": "80 excerpts from scientific works, collection of 1,645 chat conversations, collection of 200 aggregated postings",
                    "type": "extractive"
                }
            ],
            "q_uid": "8c0846879771c8f3915cc2e0718bee448f5cb007",
            "evidence": [
                {
                    "raw_evidence": [
                        "As a first corpus, we compiled INLINEFORM0 that represents a collection of 80 excerpts from scientific works including papers, dissertations, book chapters and technical reports, which we have chosen from the well-known Digital Bibliography & Library Project (DBLP) platform. Overall, the documents were written by 40 researchers, where for each author INLINEFORM1 , there are exactly two documents. Given the 80 documents, we constructed for each author INLINEFORM2 two verification problems INLINEFORM3 (a Y-case) and INLINEFORM4 (an N-case). For INLINEFORM5 we set INLINEFORM6 's first document as INLINEFORM7 and the second document as INLINEFORM8 . For INLINEFORM9 we reuse INLINEFORM10 from INLINEFORM11 as the known document and selected a text from another (random) author as the unknown document. The result of this procedure is a set of 80 verification problems, which we split into a training and test set based on a 40/60% ratio. Where possible, we tried to restrict the content of each text to the abstract and conclusion of the original work. However, since in many cases these sections were too short, we also considered other parts of the original works such as introduction or discussion sections. To ensure that the extracted text portions are appropriate for the AV task, each original work was preprocessed manually. More precisely, we removed tables, formulas, citations, quotes and sentences that include non-language content such as mathematical constructs or specific names of researchers, systems or algorithms. The average time span between both documents of an author is 15.6 years. The minimum and maximum time span are 6 and 40 years, respectively. Besides the temporal aspect of INLINEFORM12 , another challenge of this corpus is the formal (scientific) language, where the usage of stylistic devices is more restricted, in contrast to other genres such as novels or poems.",
                        "As a second corpus, we compiled INLINEFORM0 , which represents a collection of 1,645 chat conversations of 550 sex offenders crawled from the Perverted-Justice portal. The chat conversations stem from a variety of sources including emails and instant messengers (e. g., MSN, AOL or Yahoo), where for each conversation, we ensured that only chat lines from the offender were extracted. We applied the same problem construction procedure as for the corpus INLINEFORM1 , which resulted in 1,100 verification problems that again were split into a training and test set given a 40/60% ratio. In contrast to the corpus INLINEFORM2 , we only performed slight preprocessing. Essentially, we removed user names, time-stamps, URLs, multiple blanks as well as annotations that were not part of the original conversations from all chat lines. Moreover, we did not normalize words (for example, shorten words such as \u201cnooooo\u201d to \u201cno\u201d) as we believe that these represent important style markers. Furthermore, we did not remove newlines between the chat lines, as the positions of specific words might play an important role regarding the individual's writing style.",
                        "As a third corpus, we compiled INLINEFORM0 , which is a collection of 200 aggregated postings crawled from the Reddit platform. Overall, the postings were written by 100 Reddit users and stem from a variety of subreddits. In order to construct the Y-cases, we selected exactly two postings from disjoint subreddits for each user such that both the known and unknown document INLINEFORM1 and INLINEFORM2 differ in their topic. Regarding the N-cases, we applied the opposite strategy such that INLINEFORM3 and INLINEFORM4 belong to the same topic. The rationale behind this is to figure out to which extent AV methods can be fooled in cases, where the topic matches but not the authorship and vice versa. Since for this specific corpus we have to control the topics of the documents, we did not perform the same procedure applied for INLINEFORM5 and INLINEFORM6 to construct the training and test sets. Instead, we used for the resulting 100 verification problems a 40/60% hold-out split, where both training and test set are entirely disjoint."
                    ],
                    "highlighted_evidence": [
                        "As a first corpus, we compiled INLINEFORM0 that represents a collection of 80 excerpts from scientific works including papers, dissertations, book chapters and technical reports, which we have chosen from the well-known Digital Bibliography & Library Project (DBLP) platform.",
                        "As a second corpus, we compiled INLINEFORM0 , which represents a collection of 1,645 chat conversations of 550 sex offenders crawled from the Perverted-Justice portal.",
                        "As a third corpus, we compiled INLINEFORM0 , which is a collection of 200 aggregated postings crawled from the Reddit platform."
                    ]
                }
            ]
        },
        {
            "question": "What is a self-compiled corpus?",
            "answers": [
                {
                    "answer": " restrict the content of each text to the abstract and conclusion of the original work, considered other parts of the original works such as introduction or discussion sections, extracted text portions are appropriate for the AV task, each original work was preprocessed manually, removed tables, formulas, citations, quotes and sentences that include non-language content such as mathematical constructs or specific names of researchers, systems or algorithms",
                    "type": "extractive"
                }
            ],
            "q_uid": "3fae289ab1fc023bce2fa4f1ce4d9f828074f232",
            "evidence": [
                {
                    "raw_evidence": [
                        "As a first corpus, we compiled INLINEFORM0 that represents a collection of 80 excerpts from scientific works including papers, dissertations, book chapters and technical reports, which we have chosen from the well-known Digital Bibliography & Library Project (DBLP) platform. Overall, the documents were written by 40 researchers, where for each author INLINEFORM1 , there are exactly two documents. Given the 80 documents, we constructed for each author INLINEFORM2 two verification problems INLINEFORM3 (a Y-case) and INLINEFORM4 (an N-case). For INLINEFORM5 we set INLINEFORM6 's first document as INLINEFORM7 and the second document as INLINEFORM8 . For INLINEFORM9 we reuse INLINEFORM10 from INLINEFORM11 as the known document and selected a text from another (random) author as the unknown document. The result of this procedure is a set of 80 verification problems, which we split into a training and test set based on a 40/60% ratio. Where possible, we tried to restrict the content of each text to the abstract and conclusion of the original work. However, since in many cases these sections were too short, we also considered other parts of the original works such as introduction or discussion sections. To ensure that the extracted text portions are appropriate for the AV task, each original work was preprocessed manually. More precisely, we removed tables, formulas, citations, quotes and sentences that include non-language content such as mathematical constructs or specific names of researchers, systems or algorithms. The average time span between both documents of an author is 15.6 years. The minimum and maximum time span are 6 and 40 years, respectively. Besides the temporal aspect of INLINEFORM12 , another challenge of this corpus is the formal (scientific) language, where the usage of stylistic devices is more restricted, in contrast to other genres such as novels or poems."
                    ],
                    "highlighted_evidence": [
                        "Where possible, we tried to restrict the content of each text to the abstract and conclusion of the original work. However, since in many cases these sections were too short, we also considered other parts of the original works such as introduction or discussion sections. To ensure that the extracted text portions are appropriate for the AV task, each original work was preprocessed manually. More precisely, we removed tables, formulas, citations, quotes and sentences that include non-language content such as mathematical constructs or specific names of researchers, systems or algorithms."
                    ]
                }
            ]
        }
    ],
    "1912.07976": [
        {
            "question": "How much better is performance of the proposed model compared to the state of the art in these various experiments?",
            "answers": [
                {
                    "answer": "significantly improves the accuracy and F1 score of aspect polarity classification",
                    "type": "extractive"
                }
            ],
            "q_uid": "10ddac87daf153cf674589cc1c64a795907d5d9a",
            "evidence": [
                {
                    "raw_evidence": [
                        "We build a joint model for the multi-task of ATE and APC based on the BERT-BASE model. After optimizing the model parameters according to the empirical result, the joint model based on BERT-BASE achieved hopeful performance on all three datasets and even surpassed other proposed BERT based improved models on some datasets, such as BERT-PT, AEN-BERT, SDGCN-BERT, and so on. Meanwhile, we implement the joint-task model based on BERT-SPC. Compared with the BERT-BASE model, BERT-SPC significantly improves the accuracy and F1 score of aspect polarity classification. In addition, for the first time, BERT-SPC has increased the F1 score of ATE subtask on three datasets up to 99%."
                    ],
                    "highlighted_evidence": [
                        "Compared with the BERT-BASE model, BERT-SPC significantly improves the accuracy and F1 score of aspect polarity classification."
                    ]
                }
            ]
        },
        {
            "question": "What was state of the art on SemEval-2014 task4 Restaurant and Laptop dataset?",
            "answers": [
                {
                    "answer": "BERT-ADA, BERT-PT, AEN-BERT, SDGCN-BERT",
                    "type": "extractive"
                }
            ],
            "q_uid": "6cd874c4ae8e70f3c98c7176191c13a7decfbc45",
            "evidence": [
                {
                    "raw_evidence": [
                        "BERT-ADA BIBREF33 is a domain-adapted BERT-based model proposed for the APC task, which fine-tuned the BERT-BASE model on task-related corpus. This model obtained state-of-the-art accuracy on the Laptops dataset.",
                        "We build a joint model for the multi-task of ATE and APC based on the BERT-BASE model. After optimizing the model parameters according to the empirical result, the joint model based on BERT-BASE achieved hopeful performance on all three datasets and even surpassed other proposed BERT based improved models on some datasets, such as BERT-PT, AEN-BERT, SDGCN-BERT, and so on. Meanwhile, we implement the joint-task model based on BERT-SPC. Compared with the BERT-BASE model, BERT-SPC significantly improves the accuracy and F1 score of aspect polarity classification. In addition, for the first time, BERT-SPC has increased the F1 score of ATE subtask on three datasets up to 99%."
                    ],
                    "highlighted_evidence": [
                        "BERT-ADA BIBREF33 is a domain-adapted BERT-based model proposed for the APC task, which fine-tuned the BERT-BASE model on task-related corpus. This model obtained state-of-the-art accuracy on the Laptops dataset.",
                        "We build a joint model for the multi-task of ATE and APC based on the BERT-BASE model. After optimizing the model parameters according to the empirical result, the joint model based on BERT-BASE achieved hopeful performance on all three datasets and even surpassed other proposed BERT based improved models on some datasets, such as BERT-PT, AEN-BERT, SDGCN-BERT, and so on."
                    ]
                }
            ]
        },
        {
            "question": "What was previous state-of-the-art on four Chinese reviews datasets?",
            "answers": [
                {
                    "answer": "GANN obtained the state-of-the-art APC performance on the Chinese review datasets",
                    "type": "extractive"
                }
            ],
            "q_uid": "b807dd3d42251615b881632caa5e331e2203d269",
            "evidence": [
                {
                    "raw_evidence": [
                        "GANN is novel neural network model for APC task aimed to solve the shortcomings of traditional RNNs and CNNs. The GANN applied the Gate Truncation RNN (GTR) to learn informative aspect-dependent sentiment clue representations. GANN obtained the state-of-the-art APC performance on the Chinese review datasets."
                    ],
                    "highlighted_evidence": [
                        "GANN is novel neural network model for APC task aimed to solve the shortcomings of traditional RNNs and CNNs. The GANN applied the Gate Truncation RNN (GTR) to learn informative aspect-dependent sentiment clue representations. GANN obtained the state-of-the-art APC performance on the Chinese review datasets."
                    ]
                }
            ]
        },
        {
            "question": "In what four Chinese review datasets does LCF-ATEPC achieves state of the art?",
            "answers": [
                {
                    "answer": "Car, Phone, Notebook, Camera",
                    "type": "extractive"
                }
            ],
            "q_uid": "d39c911bf2479fdb7af339b59acb32073242fab3",
            "evidence": [
                {
                    "raw_evidence": [
                        "To comprehensive evaluate the performance of the proposed model, the experiments were conducted in three most commonly used ABSA datasets, the Laptops and Restaurant datasets of SemEval-2014 Task4 subtask2 BIBREF0 and an ACL Twitter social dataset BIBREF34. To evaluate our model capability with processing the Chinese language, we also tested the performance of LCF-ATEPC on four Chinese comment datasets BIBREF35, BIBREF36, BIBREF29 (Car, Phone, Notebook, Camera). We preprocessed the seven datasets. We reformatted the origin dataset and annotated each sample with the IOB labels for ATE task and polarity labels for APC tasks, respectively. The polarity of each aspect on the Laptops, Restaurants and datasets may be positive, neutral, and negative, and the conflicting labels of polarity are not considered. The reviews in the four Chinese datasets have been purged, with each aspect may be positive or negative binary polarity. To verify the effectiveness and performance of LCF-ATEPC models on multilingual datasets, we built a multilingual dataset by mixing the 7 datasets. We adopt this dataset to conduct multilingual-oriented ATE and APC experiments."
                    ],
                    "highlighted_evidence": [
                        "To evaluate our model capability with processing the Chinese language, we also tested the performance of LCF-ATEPC on four Chinese comment datasets BIBREF35, BIBREF36, BIBREF29 (Car, Phone, Notebook, Camera)."
                    ]
                }
            ]
        }
    ],
    "1904.09708": [
        {
            "question": "How does the SCAN dataset evaluate compositional generalization?",
            "answers": [
                {
                    "answer": "it systematically holds out inputs in the training set containing basic primitive verb, \"jump\", and tests on sequences containing that verb.",
                    "type": "abstractive"
                }
            ],
            "q_uid": "7182f6ed12fa990835317c57ad1ff486282594ee",
            "evidence": [
                {
                    "raw_evidence": [
                        "A recently published dataset called SCAN BIBREF2 (Simplified version of the CommAI Navigation tasks), tests compositional generalization in a sequence-to-sequence (seq2seq) setting by systematically holding out of the training set all inputs containing a basic primitive verb (\"jump\"), and testing on sequences containing that verb. Success on this difficult problem requires models to generalize knowledge gained about the other primitive verbs (\"walk\", \"run\" and \"look\") to the novel verb \"jump,\" without having seen \"jump\" in any but the most basic context (\"jump\" $\\rightarrow $ JUMP). It is trivial for human learners to generalize in this way (e.g. if I tell you that \"dax\" is a verb, you can generalize its usage to all kinds of constructions, like \"dax twice and then dax again\", without even knowing what the word means) BIBREF2 . However, standard recurrent seq2seq models fail miserably on this task, with the best-reported model (a gated recurrent unit augmented with an attention mechanism) achieving only 12.5% accuracy on the test set BIBREF2 , BIBREF4 . Recently, convolutional neural networks (CNN) were shown to perform better on this test, but still only achieved 69.2% accuracy on the test set."
                    ],
                    "highlighted_evidence": [
                        "A recently published dataset called SCAN BIBREF2 (Simplified version of the CommAI Navigation tasks), tests compositional generalization in a sequence-to-sequence (seq2seq) setting by systematically holding out of the training set all inputs containing a basic primitive verb (\"jump\"), and testing on sequences containing that verb."
                    ]
                }
            ]
        }
    ],
    "1909.10012": [
        {
            "question": "What profile metadata is used for this analysis?",
            "answers": [
                {
                    "answer": "username, display name, profile image, location, description",
                    "type": "extractive"
                },
                {
                    "answer": "username, display name, profile image, location and description",
                    "type": "extractive"
                }
            ],
            "q_uid": "e374169ee10f835f660ab8403a5701114586f167",
            "evidence": [
                {
                    "raw_evidence": [
                        "We consider 5 major profile attributes for further analysis. These attributes are username, display name, profile image, location and description respectively. The username is a unique handle or screen name associated with each twitter user that appears on the profile URL and is used to communicate with each other on Twitter. Some username examples are $@narendramodi$, $@RahulGandhi$ etc. The display name on the other hand, is merely a personal identifier that is displayed on the profile page of a user, e.g. `Narendra Modi', `Rahul Gandhi', etc. The description is a string to describe the account, while the location is user-defined profile location. We considered the 5 profile attributes as stated above since these attributes are key elements of a users identity and saliently define users likes and values BIBREF2."
                    ],
                    "highlighted_evidence": [
                        "We consider 5 major profile attributes for further analysis. These attributes are username, display name, profile image, location and description respectively."
                    ]
                },
                {
                    "raw_evidence": [
                        "We consider 5 major profile attributes for further analysis. These attributes are username, display name, profile image, location and description respectively. The username is a unique handle or screen name associated with each twitter user that appears on the profile URL and is used to communicate with each other on Twitter. Some username examples are $@narendramodi$, $@RahulGandhi$ etc. The display name on the other hand, is merely a personal identifier that is displayed on the profile page of a user, e.g. `Narendra Modi', `Rahul Gandhi', etc. The description is a string to describe the account, while the location is user-defined profile location. We considered the 5 profile attributes as stated above since these attributes are key elements of a users identity and saliently define users likes and values BIBREF2."
                    ],
                    "highlighted_evidence": [
                        "We consider 5 major profile attributes for further analysis. These attributes are username, display name, profile image, location and description respectively. "
                    ]
                }
            ]
        },
        {
            "question": "What are the organic and inorganic ways to show political affiliation through profile changes?",
            "answers": [
                {
                    "answer": "Organic: mention of political parties names in the profile attributes, specific mentions of political handles in the profile attributes.\nInorganic:  adding Chowkidar to the profile attributes, the effect of changing the profile attribute in accordance with Prime Minister's campaign, the addition of election campaign related keywords to the profile.",
                    "type": "abstractive"
                },
                {
                    "answer": "Mentioning of political parties names and political twitter handles is the organic way to show political affiliation; adding Chowkidar or its variants to the profile is the inorganic way.",
                    "type": "abstractive"
                }
            ],
            "q_uid": "82595ca5d11e541ed0c3353b41e8698af40a479b",
            "evidence": [
                {
                    "raw_evidence": [
                        "In this section, we first describe the phenomenon of mention of political parties names in the profile attributes of users. This is followed by the analysis of profiles that make specific mentions of political handles in their profile attributes. Both of these constitute an organic way of showing support to a party and does not involve any direct campaigning by the parties. We also study in detail the #MainBhiChowkidar campaign and analyze the corresponding change in profile attributes associated with it.",
                        "As discussed is Section SECREF1, @narendramodi added \u201cChowkidar\u201d to his display name in response to an opposition campaign called \u201cChowkidar chor hai\u201d. In a coordinated campaign, several other leaders and ministers of BJP also changed their Twitter profiles by adding the prefix Chowkidar to their display names. The movement, however, did not remain confined amongst the members of the party themselves and soon, several Twitter users updated their profiles as well. We opine that the whole campaign of adding Chowkidar to the profile attributes show an inorganic behavior, with political leaders acting as the catalyst. An interesting aspect of this campaign was the fact that the users used several different variants of the word Chowkidar while adding it to their Twitter profiles. Some of the most common variants of Chowkidar that were present in our dataset along with its frequency of use is shown in Figure FIGREF24.",
                        "We believe, the effect of changing the profile attribute in accordance with Prime Minister's campaign is an example of inorganic behavior contagion BIBREF6, BIBREF9. The authors in BIBREF6 argue that opinion diffuses easily in a network if it comes from opinion leaders who are considered to be users with a very high number of followers. We see a similar behavior contagion in our dataset with respect to the Chowkidar movement.",
                        "We argue that we can predict the political inclination of a user using just the profile attribute of the users. We further show that the presence of party name in the profile attribute can be considered as an organic behavior and signals support to a party. However, we argue that the addition of election campaign related keywords to the profile is a form of inorganic behavior. The inorganic behavior analysis falls inline with behavior contagion, where the followers tend to adapt to the behavior of their opinion leaders. The \u201cChowkidar movement\u201d showed a similar effect in the #LokSabhaElectios2019, which was evident by how the other political leaders and followers of BJP party added chowkidar to their profile attributes after @narendramodi did. We thus, argue that people don't shy away from showing support to political parties through profile information."
                    ],
                    "highlighted_evidence": [
                        "n this section, we first describe the phenomenon of mention of political parties names in the profile attributes of users. This is followed by the analysis of profiles that make specific mentions of political handles in their profile attributes. Both of these constitute an organic way of showing support to a party and does not involve any direct campaigning by the parties",
                        "We opine that the whole campaign of adding Chowkidar to the profile attributes show an inorganic behavior, with political leaders acting as the catalyst.",
                        "We believe, the effect of changing the profile attribute in accordance with Prime Minister's campaign is an example of inorganic behavior contagion BIBREF6, BIBREF9. ",
                        " However, we argue that the addition of election campaign related keywords to the profile is a form of inorganic behavior. "
                    ]
                },
                {
                    "raw_evidence": [
                        "In this section, we first describe the phenomenon of mention of political parties names in the profile attributes of users. This is followed by the analysis of profiles that make specific mentions of political handles in their profile attributes. Both of these constitute an organic way of showing support to a party and does not involve any direct campaigning by the parties. We also study in detail the #MainBhiChowkidar campaign and analyze the corresponding change in profile attributes associated with it.",
                        "As discussed is Section SECREF1, @narendramodi added \u201cChowkidar\u201d to his display name in response to an opposition campaign called \u201cChowkidar chor hai\u201d. In a coordinated campaign, several other leaders and ministers of BJP also changed their Twitter profiles by adding the prefix Chowkidar to their display names. The movement, however, did not remain confined amongst the members of the party themselves and soon, several Twitter users updated their profiles as well. We opine that the whole campaign of adding Chowkidar to the profile attributes show an inorganic behavior, with political leaders acting as the catalyst. An interesting aspect of this campaign was the fact that the users used several different variants of the word Chowkidar while adding it to their Twitter profiles. Some of the most common variants of Chowkidar that were present in our dataset along with its frequency of use is shown in Figure FIGREF24."
                    ],
                    "highlighted_evidence": [
                        "In this section, we first describe the phenomenon of mention of political parties names in the profile attributes of users. This is followed by the analysis of profiles that make specific mentions of political handles in their profile attributes. Both of these constitute an organic way of showing support to a party and does not involve any direct campaigning by the parties.",
                        "We opine that the whole campaign of adding Chowkidar to the profile attributes show an inorganic behavior, with political leaders acting as the catalyst. "
                    ]
                }
            ]
        },
        {
            "question": "How do profile changes vary for influential leads and their followers over the social movement?",
            "answers": [
                {
                    "answer": "Influential leaders are more likely to change their profile attributes than their followers; the leaders do not change their usernames, while their followers change their usernames a lot; the leaders  tend to make new changes related to previous attribute values, while the followers make comparatively less related changes to previous attribute values.",
                    "type": "abstractive"
                }
            ],
            "q_uid": "d4db7df65aa4ece63e1de813e5ce98ce1b4dbe7f",
            "evidence": [
                {
                    "raw_evidence": [
                        "INSIGHT 1: Political handles are more likely to engage in profile changing behavior as compared to their followers.",
                        "In Figure FIGREF15, we plot the number of changes in given attributes over all the snapshots for the users in set $S$. From this plot, we find that not all the attributes are modified at an equal rate. Profile image and Location are the most changed profile attributes and account for nearly $34\\%$ and $25\\%$ respectively of the total profile changes in our dataset. We analyze the trends in Figure FIGREF12 and find that the political handles do not change their usernames at all. This is in contrast to the trend in Figure FIGREF15 where we see that there are a lot of handles that change their usernames multiple times. The most likely reason for the same is that most of the follower handles are not verified and would not loose their verified status on changing their username.",
                        "INSIGHT 3: Political handles tend to make new changes related to previous attribute values. However, the followers make comparatively less related changes to previous attribute values."
                    ],
                    "highlighted_evidence": [
                        "INSIGHT 1: Political handles are more likely to engage in profile changing behavior as compared to their followers.",
                        " We analyze the trends in Figure FIGREF12 and find that the political handles do not change their usernames at all. This is in contrast to the trend in Figure FIGREF15 where we see that there are a lot of handles that change their usernames multiple times. ",
                        "INSIGHT 3: Political handles tend to make new changes related to previous attribute values. However, the followers make comparatively less related changes to previous attribute values."
                    ]
                }
            ]
        }
    ],
    "1907.04433": [
        {
            "question": "Do they experiment with the toolkits?",
            "answers": [
                {
                    "answer": "Yes",
                    "type": "boolean"
                },
                {
                    "answer": "Yes",
                    "type": "boolean"
                }
            ],
            "q_uid": "5f2bade0881c719ab026bc2e2962e2ada96cdb25",
            "evidence": [
                {
                    "raw_evidence": [
                        "We demonstrate the performance of GluonCV/NLP models in various computer vision and natural language processing tasks. Specifically, we evaluate popular or state-of-the-art models on standard benchmark data sets. In the experiments, we compare model performance between GluonCV/NLP and other open source implementations with Caffe, Caffe2, Theano, and TensorFlow, including ResNet BIBREF8 and MobileNet BIBREF9 for image classification (ImageNet), Faster R-CNN BIBREF10 for object detection (COCO), Mask R-CNN BIBREF11 for instance segmentation, Simple Pose BIBREF12 for pose estimation (COCO), textCNN BIBREF13 for sentiment analysis (TREC), and BERT BIBREF14 for question answering (SQuAD 1.1), sentiment analysis (SST-2), natural langauge inference (MNLI-m), and paraphrasing (MRPC). Table TABREF5 shows that the GluonCV/GluonNLP implementation matches or outperforms the compared open source implementation for the same model evaluated on the same data set."
                    ],
                    "highlighted_evidence": [
                        "In the experiments, we compare model performance between GluonCV/NLP and other open source implementations with Caffe, Caffe2, Theano, and TensorFlow, including ResNet BIBREF8 and MobileNet BIBREF9 for image classification (ImageNet), Faster R-CNN BIBREF10 for object detection (COCO), Mask R-CNN BIBREF11 for instance segmentation, Simple Pose BIBREF12 for pose estimation (COCO), textCNN BIBREF13 for sentiment analysis (TREC), and BERT BIBREF14 for question answering (SQuAD 1.1), sentiment analysis (SST-2), natural langauge inference (MNLI-m), and paraphrasing (MRPC)."
                    ]
                },
                {
                    "raw_evidence": [
                        "We demonstrate the performance of GluonCV/NLP models in various computer vision and natural language processing tasks. Specifically, we evaluate popular or state-of-the-art models on standard benchmark data sets. In the experiments, we compare model performance between GluonCV/NLP and other open source implementations with Caffe, Caffe2, Theano, and TensorFlow, including ResNet BIBREF8 and MobileNet BIBREF9 for image classification (ImageNet), Faster R-CNN BIBREF10 for object detection (COCO), Mask R-CNN BIBREF11 for instance segmentation, Simple Pose BIBREF12 for pose estimation (COCO), textCNN BIBREF13 for sentiment analysis (TREC), and BERT BIBREF14 for question answering (SQuAD 1.1), sentiment analysis (SST-2), natural langauge inference (MNLI-m), and paraphrasing (MRPC). Table TABREF5 shows that the GluonCV/GluonNLP implementation matches or outperforms the compared open source implementation for the same model evaluated on the same data set."
                    ],
                    "highlighted_evidence": [
                        "In the experiments, we compare model performance between GluonCV/NLP and other open source implementations with Caffe, Caffe2, Theano, and TensorFlow, including ResNet BIBREF8 and MobileNet BIBREF9 for image classification (ImageNet), Faster R-CNN BIBREF10 for object detection (COCO), Mask R-CNN BIBREF11 for instance segmentation, Simple Pose BIBREF12 for pose estimation (COCO), textCNN BIBREF13 for sentiment analysis (TREC), and BERT BIBREF14 for question answering (SQuAD 1.1), sentiment analysis (SST-2), natural langauge inference (MNLI-m), and paraphrasing (MRPC)."
                    ]
                }
            ]
        }
    ],
    "2003.03014": [
        {
            "question": "Do they model semantics ",
            "answers": [
                {
                    "answer": "Yes",
                    "type": "boolean"
                },
                {
                    "answer": "Yes",
                    "type": "boolean"
                }
            ],
            "q_uid": "cb384dc5366b693f28680374d31ff45356af0461",
            "evidence": [
                {
                    "raw_evidence": [
                        "We address this tension by training vector space models to represent the data, in which each unique word in a large corpus is represented by a vector (embedding) in high-dimensional space. The geometry of the resulting vector space captures many semantic relations between words. Furthermore, prior work has shown that vector space models trained on corpora from different time periods can capture semantic change BIBREF58, BIBREF59. For example, diachronic word embeddings reveal that the word gay meant \u201ccheerful\" or \u201cdapper\" in the early 20th century, but shifted to its current meaning of sexual orientation by the 1970s. Because word embeddings are created from real-world data, they contain real-world biases. For example, BIBREF37 demonstrated that gender stereotypes are deeply ingrained in these systems. Though problematic for the widespread use of these models in computational systems, these revealed biases indicate that word embeddings can actually be used to identify stereotypes about social groups and understand how they change over time BIBREF24."
                    ],
                    "highlighted_evidence": [
                        "We address this tension by training vector space models to represent the data, in which each unique word in a large corpus is represented by a vector (embedding) in high-dimensional space. The geometry of the resulting vector space captures many semantic relations between words. ",
                        "We address this tension by training vector space models to represent the data, in which each unique word in a large corpus is represented by a vector (embedding) in high-dimensional space. The geometry of the resulting vector space captures many semantic relations between words. "
                    ]
                },
                {
                    "raw_evidence": [
                        "We address this tension by training vector space models to represent the data, in which each unique word in a large corpus is represented by a vector (embedding) in high-dimensional space. The geometry of the resulting vector space captures many semantic relations between words. Furthermore, prior work has shown that vector space models trained on corpora from different time periods can capture semantic change BIBREF58, BIBREF59. For example, diachronic word embeddings reveal that the word gay meant \u201ccheerful\" or \u201cdapper\" in the early 20th century, but shifted to its current meaning of sexual orientation by the 1970s. Because word embeddings are created from real-world data, they contain real-world biases. For example, BIBREF37 demonstrated that gender stereotypes are deeply ingrained in these systems. Though problematic for the widespread use of these models in computational systems, these revealed biases indicate that word embeddings can actually be used to identify stereotypes about social groups and understand how they change over time BIBREF24."
                    ],
                    "highlighted_evidence": [
                        "We address this tension by training vector space models to represent the data, in which each unique word in a large corpus is represented by a vector (embedding) in high-dimensional space. The geometry of the resulting vector space captures many semantic relations between words. "
                    ]
                }
            ]
        },
        {
            "question": "How do they identify discussions of LGBTQ people in the New York Times?",
            "answers": [
                {
                    "answer": "act paragraphs containing any word from a predetermined list of LGTBQ terms ",
                    "type": "extractive"
                }
            ],
            "q_uid": "d41e20ec716b5904a272938e5a8f5f3f15a7779e",
            "evidence": [
                {
                    "raw_evidence": [
                        "The data for our case study spans over thirty years of articles from the New York Times, from January 1986 to December 2015, and was originally collected by BIBREF68 BIBREF68. The articles come from all sections of the newspaper, such as \u201cWorld\", \u201cNew York & Region\", \u201cOpinion\", \u201cStyle\", and \u201cSports\". Our distributional semantic methods rely on all of the available data in order to obtain the most fine-grained understanding of the relationships between words possible. For the other techniques, we extract paragraphs containing any word from a predetermined list of LGTBQ terms (shown in Table TABREF19)."
                    ],
                    "highlighted_evidence": [
                        "For the other techniques, we extract paragraphs containing any word from a predetermined list of LGTBQ terms (shown in Table TABREF19)"
                    ]
                }
            ]
        }
    ],
    "1703.09684": [
        {
            "question": "From when are many VQA datasets collected?",
            "answers": [
                {
                    "answer": "late 2014",
                    "type": "abstractive"
                }
            ],
            "q_uid": "cf93a209c8001ffb4ef505d306b6ced5936c6b63",
            "evidence": [
                {
                    "raw_evidence": [
                        "VQA research began in earnest in late 2014 when the DAQUAR dataset was released BIBREF0 . Including DAQUAR, six major VQA datasets have been released, and algorithms have rapidly improved. On the most popular dataset, `The VQA Dataset' BIBREF1 , the best algorithms are now approaching 70% accuracy BIBREF2 (human performance is 83%). While these results are promising, there are critical problems with existing datasets in terms of multiple kinds of biases. Moreover, because existing datasets do not group instances into meaningful categories, it is not easy to compare the abilities of individual algorithms. For example, one method may excel at color questions compared to answering questions requiring spatial reasoning. Because color questions are far more common in the dataset, an algorithm that performs well at spatial reasoning will not be appropriately rewarded for that feat due to the evaluation metrics that are used."
                    ],
                    "highlighted_evidence": [
                        "VQA research began in earnest in late 2014 when the DAQUAR dataset was released BIBREF0"
                    ]
                }
            ]
        }
    ],
    "1909.08306": [
        {
            "question": "What dierse domains and languages are present in new datasets?",
            "answers": [
                {
                    "answer": "movies , restaurants, English , Korean",
                    "type": "extractive"
                }
            ],
            "q_uid": "e37c32fce68759b2272adc1e44ea91c1a7c47059",
            "evidence": [
                {
                    "raw_evidence": [
                        "We provide three pairs of short/long datasets from different domains (movies and restaurants) and from different languages (English and Korean) suitable for the task: Mov_en, Res_en, and Mov_ko. Most of the datasets are from previous literature and are gathered differently The Mov_en datasets are gathered from different websites; the short dataset consists of hand-picked sentences by BIBREF19 from document-level reviews from the Rotten Tomatoes website, while the long dataset consists of reviews from the IMDB website obtained by BIBREF20. The Res_en dataset consists of reviews from Yelp, where the short dataset consists of reviews with character lengths less than 140 from BIBREF21, while reviews in the long dataset are gathered from BIBREF20. We also share new short/long datasets Mov_ko, which are gathered from two different channels, as shown in Figure FIGREF4, available in Naver Movies. Unlike previous datasets BIBREF9, BIBREF22 where they used polarity/binary (e.g., positive or negative) labels as classes, we also provide fine-grained classes, with five classes of different sentiment intensities (e.g., 1 is strong negative, 5 is strong positive), for Res_en and Mov_ko. Following the Cross Domain Transfer setting BIBREF9, BIBREF23, BIBREF24, we limit the size of the dataset to be small-scale to focus on the main task at hand. This ensures that models focus on the transfer task, and decrease the influence of other factors that can be found when using larger datasets. Finally, following BIBREF22, we provide additional unlabeled data for those models that need them BIBREF9, BIBREF23, except for the long dataset of Mov_ko, where the labeled reviews are very limited. We show the dataset statistics in Table TABREF9, and share the datasets here: https://github.com/rktamplayo/LeTraNets."
                    ],
                    "highlighted_evidence": [
                        "We provide three pairs of short/long datasets from different domains (movies and restaurants) and from different languages (English and Korean) suitable for the task: Mov_en, Res_en, and Mov_ko. Most of the datasets are from previous literature and are gathered differently The Mov_en datasets are gathered from different websites; the short dataset consists of hand-picked sentences by BIBREF19 from document-level reviews from the Rotten Tomatoes website, while the long dataset consists of reviews from the IMDB website obtained by BIBREF20."
                    ]
                }
            ]
        }
    ],
    "1806.00722": [
        {
            "question": "what are the baselines?",
            "answers": [
                {
                    "answer": " 4-layer encoder, 4-layer decoder, residual-connected model, with embedding and hidden size set as 256",
                    "type": "extractive"
                }
            ],
            "q_uid": "26b5c090f72f6d51e5d9af2e470d06b2d7fc4a98",
            "evidence": [
                {
                    "raw_evidence": [
                        "As the baseline model (BASE-4L) for IWSLT14 German-English and Turkish-English, we use a 4-layer encoder, 4-layer decoder, residual-connected model, with embedding and hidden size set as 256 by default. As a comparison, we design a densely connected model with same number of layers, but the hidden size is set as 128 in order to keep the model size consistent. The models adopting DenseAtt-1, DenseAtt-2 are named as DenseNMT-4L-1 and DenseNMT-4L-2 respectively. In order to check the effect of dense connections on deeper models, we also construct a series of 8-layer models. We set the hidden number to be 192, such that both 4-layer models and 8-layer models have similar number of parameters. For dense structured models, we set the dimension of hidden states to be 96."
                    ],
                    "highlighted_evidence": [
                        "As the baseline model (BASE-4L) for IWSLT14 German-English and Turkish-English, we use a 4-layer encoder, 4-layer decoder, residual-connected model, with embedding and hidden size set as 256 by default."
                    ]
                }
            ]
        },
        {
            "question": "did they outperform previous methods?",
            "answers": [
                {
                    "answer": "Yes",
                    "type": "boolean"
                }
            ],
            "q_uid": "8c0621016e96d86a7063cb0c9ec20c76a2dba678",
            "evidence": [
                {
                    "raw_evidence": [
                        "Table TABREF32 shows the results for De-En, Tr-En, Tr-En-morph datasets, where the best accuracy for models with the same depth and of similar sizes are marked in boldface. In almost all genres, DenseNMT models are significantly better than the baselines. With embedding size 256, where all models achieve their best scores, DenseNMT outperforms baselines by 0.7-1.0 BLEU on De-En, 0.5-1.3 BLEU on Tr-En, 0.8-1.5 BLEU on Tr-En-morph. We observe significant gain using other embedding sizes as well."
                    ],
                    "highlighted_evidence": [
                        " In almost all genres, DenseNMT models are significantly better than the baselines."
                    ]
                }
            ]
        },
        {
            "question": "what language pairs are explored?",
            "answers": [
                {
                    "answer": "German-English, Turkish-English, English-German",
                    "type": "extractive"
                }
            ],
            "q_uid": "f1214a05cc0e6d870c789aed24a8d4c768e1db2f",
            "evidence": [
                {
                    "raw_evidence": [
                        "We use three datasets for our experiments: IWSLT14 German-English, Turkish-English, and WMT14 English-German."
                    ],
                    "highlighted_evidence": [
                        "We use three datasets for our experiments: IWSLT14 German-English, Turkish-English, and WMT14 English-German."
                    ]
                }
            ]
        },
        {
            "question": "what datasets were used?",
            "answers": [
                {
                    "answer": "IWSLT14 German-English, IWSLT14 Turkish-English, WMT14 English-German",
                    "type": "abstractive"
                }
            ],
            "q_uid": "41d3ab045ef8e52e4bbe5418096551a22c5e9c43",
            "evidence": [
                {
                    "raw_evidence": [
                        "We use three datasets for our experiments: IWSLT14 German-English, Turkish-English, and WMT14 English-German."
                    ],
                    "highlighted_evidence": [
                        "We use three datasets for our experiments: IWSLT14 German-English, Turkish-English, and WMT14 English-German."
                    ]
                }
            ]
        }
    ],
    "1911.03584": [
        {
            "question": "How they prove that multi-head self-attention is at least as powerful as convolution layer? ",
            "answers": [
                {
                    "answer": "constructively by selecting the parameters of the multi-head self-attention layer so that the latter acts like a convolutional layer",
                    "type": "extractive"
                }
            ],
            "q_uid": "0e510d918456f3d2b390b501a145d92c4f125835",
            "evidence": [
                {
                    "raw_evidence": [
                        "The theorem is proven constructively by selecting the parameters of the multi-head self-attention layer so that the latter acts like a convolutional layer. In the proposed construction, the attention scores of each self-attention head should attend to a different relative shift within the set $\\Delta \\!\\!\\!\\!\\Delta _K = \\lbrace -\\lfloor K/2 \\rfloor , \\dots , \\lfloor K/2 \\rfloor \\rbrace ^2$ of all pixel shifts in a $K\\times K$ kernel. The exact condition can be found in the statement of Lemma UNKREF15."
                    ],
                    "highlighted_evidence": [
                        "The theorem is proven constructively by selecting the parameters of the multi-head self-attention layer so that the latter acts like a convolutional layer."
                    ]
                }
            ]
        },
        {
            "question": "Is there any nonnumerical experiment that also support author's claim, like analysis of attention layers in publicly available networks? ",
            "answers": [
                {
                    "answer": "No",
                    "type": "boolean"
                }
            ],
            "q_uid": "2caa8726222237af482e170c51c88099cefef6fc",
            "evidence": [
                {
                    "raw_evidence": [],
                    "highlighted_evidence": []
                }
            ]
        },
        {
            "question": "What numerical experiments they perform?",
            "answers": [
                {
                    "answer": "attention probabilities learned tend to respect the conditions of Lemma UNKREF15, corroborating our hypothesis, validate that our model learns a meaningful classifier we compare it to the standard ResNet18",
                    "type": "extractive"
                }
            ],
            "q_uid": "5367f8979488aaa420d8a69fec656851095ecacb",
            "evidence": [
                {
                    "raw_evidence": [
                        "The aim of this section is to validate the applicability of our theoretical results\u2014which state that self-attention can perform convolution\u2014and to examine whether self-attention layers in practice do actually learn to operate like convolutional layers, when being trained on standard image classification tasks. In particular, we study the relationship between self-attention and convolution with quadratic and learned relative positional encodings. We find that for both cases, the attention probabilities learned tend to respect the conditions of Lemma UNKREF15, corroborating our hypothesis.",
                        "We study a fully attentional model consisting of six multi-head self-attention layers. As it has already been shown by BIBREF9 that combining attention features with convolutional features improves performance on Cifar-100 and ImageNet, we do not focus on attaining state-of-the-art performance. Nevertheless, to validate that our model learns a meaningful classifier we compare it to the standard ResNet18 BIBREF14 on the CIFAR-10 dataset BIBREF15. In all experiments, we use a $2\\times 2$ invertible down-sampling BIBREF16 on the input to reduce the size of the image as storing the attention coefficient tensor requires a large amount of GPU memory. The fixed size representation of the input image is computed as the average pooling of the last layer representations and given to a linear classifier."
                    ],
                    "highlighted_evidence": [
                        "The aim of this section is to validate the applicability of our theoretical results\u2014which state that self-attention can perform convolution\u2014and to examine whether self-attention layers in practice do actually learn to operate like convolutional layers, when being trained on standard image classification tasks. In particular, we study the relationship between self-attention and convolution with quadratic and learned relative positional encodings. We find that for both cases, the attention probabilities learned tend to respect the conditions of Lemma UNKREF15, corroborating our hypothesis.",
                        "Nevertheless, to validate that our model learns a meaningful classifier we compare it to the standard ResNet18 BIBREF14 on the CIFAR-10 dataset BIBREF15."
                    ]
                }
            ]
        }
    ],
    "1910.06701": [
        {
            "question": "what are the existing models they compared with?",
            "answers": [
                {
                    "answer": "Syn Dep, OpenIE, SRL, BiDAF, QANet, BERT, NAQANet, NAQANet+",
                    "type": "extractive"
                }
            ],
            "q_uid": "81669c550d32d756f516dab5d2b76ff5f21c0f36",
            "evidence": [
                {
                    "raw_evidence": [
                        "Experiments ::: Baselines",
                        "For comparison, we select several public models as baselines including semantic parsing models:",
                        "BiDAF BIBREF3, an MRC model which utilizes a bi-directional attention flow network to encode the question and passage;",
                        "QANet BIBREF12, which utilizes convolutions and self-attentions as the building blocks of encoders to represent the question and passage;",
                        "BERT BIBREF23, a pre-trained bidirectional Transformer-based language model which achieves state-of-the-art performance on lots of public MRC datasets recently;",
                        "and numerical MRC models:",
                        "NAQANet BIBREF6, a numerical version of QANet model.",
                        "NAQANet+, an enhanced version of NAQANet implemented by ourselves, which further considers real number (e.g. \u201c2.5\u201d), richer arithmetic expression, data augmentation, etc. The enhancements are also used in our NumNet model and the details are given in the Appendix.",
                        "Syn Dep BIBREF6, the neural semantic parsing model (KDG) BIBREF22 with Stanford dependencies based sentence representations;",
                        "OpenIE BIBREF6, KDG with open information extraction based sentence representations;",
                        "SRL BIBREF6, KDG with semantic role labeling based sentence representations;",
                        "and traditional MRC models:"
                    ],
                    "highlighted_evidence": [
                        "Experiments ::: Baselines\nFor comparison, we select several public models as baselines including semantic parsing models:",
                        "BiDAF BIBREF3, an MRC model which utilizes a bi-directional attention flow network to encode the question and passage;\n\nQANet BIBREF12, which utilizes convolutions and self-attentions as the building blocks of encoders to represent the question and passage;\n\nBERT BIBREF23, a pre-trained bidirectional Transformer-based language model which achieves state-of-the-art performance on lots of public MRC datasets recently;\n\nand numerical MRC models:",
                        "NAQANet BIBREF6, a numerical version of QANet model.\n\nNAQANet+, an enhanced version of NAQANet implemented by ourselves, which further considers real number (e.g. \u201c2.5\u201d), richer arithmetic expression, data augmentation, etc.",
                        "Syn Dep BIBREF6, the neural semantic parsing model (KDG) BIBREF22 with Stanford dependencies based sentence representations;\n\nOpenIE BIBREF6, KDG with open information extraction based sentence representations;\n\nSRL BIBREF6, KDG with semantic role labeling based sentence representations;\n\nand traditional MRC models:"
                    ]
                }
            ]
        }
    ],
    "1811.11365": [
        {
            "question": "Why is this work different from text-only UNMT?",
            "answers": [
                {
                    "answer": "the image can play the role of a pivot \u201clanguage\" to bridge the two languages without paralleled corpus",
                    "type": "extractive"
                }
            ],
            "q_uid": "cf15c4652e23829d8fb4cf2a25e64408c18734c1",
            "evidence": [
                {
                    "raw_evidence": [
                        "Our idea is originally inspired by the text-only unsupervised MT (UMT) BIBREF8 , BIBREF9 , BIBREF0 , investigating whether it is possible to train a general MT system without any form of supervision. As BIBREF0 discussed, the text-only UMT is fundamentally an ill-posed problem, since there are potentially many ways to associate target with source sentences. Intuitively, since the visual content and language are closely related, the image can play the role of a pivot \u201clanguage\" to bridge the two languages without paralleled corpus, making the problem \u201cmore well-defined\" by reducing the problem to supervised learning. However, unlike the text translation involving word generation (usually a discrete distribution), the task to generate a dense image from a sentence description itself is a challenging problem BIBREF10 . High quality image generation usually depends on a complicated or large scale neural network architecture BIBREF11 , BIBREF12 , BIBREF13 . Thus, it is not recommended to utilize the image dataset as a pivot \u201clanguage\" BIBREF14 . Motivated by the cycle-consistency BIBREF15 , we tackle the unsupervised translation with a multi-modal framework which includes two sequence-to-sequence encoder-decoder models and one shared image feature extractor. We don't introduce the adversarial learning via a discriminator because of the non-differentiable $\\arg \\max $ operation during word generation. With five modules in our framework, there are multiple data streaming paths in the computation graph, inducing the auto-encoding loss and cycle-consistency loss, in order to achieve the unsupervised translation."
                    ],
                    "highlighted_evidence": [
                        " As BIBREF0 discussed, the text-only UMT is fundamentally an ill-posed problem, since there are potentially many ways to associate target with source sentences. Intuitively, since the visual content and language are closely related, the image can play the role of a pivot \u201clanguage\" to bridge the two languages without paralleled corpus, making the problem \u201cmore well-defined\" by reducing the problem to supervised learning."
                    ]
                }
            ]
        }
    ],
    "1909.01093": [
        {
            "question": "How does the method measure the impact of the event on market prices?",
            "answers": [
                {
                    "answer": "We collected the historical 52 week stock prices prior to this event and calculated the daily stock price change. The distribution of the daily price change of the previous 52 weeks is Figure FIGREF13 with a mean INLINEFORM1 and standard deviation INLINEFORM2 . ",
                    "type": "extractive"
                }
            ],
            "q_uid": "d82ec1003a3db7370994c7522590f7e5151b1f33",
            "evidence": [
                {
                    "raw_evidence": [
                        "We also did a qualitative study on the Starbucks (SBUX) stock movement during this event. Figure FIGREF12 is the daily percentage change of SBUX and NASDAQ index between April 11th and April 20th. SBUX did not follow the upward trend of the whole market before April 17th, and then its change on April 20th, INLINEFORM0 , is quite significant from historical norms. We collected the historical 52 week stock prices prior to this event and calculated the daily stock price change. The distribution of the daily price change of the previous 52 weeks is Figure FIGREF13 with a mean INLINEFORM1 and standard deviation INLINEFORM2 . The INLINEFORM3 down almost equals to two standard deviations below the mean. Our observation is that plausibly, there was a negative aftereffect from the event of the notable decline in Starbucks stock price due to the major public relations crisis."
                    ],
                    "highlighted_evidence": [
                        "We also did a qualitative study on the Starbucks (SBUX) stock movement during this event. Figure FIGREF12 is the daily percentage change of SBUX and NASDAQ index between April 11th and April 20th. SBUX did not follow the upward trend of the whole market before April 17th, and then its change on April 20th, INLINEFORM0 , is quite significant from historical norms. We collected the historical 52 week stock prices prior to this event and calculated the daily stock price change. The distribution of the daily price change of the previous 52 weeks is Figure FIGREF13 with a mean INLINEFORM1 and standard deviation INLINEFORM2 . "
                    ]
                }
            ]
        },
        {
            "question": "How is sentiment polarity measured?",
            "answers": [
                {
                    "answer": "For each cluster, its overall sentiment score is quantified by the mean of the sentiment scores among all tweets",
                    "type": "extractive"
                }
            ],
            "q_uid": "58f08d38bbcffb2dd9d660faa8026718d390d64b",
            "evidence": [
                {
                    "raw_evidence": [
                        "Sentiment: For each cluster, its overall sentiment score is quantified by the mean of the sentiment scores among all tweets."
                    ],
                    "highlighted_evidence": [
                        "Sentiment: For each cluster, its overall sentiment score is quantified by the mean of the sentiment scores among all tweets."
                    ]
                }
            ]
        }
    ],
    "1908.06556": [
        {
            "question": "What games are used to test author's methods?",
            "answers": [
                {
                    "answer": "Lurking Horror, Afflicted, Anchorhead, 9:05, TextWorld games",
                    "type": "extractive"
                }
            ],
            "q_uid": "5c26388a2c0b0452d529d5dd565a5375fdabdb70",
            "evidence": [
                {
                    "raw_evidence": [
                        "TextWorld uses a grammar to generate similar games. Following BIBREF7, we use TextWorld's \u201chome\u201d theme to generate the games for the question-answering system. TextWorld is a framework that uses a grammar to randomly generate game worlds and quests. This framework also gives us information such as instructions on how to finish the quest, and a list of actions that can be performed at each step based on the current world state. We do not let our agent access this additional solution information or admissible actions list. Given the relatively small quest length for TextWorld games\u2014games can be completed in as little as 5 steps\u2014we generate 50 such games and partition them into train and test sets in a 4:1 ratio. The traces are generated on the training set, and the question-answering system is evaluated on the test set.",
                        "We choose the game, 9:05 as our target task game due to similarities in structure in addition to the vocabulary overlap. Note that there are multiple possible endings to this game and we pick the simplest one for the purpose of training our agent.",
                        "For the horror domain, we choose Lurking Horror to train the question-answering system on. The source and target task games are chosen as Afflicted and Anchorhead respectively. However, due to the size and complexity of these two games some modifications to the games are required for the agent to be able to effectively solve them. We partition each of these games and make them smaller by reducing the final goal of the game to an intermediate checkpoint leading to it. This checkpoints were identified manually using walkthroughs of the game; each game has a natural intermediate goal. For example, Anchorhead is segmented into 3 chapters in the form of objectives spread across 3 days, of which we use only the first chapter. The exact details of the games after partitioning is described in Table TABREF7. For Lurking Horror, we report numbers relevant for the oracle walkthrough. We then pre-prune the action space and use only the actions that are relevant for the sections of the game that we have partitioned out. The majority of the environment is still available for the agent to explore but the game ends upon completion of the chosen intermediate checkpoint."
                    ],
                    "highlighted_evidence": [
                        "Given the relatively small quest length for TextWorld games\u2014games can be completed in as little as 5 steps\u2014we generate 50 such games and partition them into train and test sets in a 4:1 ratio.",
                        "We choose the game, 9:05 as our target task game due to similarities in structure in addition to the vocabulary overlap. ",
                        "For the horror domain, we choose Lurking Horror to train the question-answering system on. The source and target task games are chosen as Afflicted and Anchorhead respectively."
                    ]
                }
            ]
        },
        {
            "question": "How is the domain knowledge transfer represented as knowledge graph?",
            "answers": [
                {
                    "answer": "the knowledge graph is used to prune this space by ranking actions based on their presence in the current knowledge graph and the relations between the objects in the graph as in BIBREF7",
                    "type": "extractive"
                }
            ],
            "q_uid": "184e1f28f96babf468f2bb4e1734f69646590cda",
            "evidence": [
                {
                    "raw_evidence": [
                        "The agent also has access to all actions accepted by the game's parser, following BIBREF2. For general interactive fiction environments, we develop our own method to extract this information. This is done by extracting a set of templates accepted by the parser, with the objects or noun phrases in the actions replaces with a OBJ tag. An example of such a template is \"place OBJ in OBJ\". These OBJ tags are then filled in by looking at all possible objects in the given vocabulary for the game. This action space is of the order of $A=\\mathcal {O}(|V| \\times |O|^2)$ where $V$ is the number of action verbs, and $O$ is the number of distinct objects in the world that the agent can interact with. As this is too large a space for a RL agent to effectively explore, the knowledge graph is used to prune this space by ranking actions based on their presence in the current knowledge graph and the relations between the objects in the graph as in BIBREF7"
                    ],
                    "highlighted_evidence": [
                        "This action space is of the order of $A=\\mathcal {O}(|V| \\times |O|^2)$ where $V$ is the number of action verbs, and $O$ is the number of distinct objects in the world that the agent can interact with. As this is too large a space for a RL agent to effectively explore, the knowledge graph is used to prune this space by ranking actions based on their presence in the current knowledge graph and the relations between the objects in the graph as in BIBREF7"
                    ]
                }
            ]
        }
    ],
    "1908.11860": [
        {
            "question": "By how much does their model outperform the baseline in the cross-domain evaluation?",
            "answers": [
                {
                    "answer": "$2.2\\%$ absolute accuracy improvement on the laptops test set, $3.6\\%$ accuracy improvement on the restaurants test set",
                    "type": "extractive"
                }
            ],
            "q_uid": "11c77ee117cb4de825016b6ccff59ff021f84a38",
            "evidence": [
                {
                    "raw_evidence": [
                        "To answer RQ3, which is concerned with domain adaptation, we can see in the grayed out cells in tab:results, which correspond to the cross-domain adaption case where the BERT language model is trained on the target domain, that domain adaptation works well with $2.2\\%$ absolute accuracy improvement on the laptops test set and even $3.6\\%$ accuracy improvement on the restaurants test set compared to BERT-base."
                    ],
                    "highlighted_evidence": [
                        "To answer RQ3, which is concerned with domain adaptation, we can see in the grayed out cells in tab:results, which correspond to the cross-domain adaption case where the BERT language model is trained on the target domain, that domain adaptation works well with $2.2\\%$ absolute accuracy improvement on the laptops test set and even $3.6\\%$ accuracy improvement on the restaurants test set compared to BERT-base."
                    ]
                }
            ]
        },
        {
            "question": "What are the performance results?",
            "answers": [
                {
                    "answer": "results that for the in-domain training case, our models BERT-ADA Lapt and BERT-ADA Rest achieve performance close to state-of-the-art on the laptops dataset, new state-of-the-art on the restaurants dataset with accuracies of $79.19\\%$ and $87.14\\%$, respectively.",
                    "type": "extractive"
                }
            ],
            "q_uid": "0b92fb692feb35d4b4bf4665f7754d283d6ad5f3",
            "evidence": [
                {
                    "raw_evidence": [
                        "To answer RQ2, which is concerned with in-domain ATSC performance, we see in tab:results that for the in-domain training case, our models BERT-ADA Lapt and BERT-ADA Rest achieve performance close to state-of-the-art on the laptops dataset and new state-of-the-art on the restaurants dataset with accuracies of $79.19\\%$ and $87.14\\%$, respectively. On the restaurants dataset, this corresponds to an absolute improvement of $2.2\\%$ compared to the previous state-of-the-art method BERT-PT. Language model finetuning produces a larger improvement on the restaurants dataset. We think that one reason for that might be that the restaurants domain is underrepresented in the pre-training corpora of BERTBASE. Generally, we find that language model finetuning helps even if the finetuning domain does not match the evaluation domain. We think the reason for this might be that the BERT-base model is pre-trained more on knowledge-based corpora like Wikipedia than on text containing opinions. Another finding is that BERT-ADA Joint performs better on the laptops dataset than BERT-ADA Rest, although the unique amount of laptop reviews are the same in laptops- and joint-corpora. We think that confusion can be created when mixing the domains, but this needs to be investigated further. We also find that the XLNet-base baseline performs generally stronger than BERT-base and even outperforms BERT-ADA Lapt with an accuracy of $79.89\\%$ on the laptops dataset.",
                        "In general, the ATSC task generalizes well cross-domain, with about 2-$3\\%$ drop in accuracy compared to in-domain training. We think the reason for this might be that syntactical relationships between the aspect-target and the phrase expressing sentiment polarity as well as knowing the sentiment-polarity itself are sufficient to solve the ATSC task in many cases."
                    ],
                    "highlighted_evidence": [
                        "To answer RQ2, which is concerned with in-domain ATSC performance, we see in tab:results that for the in-domain training case, our models BERT-ADA Lapt and BERT-ADA Rest achieve performance close to state-of-the-art on the laptops dataset and new state-of-the-art on the restaurants dataset with accuracies of $79.19\\%$ and $87.14\\%$, respectively.",
                        "In general, the ATSC task generalizes well cross-domain, with about 2-$3\\%$ drop in accuracy compared to in-domain training."
                    ]
                }
            ]
        }
    ],
    "1908.10084": [
        {
            "question": "What transfer learning tasks are evaluated?",
            "answers": [
                {
                    "answer": "MR, CR, SUBJ, MPQA, SST, TREC, MRPC",
                    "type": "extractive"
                },
                {
                    "answer": "MR: Sentiment prediction for movie reviews snippets on a five start scale BIBREF25.\n\nCR: Sentiment prediction of customer product reviews BIBREF26.\n\nSUBJ: Subjectivity prediction of sentences from movie reviews and plot summaries BIBREF27.\n\nMPQA: Phrase level opinion polarity classification from newswire BIBREF28.\n\nSST: Stanford Sentiment Treebank with binary labels BIBREF29.\n\nTREC: Fine grained question-type classification from TREC BIBREF30.\n\nMRPC: Microsoft Research Paraphrase Corpus from parallel news sources BIBREF31.",
                    "type": "extractive"
                },
                {
                    "answer": "Semantic Textual Similarity, sentiment prediction, subjectivity prediction, phrase level opinion polarity classification, Stanford Sentiment Treebank, fine grained question-type classification.",
                    "type": "abstractive"
                }
            ],
            "q_uid": "4944cd597b836b62616a4e37c045ce48de8c82ca",
            "evidence": [
                {
                    "raw_evidence": [
                        "We compare the SBERT sentence embeddings to other sentence embeddings methods on the following seven SentEval transfer tasks:",
                        "MR: Sentiment prediction for movie reviews snippets on a five start scale BIBREF25.",
                        "CR: Sentiment prediction of customer product reviews BIBREF26.",
                        "SUBJ: Subjectivity prediction of sentences from movie reviews and plot summaries BIBREF27.",
                        "MPQA: Phrase level opinion polarity classification from newswire BIBREF28.",
                        "SST: Stanford Sentiment Treebank with binary labels BIBREF29.",
                        "TREC: Fine grained question-type classification from TREC BIBREF30.",
                        "MRPC: Microsoft Research Paraphrase Corpus from parallel news sources BIBREF31."
                    ],
                    "highlighted_evidence": [
                        "We compare the SBERT sentence embeddings to other sentence embeddings methods on the following seven SentEval transfer tasks:\n\nMR: Sentiment prediction for movie reviews snippets on a five start scale BIBREF25.\n\nCR: Sentiment prediction of customer product reviews BIBREF26.\n\nSUBJ: Subjectivity prediction of sentences from movie reviews and plot summaries BIBREF27.\n\nMPQA: Phrase level opinion polarity classification from newswire BIBREF28.\n\nSST: Stanford Sentiment Treebank with binary labels BIBREF29.\n\nTREC: Fine grained question-type classification from TREC BIBREF30.\n\nMRPC: Microsoft Research Paraphrase Corpus from parallel news sources BIBREF31."
                    ]
                },
                {
                    "raw_evidence": [
                        "The purpose of SBERT sentence embeddings are not to be used for transfer learning for other tasks. Here, we think fine-tuning BERT as described by devlin2018bert for new tasks is the more suitable method, as it updates all layers of the BERT network. However, SentEval can still give an impression on the quality of our sentence embeddings for various tasks.",
                        "We compare the SBERT sentence embeddings to other sentence embeddings methods on the following seven SentEval transfer tasks:",
                        "MR: Sentiment prediction for movie reviews snippets on a five start scale BIBREF25.",
                        "CR: Sentiment prediction of customer product reviews BIBREF26.",
                        "SUBJ: Subjectivity prediction of sentences from movie reviews and plot summaries BIBREF27.",
                        "MPQA: Phrase level opinion polarity classification from newswire BIBREF28.",
                        "SST: Stanford Sentiment Treebank with binary labels BIBREF29.",
                        "TREC: Fine grained question-type classification from TREC BIBREF30.",
                        "MRPC: Microsoft Research Paraphrase Corpus from parallel news sources BIBREF31."
                    ],
                    "highlighted_evidence": [
                        "The purpose of SBERT sentence embeddings are not to be used for transfer learning for other tasks. Here, we think fine-tuning BERT as described by devlin2018bert for new tasks is the more suitable method, as it updates all layers of the BERT network. However, SentEval can still give an impression on the quality of our sentence embeddings for various tasks.\n\nWe compare the SBERT sentence embeddings to other sentence embeddings methods on the following seven SentEval transfer tasks:\n\nMR: Sentiment prediction for movie reviews snippets on a five start scale BIBREF25.\n\nCR: Sentiment prediction of customer product reviews BIBREF26.\n\nSUBJ: Subjectivity prediction of sentences from movie reviews and plot summaries BIBREF27.\n\nMPQA: Phrase level opinion polarity classification from newswire BIBREF28.\n\nSST: Stanford Sentiment Treebank with binary labels BIBREF29.\n\nTREC: Fine grained question-type classification from TREC BIBREF30.\n\nMRPC: Microsoft Research Paraphrase Corpus from parallel news sources BIBREF31."
                    ]
                },
                {
                    "raw_evidence": [
                        "We fine-tune SBERT on NLI data, which creates sentence embeddings that significantly outperform other state-of-the-art sentence embedding methods like InferSent BIBREF4 and Universal Sentence Encoder BIBREF5. On seven Semantic Textual Similarity (STS) tasks, SBERT achieves an improvement of 11.7 points compared to InferSent and 5.5 points compared to Universal Sentence Encoder. On SentEval BIBREF6, an evaluation toolkit for sentence embeddings, we achieve an improvement of 2.1 and 2.6 points, respectively.",
                        "We compare the SBERT sentence embeddings to other sentence embeddings methods on the following seven SentEval transfer tasks:",
                        "MR: Sentiment prediction for movie reviews snippets on a five start scale BIBREF25.",
                        "CR: Sentiment prediction of customer product reviews BIBREF26.",
                        "SUBJ: Subjectivity prediction of sentences from movie reviews and plot summaries BIBREF27.",
                        "MPQA: Phrase level opinion polarity classification from newswire BIBREF28.",
                        "SST: Stanford Sentiment Treebank with binary labels BIBREF29.",
                        "TREC: Fine grained question-type classification from TREC BIBREF30.",
                        "MRPC: Microsoft Research Paraphrase Corpus from parallel news sources BIBREF31."
                    ],
                    "highlighted_evidence": [
                        "On seven Semantic Textual Similarity (STS) tasks, SBERT achieves an improvement of 11.7 points compared to InferSent and 5.5 points compared to Universal Sentence Encoder.",
                        "We compare the SBERT sentence embeddings to other sentence embeddings methods on the following seven SentEval transfer tasks:\n\nMR: Sentiment prediction for movie reviews snippets on a five start scale BIBREF25.\n\nCR: Sentiment prediction of customer product reviews BIBREF26.\n\nSUBJ: Subjectivity prediction of sentences from movie reviews and plot summaries BIBREF27.\n\nMPQA: Phrase level opinion polarity classification from newswire BIBREF28.\n\nSST: Stanford Sentiment Treebank with binary labels BIBREF29.\n\nTREC: Fine grained question-type classification from TREC BIBREF30.\n\nMRPC: Microsoft Research Paraphrase Corpus from parallel news sources BIBREF31."
                    ]
                }
            ]
        },
        {
            "question": "What metrics are used for the STS tasks?",
            "answers": [
                {
                    "answer": " Spearman's rank correlation between the cosine-similarity of the sentence embeddings and the gold labels",
                    "type": "extractive"
                },
                {
                    "answer": "Spearman's rank correlation between the cosine-similarity of the sentence embeddings and the gold labels",
                    "type": "extractive"
                }
            ],
            "q_uid": "a29c071065d26e5ee3c3bcd877e7f215c59d1d33",
            "evidence": [
                {
                    "raw_evidence": [
                        "We evaluate the performance of SBERT for STS without using any STS specific training data. We use the STS tasks 2012 - 2016 BIBREF16, BIBREF17, BIBREF18, BIBREF19, BIBREF20, the STS benchmark BIBREF10, and the SICK-Relatedness dataset BIBREF21. These datasets provide labels between 0 and 5 on the semantic relatedness of sentence pairs. We showed in BIBREF22 that Pearson correlation is badly suited for STS. Instead, we compute the Spearman's rank correlation between the cosine-similarity of the sentence embeddings and the gold labels. The setup for the other sentence embedding methods is equivalent, the similarity is computed by cosine-similarity. The results are depicted in Table TABREF6."
                    ],
                    "highlighted_evidence": [
                        "We showed in BIBREF22 that Pearson correlation is badly suited for STS. Instead, we compute the Spearman's rank correlation between the cosine-similarity of the sentence embeddings and the gold labels."
                    ]
                },
                {
                    "raw_evidence": [
                        "We evaluate the performance of SBERT for STS without using any STS specific training data. We use the STS tasks 2012 - 2016 BIBREF16, BIBREF17, BIBREF18, BIBREF19, BIBREF20, the STS benchmark BIBREF10, and the SICK-Relatedness dataset BIBREF21. These datasets provide labels between 0 and 5 on the semantic relatedness of sentence pairs. We showed in BIBREF22 that Pearson correlation is badly suited for STS. Instead, we compute the Spearman's rank correlation between the cosine-similarity of the sentence embeddings and the gold labels. The setup for the other sentence embedding methods is equivalent, the similarity is computed by cosine-similarity. The results are depicted in Table TABREF6."
                    ],
                    "highlighted_evidence": [
                        "Instead, we compute the Spearman's rank correlation between the cosine-similarity of the sentence embeddings and the gold labels. "
                    ]
                }
            ]
        },
        {
            "question": "How much time takes its training?",
            "answers": [
                {
                    "answer": "20 minutes",
                    "type": "extractive"
                }
            ],
            "q_uid": "7f207549c75f5c4388efc15ed28822672b845663",
            "evidence": [
                {
                    "raw_evidence": [
                        "Previous neural sentence embedding methods started the training from a random initialization. In this publication, we use the pre-trained BERT and RoBERTa network and only fine-tune it to yield useful sentence embeddings. This reduces significantly the needed training time: SBERT can be tuned in less than 20 minutes, while yielding better results than comparable sentence embedding methods."
                    ],
                    "highlighted_evidence": [
                        "his reduces significantly the needed training time: SBERT can be tuned in less than 20 minutes, while yielding better results than comparable sentence embedding methods."
                    ]
                }
            ]
        },
        {
            "question": "How are the siamese networks trained?",
            "answers": [
                {
                    "answer": "update the weights such that the produced sentence embeddings are semantically meaningful and can be compared with cosine-similarity., Classification Objective Function, Regression Objective Function, Triplet Objective Function",
                    "type": "extractive"
                }
            ],
            "q_uid": "2e89ebd2e4008c67bb2413699589ee55f59c4f36",
            "evidence": [
                {
                    "raw_evidence": [
                        "SBERT adds a pooling operation to the output of BERT / RoBERTa to derive a fixed sized sentence embedding. We experiment with three pooling strategies: Using the output of the CLS-token, computing the mean of all output vectors (MEAN-strategy), and computing a max-over-time of the output vectors (MAX-strategy). The default configuration is MEAN.",
                        "In order to fine-tune BERT / RoBERTa, we create siamese and triplet networks BIBREF15 to update the weights such that the produced sentence embeddings are semantically meaningful and can be compared with cosine-similarity.",
                        "The network structure depends on the available training data. We experiment with the following structures and objective functions.",
                        "Classification Objective Function. We concatenate the sentence embeddings $u$ and $v$ with the element-wise difference $|u-v|$ and multiply it with the trainable weight $W_t \\in \\mathbb {R}^{3n \\times k}$:",
                        "where $n$ is the dimension of the sentence embeddings and $k$ the number of labels. We optimize cross-entropy loss. This structure is depicted in Figure FIGREF4.",
                        "Regression Objective Function. The cosine-similarity between the two sentence embeddings $u$ and $v$ is computed (Figure FIGREF5). We use mean-squared-error loss as the objective function.",
                        "Triplet Objective Function. Given an anchor sentence $a$, a positive sentence $p$, and a negative sentence $n$, triplet loss tunes the network such that the distance between $a$ and $p$ is smaller than the distance between $a$ and $n$. Mathematically, we minimize the following loss function:",
                        "with $s_x$ the sentence embedding for $a$/$n$/$p$, $||\\cdot ||$ a distance metric and margin $\\epsilon $. Margin $\\epsilon $ ensures that $s_p$ is at least $\\epsilon $ closer to $s_a$ than $s_n$. As metric we use Euclidean distance and we set $\\epsilon =1$ in our experiments."
                    ],
                    "highlighted_evidence": [
                        "Model\nSBERT adds a pooling operation to the output of BERT / RoBERTa to derive a fixed sized sentence embedding. We experiment with three pooling strategies: Using the output of the CLS-token, computing the mean of all output vectors (MEAN-strategy), and computing a max-over-time of the output vectors (MAX-strategy). The default configuration is MEAN.\n\nIn order to fine-tune BERT / RoBERTa, we create siamese and triplet networks BIBREF15 to update the weights such that the produced sentence embeddings are semantically meaningful and can be compared with cosine-similarity.\n\nThe network structure depends on the available training data. We experiment with the following structures and objective functions.\n\nClassification Objective Function. We concatenate the sentence embeddings $u$ and $v$ with the element-wise difference $|u-v|$ and multiply it with the trainable weight $W_t \\in \\mathbb {R}^{3n \\times k}$:\n\nwhere $n$ is the dimension of the sentence embeddings and $k$ the number of labels. We optimize cross-entropy loss. This structure is depicted in Figure FIGREF4.\n\nRegression Objective Function. The cosine-similarity between the two sentence embeddings $u$ and $v$ is computed (Figure FIGREF5). We use mean-squared-error loss as the objective function.\n\nTriplet Objective Function. Given an anchor sentence $a$, a positive sentence $p$, and a negative sentence $n$, triplet loss tunes the network such that the distance between $a$ and $p$ is smaller than the distance between $a$ and $n$. Mathematically, we minimize the following loss function:\n\nwith $s_x$ the sentence embedding for $a$/$n$/$p$, $||\\cdot ||$ a distance metric and margin $\\epsilon $. Margin $\\epsilon $ ensures that $s_p$ is at least $\\epsilon $ closer to $s_a$ than $s_n$. As metric we use Euclidean distance and we set $\\epsilon =1$ in our experiments."
                    ]
                }
            ]
        }
    ],
    "1909.05016": [
        {
            "question": "What is novel in author's approach?",
            "answers": [
                {
                    "answer": "They use self-play learning , optimize the model for specific metrics, train separate models per user, use model  and response classification predictors, and filter the dataset to obtain higher quality training data.",
                    "type": "abstractive"
                }
            ],
            "q_uid": "2a6469f8f6bf16577b590732d30266fd2486a72e",
            "evidence": [
                {
                    "raw_evidence": [
                        "Our novelties include:",
                        "Using self-play learning for the neural response ranker (described in detail below).",
                        "Optimizing neural models for specific metrics (e.g. diversity, coherence) in our ensemble setup.",
                        "Training a separate dialog model for each user, personalizing our socialbot and making it more consistent.",
                        "Using a response classification predictor and a response classifier to predict and control aspects of responses such as sentiment, topic, offensiveness, diversity etc.",
                        "Using a model predictor to predict the best responding model, before the response candidates are generated, reducing computational expenses.",
                        "Using our entropy-based filtering technique to filter all dialog datasets, obtaining higher quality training data BIBREF3.",
                        "Building big, pre-trained, hierarchical BERT and GPT dialog models BIBREF6, BIBREF7, BIBREF8.",
                        "Constantly monitoring the user input through our automatic metrics, ensuring that the user stays engaged."
                    ],
                    "highlighted_evidence": [
                        "Our novelties include:\n\nUsing self-play learning for the neural response ranker (described in detail below).\n\nOptimizing neural models for specific metrics (e.g. diversity, coherence) in our ensemble setup.\n\nTraining a separate dialog model for each user, personalizing our socialbot and making it more consistent.\n\nUsing a response classification predictor and a response classifier to predict and control aspects of responses such as sentiment, topic, offensiveness, diversity etc.\n\nUsing a model predictor to predict the best responding model, before the response candidates are generated, reducing computational expenses.\n\nUsing our entropy-based filtering technique to filter all dialog datasets, obtaining higher quality training data BIBREF3.\n\nBuilding big, pre-trained, hierarchical BERT and GPT dialog models BIBREF6, BIBREF7, BIBREF8.\n\nConstantly monitoring the user input through our automatic metrics, ensuring that the user stays engaged."
                    ]
                }
            ]
        }
    ],
    "1909.00124": [
        {
            "question": "What is the dataset used to train the model?",
            "answers": [
                {
                    "answer": " movie sentence polarity dataset from BIBREF19, laptop and restaurant datasets collected from SemEval-201, we collected 2,000 reviews for each domain from the same review source",
                    "type": "extractive"
                }
            ],
            "q_uid": "c20b012ad31da46642c553ce462bc0aad56912db",
            "evidence": [
                {
                    "raw_evidence": [
                        "Clean-labeled Datasets. We use three clean labeled datasets. The first one is the movie sentence polarity dataset from BIBREF19. The other two datasets are laptop and restaurant datasets collected from SemEval-2016 . The former consists of laptop review sentences and the latter consists of restaurant review sentences. The original datasets (i.e., Laptop and Restaurant) were annotated with aspect polarity in each sentence. We used all sentences with only one polarity (positive or negative) for their aspects. That is, we only used sentences with aspects having the same sentiment label in each sentence. Thus, the sentiment of each aspect gives the ground-truth as the sentiments of all aspects are the same.",
                        "Noisy-labeled Training Datasets. For the above three domains (movie, laptop, and restaurant), we collected 2,000 reviews for each domain from the same review source. We extracted sentences from each review and assigned review's label to its sentences. Like previous work, we treat 4 or 5 stars as positive and 1 or 2 stars as negative. The data is noisy because a positive (negative) review can contain negative (positive) sentences, and there are also neutral sentences. This gives us three noisy-labeled training datasets. We still use the same test sets as those for the clean-labeled datasets. Summary statistics of all the datasets are shown in Table TABREF9."
                    ],
                    "highlighted_evidence": [
                        "Clean-labeled Datasets. We use three clean labeled datasets. The first one is the movie sentence polarity dataset from BIBREF19. The other two datasets are laptop and restaurant datasets collected from SemEval-2016 .",
                        "Noisy-labeled Training Datasets. For the above three domains (movie, laptop, and restaurant), we collected 2,000 reviews for each domain from the same review source."
                    ]
                }
            ]
        },
        {
            "question": "Is the model evaluated against a CNN baseline?",
            "answers": [
                {
                    "answer": "Yes",
                    "type": "boolean"
                }
            ],
            "q_uid": "89b9a2389166b992c42ca19939d750d88c5fa79b",
            "evidence": [
                {
                    "raw_evidence": [
                        "Baselines. We use one strong non-DNN baseline, NBSVM (with unigrams or bigrams features) BIBREF23 and six DNN baselines. The first DNN baseline is CNN BIBREF25, which does not handle noisy labels. The other five were designed to handle noisy labels.",
                        "The comparison results are shown in Table TABREF12. From the results, we can make the following observations. (1) Our NetAb model achieves the best ACC and F1 on all datasets except for F1 of negative class on Laptop. The results demonstrate the superiority of NetAb. (2) NetAb outperforms the baselines designed for learning with noisy labels. These baselines are inferior to ours as they were tailored for image classification. Note that we found no existing method to deal with noisy labels for SSC. Training Details. We use the publicly available pre-trained embedding GloVe.840B BIBREF48 to initialize the word vectors and the embedding dimension is 300."
                    ],
                    "highlighted_evidence": [
                        "Baselines. We use one strong non-DNN baseline, NBSVM (with unigrams or bigrams features) BIBREF23 and six DNN baselines. The first DNN baseline is CNN BIBREF25, which does not handle noisy labels. The other five were designed to handle noisy labels.\n\nThe comparison results are shown in Table TABREF12. From the results, we can make the following observations. (1) Our NetAb model achieves the best ACC and F1 on all datasets except for F1 of negative class on Laptop."
                    ]
                }
            ]
        }
    ],
    "1912.00819": [
        {
            "question": "How does the ensemble annotator extract the final label?",
            "answers": [
                {
                    "answer": "First preference is given to the labels that are perfectly matching in all the neural annotators., In case two out of three context models are correct, then it is being checked if that label is also produced by at least one of the non-context models., When we see that none of the context models is producing the same results, then we rank the labels with their respective confidence values produced as a probability distribution using the $softmax$ function. The labels are sorted in descending order according to confidence values. Then we check if the first three (case when one context model and both non-context models produce the same label) or at least two labels are matching, then we allow to pick that one. , Finally, when none the above conditions are fulfilled, we leave out the label with an unknown category.",
                    "type": "extractive"
                }
            ],
            "q_uid": "dcd6f18922ac5c00c22cef33c53ff5ae08b42298",
            "evidence": [
                {
                    "raw_evidence": [
                        "First preference is given to the labels that are perfectly matching in all the neural annotators. In Table TABREF11, we can see that both datasets have about 40% of exactly matching labels over all models (AM). Then priority is given to the context-based models to check if the label in all context models is matching perfectly. In case two out of three context models are correct, then it is being checked if that label is also produced by at least one of the non-context models. Then, we allow labels to rely on these at least two context models. As a result, about 47% of the labels are taken based on the context models (CM). When we see that none of the context models is producing the same results, then we rank the labels with their respective confidence values produced as a probability distribution using the $softmax$ function. The labels are sorted in descending order according to confidence values. Then we check if the first three (case when one context model and both non-context models produce the same label) or at least two labels are matching, then we allow to pick that one. There are about 3% in IEMOCAP and 5% in MELD (BM).",
                        "Finally, when none the above conditions are fulfilled, we leave out the label with an unknown category. This unknown category of the dialogue act is labeled with `xx' in the final annotations, and they are about 7% in IEMOCAP and 11% in MELD (NM). The statistics of the EDAs is reported in Table TABREF13 for both datasets. Total utterances in MELD includes training, validation and test datasets."
                    ],
                    "highlighted_evidence": [
                        "First preference is given to the labels that are perfectly matching in all the neural annotators. In Table TABREF11, we can see that both datasets have about 40% of exactly matching labels over all models (AM). Then priority is given to the context-based models to check if the label in all context models is matching perfectly. In case two out of three context models are correct, then it is being checked if that label is also produced by at least one of the non-context models. Then, we allow labels to rely on these at least two context models. As a result, about 47% of the labels are taken based on the context models (CM). When we see that none of the context models is producing the same results, then we rank the labels with their respective confidence values produced as a probability distribution using the $softmax$ function. The labels are sorted in descending order according to confidence values. Then we check if the first three (case when one context model and both non-context models produce the same label) or at least two labels are matching, then we allow to pick that one. There are about 3% in IEMOCAP and 5% in MELD (BM).\n\nFinally, when none the above conditions are fulfilled, we leave out the label with an unknown category. This unknown category of the dialogue act is labeled with `xx' in the final annotations, and they are about 7% in IEMOCAP and 11% in MELD (NM)."
                    ]
                }
            ]
        },
        {
            "question": "How were dialogue act labels defined?",
            "answers": [
                {
                    "answer": "Dialogue Act Markup in Several Layers (DAMSL) tag set",
                    "type": "extractive"
                }
            ],
            "q_uid": "2965c86467d12b79abc16e1457d848cb6ca88973",
            "evidence": [
                {
                    "raw_evidence": [
                        "There have been many taxonomies for dialogue acts: speech acts BIBREF14 refer to the utterance, not only to present information but to the action at is performed. Speech acts were later modified into five classes (Assertive, Directive, Commissive, Expressive, Declarative) BIBREF15. There are many such standard taxonomies and schemes to annotate conversational data, and most of them follow the discourse compositionality. These schemes have proven their importance for discourse or conversational analysis BIBREF16. During the increased development of dialogue systems and discourse analysis, the standard taxonomy was introduced in recent decades, called Dialogue Act Markup in Several Layers (DAMSL) tag set. According to DAMSL, each DA has a forward-looking function (such as Statement, Info-request, Thanking) and a backwards-looking function (such as Accept, Reject, Answer) BIBREF17."
                    ],
                    "highlighted_evidence": [
                        "There have been many taxonomies for dialogue acts: speech acts BIBREF14 refer to the utterance, not only to present information but to the action at is performed. Speech acts were later modified into five classes (Assertive, Directive, Commissive, Expressive, Declarative) BIBREF15. There are many such standard taxonomies and schemes to annotate conversational data, and most of them follow the discourse compositionality. These schemes have proven their importance for discourse or conversational analysis BIBREF16. During the increased development of dialogue systems and discourse analysis, the standard taxonomy was introduced in recent decades, called Dialogue Act Markup in Several Layers (DAMSL) tag set. According to DAMSL, each DA has a forward-looking function (such as Statement, Info-request, Thanking) and a backwards-looking function (such as Accept, Reject, Answer) BIBREF17."
                    ]
                }
            ]
        },
        {
            "question": "How many models were used?",
            "answers": [
                {
                    "answer": "five",
                    "type": "extractive"
                }
            ],
            "q_uid": "b99948ac4810a7fe3477f6591b8cf211d6398e67",
            "evidence": [
                {
                    "raw_evidence": [
                        "In this work, we apply an automated neural ensemble annotation process for dialogue act labeling. Several neural models are trained with the Switchboard Dialogue Act (SwDA) Corpus BIBREF9, BIBREF10 and used for inferring dialogue acts on the emotion datasets. We ensemble five model output labels by checking majority occurrences (most of the model labels are the same) and ranking confidence values of the models. We have annotated two potential multi-modal conversation datasets for emotion recognition: IEMOCAP (Interactive Emotional dyadic MOtion CAPture database) BIBREF6 and MELD (Multimodal EmotionLines Dataset) BIBREF8. Figure FIGREF2, shows an example of dialogue acts with emotion and sentiment labels from the MELD dataset. We confirmed the reliability of annotations with inter-annotator metrics. We analysed the co-occurrences of the dialogue act and emotion labels and discovered a key relationship between them; certain dialogue acts of the utterances show significant and useful association with respective emotional states. For example, Accept/Agree dialogue act often occurs with the Joy emotion while Reject with Anger, Acknowledgements with Surprise, Thanking with Joy, and Apology with Sadness, etc. The detailed analysis of the emotional dialogue acts (EDAs) and annotated datasets are being made available at the SECURE EU Project website.",
                        "We adopted the neural architectures based on Bothe et al. bothe2018discourse where two variants are: non-context model (classifying at utterance level) and context model (recognizing the dialogue act of the current utterance given a few preceding utterances). From conversational analysis using dialogue acts in Bothe et al. bothe2018interspeech, we learned that the preceding two utterances contribute significantly to recognizing the dialogue act of the current utterance. Hence, we adapt this setting for the context model and create a pool of annotators using recurrent neural networks (RNNs). RNNs can model the contextual information in the sequence of words of an utterance and in the sequence of utterances of a dialogue. Each word in an utterance is represented with a word embedding vector of dimension 1024. We use the word embedding vectors from pre-trained ELMo (Embeddings from Language Models) embeddings BIBREF22. We have a pool of five neural annotators as shown in Figure FIGREF6. Our online tool called Discourse-Wizard is available to practice automated dialogue act labeling. In this tool we use the same neural architectures but model-trained embeddings (while, in this work we use pre-trained ELMo embeddings, as they are better performant but computationally and size-wise expensive to be hosted in the online tool). The annotators are:"
                    ],
                    "highlighted_evidence": [
                        "n this work, we apply an automated neural ensemble annotation process for dialogue act labeling. Several neural models are trained with the Switchboard Dialogue Act (SwDA) Corpus BIBREF9, BIBREF10 and used for inferring dialogue acts on the emotion datasets. We ensemble five model output labels by checking majority occurrences (most of the model labels are the same) and ranking confidence values of the models.",
                        "We adopted the neural architectures based on Bothe et al. bothe2018discourse where two variants are: non-context model (classifying at utterance level) and context model (recognizing the dialogue act of the current utterance given a few preceding utterances)."
                    ]
                }
            ]
        }
    ],
    "2002.02070": [
        {
            "question": "Is car-speak language collection of abstract features that classifier is later trained on?",
            "answers": [
                {
                    "answer": "No",
                    "type": "boolean"
                },
                {
                    "answer": "No",
                    "type": "boolean"
                }
            ],
            "q_uid": "25c1c4a91f5dedd4e06d14121af3b5921db125e9",
            "evidence": [
                {
                    "raw_evidence": [
                        "The term \u201cfast\u201d is car-speak. Car-speak is abstract language that pertains to a car's physical attribute(s). In this instance the physical attributes that the term \u201cfast\u201d pertains to could be the horsepower, or it could be the car's form factor (how the car looks). However, we do not know exactly which attributes the term \u201cfast\u201d refers to.",
                        "We train a series of classifiers in order to classify car-speak. We train three classifiers on the review vectors that we prepared in Section SECREF8. The classifiers we use are K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP) BIBREF13."
                    ],
                    "highlighted_evidence": [
                        "Car-speak is abstract language that pertains to a car's physical attribute(s). In this instance the physical attributes that the term \u201cfast\u201d pertains to could be the horsepower, or it could be the car's form factor (how the car looks). However, we do not know exactly which attributes the term \u201cfast\u201d refers to.",
                        "We train a series of classifiers in order to classify car-speak. We train three classifiers on the review vectors that we prepared in Section SECREF8. The classifiers we use are K Nearest Neighbors (KNN), Random Forest (RF), Support Vector Machine (SVM), and Multi-layer Perceptron (MLP) BIBREF13."
                    ]
                },
                {
                    "raw_evidence": [
                        "The term \u201cfast\u201d is car-speak. Car-speak is abstract language that pertains to a car's physical attribute(s). In this instance the physical attributes that the term \u201cfast\u201d pertains to could be the horsepower, or it could be the car's form factor (how the car looks). However, we do not know exactly which attributes the term \u201cfast\u201d refers to.",
                        "Our data set contains $3,209$ reviews about 553 different cars from 49 different car manufacturers. In order to accomplish our goal of translating and classifying car-speak we need to filter our data set so that we only have the most relevant terms. We then need to be able to weight each word in each review, so that we can determine the most relevant ideas in each document for the purpose of classification. Finally, we need to train various classification models and evaluate them."
                    ],
                    "highlighted_evidence": [
                        "The term \u201cfast\u201d is car-speak. Car-speak is abstract language that pertains to a car's physical attribute(s). In this instance the physical attributes that the term \u201cfast\u201d pertains to could be the horsepower, or it could be the car's form factor (how the car looks). However, we do not know exactly which attributes the term \u201cfast\u201d refers to.",
                        "In order to accomplish our goal of translating and classifying car-speak we need to filter our data set so that we only have the most relevant terms. We then need to be able to weight each word in each review, so that we can determine the most relevant ideas in each document for the purpose of classification. Finally, we need to train various classification models and evaluate them."
                    ]
                }
            ]
        },
        {
            "question": "Is order of \"words\" important in car speak language?",
            "answers": [
                {
                    "answer": "No",
                    "type": "boolean"
                }
            ],
            "q_uid": "f88036174b4a0dbf4fe70ddad884d16082c5748d",
            "evidence": [
                {
                    "raw_evidence": [
                        "We would like to be able to represent each car with the most relevant car-speak terms. We can do this by filtering each review using the NLTK library BIBREF8, only retaining the most relevant words. First we token-ize each review and then keep only the nouns and adjectives from each review since they are the most salient parts of speech BIBREF9. This leaves us with $10,867$ words across all reviews. Figure FIGREF6 shows the frequency of the top 20 words that remain.",
                        "So far we have compiled the most relevant terms in from the reviews. We now need to weight these terms for each review, so that we know the car-speak terms are most associated with a car. Using TF-IDF (Term Frequency-Inverse Document Frequency) has been used as a reliable metric for finding the relevant terms in a document BIBREF10."
                    ],
                    "highlighted_evidence": [
                        " We can do this by filtering each review using the NLTK library BIBREF8, only retaining the most relevant words. First we token-ize each review and then keep only the nouns and adjectives from each review since they are the most salient parts of speech BIBREF9. This leaves us with $10,867$ words across all reviews.",
                        "Using TF-IDF (Term Frequency-Inverse Document Frequency) has been used as a reliable metric for finding the relevant terms in a document BIBREF10."
                    ]
                }
            ]
        },
        {
            "question": "What are labels in car speak language dataset?",
            "answers": [
                {
                    "answer": "car ",
                    "type": "extractive"
                },
                {
                    "answer": "the car",
                    "type": "extractive"
                }
            ],
            "q_uid": "a267d620af319b48e56c191aa4c433ea3870f6fb",
            "evidence": [
                {
                    "raw_evidence": [
                        "We represent each review as a vector of TF-IDF scores for each word in the review. The length of this vector is $10,867$. We label each review vector with the car it reviews. We ignore the year of the car being reviewed and focus specifically on the model (i.e Acura ILX, not 2013 Acura ILX). This is because there a single model of car generally retains the same characteristics over time BIBREF11, BIBREF12."
                    ],
                    "highlighted_evidence": [
                        "We label each review vector with the car it reviews. "
                    ]
                },
                {
                    "raw_evidence": [
                        "Our data set contains $3,209$ reviews about 553 different cars from 49 different car manufacturers. In order to accomplish our goal of translating and classifying car-speak we need to filter our data set so that we only have the most relevant terms. We then need to be able to weight each word in each review, so that we can determine the most relevant ideas in each document for the purpose of classification. Finally, we need to train various classification models and evaluate them.",
                        "We represent each review as a vector of TF-IDF scores for each word in the review. The length of this vector is $10,867$. We label each review vector with the car it reviews. We ignore the year of the car being reviewed and focus specifically on the model (i.e Acura ILX, not 2013 Acura ILX). This is because there a single model of car generally retains the same characteristics over time BIBREF11, BIBREF12."
                    ],
                    "highlighted_evidence": [
                        "Our data set contains $3,209$ reviews about 553 different cars from 49 different car manufacturers. In order to accomplish our goal of translating and classifying car-speak we need to filter our data set so that we only have the most relevant terms. We then need to be able to weight each word in each review, so that we can determine the most relevant ideas in each document for the purpose of classification.",
                        "We represent each review as a vector of TF-IDF scores for each word in the review. The length of this vector is $10,867$. We label each review vector with the car it reviews. We ignore the year of the car being reviewed and focus specifically on the model (i.e Acura ILX, not 2013 Acura ILX). "
                    ]
                }
            ]
        },
        {
            "question": "How big is dataset of car-speak language?",
            "answers": [
                {
                    "answer": "$3,209$ reviews ",
                    "type": "extractive"
                },
                {
                    "answer": "$3,209$ reviews about 553 different cars from 49 different car manufacturers",
                    "type": "extractive"
                }
            ],
            "q_uid": "899ed05c460bf2aa0aa65101cad1986d4f622652",
            "evidence": [
                {
                    "raw_evidence": [
                        "Our data set contains $3,209$ reviews about 553 different cars from 49 different car manufacturers. In order to accomplish our goal of translating and classifying car-speak we need to filter our data set so that we only have the most relevant terms. We then need to be able to weight each word in each review, so that we can determine the most relevant ideas in each document for the purpose of classification. Finally, we need to train various classification models and evaluate them."
                    ],
                    "highlighted_evidence": [
                        "Our data set contains $3,209$ reviews about 553 different cars from 49 different car manufacturers."
                    ]
                },
                {
                    "raw_evidence": [
                        "Our data set contains $3,209$ reviews about 553 different cars from 49 different car manufacturers. In order to accomplish our goal of translating and classifying car-speak we need to filter our data set so that we only have the most relevant terms. We then need to be able to weight each word in each review, so that we can determine the most relevant ideas in each document for the purpose of classification. Finally, we need to train various classification models and evaluate them."
                    ],
                    "highlighted_evidence": [
                        "Our data set contains $3,209$ reviews about 553 different cars from 49 different car manufacturers. In order to accomplish our goal of translating and classifying car-speak we need to filter our data set so that we only have the most relevant terms."
                    ]
                }
            ]
        },
        {
            "question": "How does car speak pertains to a car's physical attributes?",
            "answers": [
                {
                    "answer": "we do not know exactly",
                    "type": "extractive"
                }
            ],
            "q_uid": "6bf93968110c6e3e3640360440607744007a5228",
            "evidence": [
                {
                    "raw_evidence": [
                        "The term \u201cfast\u201d is car-speak. Car-speak is abstract language that pertains to a car's physical attribute(s). In this instance the physical attributes that the term \u201cfast\u201d pertains to could be the horsepower, or it could be the car's form factor (how the car looks). However, we do not know exactly which attributes the term \u201cfast\u201d refers to."
                    ],
                    "highlighted_evidence": [
                        "Car-speak is abstract language that pertains to a car's physical attribute(s). In this instance the physical attributes that the term \u201cfast\u201d pertains to could be the horsepower, or it could be the car's form factor (how the car looks). However, we do not know exactly which attributes the term \u201cfast\u201d refers to."
                    ]
                }
            ]
        }
    ],
    "1904.02357": [
        {
            "question": "How is human interaction consumed by the model?",
            "answers": [
                {
                    "answer": "displays three different versions of a story written by three distinct models for a human to compare, human can select the model to interact with (potentially after having chosen it via cross-model), and can collaborate at all stages",
                    "type": "extractive"
                }
            ],
            "q_uid": "5c70fdd3d6b67031768d3e28336942e49bf9a500",
            "evidence": [
                {
                    "raw_evidence": [
                        "gordon2009sayanything use an information retrieval based system to write by alternating turns between a human and their system. clark2018mil use a similar turn-taking approach to interactivity, but employ a neural model for generation and allow the user to edit the generated sentence before accepting it. They find that users prefer a full-sentence collaborative setup (vs. shorter fragments) but are mixed with regard to the system-driven approach to interaction. roemmele2017eval experiment with a user-driven setup, where the machine doesn't generate until the user requests it to, and then the user can edit or delete at will. They leverage user-acceptance or rejection of suggestions as a tool for understanding the characteristics of a helpful generation. All of these systems involve the user in the story-writing process, but lack user involvement in the story-planning process, and so they lean on the user's ability to knit a coherent overall story together out of locally related sentences. They also do not allow a user to control the novelty or \u201cunexpectedness\u201d of the generations, which clark2018mil find to be a weakness. Nor do they enable iteration; a user cannot revise earlier sentences and have the system update later generations. We develop a system that allows a user to interact in all of these ways that were limitations in previous systems; it enables involvement in planning, editing, iterative revising, and control of novelty. We conduct experiments to understand which types of interaction are most effective for improving stories and for making users satisfied and engaged. We have two main interfaces that enable human interaction with the computer. There is cross-model interaction, where the machine does all the composition work, and displays three different versions of a story written by three distinct models for a human to compare. The user guides generation by providing a topic for story-writing and by tweaking decoding parameters to control novelty, or diversity. The second interface is intra-model interaction, where a human can select the model to interact with (potentially after having chosen it via cross-model), and can collaborate at all stages to jointly create better stories. The full range of interactions available to a user is: select a model, provide a topic, change diversity of content, collaborate on the planning for the story, and collaborate on the story sentences. It is entirely user-driven, as the users control how much is their own work and how much is the machine's at every stage. It supports revision; a user can modify an earlier part of a written story or of the story plan at any point, and observe how this affects later generations."
                    ],
                    "highlighted_evidence": [
                        "We have two main interfaces that enable human interaction with the computer. There is cross-model interaction, where the machine does all the composition work, and displays three different versions of a story written by three distinct models for a human to compare. The user guides generation by providing a topic for story-writing and by tweaking decoding parameters to control novelty, or diversity. The second interface is intra-model interaction, where a human can select the model to interact with (potentially after having chosen it via cross-model), and can collaborate at all stages to jointly create better stories."
                    ]
                }
            ]
        },
        {
            "question": "How do they evaluate generated stories?",
            "answers": [
                {
                    "answer": "separate set of Turkers to rate the stories for overall quality and the three improvement areas",
                    "type": "extractive"
                }
            ],
            "q_uid": "f27502c3ece9ade265389d5ace90ca9ca42b46f3",
            "evidence": [
                {
                    "raw_evidence": [
                        "We then ask a separate set of Turkers to rate the stories for overall quality and the three improvement areas. All ratings are on a five-point scale. We collect two ratings per story, and throw out ratings that disagree by more than 2 points. A total of 11% of ratings were thrown out, leaving four metrics across 241 stories for analysis."
                    ],
                    "highlighted_evidence": [
                        "We then ask a separate set of Turkers to rate the stories for overall quality and the three improvement areas. All ratings are on a five-point scale. We collect two ratings per story, and throw out ratings that disagree by more than 2 points. A total of 11% of ratings were thrown out, leaving four metrics across 241 stories for analysis."
                    ]
                }
            ]
        },
        {
            "question": "What are the baselines?",
            "answers": [
                {
                    "answer": "Title-to-Story system",
                    "type": "extractive"
                }
            ],
            "q_uid": "aa4b38f601cc87bf93849245d5f65124da3dc112",
            "evidence": [
                {
                    "raw_evidence": [
                        "The Title-to-Story system is a baseline, which generates directly from topic."
                    ],
                    "highlighted_evidence": [
                        "The Title-to-Story system is a baseline, which generates directly from topic."
                    ]
                }
            ]
        }
    ],
    "1909.12642": [
        {
            "question": "What is the performance of the model for the German sub-task A?",
            "answers": [
                {
                    "answer": "macro F1 score of 0.62",
                    "type": "extractive"
                }
            ],
            "q_uid": "d10e256f2f724ad611fd3ff82ce88f7a78bad7f7",
            "evidence": [
                {
                    "raw_evidence": [
                        "The performance of our models across different languages for sub-task A are shown in table TABREF19. Our model got the first position in the German sub-task with a macro F1 score of 0.62. The results of sub-task B and sub-task C is shown in table TABREF20 and TABREF21 respectively."
                    ],
                    "highlighted_evidence": [
                        "Our model got the first position in the German sub-task with a macro F1 score of 0.62."
                    ]
                }
            ]
        },
        {
            "question": "Is the model tested for language identification?",
            "answers": [
                {
                    "answer": "No",
                    "type": "boolean"
                }
            ],
            "q_uid": "c691b47c0380c9529e34e8ca6c1805f98288affa",
            "evidence": [
                {
                    "raw_evidence": [],
                    "highlighted_evidence": []
                }
            ]
        },
        {
            "question": "Is the model compared to a baseline model?",
            "answers": [
                {
                    "answer": "No",
                    "type": "boolean"
                }
            ],
            "q_uid": "892e42137b14d9fabd34084b3016cf3f12cac68a",
            "evidence": [
                {
                    "raw_evidence": [],
                    "highlighted_evidence": []
                }
            ]
        },
        {
            "question": "What are the languages used to test the model?",
            "answers": [
                {
                    "answer": "Hindi, English and German (German task won)",
                    "type": "abstractive"
                }
            ],
            "q_uid": "dc69256bdfe76fa30ce4404b697f1bedfd6125fe",
            "evidence": [
                {
                    "raw_evidence": [
                        "The dataset at HASOC 2019 were given in three languages: Hindi, English, and German. Dataset in Hindi and English had three subtasks each, while German had only two subtasks. We participated in all the tasks provided by the organisers and decided to develop a single model that would be language agnostic. We used the same model architecture for all the three languages.",
                        "In the results of subtask A, models are mainly affected by imbalance of the dataset. The training dataset of Hindi dataset was more balanced than English or German dataset. Hence, the results were around 0.78. As the dataset in German language was highly imbalanced, the results drops to 0.62. In subtask B, the highest F1 score reached was by the profane class for each language in table TABREF20. The model got confused between OFFN, HATE and PRFN labels which suggests that these models are not able to capture the context in the sentence. The subtask C was again a case of imbalanced dataset as targeted(TIN) label gets the highest F1 score in table TABREF21."
                    ],
                    "highlighted_evidence": [
                        "The dataset at HASOC 2019 were given in three languages: Hindi, English, and German. Dataset in Hindi and English had three subtasks each, while German had only two subtasks. We participated in all the tasks provided by the organisers and decided to develop a single model that would be language agnostic. We used the same model architecture for all the three languages.",
                        "The training dataset of Hindi dataset was more balanced than English or German dataset. Hence, the results were around 0.78. As the dataset in German language was highly imbalanced, the results drops to 0.62."
                    ]
                }
            ]
        }
    ],
    "1603.04553": [
        {
            "question": "Are resolution mode variables hand crafted?",
            "answers": [
                {
                    "answer": "No",
                    "type": "boolean"
                }
            ],
            "q_uid": "80de3baf97a55ea33e0fe0cafa6f6221ba347d0a",
            "evidence": [
                {
                    "raw_evidence": [
                        "According to previous work BIBREF17 , BIBREF18 , BIBREF1 , antecedents are resolved by different categories of information for different mentions. For example, the Stanford system BIBREF1 uses string-matching sieves to link two mentions with similar text and precise-construct sieve to link two mentions which satisfy special syntactic or semantic relations such as apposition or acronym. Motivated by this, we introduce resolution mode variables $\\Pi = \\lbrace \\pi _1, \\ldots , \\pi _n\\rbrace $ , where for each mention $j$ the variable $\\pi _j \\in \\lbrace str, prec, attr\\rbrace $ indicates in which mode the mention should be resolved. In our model, we define three resolution modes \u2014 string-matching (str), precise-construct (prec), and attribute-matching (attr) \u2014 and $\\Pi $ is deterministic when $D$ is given (i.e. $P(\\Pi |D)$ is a point distribution). We determine $\\pi _j$ for each mention $m_j$ in the following way:",
                        "$\\pi _j = str$ , if there exists a mention $m_i, i < j$ such that the two mentions satisfy the String Match sieve, the Relaxed String Match sieve, or the Strict Head Match A sieve in the Stanford multi-sieve system BIBREF1 .",
                        "$\\pi _j = prec$ , if there exists a mention $m_i, i < j$ such that the two mentions satisfy the Speaker Identification sieve, or the Precise Constructs sieve.",
                        "$\\pi _j = attr$ , if there is no mention $m_i, i < j$ satisfies the above two conditions."
                    ],
                    "highlighted_evidence": [
                        "Motivated by this, we introduce resolution mode variables $\\Pi = \\lbrace \\pi _1, \\ldots , \\pi _n\\rbrace $ , where for each mention $j$ the variable $\\pi _j \\in \\lbrace str, prec, attr\\rbrace $ indicates in which mode the mention should be resolved. In our model, we define three resolution modes \u2014 string-matching (str), precise-construct (prec), and attribute-matching (attr) \u2014 and $\\Pi $ is deterministic when $D$ is given (i.e. $P(\\Pi |D)$ is a point distribution). We determine $\\pi _j$ for each mention $m_j$ in the following way:\n\n$\\pi _j = str$ , if there exists a mention $m_i, i < j$ such that the two mentions satisfy the String Match sieve, the Relaxed String Match sieve, or the Strict Head Match A sieve in the Stanford multi-sieve system BIBREF1 .\n\n$\\pi _j = prec$ , if there exists a mention $m_i, i < j$ such that the two mentions satisfy the Speaker Identification sieve, or the Precise Constructs sieve.\n\n$\\pi _j = attr$ , if there is no mention $m_i, i < j$ satisfies the above two conditions."
                    ]
                }
            ]
        },
        {
            "question": "What are resolution model variables?",
            "answers": [
                {
                    "answer": "Variables in the set {str, prec, attr} indicating in which mode the mention should be resolved.",
                    "type": "abstractive"
                }
            ],
            "q_uid": "f5707610dc8ae2a3dc23aec63d4afa4b40b7ec1e",
            "evidence": [
                {
                    "raw_evidence": [
                        "According to previous work BIBREF17 , BIBREF18 , BIBREF1 , antecedents are resolved by different categories of information for different mentions. For example, the Stanford system BIBREF1 uses string-matching sieves to link two mentions with similar text and precise-construct sieve to link two mentions which satisfy special syntactic or semantic relations such as apposition or acronym. Motivated by this, we introduce resolution mode variables $\\Pi = \\lbrace \\pi _1, \\ldots , \\pi _n\\rbrace $ , where for each mention $j$ the variable $\\pi _j \\in \\lbrace str, prec, attr\\rbrace $ indicates in which mode the mention should be resolved. In our model, we define three resolution modes \u2014 string-matching (str), precise-construct (prec), and attribute-matching (attr) \u2014 and $\\Pi $ is deterministic when $D$ is given (i.e. $P(\\Pi |D)$ is a point distribution). We determine $\\pi _j$ for each mention $m_j$ in the following way:",
                        "$\\pi _j = str$ , if there exists a mention $m_i, i < j$ such that the two mentions satisfy the String Match sieve, the Relaxed String Match sieve, or the Strict Head Match A sieve in the Stanford multi-sieve system BIBREF1 .",
                        "$\\pi _j = prec$ , if there exists a mention $m_i, i < j$ such that the two mentions satisfy the Speaker Identification sieve, or the Precise Constructs sieve.",
                        "$\\pi _j = attr$ , if there is no mention $m_i, i < j$ satisfies the above two conditions."
                    ],
                    "highlighted_evidence": [
                        "Motivated by this, we introduce resolution mode variables $\\Pi = \\lbrace \\pi _1, \\ldots , \\pi _n\\rbrace $ , where for each mention $j$ the variable $\\pi _j \\in \\lbrace str, prec, attr\\rbrace $ indicates in which mode the mention should be resolved. In our model, we define three resolution modes \u2014 string-matching (str), precise-construct (prec), and attribute-matching (attr) \u2014 and $\\Pi $ is deterministic when $D$ is given (i.e. $P(\\Pi |D)$ is a point distribution). We determine $\\pi _j$ for each mention $m_j$ in the following way:\n\n$\\pi _j = str$ , if there exists a mention $m_i, i < j$ such that the two mentions satisfy the String Match sieve, the Relaxed String Match sieve, or the Strict Head Match A sieve in the Stanford multi-sieve system BIBREF1 .\n\n$\\pi _j = prec$ , if there exists a mention $m_i, i < j$ such that the two mentions satisfy the Speaker Identification sieve, or the Precise Constructs sieve.\n\n$\\pi _j = attr$ , if there is no mention $m_i, i < j$ satisfies the above two conditions."
                    ]
                }
            ]
        },
        {
            "question": "Is the model presented in the paper state of the art?",
            "answers": [
                {
                    "answer": "No, supervised models perform better for this task.",
                    "type": "abstractive"
                }
            ],
            "q_uid": "e76139c63da0f861c097466983fbe0c94d1d9810",
            "evidence": [
                {
                    "raw_evidence": [
                        "To make a thorough empirical comparison with previous studies, Table 3 (below the dashed line) also shows the results of some state-of-the-art supervised coreference resolution systems \u2014 IMS: the second best system in the CoNLL 2012 shared task BIBREF28 ; Latent-Tree: the latent tree model BIBREF29 obtaining the best results in the shared task; Berkeley: the Berkeley system with the final feature set BIBREF12 ; LaSO: the structured perceptron system with non-local features BIBREF30 ; Latent-Strc: the latent structure system BIBREF31 ; Model-Stack: the entity-centric system with model stacking BIBREF32 ; and Non-Linear: the non-linear mention-ranking model with feature representations BIBREF33 . Our unsupervised ranking model outperforms the supervised IMS system by 1.02% on the CoNLL F1 score, and achieves competitive performance with the latent tree model. Moreover, our approach considerably narrows the gap to other supervised systems listed in Table 3 ."
                    ],
                    "highlighted_evidence": [
                        "Our unsupervised ranking model outperforms the supervised IMS system by 1.02% on the CoNLL F1 score, and achieves competitive performance with the latent tree model. Moreover, our approach considerably narrows the gap to other supervised systems listed in Table 3 ."
                    ]
                }
            ]
        }
    ],
    "2002.09637": [
        {
            "question": "Is the proposed method compared to previous methods?",
            "answers": [
                {
                    "answer": "Yes",
                    "type": "boolean"
                }
            ],
            "q_uid": "0bd992a6a218331aa771d922e3c7bb60b653949a",
            "evidence": [
                {
                    "raw_evidence": [
                        "Following the algorithms above, with the consideration of both the advantages and disadvantages of them, in this project, I am going to use a modified method: sound-class based skip-grams with bipartite networks (BipSkip). The whole procedure is quite straightforward and could be divided into three steps. First step: the word pair and their skip-grams are two sets of the bipartite networks. The second step is optional, which is to refine the bipartite network. Before I run the program, I will be asked to input a threshold, which determines if the program should delete the skip-gram nodes linked to fewer word nodes than the threshold itself. According to the experiment, even though I did not input any threshold as one of the parameters, the algorithm could still give the same answer but with more executing time. In the last step, the final generated bipartite graph would be connected to a monopartite graph and partitioned into cognate sets with the help of graph partitioning algorithms. Here I would use Informap algorithm BIBREF12. To make a comparison to this method, I am using CCM and SCA for distance measurement in this experiment, too. UPGMA algorithm would be used accordingly in these two cases."
                    ],
                    "highlighted_evidence": [
                        "Here I would use Informap algorithm BIBREF12. To make a comparison to this method, I am using CCM and SCA for distance measurement in this experiment, too. UPGMA algorithm would be used accordingly in these two cases."
                    ]
                }
            ]
        }
    ],
    "2003.06279": [
        {
            "question": "What other natural processing tasks authors think could be studied by using word embeddings?",
            "answers": [
                {
                    "answer": "general classification tasks, use of the methodology in other networked systems, a network could be enriched with embeddings obtained from graph embeddings techniques",
                    "type": "extractive"
                }
            ],
            "q_uid": "ec8043290356fcb871c2f5d752a9fe93a94c2f71",
            "evidence": [
                {
                    "raw_evidence": [
                        "Our findings paves the way for research in several new directions. While we probed the effectiveness of virtual edges in a specific text classification task, we could extend this approach for general classification tasks. A systematic comparison of embeddings techniques could also be performed to include other recent techniques BIBREF54, BIBREF55. We could also identify other relevant techniques to create virtual edges, allowing thus the use of the methodology in other networked systems other than texts. For example, a network could be enriched with embeddings obtained from graph embeddings techniques. A simpler approach could also consider link prediction BIBREF56 to create virtual edges. Finally, other interesting family of studies concerns the discrimination between co-occurrence and virtual edges, possibly by creating novel network measurements considering heterogeneous links."
                    ],
                    "highlighted_evidence": [
                        "Our findings paves the way for research in several new directions. While we probed the effectiveness of virtual edges in a specific text classification task, we could extend this approach for general classification tasks. A systematic comparison of embeddings techniques could also be performed to include other recent techniques BIBREF54, BIBREF55. We could also identify other relevant techniques to create virtual edges, allowing thus the use of the methodology in other networked systems other than texts. For example, a network could be enriched with embeddings obtained from graph embeddings techniques. A simpler approach could also consider link prediction BIBREF56 to create virtual edges. Finally, other interesting family of studies concerns the discrimination between co-occurrence and virtual edges, possibly by creating novel network measurements considering heterogeneous links."
                    ]
                }
            ]
        },
        {
            "question": "What is the reason that traditional co-occurrence networks fail in establishing links between similar words whenever they appear distant in the text?",
            "answers": [
                {
                    "answer": "long-range syntactical links, though less frequent than adjacent syntactical relationships, might be disregarded from a simple word adjacency approach",
                    "type": "extractive"
                }
            ],
            "q_uid": "728c2fb445173fe117154a2a5482079caa42fe24",
            "evidence": [
                {
                    "raw_evidence": [
                        "In a more practical scenario, text networks have been used in text classification tasks BIBREF8, BIBREF9, BIBREF10. The main advantage of the model is that it does not rely on deep semantical information to obtain competitive results. Another advantage of graph-based approaches is that, when combined with other approaches, it yields competitive results BIBREF11. A simple, yet recurrent text model is the well-known word co-occurrence network. After optional textual pre-processing steps, in a co-occurrence network each different word becomes a node and edges are established via co-occurrence in a desired window. A common strategy connects only adjacent words in the so called word adjacency networks.",
                        "While the co-occurrence representation yields good results in classification scenarios, some important features are not considered in the model. For example, long-range syntactical links, though less frequent than adjacent syntactical relationships, might be disregarded from a simple word adjacency approach BIBREF12. In addition, semantically similar words not sharing the same lemma are mapped into distinct nodes. In order to address these issues, here we introduce a modification of the traditional network representation by establishing additional edges, referred to as \u201cvirtual\u201d edges. In the proposed model, in addition to the co-occurrence edges, we link two nodes (words) if the corresponding word embedding representation is similar. While this approach still does not merge similar nodes into the same concept, similar nodes are explicitly linked via virtual edges."
                    ],
                    "highlighted_evidence": [
                        "A simple, yet recurrent text model is the well-known word co-occurrence network. After optional textual pre-processing steps, in a co-occurrence network each different word becomes a node and edges are established via co-occurrence in a desired window. A common strategy connects only adjacent words in the so called word adjacency networks.\n\nWhile the co-occurrence representation yields good results in classification scenarios, some important features are not considered in the model. For example, long-range syntactical links, though less frequent than adjacent syntactical relationships, might be disregarded from a simple word adjacency approach BIBREF12."
                    ]
                }
            ]
        },
        {
            "question": "Do the use word embeddings alone or they replace some previous features of the model with word embeddings?",
            "answers": [
                {
                    "answer": "They use it as addition to previous model - they add new edge between words if word embeddings are similar.",
                    "type": "abstractive"
                }
            ],
            "q_uid": "23d32666dfc29ed124f3aa4109e2527efa225fbc",
            "evidence": [
                {
                    "raw_evidence": [
                        "While the co-occurrence representation yields good results in classification scenarios, some important features are not considered in the model. For example, long-range syntactical links, though less frequent than adjacent syntactical relationships, might be disregarded from a simple word adjacency approach BIBREF12. In addition, semantically similar words not sharing the same lemma are mapped into distinct nodes. In order to address these issues, here we introduce a modification of the traditional network representation by establishing additional edges, referred to as \u201cvirtual\u201d edges. In the proposed model, in addition to the co-occurrence edges, we link two nodes (words) if the corresponding word embedding representation is similar. While this approach still does not merge similar nodes into the same concept, similar nodes are explicitly linked via virtual edges."
                    ],
                    "highlighted_evidence": [
                        "In the proposed model, in addition to the co-occurrence edges, we link two nodes (words) if the corresponding word embedding representation is similar."
                    ]
                }
            ]
        },
        {
            "question": "On what model architectures are previous co-occurence networks based?",
            "answers": [
                {
                    "answer": "in a co-occurrence network each different word becomes a node and edges are established via co-occurrence in a desired window, connects only adjacent words in the so called word adjacency networks",
                    "type": "extractive"
                }
            ],
            "q_uid": "076928bebde4dffcb404be216846d9d680310622",
            "evidence": [
                {
                    "raw_evidence": [
                        "In a more practical scenario, text networks have been used in text classification tasks BIBREF8, BIBREF9, BIBREF10. The main advantage of the model is that it does not rely on deep semantical information to obtain competitive results. Another advantage of graph-based approaches is that, when combined with other approaches, it yields competitive results BIBREF11. A simple, yet recurrent text model is the well-known word co-occurrence network. After optional textual pre-processing steps, in a co-occurrence network each different word becomes a node and edges are established via co-occurrence in a desired window. A common strategy connects only adjacent words in the so called word adjacency networks."
                    ],
                    "highlighted_evidence": [
                        "A simple, yet recurrent text model is the well-known word co-occurrence network. After optional textual pre-processing steps, in a co-occurrence network each different word becomes a node and edges are established via co-occurrence in a desired window. A common strategy connects only adjacent words in the so called word adjacency networks."
                    ]
                }
            ]
        }
    ],
    "1707.08559": [
        {
            "question": "What is the average length of the recordings?",
            "answers": [
                {
                    "answer": "40 minutes",
                    "type": "abstractive"
                }
            ],
            "q_uid": "e414d819f10c443cbefa8bdb9bd486ffc6d1fc6a",
            "evidence": [
                {
                    "raw_evidence": [
                        "Each game's video ranges from 30 to 50 minutes in length which contains image and chat data linked to the specific timestamp of the game. The average number of chats per video is 7490 with a standard deviation of 4922. The high value of standard deviation is mostly due to the fact that NALCS simultaneously broadcasts matches in two different channels (nalcs1 and nalcs2) which often leads to the majority of users watching the channel with a relatively more popular team causing an imbalance in the number of chats. If we only consider LMS which broadcasts with a single channel, the average number of chats are 7210 with standard deviation of 2719. The number of viewers for each game averages about 21526, and the number of unique users who type in chat is on average 2185, i.e., roughly 10% of the viewers."
                    ],
                    "highlighted_evidence": [
                        "Each game's video ranges from 30 to 50 minutes in length which contains image and chat data linked to the specific timestamp of the game."
                    ]
                }
            ]
        },
        {
            "question": "How big was the dataset presented?",
            "answers": [
                {
                    "answer": "321 videos",
                    "type": "extractive"
                }
            ],
            "q_uid": "2e73006e5d007aa08c62030a4d5a7e2e7e0eaf6c",
            "evidence": [
                {
                    "raw_evidence": [
                        "Our dataset covers 218 videos from NALCS and 103 from LMS for a total of 321 videos from week 1 to week 9 in 2017 spring series from each tournament. Each week there are 10 matches for NALCS and 6 matches for LMS. Matches are best of 3, so consist of two games or three games. The first and third games are used for training. The second games in the first 4 weeks are used as validation and the remainder of second games are used as test. Table TABREF3 lists the numbers of videos in train, validation, and test subsets."
                    ],
                    "highlighted_evidence": [
                        "Our dataset covers 218 videos from NALCS and 103 from LMS for a total of 321 videos from week 1 to week 9 in 2017 spring series from each tournament. "
                    ]
                }
            ]
        }
    ],
    "1910.08772": [
        {
            "question": "Do they beat current state-of-the-art on SICK?",
            "answers": [
                {
                    "answer": "No",
                    "type": "boolean"
                }
            ],
            "q_uid": "ee27e5b56e439546d710ce113c9be76e1bfa1a3d",
            "evidence": [
                {
                    "raw_evidence": [
                        "To show the effectiveness of our approach, we show results on the SICK dataset BIBREF1, a common benchmark for logic-based NLI, and find MonaLog to be competitive with more complicated logic-based approaches (many of which require full semantic parsing and more complex logical machinery). We also introduce a supplementary version of SICK that corrects several common annotation mistakes (e.g., asymmetrical inference annotations) based on previous work by kalouli2017entail,kalouli2018. Positive results on both these datasets show the ability of lightweight monotonicity models to handle many of the inferences found in current NLI datasets, hence putting a more reliable lower-bound on what results the simplest logical approach is capable of achieving on this benchmark."
                    ],
                    "highlighted_evidence": [
                        "To show the effectiveness of our approach, we show results on the SICK dataset BIBREF1, a common benchmark for logic-based NLI, and find MonaLog to be competitive with more complicated logic-based approaches (many of which require full semantic parsing and more complex logical machinery)."
                    ]
                }
            ]
        },
        {
            "question": "How do they combine MonaLog with BERT?",
            "answers": [
                {
                    "answer": "They use Monalog for data-augmentation to fine-tune BERT on this task",
                    "type": "abstractive"
                }
            ],
            "q_uid": "4688534a07a3cbd8afa738eea02cc6981a4fd285",
            "evidence": [
                {
                    "raw_evidence": [
                        "In this work, we introduce a new logical inference engine called MonaLog, which is based on natural logic and work on monotonicity stemming from vanBenthemEssays86. In contrast to the logical approaches cited above, our starting point is different in that we begin with the following two questions: 1) what is the simplest logical system that one can come up with to solve empirical NLI problems (i.e., the system with minimal amounts of primitives and background knowledge)?; and 2) what is the lower-bound performance of such a model? Like other approaches to natural logic BIBREF15, BIBREF16, our model works by reasoning over surface forms (as opposed to translating to symbolic representations) using a small inventory of monotonicity facts about quantifiers, lexical items and token-level polarity BIBREF17; proofs in the calculus are hence fully interpretable and expressible in ordinary language. Unlike existing work on natural logic, however, our model avoids the need for having expensive alignment and search sub-procedures BIBREF18, BIBREF19, and relies on a much smaller set of background knowledge and primitive relations than MacCartneyManning.",
                        "Since our logic operates over surface forms, it is straightforward to hybridize our models. We investigate using MonaLog in combination with the language model BERT BIBREF20, including for compositional data augmentation, i.e, re-generating entailed versions of examples in our training sets. To our knowledge, our approach is the first attempt to use monotonicity for data augmentation, and we show that such augmentation can generate high-quality training data with which models like BERT can improve performance.",
                        "We perform two experiments to test MonaLog. We first use MonaLog to solve the problems in a commonly used natural language inference dataset, SICK BIBREF1, comparing our results with previous systems. Second, we test the quality of the data generated by MonaLog. To do this, we generate more training data (sentence pairs) from the SICK training data using our system, and performe fine-tuning on BERT BIBREF20, a language model based on the transformer architecture BIBREF23, with the expanded dataset. In all experiments, we use the Base, Uncased model of BERT."
                    ],
                    "highlighted_evidence": [
                        "In this work, we introduce a new logical inference engine called MonaLog, which is based on natural logic and work on monotonicity stemming from vanBenthemEssays86.",
                        "Since our logic operates over surface forms, it is straightforward to hybridize our models. We investigate using MonaLog in combination with the language model BERT BIBREF20, including for compositional data augmentation, i.e, re-generating entailed versions of examples in our training sets. ",
                        "We perform two experiments to test MonaLog. We first use MonaLog to solve the problems in a commonly used natural language inference dataset, SICK BIBREF1, comparing our results with previous systems. Second, we test the quality of the data generated by MonaLog. To do this, we generate more training data (sentence pairs) from the SICK training data using our system, and performe fine-tuning on BERT BIBREF20, a language model based on the transformer architecture BIBREF23, with the expanded dataset. "
                    ]
                }
            ]
        },
        {
            "question": "How do they select monotonicity facts?",
            "answers": [
                {
                    "answer": "They derive it from Wordnet",
                    "type": "abstractive"
                }
            ],
            "q_uid": "45893f31ef07f0cca5783bd39c4e60630d6b93b3",
            "evidence": [
                {
                    "raw_evidence": [
                        "MonaLog utilizes two auxiliary sets. First, a knowledge base ${K}$ that stores the world knowledge needed for inference, e.g., semanticist $\\le $ linguist and swim $\\le $ move, which captures the facts that $[\\![\\mbox{\\em semanticist}]\\!]$ denotes a subset of $[\\![\\mbox{\\em linguist}]\\!]$, and that $[\\![\\mbox{\\em swim}]\\!]$ denotes a subset of $[\\![\\mbox{\\em move}]\\!]$, respectively. Such world knowledge can be created manually for the problem at hand, or derived easily from existing resources such as WordNet BIBREF22. Note that we do not blindly add all relations from WordNet to our knowledge base, since this would hinge heavily on word sense disambiguation (we need to know whether the \u201cbank\u201d is a financial institution or a river bank to extract its relations correctly). In the current implementation, we avoid this by adding x $\\le $ y or x $\\perp $ y relations only if both x and y are words in the premise-hypothesis pair. Additionally, some relations that involve quantifiers and prepositions need to be hard-coded, since WordNet does not include them: every $=$ all $=$ each $\\le $ most $\\le $ many $\\le $ a few $=$ several $\\le $ some $=$ a; the $\\le $ some $=$ a; on $\\perp $ off; up $\\perp $ down; etc."
                    ],
                    "highlighted_evidence": [
                        "MonaLog utilizes two auxiliary sets. First, a knowledge base ${K}$ that stores the world knowledge needed for inference, e.g., semanticist $\\le $ linguist and swim $\\le $ move, which captures the facts that $[\\![\\mbox{\\em semanticist}]\\!]$ denotes a subset of $[\\![\\mbox{\\em linguist}]\\!]$, and that $[\\![\\mbox{\\em swim}]\\!]$ denotes a subset of $[\\![\\mbox{\\em move}]\\!]$, respectively. Such world knowledge can be created manually for the problem at hand, or derived easily from existing resources such as WordNet BIBREF22. Note that we do not blindly add all relations from WordNet to our knowledge base, since this would hinge heavily on word sense disambiguation (we need to know whether the \u201cbank\u201d is a financial institution or a river bank to extract its relations correctly). In the current implementation, we avoid this by adding x $\\le $ y or x $\\perp $ y relations only if both x and y are words in the premise-hypothesis pair. Additionally, some relations that involve quantifiers and prepositions need to be hard-coded, since WordNet does not include them: every $=$ all $=$ each $\\le $ most $\\le $ many $\\le $ a few $=$ several $\\le $ some $=$ a; the $\\le $ some $=$ a; on $\\perp $ off; up $\\perp $ down; etc."
                    ]
                }
            ]
        }
    ],
    "1911.12579": [
        {
            "question": "How does proposed word embeddings compare to Sindhi fastText word representations?",
            "answers": [
                {
                    "answer": "Proposed SG model vs SINDHI FASTTEXT:\nAverage cosine similarity score: 0.650 vs 0.388\nAverage semantic relatedness similarity score between countries and their capitals: 0.663 vs 0.391",
                    "type": "abstractive"
                }
            ],
            "q_uid": "5b6aec1b88c9832075cd343f59158078a91f3597",
            "evidence": [
                {
                    "raw_evidence": [
                        "Generally, closer words are considered more important to a word\u2019s meaning. The word embeddings models have the ability to capture the lexical relations between words. Identifying such relationship that connects words is important in NLP applications. We measure that semantic relationship by calculating the dot product of two vectors using Eq. DISPLAY_FORM48. The high cosine similarity score denotes the closer words in the embedding matrix, while less cosine similarity score means the higher distance between word pairs. We present the cosine similarity score of different semantically or syntactically related word pairs taken from the vocabulary in Table TABREF77 along with English translation, which shows the average similarity of 0.632, 0.650, 0.591 yields by CBoW, SG and GloVe respectively. The SG model achieved a high average similarity score of 0.650 followed by CBoW with a 0.632 average similarity score. The GloVe also achieved a considerable average score of 0.591 respectively. However, the average similarity score of SdfastText is 0.388 and the word pair Microsoft-Bill Gates is not available in the vocabulary of SdfastText. This shows that along with performance, the vocabulary in SdfastText is also limited as compared to our proposed word embeddings.",
                        "Moreover, the average semantic relatedness similarity score between countries and their capitals is shown in Table TABREF78 with English translation, where SG also yields the best average score of 0.663 followed by CBoW with 0.611 similarity score. The GloVe also yields better semantic relatedness of 0.576 and the SdfastText yield an average score of 0.391. The first query word China-Beijing is not available the vocabulary of SdfastText. However, the similarity score between Afghanistan-Kabul is lower in our proposed CBoW, SG, GloVe models because the word Kabul is the name of the capital of Afghanistan as well as it frequently appears as an adjective in Sindhi text which means able."
                    ],
                    "highlighted_evidence": [
                        "The SG model achieved a high average similarity score of 0.650 followed by CBoW with a 0.632 average similarity score. The GloVe also achieved a considerable average score of 0.591 respectively. However, the average similarity score of SdfastText is 0.388 and the word pair Microsoft-Bill Gates is not available in the vocabulary of SdfastText.",
                        "Moreover, the average semantic relatedness similarity score between countries and their capitals is shown in Table TABREF78 with English translation, where SG also yields the best average score of 0.663 followed by CBoW with 0.611 similarity score. The GloVe also yields better semantic relatedness of 0.576 and the SdfastText yield an average score of 0.391."
                    ]
                }
            ]
        },
        {
            "question": "Are trained word embeddings used for any other NLP task?",
            "answers": [
                {
                    "answer": "No",
                    "type": "boolean"
                }
            ],
            "q_uid": "a6717e334c53ebbb87e5ef878a77ef46866e3aed",
            "evidence": [
                {
                    "raw_evidence": [
                        "In this era of the information age, the existence of LRs plays a vital role in the digital survival of natural languages because the NLP tools are used to process a flow of un-structured data from disparate sources. It is imperative to mention that presently, Sindhi Persian-Arabic is frequently used in online communication, newspapers, public institutions in Pakistan and India. Due to the growing use of Sindhi on web platforms, the need for its LRs is also increasing for the development of language technology tools. But little work has been carried out for the development of resources which is not sufficient to design a language independent or machine learning algorithms. The present work is a first comprehensive initiative on resource development along with their evaluation for statistical Sindhi language processing. More recently, the NN based approaches have produced a state-of-the-art performance in NLP by exploiting unsupervised word embeddings learned from the large unlabelled corpus. Such word embeddings have also motivated the work on low-resourced languages. Our work mainly consists of novel contributions of resource development along with comprehensive evaluation for the utilization of NN based approaches in SNLP applications. The large corpus obtained from multiple web resources is utilized for the training of word embeddings using SG, CBoW and Glove models. The intrinsic evaluation along with comparative results demonstrates that the proposed Sindhi word embeddings have accurately captured the semantic information as compare to recently revealed SdfastText word vectors. The SG yield best results in nearest neighbors, word pair relationship and semantic similarity. The performance of CBoW is also close to SG in all the evaluation matrices. The GloVe also yields better word representations; however SG and CBoW models surpass the GloVe model in all evaluation matrices. Hyperparameter optimization is as important as designing a new algorithm. The choice of optimal parameters is a key aspect of performance gain in learning robust word embeddings. Moreover, We analysed that the size of the corpus and careful preprocessing steps have a large impact on the quality of word embeddings. However, in algorithmic perspective, the character-level learning approach in SG and CBoW improves the quality of representation learning, and overall window size, learning rate, number of epochs are the core parameters that largely influence the performance of word embeddings models. Ultimately, the new corpus of low-resourced Sindhi language, list of stop words and pretrained word embeddings along with empirical evaluation, will be a good supplement for future research in SSLP applications. In the future, we aim to use the corpus for annotation projects such as parts-of-speech tagging, named entity recognition. The proposed word embeddings will be refined further by creating custom benchmarks and the extrinsic evaluation approach will be employed for the performance analysis of proposed word embeddings. Moreover, we will also utilize the corpus using Bi-directional Encoder Representation Transformer BIBREF13 for learning deep contextualized Sindhi word representations. Furthermore, the generated word embeddings will be utilized for the automatic construction of Sindhi WordNet."
                    ],
                    "highlighted_evidence": [
                        "In the future, we aim to use the corpus for annotation projects such as parts-of-speech tagging, named entity recognition.",
                        "Furthermore, the generated word embeddings will be utilized for the automatic construction of Sindhi WordNet."
                    ]
                }
            ]
        },
        {
            "question": "How is the data collected, which web resources were used?",
            "answers": [
                {
                    "answer": "daily Kawish and Awami Awaz Sindhi newspapers, Wikipedia dumps, short stories and sports news from Wichaar social blog, news from Focus Word press blog, historical writings, novels, stories, books from Sindh Salamat literary website, novels, history and religious books from Sindhi Adabi Board,  tweets regarding news and sports are collected from twitter",
                    "type": "extractive"
                }
            ],
            "q_uid": "8cb9006bcbd2f390aadc6b70d54ee98c674e45cc",
            "evidence": [
                {
                    "raw_evidence": [
                        "The corpus is a collection of human language text BIBREF31 built with a specific purpose. However, the statistical analysis of the corpus provides quantitative, reusable data, and an opportunity to examine intuitions and ideas about language. Therefore, the corpus has great importance for the study of written language to examine the text. In fact, realizing the necessity of large text corpus for Sindhi, we started this research by collecting raw corpus from multiple web resource using web-scrappy framwork for extraction of news columns of daily Kawish and Awami Awaz Sindhi newspapers, Wikipedia dumps, short stories and sports news from Wichaar social blog, news from Focus Word press blog, historical writings, novels, stories, books from Sindh Salamat literary websites, novels, history and religious books from Sindhi Adabi Board and tweets regarding news and sports are collected from twitter."
                    ],
                    "highlighted_evidence": [
                        "In fact, realizing the necessity of large text corpus for Sindhi, we started this research by collecting raw corpus from multiple web resource using web-scrappy framwork for extraction of news columns of daily Kawish and Awami Awaz Sindhi newspapers, Wikipedia dumps, short stories and sports news from Wichaar social blog, news from Focus Word press blog, historical writings, novels, stories, books from Sindh Salamat literary websites, novels, history and religious books from Sindhi Adabi Board and tweets regarding news and sports are collected from twitter."
                    ]
                }
            ]
        }
    ],
    "1706.06894": [
        {
            "question": "How were the tweets annotated?",
            "answers": [
                {
                    "answer": "tweets are annotated with only Favor or Against for two targets - Galatasaray and Fenerbah\u00e7e",
                    "type": "abstractive"
                }
            ],
            "q_uid": "9ca447c8959a693a3f7bdd0a2c516f4b86f95718",
            "evidence": [
                {
                    "raw_evidence": [
                        "We have decided to consider tweets about popular sports clubs as our domain for stance detection. Considerable amounts of tweets are being published for sports-related events at every instant. Hence we have determined our targets as Galatasaray (namely Target-1) and Fenerbah\u00e7e (namely, Target-2) which are two of the most popular football clubs in Turkey. As is the case for the sentiment analysis tools, the outputs of the stance detection systems on a stream of tweets about these clubs can facilitate the use of the opinions of the football followers by these clubs.",
                        "In a previous study on the identification of public health-related tweets, two tweet data sets in Turkish (each set containing 1 million random tweets) have been compiled where these sets belong to two different periods of 20 consecutive days BIBREF11 . We have decided to use one of these sets (corresponding to the period between August 18 and September 6, 2015) and firstly filtered the tweets using the possible names used to refer to the target clubs. Then, we have annotated the stance information in the tweets for these targets as Favor or Against. Within the course of this study, we have not considered those tweets in which the target is not explicitly mentioned, as our initial filtering process reveals.",
                        "For the purposes of the current study, we have not annotated any tweets with the Neither class. This stance class and even finer-grained classes can be considered in further annotation studies. We should also note that in a few tweets, the target of the stance was the management of the club while in some others a particular footballer of the club is praised or criticised. Still, we have considered the club as the target of the stance in all of the cases and carried out our annotations accordingly."
                    ],
                    "highlighted_evidence": [
                        "Fenerbah\u00e7e",
                        "We have decided to consider tweets about popular sports clubs as our domain for stance detection. ",
                        "Hence we have determined our targets as Galatasaray (namely Target-1) and Fenerbah\u00e7e (namely, Target-2) which are two of the most popular football clubs in Turkey. ",
                        "Then, we have annotated the stance information in the tweets for these targets as Favor or Against.",
                        "For the purposes of the current study, we have not annotated any tweets with the Neither class."
                    ]
                }
            ]
        },
        {
            "question": "Which SVM approach resulted in the best performance?",
            "answers": [
                {
                    "answer": "Target-1",
                    "type": "extractive"
                }
            ],
            "q_uid": "05887a8466e0a2f0df4d6a5ffc5815acd7d9066a",
            "evidence": [
                {
                    "raw_evidence": [
                        "The evaluation results are quite favorable for both targets and particularly higher for Target-1, considering the fact that they are the initial experiments on the data set. The performance of the classifiers is better for the Favor class for both targets when compared with the performance results for the Against class. This outcome may be due to the common use of some terms when expressing positive stance towards sports clubs in Turkish tweets. The same percentage of common terms may not have been observed in tweets during the expression of negative stances towards the targets. Yet, completely the opposite pattern is observed in stance detection results of baseline systems given in BIBREF0 , i.e., better F-Measure rates have been obtained for the Against class when compared with the Favor class BIBREF0 . Some of the baseline systems reported in BIBREF0 are SVM-based systems using unigrams and ngrams as features similar to our study, but their data sets include all three stance classes of Favor, Against, and Neither, while our data set comprises only tweets classified as belonging to Favor or Against classes. Another difference is that the data sets in BIBREF0 have been divided into training and test sets, while in our study we provide 10-fold cross-validation results on the whole data set. On the other hand, we should also note that SVM-based sentiment analysis systems (such as those given in BIBREF16 ) have been reported to achieve better F-Measure rates for the Positive sentiment class when compared with the results obtained for the Negative class. Therefore, our evaluation results for each stance class seem to be in line with such sentiment analysis systems. Yet, further experiments on the extended versions of our data set should be conducted and the results should again be compared with the stance detection results given in the literature."
                    ],
                    "highlighted_evidence": [
                        "The evaluation results are quite favorable for both targets and particularly higher for Target-1, considering the fact that they are the initial experiments on the data set."
                    ]
                }
            ]
        },
        {
            "question": "What are hashtag features?",
            "answers": [
                {
                    "answer": "hashtag features contain whether there is any hashtag in the tweet",
                    "type": "abstractive"
                }
            ],
            "q_uid": "c87fcc98625e82fdb494ff0f5309319620d69040",
            "evidence": [
                {
                    "raw_evidence": [
                        "With an intention to exploit the contribution of hashtag use to stance detection, we have also used the existence of hashtags in tweets as an additional feature to unigrams. The corresponding evaluation results of the SVM classifiers using unigrams together the existence of hashtags as features are provided in Table TABREF2 ."
                    ],
                    "highlighted_evidence": [
                        "With an intention to exploit the contribution of hashtag use to stance detection, we have also used the existence of hashtags in tweets as an additional feature to unigrams."
                    ]
                }
            ]
        },
        {
            "question": "How many tweets did they collect?",
            "answers": [
                {
                    "answer": "700 ",
                    "type": "extractive"
                },
                {
                    "answer": "700",
                    "type": "extractive"
                }
            ],
            "q_uid": "500a8ec1c56502529d6e59ba6424331f797f31f0",
            "evidence": [
                {
                    "raw_evidence": [
                        "At the end of the annotation process, we have annotated 700 tweets, where 175 tweets are in favor of and 175 tweets are against Target-1, and similarly 175 tweets are in favor of and 175 are against Target-2. Hence, our data set is a balanced one although it is currently limited in size. The corresponding stance annotations are made publicly available at http://ceng.metu.edu.tr/ INLINEFORM0 e120329/ Turkish_Stance_Detection_Tweet_Dataset.csv in Comma Separated Values (CSV) format. The file contains three columns with the corresponding headers. The first column is the tweet id of the corresponding tweet, the second column contains the name of the stance target, and the last column includes the stance of the tweet for the target as Favor or Against."
                    ],
                    "highlighted_evidence": [
                        "At the end of the annotation process, we have annotated 700 tweets, where 175 tweets are in favor of and 175 tweets are against Target-1, and similarly 175 tweets are in favor of and 175 are against Target-2. "
                    ]
                },
                {
                    "raw_evidence": [
                        "At the end of the annotation process, we have annotated 700 tweets, where 175 tweets are in favor of and 175 tweets are against Target-1, and similarly 175 tweets are in favor of and 175 are against Target-2. Hence, our data set is a balanced one although it is currently limited in size. The corresponding stance annotations are made publicly available at http://ceng.metu.edu.tr/ INLINEFORM0 e120329/ Turkish_Stance_Detection_Tweet_Dataset.csv in Comma Separated Values (CSV) format. The file contains three columns with the corresponding headers. The first column is the tweet id of the corresponding tweet, the second column contains the name of the stance target, and the last column includes the stance of the tweet for the target as Favor or Against."
                    ],
                    "highlighted_evidence": [
                        "At the end of the annotation process, we have annotated 700 tweets, where 175 tweets are in favor of and 175 tweets are against Target-1, and similarly 175 tweets are in favor of and 175 are against Target-2. "
                    ]
                }
            ]
        },
        {
            "question": "Which sports clubs are the targets?",
            "answers": [
                {
                    "answer": "Galatasaray, Fenerbah\u00e7e",
                    "type": "extractive"
                },
                {
                    "answer": "Galatasaray , Fenerbah\u00e7e ",
                    "type": "extractive"
                }
            ],
            "q_uid": "ff6c9af28f0e2bb4fb6a69f124665f8ceb966fbc",
            "evidence": [
                {
                    "raw_evidence": [
                        "We have decided to consider tweets about popular sports clubs as our domain for stance detection. Considerable amounts of tweets are being published for sports-related events at every instant. Hence we have determined our targets as Galatasaray (namely Target-1) and Fenerbah\u00e7e (namely, Target-2) which are two of the most popular football clubs in Turkey. As is the case for the sentiment analysis tools, the outputs of the stance detection systems on a stream of tweets about these clubs can facilitate the use of the opinions of the football followers by these clubs."
                    ],
                    "highlighted_evidence": [
                        "Hence we have determined our targets as Galatasaray (namely Target-1) and Fenerbah\u00e7e (namely, Target-2) which are two of the most popular football clubs in Turkey."
                    ]
                },
                {
                    "raw_evidence": [
                        "We have decided to consider tweets about popular sports clubs as our domain for stance detection. Considerable amounts of tweets are being published for sports-related events at every instant. Hence we have determined our targets as Galatasaray (namely Target-1) and Fenerbah\u00e7e (namely, Target-2) which are two of the most popular football clubs in Turkey. As is the case for the sentiment analysis tools, the outputs of the stance detection systems on a stream of tweets about these clubs can facilitate the use of the opinions of the football followers by these clubs."
                    ],
                    "highlighted_evidence": [
                        "Hence we have determined our targets as Galatasaray (namely Target-1) and Fenerbah\u00e7e (namely, Target-2) which are two of the most popular football clubs in Turkey. "
                    ]
                }
            ]
        }
    ],
    "1703.08098": [
        {
            "question": "What models does this overview cover?",
            "answers": [
                {
                    "answer": "This article presented a brief overview of embedding models of entity and relationships for KB completion. ",
                    "type": "extractive"
                }
            ],
            "q_uid": "47ecaca8adc7306e3014e8c4358e306a5f0e1716",
            "evidence": [
                {
                    "raw_evidence": [
                        "This intuition inspired the TransE model\u2014a well-known embedding model for KB completion or link prediction in KBs BIBREF2 .",
                        "Embedding models for KB completion have been proven to give state-of-the-art link prediction performances, in which entities are represented by latent feature vectors while relation types are represented by latent feature vectors and/or matrices and/or third-order tensors BIBREF24 , BIBREF25 , BIBREF2 , BIBREF26 , BIBREF27 , BIBREF28 , BIBREF29 , BIBREF19 , BIBREF30 , BIBREF3 , BIBREF31 , BIBREF32 , BIBREF33 . This article briefly overviews the embedding models for KB completion, and then summarizes up-to-date experimental results on two standard evaluation tasks: i) the entity prediction task\u2014which is also referred to as the link prediction task BIBREF2 \u2014and ii) the triple classification task BIBREF34 .",
                        "The Unstructured model BIBREF22 assumes that the head and tail entity vectors are similar. As the Unstructured model does not take the relationship into account, it cannot distinguish different relation types. The Structured Embedding (SE) model BIBREF35 assumes that the head and tail entities are similar only in a relation-dependent subspace, where each relation is represented by two different matrices. Furthermore, the SME model BIBREF22 uses four different matrices to project entity and relation vectors into a subspace. The TransE model BIBREF2 is inspired by models such as the Word2Vec Skip-gram model BIBREF0 where relationships between words often correspond to translations in latent feature space. TorusE BIBREF36 embeds entities and relations on a torus to handle TransE's regularization problem.",
                        "The TransH model BIBREF26 associates each relation with a relation-specific hyperplane and uses a projection vector to project entity vectors onto that hyperplane. TransD BIBREF37 and TransR/CTransR BIBREF28 extend the TransH model by using two projection vectors and a matrix to project entity vectors into a relation-specific space, respectively. Similar to TransR, TransR-FT BIBREF38 also uses a matrix to project head and tail entity vectors. TEKE_H BIBREF39 extends TransH to incorporate rich context information in an external text corpus. lppTransD BIBREF40 extends TransD to additionally use two projection vectors for representing each relation. STransE BIBREF41 and TranSparse BIBREF42 can be viewed as direct extensions of the TransR model, where head and tail entities are associated with their own projection matrices. Unlike STransE, the TranSparse model uses adaptive sparse matrices, whose sparse degrees are defined based on the number of entities linked by relations. TranSparse-DT BIBREF43 is an extension of TranSparse with a dynamic translation. ITransF BIBREF44 can be considered as a generalization of STransE, which allows sharing statistic regularities between relation projection matrices and alleviates data sparsity issue.",
                        "DISTMULT BIBREF45 is based on the Bilinear model BIBREF24 , BIBREF22 , BIBREF25 where each relation is represented by a diagonal rather than a full matrix. The neural tensor network (NTN) model BIBREF34 uses a bilinear tensor operator to represent each relation while ER-MLP BIBREF27 and ProjE BIBREF46 can be viewed as simplified versions of NTN. Such quadratic forms are also used to model entities and relations in KG2E BIBREF47 , TransG BIBREF48 , ComplEx BIBREF31 , TATEC BIBREF3 , RSTE BIBREF49 and ANALOGY BIBREF50 . In addition, the HolE model BIBREF33 uses circular correlation\u2013a compositional operator\u2013which can be interpreted as a compression of the tensor product.",
                        "ConvE BIBREF51 and ConvKB BIBREF52 are based on convolutional neural networks. ConvE uses a 2D convolutional layer directly over head-entity and relation vector embeddings while ConvKB applies a convolutional layer over embedding triples. Unlike ConvE and ConvKB, the IRN model BIBREF53 uses a shared memory and recurrent neural network-based controller to implicitly model multi-step structured relationships.",
                        "The Path Ranking Algorithm (PRA) BIBREF21 is a random walk inference technique which was proposed to predict a new relationship between two entities in KBs. BIBREF61 used PRA to estimate the probability of an unseen triple as a combination of weighted random walks that follow different paths linking the head entity and tail entity in the KB. BIBREF23 made use of an external text corpus to increase the connectivity of the KB used as the input to PRA. BIBREF62 improved PRA by proposing a subgraph feature extraction technique to make the generation of random walks in KBs more efficient and expressive, while BIBREF63 extended PRA to couple the path ranking of multiple relations. PRA can also be used in conjunction with first-order logic in the discriminative Gaifman model BIBREF64 . In addition, BIBREF65 used a recurrent neural network to learn vector representations of PRA-style relation paths between entities in the KB. Other random-walk based learning algorithms for KB completion can be also found in BIBREF66 , BIBREF67 , BIBREF68 and BIBREF69 . Recently, BIBREF70 have proposed a Neural Logic Programming (LP) framework to learning probabilistic first-order logical rules for KB reasoning, producing competitive link prediction performances. See other methods for learning from KBs and multi-relational data in BIBREF4 ."
                    ],
                    "highlighted_evidence": [
                        " the TransE model",
                        "Embedding models",
                        "The Unstructured model BIBREF22",
                        "The TransH model BIBREF26",
                        "DISTMULT BIBREF45",
                        "ConvE BIBREF51 and ConvKB BIBREF52",
                        "The Path Ranking Algorithm (PRA) BIBREF21"
                    ]
                }
            ]
        }
    ],
    "1604.00727": [
        {
            "question": "Do the authors also try the model on other datasets?",
            "answers": [
                {
                    "answer": "No",
                    "type": "boolean"
                },
                {
                    "answer": "No",
                    "type": "boolean"
                }
            ],
            "q_uid": "784ce5a983c5f2cc95a2c60ce66f2a8a50f3636f",
            "evidence": [
                {
                    "raw_evidence": [
                        "In our experiments, the Memory Neural Networks (MemNNs) proposed in babidataset serve as the baselines. For training, in addition to the 76K questions in the training set, the MemNNs use 3K training questions from WebQuestions BIBREF27 , 15M paraphrases from WikiAnswers BIBREF2 , and 11M and 12M automatically generated questions from the KB for the FB2M and FB5M settings, respectively. In contrast, our models are trained only on the 76K questions in the training set."
                    ],
                    "highlighted_evidence": [
                        "In contrast, our models are trained only on the 76K questions in the training set."
                    ]
                },
                {
                    "raw_evidence": [],
                    "highlighted_evidence": []
                }
            ]
        },
        {
            "question": "What word level and character level model baselines are used?",
            "answers": [
                {
                    "answer": "None",
                    "type": "abstractive"
                },
                {
                    "answer": "Word-level Memory Neural Networks (MemNNs) proposed in Bordes et al. (2015)",
                    "type": "abstractive"
                }
            ],
            "q_uid": "7705dd04acedaefee30d8b2c9978537afb2040dc",
            "evidence": [
                {
                    "raw_evidence": [
                        "In our experiments, the Memory Neural Networks (MemNNs) proposed in babidataset serve as the baselines. For training, in addition to the 76K questions in the training set, the MemNNs use 3K training questions from WebQuestions BIBREF27 , 15M paraphrases from WikiAnswers BIBREF2 , and 11M and 12M automatically generated questions from the KB for the FB2M and FB5M settings, respectively. In contrast, our models are trained only on the 76K questions in the training set."
                    ],
                    "highlighted_evidence": [
                        "In our experiments, the Memory Neural Networks (MemNNs) proposed in babidataset serve as the baselines."
                    ]
                },
                {
                    "raw_evidence": [
                        "In our experiments, the Memory Neural Networks (MemNNs) proposed in babidataset serve as the baselines. For training, in addition to the 76K questions in the training set, the MemNNs use 3K training questions from WebQuestions BIBREF27 , 15M paraphrases from WikiAnswers BIBREF2 , and 11M and 12M automatically generated questions from the KB for the FB2M and FB5M settings, respectively. In contrast, our models are trained only on the 76K questions in the training set.",
                        "We evaluate the proposed model on the SimpleQuestions dataset BIBREF0 . The dataset consists of 108,442 single-relation questions and their corresponding (topic entity, predicate, answer entity) triples from Freebase. It is split into 75,910 train, 10,845 validation, and 21,687 test questions. Only 10,843 of the 45,335 unique words in entity aliases and 886 out of 1,034 unique predicates in the test set were present in the train set. For the proposed dataset, there are two evaluation settings, called FB2M and FB5M, respectively. The former uses a KB for candidate generation which is a subset of Freebase and contains 2M entities, while the latter uses subset of Freebase with 5M entities."
                    ],
                    "highlighted_evidence": [
                        "In our experiments, the Memory Neural Networks (MemNNs) proposed in babidataset serve as the baselines. For training, in addition to the 76K questions in the training set, the MemNNs use 3K training questions from WebQuestions BIBREF27 , 15M paraphrases from WikiAnswers BIBREF2 , and 11M and 12M automatically generated questions from the KB for the FB2M and FB5M settings, respectively. ",
                        "For the proposed dataset, there are two evaluation settings, called FB2M and FB5M, respectively. The former uses a KB for candidate generation which is a subset of Freebase and contains 2M entities, while the latter uses subset of Freebase with 5M entities."
                    ]
                }
            ]
        }
    ],
    "1611.01576": [
        {
            "question": "What languages pairs are used in machine translation?",
            "answers": [
                {
                    "answer": "German\u2013English",
                    "type": "extractive"
                },
                {
                    "answer": "German\u2013English",
                    "type": "extractive"
                }
            ],
            "q_uid": "2f901dab6b757e12763b23ae8b37ae2e517a2271",
            "evidence": [
                {
                    "raw_evidence": [
                        "We evaluate the sequence-to-sequence QRNN architecture described in SECREF5 on a challenging neural machine translation task, IWSLT German\u2013English spoken-domain translation, applying fully character-level segmentation. This dataset consists of 209,772 sentence pairs of parallel training data from transcribed TED and TEDx presentations, with a mean sentence length of 103 characters for German and 93 for English. We remove training sentences with more than 300 characters in English or German, and use a unified vocabulary of 187 Unicode code points."
                    ],
                    "highlighted_evidence": [
                        "We evaluate the sequence-to-sequence QRNN architecture described in SECREF5 on a challenging neural machine translation task, IWSLT German\u2013English spoken-domain translation, applying fully character-level segmentation. "
                    ]
                },
                {
                    "raw_evidence": [
                        "We evaluate the sequence-to-sequence QRNN architecture described in SECREF5 on a challenging neural machine translation task, IWSLT German\u2013English spoken-domain translation, applying fully character-level segmentation. This dataset consists of 209,772 sentence pairs of parallel training data from transcribed TED and TEDx presentations, with a mean sentence length of 103 characters for German and 93 for English. We remove training sentences with more than 300 characters in English or German, and use a unified vocabulary of 187 Unicode code points."
                    ],
                    "highlighted_evidence": [
                        "We evaluate the sequence-to-sequence QRNN architecture described in SECREF5 on a challenging neural machine translation task, IWSLT German\u2013English spoken-domain translation, applying fully character-level segmentation. This dataset consists of 209,772 sentence pairs of parallel training data from transcribed TED and TEDx presentations, with a mean sentence length of 103 characters for German and 93 for English. We remove training sentences with more than 300 characters in English or German, and use a unified vocabulary of 187 Unicode code points."
                    ]
                }
            ]
        },
        {
            "question": "What sentiment classification dataset is used?",
            "answers": [
                {
                    "answer": "the IMDb movie review dataset BIBREF17",
                    "type": "extractive"
                },
                {
                    "answer": "IMDb movie review",
                    "type": "extractive"
                }
            ],
            "q_uid": "b591853e938984e6069d738371500ebdec50d256",
            "evidence": [
                {
                    "raw_evidence": [
                        "We evaluate the QRNN architecture on a popular document-level sentiment classification benchmark, the IMDb movie review dataset BIBREF17 . The dataset consists of a balanced sample of 25,000 positive and 25,000 negative reviews, divided into equal-size train and test sets, with an average document length of 231 words BIBREF18 . We compare only to other results that do not make use of additional unlabeled data (thus excluding e.g., BIBREF19 )."
                    ],
                    "highlighted_evidence": [
                        "We evaluate the QRNN architecture on a popular document-level sentiment classification benchmark, the IMDb movie review dataset BIBREF17 . The dataset consists of a balanced sample of 25,000 positive and 25,000 negative reviews, divided into equal-size train and test sets, with an average document length of 231 words BIBREF18 . We compare only to other results that do not make use of additional unlabeled data (thus excluding e.g., BIBREF19 )."
                    ]
                },
                {
                    "raw_evidence": [
                        "We evaluate the QRNN architecture on a popular document-level sentiment classification benchmark, the IMDb movie review dataset BIBREF17 . The dataset consists of a balanced sample of 25,000 positive and 25,000 negative reviews, divided into equal-size train and test sets, with an average document length of 231 words BIBREF18 . We compare only to other results that do not make use of additional unlabeled data (thus excluding e.g., BIBREF19 )."
                    ],
                    "highlighted_evidence": [
                        "We evaluate the QRNN architecture on a popular document-level sentiment classification benchmark, the IMDb movie review dataset BIBREF17 . "
                    ]
                }
            ]
        },
        {
            "question": "What pooling function is used?",
            "answers": [
                {
                    "answer": "dynamic average pooling",
                    "type": "extractive"
                },
                {
                    "answer": " f-pooling, fo-pooling, and ifo-pooling ",
                    "type": "extractive"
                }
            ],
            "q_uid": "a130306c6662ff489df13fb3f8faa7cba8c52a21",
            "evidence": [
                {
                    "raw_evidence": [
                        "Suitable functions for the pooling subcomponent can be constructed from the familiar elementwise gates of the traditional LSTM cell. We seek a function controlled by gates that can mix states across timesteps, but which acts independently on each channel of the state vector. The simplest option, which BIBREF12 term \u201cdynamic average pooling\u201d, uses only a forget gate: DISPLAYFORM0"
                    ],
                    "highlighted_evidence": [
                        "We seek a function controlled by gates that can mix states across timesteps, but which acts independently on each channel of the state vector. The simplest option, which BIBREF12 term \u201cdynamic average pooling\u201d, uses only a forget gate: DISPLAYFORM0"
                    ]
                },
                {
                    "raw_evidence": [
                        "We term these three options f-pooling, fo-pooling, and ifo-pooling respectively; in each case we initialize INLINEFORM0 or INLINEFORM1 to zero. Although the recurrent parts of these functions must be calculated for each timestep in sequence, their simplicity and parallelism along feature dimensions means that, in practice, evaluating them over even long sequences requires a negligible amount of computation time."
                    ],
                    "highlighted_evidence": [
                        "We term these three options f-pooling, fo-pooling, and ifo-pooling respectively; in each case we initialize INLINEFORM0 or INLINEFORM1 to zero. Although the recurrent parts of these functions must be calculated for each timestep in sequence, their simplicity and parallelism along feature dimensions means that, in practice, evaluating them over even long sequences requires a negligible amount of computation time."
                    ]
                }
            ]
        }
    ],
    "1708.00214": [
        {
            "question": "What NLP tasks do the authors evaluate feed-forward networks on?",
            "answers": [
                {
                    "answer": "language identification, part-of-speech tagging, word segmentation, and preordering for statistical machine translation",
                    "type": "extractive"
                }
            ],
            "q_uid": "56836afc57cae60210fa1e5294c88e40bb10cc0e",
            "evidence": [
                {
                    "raw_evidence": [
                        "We experiment with small feed-forward networks for four diverse NLP tasks: language identification, part-of-speech tagging, word segmentation, and preordering for statistical machine translation."
                    ],
                    "highlighted_evidence": [
                        "We experiment with small feed-forward networks for four diverse NLP tasks: language identification, part-of-speech tagging, word segmentation, and preordering for statistical machine translation."
                    ]
                }
            ]
        }
    ],
    "1904.02815": [
        {
            "question": "Do the authors do manual evaluation?",
            "answers": [
                {
                    "answer": "No",
                    "type": "boolean"
                }
            ],
            "q_uid": "b3ac67232c8c7d5a759ae025aee85e9c838584eb",
            "evidence": [
                {
                    "raw_evidence": [
                        "We compare the performance of our model (Table 2 ) with traditional Bag of Words (BoW), TF-IDF, and n-grams features based classifiers. We also compare against averaged Skip-Gram BIBREF29 , Doc2Vec BIBREF30 , CNN BIBREF23 , Hierarchical Attention (HN-ATT) BIBREF24 and hierarchical network (HN) models. HN it is similar to our model HN-SA but without any self attention.",
                        "Analysis: As is evident from the experiments on both the versions of SWBD, our model (HN-SA) outperforms traditional feature based topic spotting models and deep learning based document classification models. It is interesting to see that simple BoW and n-gram baselines are quite competitive and outperform some of the deep learning based document classification model. Similar observation has also been reported by BIBREF31 ( BIBREF31 ) for the task of sentiment analysis. The task of topic spotting is arguably more challenging than document classification. In the topic spotting task, the number of output classes (66/42 classes) is much more than those in document classification (5/6 classes), which is done mainly on the texts from customer reviews. Dialogues in SWBD have on an average 200 utterances and are much longer texts than customer reviews. Additionally, the number of dialogues available for training the model is significantly lesser than customer reviews. We further investigated the performance on SWBD2 by examining the confusion matrix of the model. Figure 2 shows the heatmap of the normalized confusion matrix of the model on SWBD2. For most of the classes the classifier is able to predict accurately. However, the model gets confused between the classes which are semantically close (w.r.t. terms used) to each other, for example, the model gets confused between pragmatically similar topics e.g. HOBBIES\u0080\u0099 vs \u0080\u0098GARDENING\u0080\u0099, \u0080\u0098MOVIES vs \u0080\u0098TV PROGRAMS\u00e2\u0080\u0099, \u0080\u0098RIGHT TO PRIVACY vs\u0080\u0098 DRUG TESTING\u0080\u0099."
                    ],
                    "highlighted_evidence": [
                        "We compare the performance of our model (Table 2 ) with traditional Bag of Words (BoW), TF-IDF, and n-grams features based classifiers. We also compare against averaged Skip-Gram BIBREF29 , Doc2Vec BIBREF30 , CNN BIBREF23 , Hierarchical Attention (HN-ATT) BIBREF24 and hierarchical network (HN) models. HN it is similar to our model HN-SA but without any self attention.",
                        "We further investigated the performance on SWBD2 by examining the confusion matrix of the model. Figure 2 shows the heatmap of the normalized confusion matrix of the model on SWBD2."
                    ]
                }
            ]
        }
    ],
    "1709.06671": [
        {
            "question": "What is the introduced meta-embedding method introduced in this paper?",
            "answers": [
                {
                    "answer": "proposed method comprises of two steps: a neighbourhood reconstruction step (Section \"Nearest Neighbour Reconstruction\" ), and a projection step (Section \"Projection to Meta-Embedding Space\" ). In the reconstruction step, we represent the embedding of a word by the linearly weighted combination of the embeddings of its nearest neighbours in each source embedding space. ",
                    "type": "extractive"
                }
            ],
            "q_uid": "b5484a0f03d63d091398d3ce4f841a45062438a7",
            "evidence": [
                {
                    "raw_evidence": [
                        "To overcome the above-mentioned challenges, we propose a locally-linear meta-embedding learning method that (a) requires only the words in the vocabulary of each source embedding, without having to predict embeddings for missing words, (b) can meta-embed source embeddings with different dimensionalities, (c) is sensitive to the diversity of the neighbourhoods of the source embeddings.",
                        "Our proposed method comprises of two steps: a neighbourhood reconstruction step (Section \"Nearest Neighbour Reconstruction\" ), and a projection step (Section \"Projection to Meta-Embedding Space\" ). In the reconstruction step, we represent the embedding of a word by the linearly weighted combination of the embeddings of its nearest neighbours in each source embedding space. Although the number of words in the vocabulary of a particular source embedding can be potentially large, the consideration of nearest neighbours enables us to limit the representation to a handful of parameters per each word, not exceeding the neighbourhood size. The weights we learn are shared across different source embeddings, thereby incorporating the information from different source embeddings in the meta-embedding. Interestingly, vector concatenation, which has found to be an accurate meta-embedding method, can be derived as a special case of this reconstruction step."
                    ],
                    "highlighted_evidence": [
                        "To overcome the above-mentioned challenges, we propose a locally-linear meta-embedding learning method that (a) requires only the words in the vocabulary of each source embedding, without having to predict embeddings for missing words, (b) can meta-embed source embeddings with different dimensionalities, (c) is sensitive to the diversity of the neighbourhoods of the source embeddings.\n\nOur proposed method comprises of two steps: a neighbourhood reconstruction step (Section \"Nearest Neighbour Reconstruction\" ), and a projection step (Section \"Projection to Meta-Embedding Space\" ). In the reconstruction step, we represent the embedding of a word by the linearly weighted combination of the embeddings of its nearest neighbours in each source embedding space."
                    ]
                }
            ]
        }
    ],
    "1911.08829": [
        {
            "question": "How big PIE datasets are obtained from dictionaries?",
            "answers": [
                {
                    "answer": "46 documents makes up our base corpus",
                    "type": "extractive"
                }
            ],
            "q_uid": "346f10ddb34503dfba72b0e49afcdf6a08ecacfa",
            "evidence": [
                {
                    "raw_evidence": [
                        "We use only the written part of the BNC. From this, we extract a set of documents with the aim of having as much genre variation as possible. To achieve this, we select the first document in each genre, as defined by the classCode attribute (e.g. nonAc, commerce, letters). The resulting set of 46 documents makes up our base corpus. Note that these documents vary greatly in size, which means the resulting corpus is varied, but not balanced in terms of size (Table TABREF43). The documents are split across a development and test set, as specified at the end of Section SECREF46. We exclude documents with IDs starting with A0 from all annotation and evaluation procedures, as these were used during development of the extraction tool and annotation guidelines."
                    ],
                    "highlighted_evidence": [
                        "The resulting set of 46 documents makes up our base corpus. Note that these documents vary greatly in size, which means the resulting corpus is varied, but not balanced in terms of size (Table TABREF43)."
                    ]
                }
            ]
        },
        {
            "question": "What compleentary PIE extraction methods are used to increase reliability further?",
            "answers": [
                {
                    "answer": "exact string matching, inflectional string matching",
                    "type": "extractive"
                }
            ],
            "q_uid": "2480dfe2d996afef840a81bd920aeb9c26e5b31d",
            "evidence": [
                {
                    "raw_evidence": [
                        "We experiment with two such combinations, by simply taking the union of the sets of extracted idioms of both systems, and filtering out duplicates. Results are shown in Table TABREF77. Both combinations show the expected effect: a clear gain in recall at a minimal loss in precision. Compared to the in-context-parsing-based system, the combination with exact string matching yields a gain in recall of over 6%, and the combination with inflectional string matching yields an even bigger gain of almost 8%, at precision losses of 0.6% and 0.8%, respectively. This indicates that the systems are very much complementary in the PIEs they extract. It also means that, when used in practice, combining inflectional string matching and parse-based extraction is the most reliable configuration."
                    ],
                    "highlighted_evidence": [
                        "Compared to the in-context-parsing-based system, the combination with exact string matching yields a gain in recall of over 6%, and the combination with inflectional string matching yields an even bigger gain of almost 8%, at precision losses of 0.6% and 0.8%, respectively."
                    ]
                }
            ]
        },
        {
            "question": "Are PIEs extracted automatically subjected to human evaluation?",
            "answers": [
                {
                    "answer": "Yes",
                    "type": "boolean"
                }
            ],
            "q_uid": "0fec9da2bc80a12a7a6d6600b9ecf3e122732b60",
            "evidence": [
                {
                    "raw_evidence": [
                        "For parser-based extraction, systems with and without in-context parsing, ignoring labels, and ignoring directionality are tested. For the three string-based extraction methods, varying numbers of intervening words and case sensitivity are evaluated. Evaluation is done using the development set, consisting of 22 documents and 1112 PIE candidates, and the test set, which consists of 23 documents and 1127 PIE candidates. For each method the best set of parameters and/or options is determined using the development set, after which the best variant by F1-score of each method is evaluated on the test set.",
                        "Since these documents in the corpus are exhaustively annotated for PIEs (see Section SECREF40), we can calculate true and false positives, and false negatives, and thus precision, recall and F1-score. The exact spans are ignored, because the spans annotated in the evaluation corpus are not completely reliable. These were automatically generated during candidate extraction, as described in Section SECREF45. Rather, we count an extraction as a true positive if it finds the correct PIE type in the correct sentence."
                    ],
                    "highlighted_evidence": [
                        "Evaluation is done using the development set, consisting of 22 documents and 1112 PIE candidates, and the test set, which consists of 23 documents and 1127 PIE candidates. For each method the best set of parameters and/or options is determined using the development set, after which the best variant by F1-score of each method is evaluated on the test set.\n\nSince these documents in the corpus are exhaustively annotated for PIEs (see Section SECREF40), we can calculate true and false positives, and false negatives, and thus precision, recall and F1-score."
                    ]
                }
            ]
        },
        {
            "question": "What dictionaries are used for automatic extraction of PIEs?",
            "answers": [
                {
                    "answer": "Wiktionary, Oxford Dictionary of English Idioms, UsingEnglish.com (UE), Sporleder corpus, VNC dataset, SemEval-2013 Task 5 dataset",
                    "type": "extractive"
                }
            ],
            "q_uid": "5499527beadb7f5dd908bd659cad83d6a81119bd",
            "evidence": [
                {
                    "raw_evidence": [
                        "We evaluate the quality of three idiom dictionaries by comparing them to each other and to three idiom corpora. Before we report on the comparison we first describe why we select and how we prepare these resources. We investigate the following six idiom resources:",
                        "Wiktionary;",
                        "the Oxford Dictionary of English Idioms (ODEI, BIBREF31);",
                        "UsingEnglish.com (UE);",
                        "the Sporleder corpus BIBREF10;",
                        "the VNC dataset BIBREF9;",
                        "There are four sizeable sense-annotated PIE corpora for English: the VNC-Tokens Dataset BIBREF9, the Gigaword dataset BIBREF14, the IDIX Corpus BIBREF10, and the SemEval-2013 Task 5 dataset BIBREF15. An overview of these corpora is presented in Table TABREF7."
                    ],
                    "highlighted_evidence": [
                        "We investigate the following six idiom resources:\n\nWiktionary;\n\nthe Oxford Dictionary of English Idioms (ODEI, BIBREF31);\n\nUsingEnglish.com (UE);\n\nthe Sporleder corpus BIBREF10;\n\nthe VNC dataset BIBREF9;\n\nand the SemEval-2013 Task 5 dataset BIBREF15."
                    ]
                }
            ]
        }
    ],
    "1608.08188": [
        {
            "question": "What is the model architecture used?",
            "answers": [
                {
                    "answer": "LSTM to encode the question, VGG16 to extract visual features. The outputs of LSTM and VGG16 are multiplied element-wise and sent to a softmax layer.",
                    "type": "abstractive"
                },
                {
                    "answer": "random forest, The question is encoded with a 1024-dimensional LSTM model that takes in a one-hot descriptor of each word in the question. The image is described with the 4096-dimensional output from the last fully connected layer of the Convolutional Neural Network (CNN), VGG16 BIBREF25 . The system performs an element-wise multiplication of the image and question features, after linearly transforming the image descriptor to 1024 dimensions. The final layer of the architecture is a softmax layer.",
                    "type": "extractive"
                }
            ],
            "q_uid": "ecd5770cf8cb12cb34285e26ab834301c17c53e1",
            "evidence": [
                {
                    "raw_evidence": [
                        "We next adapt a VQA deep learning architecture BIBREF24 to learn the predictive combination of visual and textual features. The question is encoded with a 1024-dimensional LSTM model that takes in a one-hot descriptor of each word in the question. The image is described with the 4096-dimensional output from the last fully connected layer of the Convolutional Neural Network (CNN), VGG16 BIBREF25 . The system performs an element-wise multiplication of the image and question features, after linearly transforming the image descriptor to 1024 dimensions. The final layer of the architecture is a softmax layer."
                    ],
                    "highlighted_evidence": [
                        "The question is encoded with a 1024-dimensional LSTM model that takes in a one-hot descriptor of each word in the question. The image is described with the 4096-dimensional output from the last fully connected layer of the Convolutional Neural Network (CNN), VGG16 BIBREF25 . The system performs an element-wise multiplication of the image and question features, after linearly transforming the image descriptor to 1024 dimensions. The final layer of the architecture is a softmax layer."
                    ]
                },
                {
                    "raw_evidence": [
                        "We leverage a random forest classification model BIBREF23 to predict an answer (dis)agreement label for a given visual question. This model consists of an ensemble of decision tree classifiers. We train the system to learn the unique weighted combinations of the aforementioned 2,497 features that each decision tree applies to make a prediction. At test time, given a novel visual question, the trained system converts a 2,497 feature descriptor of the visual question into a final prediction that reflects the majority vote prediction from the ensemble of decision trees. The system returns the final prediction along with a probability indicating the system's confidence in that prediction. We employ the Matlab implementation of random forests, using 25 trees and the default parameters.",
                        "We next adapt a VQA deep learning architecture BIBREF24 to learn the predictive combination of visual and textual features. The question is encoded with a 1024-dimensional LSTM model that takes in a one-hot descriptor of each word in the question. The image is described with the 4096-dimensional output from the last fully connected layer of the Convolutional Neural Network (CNN), VGG16 BIBREF25 . The system performs an element-wise multiplication of the image and question features, after linearly transforming the image descriptor to 1024 dimensions. The final layer of the architecture is a softmax layer."
                    ],
                    "highlighted_evidence": [
                        "We leverage a random forest classification model BIBREF23 to predict an answer (dis)agreement label for a given visual question.",
                        "We next adapt a VQA deep learning architecture BIBREF24 to learn the predictive combination of visual and textual features. The question is encoded with a 1024-dimensional LSTM model that takes in a one-hot descriptor of each word in the question. The image is described with the 4096-dimensional output from the last fully connected layer of the Convolutional Neural Network (CNN), VGG16 BIBREF25 . The system performs an element-wise multiplication of the image and question features, after linearly transforming the image descriptor to 1024 dimensions. The final layer of the architecture is a softmax layer."
                    ]
                }
            ]
        },
        {
            "question": "How is the data used for training annotated?",
            "answers": [
                {
                    "answer": "The number of redundant answers to collect from the crowd is predicted to efficiently capture the diversity of all answers from all visual questions.",
                    "type": "abstractive"
                }
            ],
            "q_uid": "4a4ce942a7a6efd1fa1d6c91dedf7a89af64b729",
            "evidence": [
                {
                    "raw_evidence": [
                        "FLOAT SELECTED: Fig. 6: We propose a novel application of predicting the number of redundant answers to collect from the crowd per visual question to efficiently capture the diversity of all answers for all visual questions. (a) For a batch of visual questions, our system first produces a relative ordering using the predicted confidence in whether a crowd would agree on an answer (upper half). Then, the system allocates a minimum number of annotations to all visual questions (bottom, left half) and then the extra available human budget to visual questions most confidently predicted to lead to crowd disagreement (bottom, right half). (b) For 121,512 visual questions, we show results for our system, a related VQA algorithm, and today\u2019s status quo of random predictions. Boundary conditions are one answer (leftmost) and five answers (rightmost) for all visual questions. Our approach typically accelerates the capture of answer diversity by over 20% from today\u2019s Status Quo selection; e.g., 21% for 70% of the answer diversity and 23% for 86% of the answer diversity. This translates to saving over 19 40-hour work weeks and $1800, assuming 30 seconds and $0.02 per answer."
                    ],
                    "highlighted_evidence": [
                        "FLOAT SELECTED: Fig. 6: We propose a novel application of predicting the number of redundant answers to collect from the crowd per visual question to efficiently capture the diversity of all answers for all visual questions. (a) For a batch of visual questions, our system first produces a relative ordering using the predicted confidence in whether a crowd would agree on an answer (upper half). Then, the system allocates a minimum number of annotations to all visual questions (bottom, left half) and then the extra available human budget to visual questions most confidently predicted to lead to crowd disagreement (bottom, right half). (b) For 121,512 visual questions, we show results for our system, a related VQA algorithm, and today\u2019s status quo of random predictions. Boundary conditions are one answer (leftmost) and five answers (rightmost) for all visual questions. Our approach typically accelerates the capture of answer diversity by over 20% from today\u2019s Status Quo selection; e.g., 21% for 70% of the answer diversity and 23% for 86% of the answer diversity. This translates to saving over 19 40-hour work weeks and $1800, assuming 30 seconds and $0.02 per answer."
                    ]
                }
            ]
        }
    ],
    "1711.02013": [
        {
            "question": "How do they show their model discovers underlying syntactic structure?",
            "answers": [
                {
                    "answer": "By visualizing syntactic distance estimated by the parsing network",
                    "type": "abstractive"
                }
            ],
            "q_uid": "d824f837d8bc17f399e9b8ce8b30795944df0d51",
            "evidence": [
                {
                    "raw_evidence": [
                        "In Figure FIGREF32 , we visualize the syntactic distance estimated by the Parsing Network, while reading three different sequences from the PTB test set. We observe that the syntactic distance tends to be higher between the last character of a word and a space, which is a reasonable breakpoint to separate between words. In other words, if the model sees a space, it will attend on all previous step. If the model sees a letter, it will attend no further then the last space step. The model autonomously discovered to avoid inter-word attention connection, and use the hidden states of space (separator) tokens to summarize previous information. This is strong proof that the model can understand the latent structure of data. As a result our model achieve state-of-the-art performance and significantly outperform baseline models. It is worth noting that HM-LSTM BIBREF6 also unsupervisedly induce similar structure from data. But discrete operations in HM-LSTM make their training procedure more complicated then ours."
                    ],
                    "highlighted_evidence": [
                        "In Figure FIGREF32 , we visualize the syntactic distance estimated by the Parsing Network, while reading three different sequences from the PTB test set. We observe that the syntactic distance tends to be higher between the last character of a word and a space, which is a reasonable breakpoint to separate between words. ",
                        "The model autonomously discovered to avoid inter-word attention connection, and use the hidden states of space (separator) tokens to summarize previous information. This is strong proof that the model can understand the latent structure of data."
                    ]
                }
            ]
        },
        {
            "question": "Which dataset do they experiment with?",
            "answers": [
                {
                    "answer": "Penn Treebank, Text8, WSJ10",
                    "type": "extractive"
                }
            ],
            "q_uid": "2ff3898fbb5954aa82dd2f60b37dd303449c81ba",
            "evidence": [
                {
                    "raw_evidence": [
                        "From a character-level view, natural language is a discrete sequence of data, where discrete symbols form a distinct and shallow tree structure: the sentence is the root, words are children of the root, and characters are leafs. However, compared to word-level language modeling, character-level language modeling requires the model to handle longer-term dependencies. We evaluate a character-level variant of our proposed language model over a preprocessed version of the Penn Treebank (PTB) and Text8 datasets.",
                        "The unsupervised constituency parsing task compares hte tree structure inferred by the model with those annotated by human experts. The experiment is performed on WSJ10 dataset. WSJ10 is the 7422 sentences in the Penn Treebank Wall Street Journal section which contained 10 words or less after the removal of punctuation and null elements. Evaluation was done by seeing whether proposed constituent spans are also in the Treebank parse, measuring unlabeled F1 ( INLINEFORM0 ) of unlabeled constituent precision and recall. Constituents which could not be gotten wrong (those of span one and those spanning entire sentences) were discarded. Given the mechanism discussed in Section SECREF14 , our model generates a binary tree. Although standard constituency parsing tree is not limited to binary tree. Previous unsupervised constituency parsing model also generate binary trees BIBREF11 , BIBREF13 . Our model is compared with the several baseline methods, that are explained in Appendix ."
                    ],
                    "highlighted_evidence": [
                        "We evaluate a character-level variant of our proposed language model over a preprocessed version of the Penn Treebank (PTB) and Text8 datasets.",
                        "The unsupervised constituency parsing task compares hte tree structure inferred by the model with those annotated by human experts. The experiment is performed on WSJ10 dataset."
                    ]
                }
            ]
        }
    ],
    "1911.00547": [
        {
            "question": "What is the size of the dataset?",
            "answers": [
                {
                    "answer": " 9,892 stories of sexual harassment incidents",
                    "type": "extractive"
                }
            ],
            "q_uid": "acd05f31e25856b9986daa1651843b8dc92c2d99",
            "evidence": [
                {
                    "raw_evidence": [
                        "We obtained 9,892 stories of sexual harassment incidents that was reported on Safecity. Those stories include a text description, along with tags of the forms of harassment, e.g. commenting, ogling and groping. A dataset of these stories was published by Karlekar and Bansal karlekar2018safecity. In addition to the forms of harassment, we manually annotated each story with the key elements (i.e. \u201charasser\", \u201ctime\", \u201clocation\", \u201ctrigger\"), because they are essential to uncover the harassment patterns. An example is shown in Figure FIGREF3. Furthermore, we also assigned each story classification labels in five dimensions (Table TABREF4). The detailed definitions of classifications in all dimensions are explained below."
                    ],
                    "highlighted_evidence": [
                        "We obtained 9,892 stories of sexual harassment incidents that was reported on Safecity. Those stories include a text description, along with tags of the forms of harassment, e.g. commenting, ogling and groping. A dataset of these stories was published by Karlekar and Bansal karlekar2018safecity. In addition to the forms of harassment, we manually annotated each story with the key elements (i.e. \u201charasser\", \u201ctime\", \u201clocation\", \u201ctrigger\"), because they are essential to uncover the harassment patterns. An example is shown in Figure FIGREF3. Furthermore, we also assigned each story classification labels in five dimensions (Table TABREF4). The detailed definitions of classifications in all dimensions are explained below."
                    ]
                }
            ]
        },
        {
            "question": "What model did they use?",
            "answers": [
                {
                    "answer": "joint learning NLP models that use convolutional neural network (CNN) BIBREF8 and bi-directional long short-term memory (BiLSTM)",
                    "type": "extractive"
                }
            ],
            "q_uid": "8c78b21ec966a5e8405e8b9d3d6e7099e95ea5fb",
            "evidence": [
                {
                    "raw_evidence": [
                        "2. We proposed joint learning NLP models that use convolutional neural network (CNN) BIBREF8 and bi-directional long short-term memory (BiLSTM) BIBREF9, BIBREF10 as basic units. Our models can automatically extract the key elements from the sexual harassment stories and at the same time categorize the stories in different dimensions. The proposed models outperformed the single task models, and achieved higher than previously reported accuracy in classifications of harassment forms BIBREF6."
                    ],
                    "highlighted_evidence": [
                        "We proposed joint learning NLP models that use convolutional neural network (CNN) BIBREF8 and bi-directional long short-term memory (BiLSTM) BIBREF9, BIBREF10 as basic units. Our models can automatically extract the key elements from the sexual harassment stories and at the same time categorize the stories in different dimensions. The proposed models outperformed the single task models, and achieved higher than previously reported accuracy in classifications of harassment forms BIBREF6."
                    ]
                }
            ]
        },
        {
            "question": "What patterns were discovered from the stories?",
            "answers": [
                {
                    "answer": "we demonstrate that harassment occurred more frequently during the night time than the day time, it shows that besides unspecified strangers (not shown in the figure), conductors and drivers are top the list of identified types of harassers, followed by friends and relatives, we uncovered that there exist strong correlations between the age of perpetrators and the location of harassment, between the single/multiple harasser(s) and location, and between age and single/multiple harasser(s) , We also found that the majority of young perpetrators engaged in harassment behaviors on the streets, we found that adult perpetrators of sexual harassment are more likely to act alone, we also found that the correlations between the forms of harassment with the age, single/multiple harasser, type of harasser, and location , commenting happened more frequently when harassers were in groups. Last but not least, public transportation is where people got indecently touched most frequently both by fellow passengers and by conductors and drivers.",
                    "type": "extractive"
                }
            ],
            "q_uid": "af60462881b2d723adeb4acb5fbc07ea27b6bde2",
            "evidence": [
                {
                    "raw_evidence": [
                        "We plotted the distribution of harassment incidents in each categorization dimension (Figure FIGREF19). It displays statistics that provide important evidence as to the scale of harassment and that can serve as the basis for more effective interventions to be developed by authorities ranging from advocacy organizations to policy makers. It provides evidence to support some commonly assumed factors about harassment: First, we demonstrate that harassment occurred more frequently during the night time than the day time. Second, it shows that besides unspecified strangers (not shown in the figure), conductors and drivers are top the list of identified types of harassers, followed by friends and relatives.",
                        "Furthermore, we uncovered that there exist strong correlations between the age of perpetrators and the location of harassment, between the single/multiple harasser(s) and location, and between age and single/multiple harasser(s) (Figure FIGREF20). The significance of the correlation is tested by chi-square independence with p value less than 0.05. Identifying these patterns will enable interventions to be differentiated for and targeted at specific populations. For instance, the young harassers often engage in harassment activities as groups. This points to the influence of peer pressure and masculine behavioral norms for men and boys on these activities. We also found that the majority of young perpetrators engaged in harassment behaviors on the streets. These findings suggest that interventions with young men and boys, who are readily influenced by peers, might be most effective when education is done peer-to-peer. It also points to the locations where such efforts could be made, including both in schools and on the streets. In contrast, we found that adult perpetrators of sexual harassment are more likely to act alone. Most of the adult harassers engaged in harassment on public transportation. These differences in adult harassment activities and locations, mean that interventions should be responsive to these factors. For example, increasing the security measures on transit at key times and locations.",
                        "In addition, we also found that the correlations between the forms of harassment with the age, single/multiple harasser, type of harasser, and location (Figure FIGREF21). For example, young harassers are more likely to engage in behaviors of verbal harassment, rather than physical harassment as compared to adults. It was a single perpetrator that engaged in touching or groping more often, rather than groups of perpetrators. In contrast, commenting happened more frequently when harassers were in groups. Last but not least, public transportation is where people got indecently touched most frequently both by fellow passengers and by conductors and drivers. The nature and location of the harassment are particularly significant in developing strategies for those who are harassed or who witness the harassment to respond and manage the everyday threat of harassment. For example, some strategies will work best on public transport, a particular closed, shared space setting, while other strategies might be more effective on the open space of the street."
                    ],
                    "highlighted_evidence": [
                        "We plotted the distribution of harassment incidents in each categorization dimension (Figure FIGREF19). It displays statistics that provide important evidence as to the scale of harassment and that can serve as the basis for more effective interventions to be developed by authorities ranging from advocacy organizations to policy makers. It provides evidence to support some commonly assumed factors about harassment: First, we demonstrate that harassment occurred more frequently during the night time than the day time. Second, it shows that besides unspecified strangers (not shown in the figure), conductors and drivers are top the list of identified types of harassers, followed by friends and relatives.",
                        "Furthermore, we uncovered that there exist strong correlations between the age of perpetrators and the location of harassment, between the single/multiple harasser(s) and location, and between age and single/multiple harasser(s) (Figure FIGREF20). ",
                        "We also found that the majority of young perpetrators engaged in harassment behaviors on the streets. These findings suggest that interventions with young men and boys, who are readily influenced by peers, might be most effective when education is done peer-to-peer. It also points to the locations where such efforts could be made, including both in schools and on the streets. ",
                        "In contrast, we found that adult perpetrators of sexual harassment are more likely to act alone. Most of the adult harassers engaged in harassment on public transportation. These differences in adult harassment activities and locations, mean that interventions should be responsive to these factors. For example, increasing the security measures on transit at key times and locations.",
                        "In addition, we also found that the correlations between the forms of harassment with the age, single/multiple harasser, type of harasser, and location (Figure FIGREF21). For example, young harassers are more likely to engage in behaviors of verbal harassment, rather than physical harassment as compared to adults. It was a single perpetrator that engaged in touching or groping more often, rather than groups of perpetrators.",
                        "In contrast, commenting happened more frequently when harassers were in groups. Last but not least, public transportation is where people got indecently touched most frequently both by fellow passengers and by conductors and drivers. The nature and location of the harassment are particularly significant in developing strategies for those who are harassed or who witness the harassment to respond and manage the everyday threat of harassment. For example, some strategies will work best on public transport, a particular closed, shared space setting, while other strategies might be more effective on the open space of the street."
                    ]
                }
            ]
        }
    ],
    "1701.05574": [
        {
            "question": "What other evaluation metrics are looked at?",
            "answers": [
                {
                    "answer": "F-score, Kappa",
                    "type": "extractive"
                }
            ],
            "q_uid": "49c32a2a64eb41381e5f12ccea4150cac9f3303d",
            "evidence": [
                {
                    "raw_evidence": [
                        "For all the classifiers, our feature combination outperforms the baselines (considering only unigram features) as well as BIBREF3 , with the MILR classifier getting an F-score improvement of 3.7% and Kappa difference of 0.08. We also achieve an improvement of 2% over the baseline, using SVM classifier, when we employ our feature set. We also observe that the gaze features alone, also capture the differences between sarcasm and non-sarcasm classes with a high-precision but a low recall."
                    ],
                    "highlighted_evidence": [
                        "For all the classifiers, our feature combination outperforms the baselines (considering only unigram features) as well as BIBREF3 , with the MILR classifier getting an F-score improvement of 3.7% and Kappa difference of 0.08. We also achieve an improvement of 2% over the baseline, using SVM classifier, when we employ our feature set. We also observe that the gaze features alone, also capture the differences between sarcasm and non-sarcasm classes with a high-precision but a low recall."
                    ]
                }
            ]
        }
    ],
    "1606.04631": [
        {
            "question": "what metrics were used for evaluation?",
            "answers": [
                {
                    "answer": "METEOR",
                    "type": "extractive"
                }
            ],
            "q_uid": "7c792cda220916df40edb3107e405c86455822ed",
            "evidence": [
                {
                    "raw_evidence": [
                        "BLEU BIBREF28 , METEOR BIBREF29 , ROUGE-L BIBREF30 and CIDEr BIBREF31 are common evaluation metrics in image and video description, the first three were originally proposed to evaluate machine translation at the earliest and CIDEr was proposed to evaluate image description with sufficient reference sentences. To quantitatively evaluate the performance of our bidirectional recurrent based approach, we adopt METEOR metric because of its robust performance. Contrasting to the other three metrics, METEOR could capture semantic aspect since it identifies all possible matches by extracting exact matcher, stem matcher, paraphrase matcher and synonym matcher using WordNet database, and compute sentence level similarity scores according to matcher weights. The authors of CIDEr also argued for that METEOR outperforms CIDEr when the reference set is small BIBREF31 ."
                    ],
                    "highlighted_evidence": [
                        "To quantitatively evaluate the performance of our bidirectional recurrent based approach, we adopt METEOR metric because of its robust performance."
                    ]
                }
            ]
        }
    ],
    "1909.11467": [
        {
            "question": "Is the corpus annotated?",
            "answers": [
                {
                    "answer": "No",
                    "type": "boolean"
                }
            ],
            "q_uid": "fb1c2ff0872084241b9725b4f07750bd3e1df793",
            "evidence": [
                {
                    "raw_evidence": [],
                    "highlighted_evidence": []
                }
            ]
        },
        {
            "question": "How is the corpus normalized?",
            "answers": [
                {
                    "answer": "by replacing zero-width-non-joiner (ZWNJ) BIBREF2 and manually verifying the orthography",
                    "type": "extractive"
                }
            ],
            "q_uid": "9d9f6cc0f026f7168fcea461baff4b8a925a185f",
            "evidence": [
                {
                    "raw_evidence": [
                        "KTC is composed of 31 educational textbooks published from 2011 to 2018 in various topics by the MoE. We received the material from the MoE partly in different versions of Microsoft Word and partly in Adobe InDesign formats. In the first step, we categorized each textbook based on the topics and chapters. As the original texts were not in Unicode, we converted the content to Unicode. This step was followed by a pre-processing stage where the texts were normalized by replacing zero-width-non-joiner (ZWNJ) BIBREF2 and manually verifying the orthography based on the reference orthography of the Kurdistan Region of Iraq. In the normalization process, we did not remove punctuation and special characters so that the corpus can be easily adapted our current task and also to future tasks where the integrity of the text may be required."
                    ],
                    "highlighted_evidence": [
                        "This step was followed by a pre-processing stage where the texts were normalized by replacing zero-width-non-joiner (ZWNJ) BIBREF2 and manually verifying the orthography based on the reference orthography of the Kurdistan Region of Iraq."
                    ]
                }
            ]
        },
        {
            "question": "Is the corpus annotated with a phonetic transcription?",
            "answers": [
                {
                    "answer": "No",
                    "type": "boolean"
                }
            ],
            "q_uid": "2cc63f42410eff3bcb15cfddc593d8aab9413eea",
            "evidence": [
                {
                    "raw_evidence": [],
                    "highlighted_evidence": []
                }
            ]
        },
        {
            "question": "Is the corpus annotated with Part-of-Speech tags?",
            "answers": [
                {
                    "answer": "No",
                    "type": "boolean"
                }
            ],
            "q_uid": "0a9ced54324e70973354978cccef1c70dee5a543",
            "evidence": [
                {
                    "raw_evidence": [],
                    "highlighted_evidence": []
                }
            ]
        }
    ],
    "1912.13109": [
        {
            "question": "What is the previous work's model?",
            "answers": [
                {
                    "answer": "Ternary Trans-CNN",
                    "type": "extractive"
                }
            ],
            "q_uid": "792d7b579cbf7bfad8fe125b0d66c2059a174cf9",
            "evidence": [
                {
                    "raw_evidence": [
                        "Mathur et al. in their paper for detecting offensive tweets proposed a Ternary Trans-CNN model where they train a model architecture comprising of 3 layers of Convolution 1D having filter sizes of 15, 12 and 10 and kernel size of 3 followed by 2 dense fully connected layer of size 64 and 3. The first dense FC layer has ReLU activation while the last Dense layer had Softmax activation. They were able to train this network on a parallel English dataset provided by Davidson et al. The authors were able to achieve Accuracy of 83.9%, Precision of 80.2%, Recall of 69.8%.",
                        "The approach looked promising given that the dataset was merely 3189 sentences divided into three categories and thus we replicated the experiment but failed to replicate the results. The results were poor than what the original authors achieved. But, most of the model hyper-parameter choices where inspired from this work."
                    ],
                    "highlighted_evidence": [
                        "Mathur et al. in their paper for detecting offensive tweets proposed a Ternary Trans-CNN model where they train a model architecture comprising of 3 layers of Convolution 1D having filter sizes of 15, 12 and 10 and kernel size of 3 followed by 2 dense fully connected layer of size 64 and 3. The first dense FC layer has ReLU activation while the last Dense layer had Softmax activation. They were able to train this network on a parallel English dataset provided by Davidson et al. The authors were able to achieve Accuracy of 83.9%, Precision of 80.2%, Recall of 69.8%.\n\nThe approach looked promising given that the dataset was merely 3189 sentences divided into three categories and thus we replicated the experiment but failed to replicate the results. The results were poor than what the original authors achieved. But, most of the model hyper-parameter choices where inspired from this work."
                    ]
                }
            ]
        },
        {
            "question": "What dataset is used?",
            "answers": [
                {
                    "answer": "HEOT , A labelled dataset for a corresponding english tweets",
                    "type": "extractive"
                },
                {
                    "answer": "HEOT",
                    "type": "extractive"
                }
            ],
            "q_uid": "44a2a8e187f8adbd7d63a51cd2f9d2d324d0c98d",
            "evidence": [
                {
                    "raw_evidence": [
                        "We used dataset, HEOT obtained from one of the past studies done by Mathur et al. where they annotated a set of cleaned tweets obtained from twitter for the conversations happening in Indian subcontinent. A labelled dataset for a corresponding english tweets were also obtained from a study conducted by Davidson et al. This dataset was important to employ Transfer Learning to our task since the number of labeled dataset was very small. Basic summary and examples of the data from the dataset are below:"
                    ],
                    "highlighted_evidence": [
                        "We used dataset, HEOT obtained from one of the past studies done by Mathur et al. where they annotated a set of cleaned tweets obtained from twitter for the conversations happening in Indian subcontinent. A labelled dataset for a corresponding english tweets were also obtained from a study conducted by Davidson et al."
                    ]
                },
                {
                    "raw_evidence": [
                        "We used dataset, HEOT obtained from one of the past studies done by Mathur et al. where they annotated a set of cleaned tweets obtained from twitter for the conversations happening in Indian subcontinent. A labelled dataset for a corresponding english tweets were also obtained from a study conducted by Davidson et al. This dataset was important to employ Transfer Learning to our task since the number of labeled dataset was very small. Basic summary and examples of the data from the dataset are below:"
                    ],
                    "highlighted_evidence": [
                        "We used dataset, HEOT obtained from one of the past studies done by Mathur et al. where they annotated a set of cleaned tweets obtained from twitter for the conversations happening in Indian subcontinent. A labelled dataset for a corresponding english tweets were also obtained from a study conducted by Davidson et al. This dataset was important to employ Transfer Learning to our task since the number of labeled dataset was very small."
                    ]
                }
            ]
        },
        {
            "question": "How is the dataset collected?",
            "answers": [
                {
                    "answer": "A labelled dataset for a corresponding english tweets were also obtained from a study conducted by Davidson et al, HEOT obtained from one of the past studies done by Mathur et al",
                    "type": "extractive"
                }
            ],
            "q_uid": "cca3301f20db16f82b5d65a102436bebc88a2026",
            "evidence": [
                {
                    "raw_evidence": [
                        "We used dataset, HEOT obtained from one of the past studies done by Mathur et al. where they annotated a set of cleaned tweets obtained from twitter for the conversations happening in Indian subcontinent. A labelled dataset for a corresponding english tweets were also obtained from a study conducted by Davidson et al. This dataset was important to employ Transfer Learning to our task since the number of labeled dataset was very small. Basic summary and examples of the data from the dataset are below:"
                    ],
                    "highlighted_evidence": [
                        "We used dataset, HEOT obtained from one of the past studies done by Mathur et al. where they annotated a set of cleaned tweets obtained from twitter for the conversations happening in Indian subcontinent. A labelled dataset for a corresponding english tweets were also obtained from a study conducted by Davidson et al."
                    ]
                }
            ]
        },
        {
            "question": "Was each text augmentation technique experimented individually?",
            "answers": [
                {
                    "answer": "No",
                    "type": "boolean"
                }
            ],
            "q_uid": "cfd67b9eeb10e5ad028097d192475d21d0b6845b",
            "evidence": [
                {
                    "raw_evidence": [],
                    "highlighted_evidence": []
                }
            ]
        },
        {
            "question": "What models do previous work use?",
            "answers": [
                {
                    "answer": "Ternary Trans-CNN , Hybrid multi-channel CNN and LSTM",
                    "type": "extractive"
                }
            ],
            "q_uid": "e1c681280b5667671c7f78b1579d0069cba72b0e",
            "evidence": [
                {
                    "raw_evidence": [
                        "Related Work ::: Transfer learning based approaches",
                        "Mathur et al. in their paper for detecting offensive tweets proposed a Ternary Trans-CNN model where they train a model architecture comprising of 3 layers of Convolution 1D having filter sizes of 15, 12 and 10 and kernel size of 3 followed by 2 dense fully connected layer of size 64 and 3. The first dense FC layer has ReLU activation while the last Dense layer had Softmax activation. They were able to train this network on a parallel English dataset provided by Davidson et al. The authors were able to achieve Accuracy of 83.9%, Precision of 80.2%, Recall of 69.8%.",
                        "The approach looked promising given that the dataset was merely 3189 sentences divided into three categories and thus we replicated the experiment but failed to replicate the results. The results were poor than what the original authors achieved. But, most of the model hyper-parameter choices where inspired from this work.",
                        "Related Work ::: Hybrid models",
                        "In another localized setting of Vietnamese language, Nguyen et al. in 2017 proposed a Hybrid multi-channel CNN and LSTM model where they build feature maps for Vietnamese language using CNN to capture shorterm dependencies and LSTM to capture long term dependencies and concatenate both these feature sets to learn a unified set of features on the messages. These concatenated feature vectors are then sent to a few fully connected layers. They achieved an accuracy rate of 87.3% with this architecture."
                    ],
                    "highlighted_evidence": [
                        " Transfer learning based approaches\nMathur et al. in their paper for detecting offensive tweets proposed a Ternary Trans-CNN model where they train a model architecture comprising of 3 layers of Convolution 1D having filter sizes of 15, 12 and 10 and kernel size of 3 followed by 2 dense fully connected layer of size 64 and 3. The first dense FC layer has ReLU activation while the last Dense layer had Softmax activation. They were able to train this network on a parallel English dataset provided by Davidson et al. The authors were able to achieve Accuracy of 83.9%, Precision of 80.2%, Recall of 69.8%.\n\nThe approach looked promising given that the dataset was merely 3189 sentences divided into three categories and thus we replicated the experiment but failed to replicate the results. The results were poor than what the original authors achieved. But, most of the model hyper-parameter choices where inspired from this work.\n\nRelated Work ::: Hybrid models\nIn another localized setting of Vietnamese language, Nguyen et al. in 2017 proposed a Hybrid multi-channel CNN and LSTM model where they build feature maps for Vietnamese language using CNN to capture shorterm dependencies and LSTM to capture long term dependencies and concatenate both these feature sets to learn a unified set of features on the messages. These concatenated feature vectors are then sent to a few fully connected layers. They achieved an accuracy rate of 87.3% with this architecture."
                    ]
                }
            ]
        },
        {
            "question": "Does the dataset contain content from various social media platforms?",
            "answers": [
                {
                    "answer": "No",
                    "type": "boolean"
                }
            ],
            "q_uid": "58d50567df71fa6c3792a0964160af390556757d",
            "evidence": [
                {
                    "raw_evidence": [
                        "Hinglish is a linguistic blend of Hindi (very widely spoken language in India) and English (an associate language of urban areas) and is spoken by upwards of 350 million people in India. While the name is based on the Hindi language, it does not refer exclusively to Hindi, but is used in India, with English words blending with Punjabi, Gujarati, Marathi and Hindi. Sometimes, though rarely, Hinglish is used to refer to Hindi written in English script and mixing with English words or phrases. This makes analyzing the language very interesting. Its rampant usage in social media like Twitter, Facebook, Online blogs and reviews has also led to its usage in delivering hate and abuses in similar platforms. We aim to find such content in the social media focusing on the tweets. Hypothetically, if we can classify such tweets, we might be able to detect them and isolate them for further analysis before it reaches public. This will a great application of AI to the social cause and thus is motivating. An example of a simple, non offensive message written in Hinglish could be:"
                    ],
                    "highlighted_evidence": [
                        "We aim to find such content in the social media focusing on the tweets."
                    ]
                }
            ]
        },
        {
            "question": "What dataset is used?",
            "answers": [
                {
                    "answer": "HEOT , A labelled dataset for a corresponding english tweets ",
                    "type": "extractive"
                }
            ],
            "q_uid": "07c79edd4c29635dbc1c2c32b8df68193b7701c6",
            "evidence": [
                {
                    "raw_evidence": [
                        "We used dataset, HEOT obtained from one of the past studies done by Mathur et al. where they annotated a set of cleaned tweets obtained from twitter for the conversations happening in Indian subcontinent. A labelled dataset for a corresponding english tweets were also obtained from a study conducted by Davidson et al. This dataset was important to employ Transfer Learning to our task since the number of labeled dataset was very small. Basic summary and examples of the data from the dataset are below:"
                    ],
                    "highlighted_evidence": [
                        "We used dataset, HEOT obtained from one of the past studies done by Mathur et al. where they annotated a set of cleaned tweets obtained from twitter for the conversations happening in Indian subcontinent. A labelled dataset for a corresponding english tweets were also obtained from a study conducted by Davidson et al."
                    ]
                }
            ]
        }
    ],
    "2004.01694": [
        {
            "question": "What empricial investigations do they reference?",
            "answers": [
                {
                    "answer": "empirically test to what extent changes in the evaluation design affect the outcome of the human evaluation",
                    "type": "extractive"
                }
            ],
            "q_uid": "c1429f7fed5a4dda11ac7d9643f97af87a83508b",
            "evidence": [
                {
                    "raw_evidence": [
                        "Our paper investigates three aspects of human MT evaluation, with a special focus on assessing human\u2013machine parity: the choice of raters, the use of linguistic context, and the creation of reference translations. We focus on the data shared by BIBREF3, and empirically test to what extent changes in the evaluation design affect the outcome of the human evaluation. We find that for all three aspects, human translations are judged more favourably, and significantly better than MT, when we make changes that we believe strengthen the evaluation design. Based on our empirical findings, we formulate a set of recommendations for human MT evaluation in general, and assessing human\u2013machine parity in particular. All of our data are made publicly available for external validation and further analysis.",
                        "In this study, we address three aspects that we consider to be particularly relevant for human evaluation of MT, with a special focus on testing human\u2013machine parity: the choice of raters, the use of linguistic context, and the construction of reference translations.",
                        "We empirically test and discuss the impact of these factors on human evaluation of MT in Sections SECREF3\u2013SECREF5. Based on our findings, we then distil a set of recommendations for human evaluation of strong MT systems, with a focus on assessing human\u2013machine parity (Section SECREF6)."
                    ],
                    "highlighted_evidence": [
                        "We focus on the data shared by BIBREF3, and empirically test to what extent changes in the evaluation design affect the outcome of the human evaluation.",
                        "In this study, we address three aspects that we consider to be particularly relevant for human evaluation of MT, with a special focus on testing human\u2013machine parity: the choice of raters, the use of linguistic context, and the construction of reference translations.",
                        "We empirically test and discuss the impact of these factors on human evaluation of MT in Sections SECREF3\u2013SECREF5. "
                    ]
                }
            ]
        },
        {
            "question": "What languages do they investigate for machine translation?",
            "answers": [
                {
                    "answer": "English , Chinese ",
                    "type": "extractive"
                }
            ],
            "q_uid": "a93d4aa89ac3abbd08d725f3765c4f1bed35c889",
            "evidence": [
                {
                    "raw_evidence": [
                        "We use English translations of the Chinese source texts in the WMT 2017 English\u2013Chinese test set BIBREF18 for all experiments presented in this article:"
                    ],
                    "highlighted_evidence": [
                        "We use English translations of the Chinese source texts in the WMT 2017 English\u2013Chinese test set BIBREF18 for all experiments presented in this article:"
                    ]
                }
            ]
        },
        {
            "question": "What recommendations do they offer?",
            "answers": [
                {
                    "answer": " Choose professional translators as raters,  Evaluate documents, not sentences, Evaluate fluency in addition to adequacy, Do not heavily edit reference translations for fluency, Use original source texts",
                    "type": "extractive"
                }
            ],
            "q_uid": "bc473c5bd0e1a8be9b2037aa7006fd68217c3f47",
            "evidence": [
                {
                    "raw_evidence": [
                        "Our experiments in Sections SECREF3\u2013SECREF5 show that machine translation quality has not yet reached the level of professional human translation, and that human evaluation methods which are currently considered best practice fail to reveal errors in the output of strong NMT systems. In this section, we recommend a set of evaluation design changes that we believe are needed for assessing human\u2013machine parity, and will strengthen the human evaluation of MT in general.",
                        "Recommendations ::: (R1) Choose professional translators as raters.",
                        "In our blind experiment (Section SECREF3), non-experts assess parity between human and machine translation where professional translators do not, indicating that the former neglect more subtle differences between different translation outputs.",
                        "Recommendations ::: (R2) Evaluate documents, not sentences.",
                        "When evaluating sentences in random order, professional translators judge machine translation more favourably as they cannot identify errors related to textual coherence and cohesion, such as different translations of the same product name. Our experiments show that using whole documents (i. e., full news articles) as unit of evaluation increases the rating gap between human and machine translation (Section SECREF4).",
                        "Recommendations ::: (R3) Evaluate fluency in addition to adequacy.",
                        "Raters who judge target language fluency without access to the source texts show a stronger preference for human translation than raters with access to the source texts (Sections SECREF4 and SECREF24). In all of our experiments, raters prefer human translation in terms of fluency while, just as in BIBREF3's BIBREF3 evaluation, they find no significant difference between human and machine translation in sentence-level adequacy (Tables TABREF21 and TABREF30). Our error analysis in Table TABREF34 also indicates that MT still lags behind human translation in fluency, specifically in grammaticality.",
                        "Recommendations ::: (R4) Do not heavily edit reference translations for fluency.",
                        "In professional translation workflows, texts are typically revised with a focus on target language fluency after an initial translation step. As shown in our experiment in Section SECREF24, aggressive revision can make translations more fluent but less accurate, to the degree that they become indistinguishable from MT in terms of accuracy (Table TABREF30).",
                        "Recommendations ::: (R5) Use original source texts.",
                        "Raters show a significant preference for human over machine translations of texts that were originally written in the source language, but not for source texts that are translations themselves (Section SECREF35). Our results are further evidence that translated texts tend to be simpler than original texts, and in turn easier to translate with MT."
                    ],
                    "highlighted_evidence": [
                        " In this section, we recommend a set of evaluation design changes that we believe are needed for assessing human\u2013machine parity, and will strengthen the human evaluation of MT in general.\n\nRecommendations ::: (R1) Choose professional translators as raters.\nIn our blind experiment (Section SECREF3), non-experts assess parity between human and machine translation where professional translators do not, indicating that the former neglect more subtle differences between different translation outputs.\n\nRecommendations ::: (R2) Evaluate documents, not sentences.\nWhen evaluating sentences in random order, professional translators judge machine translation more favourably as they cannot identify errors related to textual coherence and cohesion, such as different translations of the same product name. Our experiments show that using whole documents (i. e., full news articles) as unit of evaluation increases the rating gap between human and machine translation (Section SECREF4).\n\nRecommendations ::: (R3) Evaluate fluency in addition to adequacy.\nRaters who judge target language fluency without access to the source texts show a stronger preference for human translation than raters with access to the source texts (Sections SECREF4 and SECREF24). In all of our experiments, raters prefer human translation in terms of fluency while, just as in BIBREF3's BIBREF3 evaluation, they find no significant difference between human and machine translation in sentence-level adequacy (Tables TABREF21 and TABREF30). Our error analysis in Table TABREF34 also indicates that MT still lags behind human translation in fluency, specifically in grammaticality.\n\nRecommendations ::: (R4) Do not heavily edit reference translations for fluency.\nIn professional translation workflows, texts are typically revised with a focus on target language fluency after an initial translation step. As shown in our experiment in Section SECREF24, aggressive revision can make translations more fluent but less accurate, to the degree that they become indistinguishable from MT in terms of accuracy (Table TABREF30).\n\nRecommendations ::: (R5) Use original source texts.\nRaters show a significant preference for human over machine translations of texts that were originally written in the source language, but not for source texts that are translations themselves (Section SECREF35). Our results are further evidence that translated texts tend to be simpler than original texts, and in turn easier to translate with MT."
                    ]
                }
            ]
        },
        {
            "question": "What was the weakness in Hassan et al's evaluation design?",
            "answers": [
                {
                    "answer": "MT developers to which crowd workers were compared are usually not professional translators, evaluation of sentences in isolation prevents raters from detecting translation errors, used not originally written Chinese test set\n",
                    "type": "abstractive"
                }
            ],
            "q_uid": "9299fe72f19c1974564ea60278e03a423eb335dc",
            "evidence": [
                {
                    "raw_evidence": [
                        "The human evaluation of MT output in research scenarios is typically conducted by crowd workers in order to minimise costs. BIBREF13 shows that aggregated assessments of bilingual crowd workers are very similar to those of MT developers, and BIBREF14, based on experiments with data from WMT 2012, similarly conclude that with proper quality control, MT systems can be evaluated by crowd workers. BIBREF3 also use bilingual crowd workers, but the studies supporting the use of crowdsourcing for MT evaluation were performed with older MT systems, and their findings may not carry over to the evaluation of contemporary higher-quality neural machine translation (NMT) systems. In addition, the MT developers to which crowd workers were compared are usually not professional translators. We hypothesise that expert translators will provide more nuanced ratings than non-experts, and that their ratings will show a higher difference between MT outputs and human translations.",
                        "MT has been evaluated almost exclusively at the sentence level, owing to the fact that most MT systems do not yet take context across sentence boundaries into account. However, when machine translations are compared to those of professional translators, the omission of linguistic context\u2014e. g., by random ordering of the sentences to be evaluated\u2014does not do justice to humans who, in contrast to most MT systems, can and do take inter-sentential context into account BIBREF15, BIBREF16. We hypothesise that an evaluation of sentences in isolation, as applied by BIBREF3, precludes raters from detecting translation errors that become apparent only when inter-sentential context is available, and that they will judge MT quality less favourably when evaluating full documents.",
                        "The human reference translations with which machine translations are compared within the scope of a human\u2013machine parity assessment play an important role. BIBREF3 used all source texts of the WMT 2017 Chinese\u2013English test set for their experiments, of which only half were originally written in Chinese; the other half were translated from English into Chinese. Since translated texts are usually simpler than their original counterparts BIBREF17, they should be easier to translate for MT systems. Moreover, different human translations of the same source text sometimes show considerable differences in quality, and a comparison with an MT system only makes sense if the human reference translations are of high quality. BIBREF3, for example, had the WMT source texts re-translated as they were not convinced of the quality of the human translations in the test set. At WMT 2018, the organisers themselves noted that the manual evaluation included several reports of ill-formed reference translations BIBREF5. We hypothesise that the quality of the human translations has a significant effect on findings of human\u2013machine parity, which would indicate that it is necessary to ensure that human translations used to assess parity claims need to be carefully vetted for their quality."
                    ],
                    "highlighted_evidence": [
                        " BIBREF3 also use bilingual crowd workers, but the studies supporting the use of crowdsourcing for MT evaluation were performed with older MT systems, and their findings may not carry over to the evaluation of contemporary higher-quality neural machine translation (NMT) systems. In addition, the MT developers to which crowd workers were compared are usually not professional translators. ",
                        "We hypothesise that an evaluation of sentences in isolation, as applied by BIBREF3, precludes raters from detecting translation errors that become apparent only when inter-sentential context is available, and that they will judge MT quality less favourably when evaluating full documents.",
                        "BIBREF3 used all source texts of the WMT 2017 Chinese\u2013English test set for their experiments, of which only half were originally written in Chinese; "
                    ]
                }
            ]
        }
    ],
    "1605.06083": [
        {
            "question": "What is the size of the dataset?",
            "answers": [
                {
                    "answer": "30,000",
                    "type": "extractive"
                },
                {
                    "answer": "collection of over 30,000 images with 5 crowdsourced descriptions each",
                    "type": "extractive"
                }
            ],
            "q_uid": "7561a968470a8936d10e1ba722d2f38b5a9a4d38",
            "evidence": [
                {
                    "raw_evidence": [
                        "The Flickr30K dataset BIBREF0 is a collection of over 30,000 images with 5 crowdsourced descriptions each. It is commonly used to train and evaluate neural network models that generate image descriptions (e.g. BIBREF2 ). An untested assumption behind the dataset is that the descriptions are based on the images, and nothing else. Here are the authors (about the Flickr8K dataset, a subset of Flickr30K):",
                        "This paper aims to give an overview of linguistic bias and unwarranted inferences resulting from stereotypes and prejudices. I will build on earlier work on linguistic bias in general BIBREF3 , providing examples from the Flickr30K data, and present a taxonomy of unwarranted inferences. Finally, I will discuss several methods to analyze the data in order to detect biases."
                    ],
                    "highlighted_evidence": [
                        "The Flickr30K dataset BIBREF0 is a collection of over 30,000 images with 5 crowdsourced descriptions each. It is commonly used to train and evaluate neural network models that generate image descriptions (e.g. BIBREF2 ).",
                        "This paper aims to give an overview of linguistic bias and unwarranted inferences resulting from stereotypes and prejudices. I will build on earlier work on linguistic bias in general BIBREF3 , providing examples from the Flickr30K data, and present a taxonomy of unwarranted inferences. Finally, I will discuss several methods to analyze the data in order to detect biases."
                    ]
                },
                {
                    "raw_evidence": [
                        "The Flickr30K dataset BIBREF0 is a collection of over 30,000 images with 5 crowdsourced descriptions each. It is commonly used to train and evaluate neural network models that generate image descriptions (e.g. BIBREF2 ). An untested assumption behind the dataset is that the descriptions are based on the images, and nothing else. Here are the authors (about the Flickr8K dataset, a subset of Flickr30K):"
                    ],
                    "highlighted_evidence": [
                        "The Flickr30K dataset BIBREF0 is a collection of over 30,000 images with 5 crowdsourced descriptions each."
                    ]
                }
            ]
        },
        {
            "question": "Which methods are considered to find examples of biases and unwarranted inferences??",
            "answers": [
                {
                    "answer": "spot patterns by just looking at a collection of images, tag all descriptions with part-of-speech information, I applied Louvain clustering",
                    "type": "extractive"
                },
                {
                    "answer": "Looking for adjectives marking the noun \"baby\" and also looking for most-common adjectives related to certain nouns using POS-tagging",
                    "type": "abstractive"
                }
            ],
            "q_uid": "6d4400f45bd97b812e946b8a682b018826e841f1",
            "evidence": [
                {
                    "raw_evidence": [
                        "It may be difficult to spot patterns by just looking at a collection of images. Another method is to tag all descriptions with part-of-speech information, so that it becomes possible to see e.g. which adjectives are most commonly used for particular nouns. One method readers may find particularly useful is to leverage the structure of Flickr30K Entities BIBREF8 . This dataset enriches Flickr30K by adding coreference annotations, i.e. which phrase in each description refers to the same entity in the corresponding image. I have used this data to create a coreference graph by linking all phrases that refer to the same entity. Following this, I applied Louvain clustering BIBREF9 to the coreference graph, resulting in clusters of expressions that refer to similar entities. Looking at those clusters helps to get a sense of the enormous variation in referring expressions. To get an idea of the richness of this data, here is a small sample of the phrases used to describe beards (cluster 268): a scruffy beard; a thick beard; large white beard; a bubble beard; red facial hair; a braided beard; a flaming red beard. In this case, `red facial hair' really stands out as a description; why not choose the simpler `beard' instead?"
                    ],
                    "highlighted_evidence": [
                        "It may be difficult to spot patterns by just looking at a collection of images. Another method is to tag all descriptions with part-of-speech information, so that it becomes possible to see e.g. which adjectives are most commonly used for particular nouns. One method readers may find particularly useful is to leverage the structure of Flickr30K Entities BIBREF8 .",
                        "Following this, I applied Louvain clustering BIBREF9 to the coreference graph, resulting in clusters of expressions that refer to similar entities."
                    ]
                },
                {
                    "raw_evidence": [
                        "We don't know whether or not an entity belongs to a particular social class (in this case: ethnic group) until it is marked as such. But we can approximate the proportion by looking at all the images where the annotators have used a marker (in this case: adjectives like black, white, asian), and for those images count how many descriptions (out of five) contain a marker. This gives us an upper bound that tells us how often ethnicity is indicated by the annotators. Note that this upper bound lies somewhere between 20% (one description) and 100% (5 descriptions). Figure TABREF22 presents count data for the ethnic marking of babies. It includes two false positives (talking about a white baby stroller rather than a white baby). In the Asian group there is an additional complication: sometimes the mother gets marked rather than the baby. E.g. An Asian woman holds a baby girl. I have counted these occurrences as well.",
                        "One interesting pattern is that the ethnicity/race of babies doesn't seem to be mentioned unless the baby is black or asian. In other words: white seems to be the default, and others seem to be marked. How can we tell whether or not the data is actually biased?",
                        "It may be difficult to spot patterns by just looking at a collection of images. Another method is to tag all descriptions with part-of-speech information, so that it becomes possible to see e.g. which adjectives are most commonly used for particular nouns. One method readers may find particularly useful is to leverage the structure of Flickr30K Entities BIBREF8 . This dataset enriches Flickr30K by adding coreference annotations, i.e. which phrase in each description refers to the same entity in the corresponding image. I have used this data to create a coreference graph by linking all phrases that refer to the same entity. Following this, I applied Louvain clustering BIBREF9 to the coreference graph, resulting in clusters of expressions that refer to similar entities. Looking at those clusters helps to get a sense of the enormous variation in referring expressions. To get an idea of the richness of this data, here is a small sample of the phrases used to describe beards (cluster 268): a scruffy beard; a thick beard; large white beard; a bubble beard; red facial hair; a braided beard; a flaming red beard. In this case, `red facial hair' really stands out as a description; why not choose the simpler `beard' instead?"
                    ],
                    "highlighted_evidence": [
                        "We don't know whether or not an entity belongs to a particular social class (in this case: ethnic group) until it is marked as such. But we can approximate the proportion by looking at all the images where the annotators have used a marker (in this case: adjectives like black, white, asian), and for those images count how many descriptions (out of five) contain a marker. This gives us an upper bound that tells us how often ethnicity is indicated by the annotators.",
                        "One interesting pattern is that the ethnicity/race of babies doesn't seem to be mentioned unless the baby is black or asian. In other words: white seems to be the default, and others seem to be marked. How can we tell whether or not the data is actually biased?",
                        "Another method is to tag all descriptions with part-of-speech information, so that it becomes possible to see e.g. which adjectives are most commonly used for particular nouns. One method readers may find particularly useful is to leverage the structure of Flickr30K Entities BIBREF8 . This dataset enriches Flickr30K by adding coreference annotations, i.e. which phrase in each description refers to the same entity in the corresponding image. I have used this data to create a coreference graph by linking all phrases that refer to the same entity. Following this, I applied Louvain clustering BIBREF9 to the coreference graph, resulting in clusters of expressions that refer to similar entities. Looking at those clusters helps to get a sense of the enormous variation in referring expressions."
                    ]
                }
            ]
        },
        {
            "question": "What biases are found in the dataset?",
            "answers": [
                {
                    "answer": "Ethnic bias",
                    "type": "abstractive"
                },
                {
                    "answer": "adjectives are used to create \u201cmore narrow labels [or subtypes] for individuals who do not fit with general social category expectations\u201d",
                    "type": "extractive"
                }
            ],
            "q_uid": "26c2e1eb12143d985e4fb50543cf0d1eb4395e67",
            "evidence": [
                {
                    "raw_evidence": [
                        "Ethnicity/race",
                        "One interesting pattern is that the ethnicity/race of babies doesn't seem to be mentioned unless the baby is black or asian. In other words: white seems to be the default, and others seem to be marked. How can we tell whether or not the data is actually biased?",
                        "The numbers in Table TABREF22 are striking: there seems to be a real, systematic difference in ethnicity marking between the groups. We can take one step further and look at all the 697 pictures with the word `baby' in it. If there turn out to be disproportionately many white babies, this strengthens the conclusion that the dataset is biased."
                    ],
                    "highlighted_evidence": [
                        "Ethnicity/race\nOne interesting pattern is that the ethnicity/race of babies doesn't seem to be mentioned unless the baby is black or asian. In other words: white seems to be the default, and others seem to be marked. How can we tell whether or not the data is actually biased?\n\n",
                        "The numbers in Table TABREF22 are striking: there seems to be a real, systematic difference in ethnicity marking between the groups. We can take one step further and look at all the 697 pictures with the word `baby' in it. If there turn out to be disproportionately many white babies, this strengthens the conclusion that the dataset is biased."
                    ]
                },
                {
                    "raw_evidence": [
                        "One well-studied example BIBREF4 , BIBREF5 is sexist language, where the sex of a person tends to be mentioned more frequently if their role or occupation is inconsistent with `traditional' gender roles (e.g. female surgeon, male nurse). Beukeboom also notes that adjectives are used to create \u201cmore narrow labels [or subtypes] for individuals who do not fit with general social category expectations\u201d (p. 3). E.g. tough woman makes an exception to the `rule' that women aren't considered to be tough."
                    ],
                    "highlighted_evidence": [
                        "One well-studied example BIBREF4 , BIBREF5 is sexist language, where the sex of a person tends to be mentioned more frequently if their role or occupation is inconsistent with `traditional' gender roles (e.g. female surgeon, male nurse).",
                        "Beukeboom also notes that adjectives are used to create \u201cmore narrow labels [or subtypes] for individuals who do not fit with general social category expectations\u201d (p. 3). E.g. tough woman makes an exception to the `rule' that women aren't considered to be tough."
                    ]
                }
            ]
        }
    ],
    "2001.08868": [
        {
            "question": "How is trajectory with how rewards extracted?",
            "answers": [
                {
                    "answer": "explores the state space through keeping track of previously visited states by maintaining an archive",
                    "type": "extractive"
                }
            ],
            "q_uid": "568fb7989a133564d84911e7cb58e4d8748243ef",
            "evidence": [
                {
                    "raw_evidence": [
                        "Go-Explore BIBREF0 differs from the exploration-based algorithms discussed above in that it explicitly keeps track of under-explored areas of the state space and in that it utilizes the determinism of the simulator in order to return to those states, allowing it to explore sparse-reward environments in a sample efficient way (see BIBREF0 as well as section SECREF27). For the experiments in this paper we mainly focus on the final performance of our policy, not how that policy is trained, thus making Go-Explore a suitable algorithm for our experiments. Go-Explore is composed of two phases. In phase 1 (also referred to as the \u201cexploration\u201d phase) the algorithm explores the state space through keeping track of previously visited states by maintaining an archive. During this phase, instead of resuming the exploration from scratch, the algorithm starts exploring from promising states in the archive to find high performing trajectories. In phase 2 (also referred to as the \u201crobustification\u201d phase, while in our variant we will call it \u201cgeneralization\u201d) the algorithm trains a policy using the trajectories found in phase 1. Following this framework, which is also shown in Figure FIGREF56 (Appendix A.2), we define the Go-Explore phases for text-based games."
                    ],
                    "highlighted_evidence": [
                        "In phase 1 (also referred to as the \u201cexploration\u201d phase) the algorithm explores the state space through keeping track of previously visited states by maintaining an archive. During this phase, instead of resuming the exploration from scratch, the algorithm starts exploring from promising states in the archive to find high performing trajectories."
                    ]
                }
            ]
        },
        {
            "question": "On what Text-Based Games are experiments performed?",
            "answers": [
                {
                    "answer": "CoinCollector , CookingWorld ",
                    "type": "extractive"
                },
                {
                    "answer": "CoinCollector, CookingWorld",
                    "type": "extractive"
                }
            ],
            "q_uid": "2c947447d81252397839d58c75ebcc71b34379b5",
            "evidence": [
                {
                    "raw_evidence": [
                        "CoinCollector BIBREF8 is a class of text-based games where the objective is to find and collect a coin from a specific location in a given set of connected rooms . The agent wins the game after it collects the coin, at which point (for the first and only time) a reward of +1 is received by the agent. The environment parses only five admissible commands (go north, go east, go south, go west, and take coin) made by two worlds;",
                        "CookingWorld BIBREF14 in this challenge, there are 4,440 games with 222 different levels of difficulty, with 20 games per level of difficulty, each with different entities and maps. The goal of each game is to cook and eat food from a given recipe, which includes the task of collecting ingredients (e.g. tomato, potato, etc.), objects (e.g. knife), and processing them according to the recipe (e.g. cook potato, slice tomato, etc.). The parser of each game accepts 18 verbs and 51 entities with a predefined grammar, but the overall size of the vocabulary of the observations is 20,000. In Appendix SECREF36 we provide more details about the levels and the games' grammar."
                    ],
                    "highlighted_evidence": [
                        "CoinCollector BIBREF8 is a class of text-based games where the objective is to find and collect a coin from a specific location in a given set of connected rooms . The agent wins the game after it collects the coin, at which point (for the first and only time) a reward of +1 is received by the agent. The environment parses only five admissible commands (go north, go east, go south, go west, and take coin) made by two worlds;",
                        "CookingWorld BIBREF14 in this challenge, there are 4,440 games with 222 different levels of difficulty, with 20 games per level of difficulty, each with different entities and maps. The goal of each game is to cook and eat food from a given recipe, which includes the task of collecting ingredients (e.g. tomato, potato, etc.), objects (e.g. knife), and processing them according to the recipe (e.g. cook potato, slice tomato, etc.). The parser of each game accepts 18 verbs and 51 entities with a predefined grammar, but the overall size of the vocabulary of the observations is 20,000. In Appendix SECREF36 we provide more details about the levels and the games' grammar."
                    ]
                },
                {
                    "raw_evidence": [
                        "CoinCollector BIBREF8 is a class of text-based games where the objective is to find and collect a coin from a specific location in a given set of connected rooms . The agent wins the game after it collects the coin, at which point (for the first and only time) a reward of +1 is received by the agent. The environment parses only five admissible commands (go north, go east, go south, go west, and take coin) made by two worlds;",
                        "CookingWorld BIBREF14 in this challenge, there are 4,440 games with 222 different levels of difficulty, with 20 games per level of difficulty, each with different entities and maps. The goal of each game is to cook and eat food from a given recipe, which includes the task of collecting ingredients (e.g. tomato, potato, etc.), objects (e.g. knife), and processing them according to the recipe (e.g. cook potato, slice tomato, etc.). The parser of each game accepts 18 verbs and 51 entities with a predefined grammar, but the overall size of the vocabulary of the observations is 20,000. In Appendix SECREF36 we provide more details about the levels and the games' grammar."
                    ],
                    "highlighted_evidence": [
                        "CoinCollector BIBREF8 is a class of text-based games where the objective is to find and collect a coin from a specific location in a given set of connected rooms .",
                        "CookingWorld BIBREF14 in this challenge, there are 4,440 games with 222 different levels of difficulty, with 20 games per level of difficulty, each with different entities and maps. The goal of each game is to cook and eat food from a given recipe, which includes the task of collecting ingredients (e.g. tomato, potato, etc.), objects (e.g. knife), and processing them according to the recipe (e.g. cook potato, slice tomato, etc.)."
                    ]
                }
            ]
        },
        {
            "question": "How do the authors show that their learned policy generalize better than existing solutions to unseen games?",
            "answers": [
                {
                    "answer": "promising results by solving almost half of the unseen games, most of the lost games are in the hardest set, where a very long sequence of actions is required for winning the game",
                    "type": "extractive"
                }
            ],
            "q_uid": "c01784b995f6594fdb23d7b62f20a35ae73eaa77",
            "evidence": [
                {
                    "raw_evidence": [
                        "In this setting the 4,440 games are split into training, validation, and test games. The split is done randomly but in a way that different difficulty levels (recipes 1, 2 and 3), are represented with equal ratios in all the 3 splits, i.e. stratified by difficulty. As shown in Table TABREF26, the zero-shot performance of the RL baselines is poor, which could be attributed to the same reasons why RL baselines under-perform in the Joint case. Especially interesting is that the performance of DRRN is substantially lower than that of the Go-Explore Seq2Seq model, even though the DRRN model has access to the admissible actions at test time, while the Seq2Seq model (as well as the LSTM-DQN model) has to construct actions token-by-token from the entire vocabulary of 20,000 tokens. On the other hand, Go-Explore Seq2Seq shows promising results by solving almost half of the unseen games. Figure FIGREF62 (in Appendix SECREF60) shows that most of the lost games are in the hardest set, where a very long sequence of actions is required for winning the game. These results demonstrate both the relative effectiveness of training a Seq2Seq model on Go-Explore trajectories, but they also indicate that additional effort needed for designing reinforcement learning algorithms that effectively generalize to unseen games."
                    ],
                    "highlighted_evidence": [
                        "On the other hand, Go-Explore Seq2Seq shows promising results by solving almost half of the unseen games. Figure FIGREF62 (in Appendix SECREF60) shows that most of the lost games are in the hardest set, where a very long sequence of actions is required for winning the game. These results demonstrate both the relative effectiveness of training a Seq2Seq model on Go-Explore trajectories, but they also indicate that additional effort needed for designing reinforcement learning algorithms that effectively generalize to unseen games."
                    ]
                }
            ]
        }
    ],
    "1702.06777": [
        {
            "question": "Do the authors mention any possible confounds in their study?",
            "answers": [
                {
                    "answer": "Yes",
                    "type": "boolean"
                }
            ],
            "q_uid": "0d755ff58a7e22eb4d02fca45d4a7a3920f4e725",
            "evidence": [
                {
                    "raw_evidence": [
                        "After averaging over all concepts, we lose information on the lexical variation that each concept presents but on the other hand one can now investigate which regions show similar geolectal variation, yielding well defined linguistic varieties. Those cells that have similar colors in either figure FIGREF16 or figure FIGREF17 are expected to be ascribed to the same dialect zone. Thus, we can distinguish two main regions or clusters in the maps. The purple background covers most of the map and represents rural regions with small, scattered population. Our analysis shows that this group of cells possesses more specific words in their lexicon. In contrast, the green and yellow cells form a second cluster that is largely concentrated on the center and along the coastline, which correspond to big cities and industrialized areas. In these cells, the use of standard Spanish language is widespread due probably to school education, media, travelers, etc. The character of its vocabulary is more uniform as compared with the purple group. While the purple cluster prefer particular utterances, the lexicon of the urban group includes most of the keywords. Importantly, we emphasize that both distance measures (cosine similarity and Jensen-Shanon) give rise to the same result, with little discrepancies on the numerical values that are not significant. The presence of two Twitter superdialects (urban and rural) has been recently suggested BIBREF10 based on a machine learning approach. Here, we arrive at the same conclusion but with a totally distinct model and corpus. The advantage of our proposal is that it may serve as a useful tool for dialectometric purposes."
                    ],
                    "highlighted_evidence": [
                        "Thus, we can distinguish two main regions or clusters in the maps. The purple background covers most of the map and represents rural regions with small, scattered population. Our analysis shows that this group of cells possesses more specific words in their lexicon. In contrast, the green and yellow cells form a second cluster that is largely concentrated on the center and along the coastline, which correspond to big cities and industrialized areas.",
                        " In these cells, the use of standard Spanish language is widespread due probably to school education, media, travelers, etc."
                    ]
                }
            ]
        },
        {
            "question": "What are the characteristics of the city dialect?",
            "answers": [
                {
                    "answer": "Lexicon of the cities tend to use most forms of a particular concept",
                    "type": "abstractive"
                }
            ],
            "q_uid": "ff2bcf2d8ffee586751ce91cf15176301267b779",
            "evidence": [
                {
                    "raw_evidence": [
                        "After averaging over all concepts, we lose information on the lexical variation that each concept presents but on the other hand one can now investigate which regions show similar geolectal variation, yielding well defined linguistic varieties. Those cells that have similar colors in either figure FIGREF16 or figure FIGREF17 are expected to be ascribed to the same dialect zone. Thus, we can distinguish two main regions or clusters in the maps. The purple background covers most of the map and represents rural regions with small, scattered population. Our analysis shows that this group of cells possesses more specific words in their lexicon. In contrast, the green and yellow cells form a second cluster that is largely concentrated on the center and along the coastline, which correspond to big cities and industrialized areas. In these cells, the use of standard Spanish language is widespread due probably to school education, media, travelers, etc. The character of its vocabulary is more uniform as compared with the purple group. While the purple cluster prefer particular utterances, the lexicon of the urban group includes most of the keywords. Importantly, we emphasize that both distance measures (cosine similarity and Jensen-Shanon) give rise to the same result, with little discrepancies on the numerical values that are not significant. The presence of two Twitter superdialects (urban and rural) has been recently suggested BIBREF10 based on a machine learning approach. Here, we arrive at the same conclusion but with a totally distinct model and corpus. The advantage of our proposal is that it may serve as a useful tool for dialectometric purposes."
                    ],
                    "highlighted_evidence": [
                        "After averaging over all concepts, we lose information on the lexical variation that each concept presents but on the other hand one can now investigate which regions show similar geolectal variation, yielding well defined linguistic varieties. Those cells that have similar colors in either figure FIGREF16 or figure FIGREF17 are expected to be ascribed to the same dialect zone. Thus, we can distinguish two main regions or clusters in the maps. The purple background covers most of the map and represents rural regions with small, scattered population. Our analysis shows that this group of cells possesses more specific words in their lexicon. In contrast, the green and yellow cells form a second cluster that is largely concentrated on the center and along the coastline, which correspond to big cities and industrialized areas. In these cells, the use of standard Spanish language is widespread due probably to school education, media, travelers, etc. The character of its vocabulary is more uniform as compared with the purple group. While the purple cluster prefer particular utterances, the lexicon of the urban group includes most of the keywords."
                    ]
                }
            ]
        },
        {
            "question": "What are the characteristics of the rural dialect?",
            "answers": [
                {
                    "answer": "It uses particular forms of a concept rather than all of them uniformly",
                    "type": "abstractive"
                }
            ],
            "q_uid": "55588ae77496e7753bff18763a21ca07d9f93240",
            "evidence": [
                {
                    "raw_evidence": [
                        "After averaging over all concepts, we lose information on the lexical variation that each concept presents but on the other hand one can now investigate which regions show similar geolectal variation, yielding well defined linguistic varieties. Those cells that have similar colors in either figure FIGREF16 or figure FIGREF17 are expected to be ascribed to the same dialect zone. Thus, we can distinguish two main regions or clusters in the maps. The purple background covers most of the map and represents rural regions with small, scattered population. Our analysis shows that this group of cells possesses more specific words in their lexicon. In contrast, the green and yellow cells form a second cluster that is largely concentrated on the center and along the coastline, which correspond to big cities and industrialized areas. In these cells, the use of standard Spanish language is widespread due probably to school education, media, travelers, etc. The character of its vocabulary is more uniform as compared with the purple group. While the purple cluster prefer particular utterances, the lexicon of the urban group includes most of the keywords. Importantly, we emphasize that both distance measures (cosine similarity and Jensen-Shanon) give rise to the same result, with little discrepancies on the numerical values that are not significant. The presence of two Twitter superdialects (urban and rural) has been recently suggested BIBREF10 based on a machine learning approach. Here, we arrive at the same conclusion but with a totally distinct model and corpus. The advantage of our proposal is that it may serve as a useful tool for dialectometric purposes."
                    ],
                    "highlighted_evidence": [
                        "After averaging over all concepts, we lose information on the lexical variation that each concept presents but on the other hand one can now investigate which regions show similar geolectal variation, yielding well defined linguistic varieties. Those cells that have similar colors in either figure FIGREF16 or figure FIGREF17 are expected to be ascribed to the same dialect zone. Thus, we can distinguish two main regions or clusters in the maps. The purple background covers most of the map and represents rural regions with small, scattered population. Our analysis shows that this group of cells possesses more specific words in their lexicon. In contrast, the green and yellow cells form a second cluster that is largely concentrated on the center and along the coastline, which correspond to big cities and industrialized areas. In these cells, the use of standard Spanish language is widespread due probably to school education, media, travelers, etc. The character of its vocabulary is more uniform as compared with the purple group. While the purple cluster prefer particular utterances, the lexicon of the urban group includes most of the keywords. "
                    ]
                }
            ]
        }
    ],
    "2003.04032": [
        {
            "question": "Do they build a model to recognize discourse relations on their dataset?",
            "answers": [
                {
                    "answer": "No",
                    "type": "boolean"
                }
            ],
            "q_uid": "58e65741184c81c9e7fe0ca15832df2d496beb6f",
            "evidence": [
                {
                    "raw_evidence": [],
                    "highlighted_evidence": []
                }
            ]
        },
        {
            "question": "Which inter-annotator metric do they use?",
            "answers": [
                {
                    "answer": "agreement rates, Kappa value",
                    "type": "extractive"
                }
            ],
            "q_uid": "269b05b74d5215b09c16e95a91ae50caedd9e2aa",
            "evidence": [
                {
                    "raw_evidence": [
                        "We measured intra-annotator agreement between two annotators in three aspects: relations, senses, arguments. To be specific, the annotators\u2019 consistency in annotating the type of a specific relation or sense and the position and scope of arguments are measured. To assess the consistency of annotations and also eliminate coincidental annotations, we used agreement rates, which is calculated by dividing the number of senses under each category where the annotators annotate consistently by the total number of each kind of sense. And considering the potential impact of unbalanced distribution of senses, we also used the Kappa value. And the final agreement study was carried out for the first 300 relations in our corpus. We obtained high agreement results and Kappa value for the discourse relation type and top-level senses ($\\ge {0.9} $ ). However, what we did was more than this, and we also achieved great results on the second-level and third-level senses for the sake of our self-demand for high-quality, finally achieving agreement of 0.85 and Kappa value of 0.83 for these two deeper levels of senses."
                    ],
                    "highlighted_evidence": [
                        "To assess the consistency of annotations and also eliminate coincidental annotations, we used agreement rates, which is calculated by dividing the number of senses under each category where the annotators annotate consistently by the total number of each kind of sense. And considering the potential impact of unbalanced distribution of senses, we also used the Kappa value."
                    ]
                }
            ]
        },
        {
            "question": "How high is the inter-annotator agreement?",
            "answers": [
                {
                    "answer": "agreement of 0.85 and Kappa value of 0.83",
                    "type": "extractive"
                }
            ],
            "q_uid": "0d7f514f04150468b2d1de9174c12c28e52c5511",
            "evidence": [
                {
                    "raw_evidence": [
                        "We measured intra-annotator agreement between two annotators in three aspects: relations, senses, arguments. To be specific, the annotators\u2019 consistency in annotating the type of a specific relation or sense and the position and scope of arguments are measured. To assess the consistency of annotations and also eliminate coincidental annotations, we used agreement rates, which is calculated by dividing the number of senses under each category where the annotators annotate consistently by the total number of each kind of sense. And considering the potential impact of unbalanced distribution of senses, we also used the Kappa value. And the final agreement study was carried out for the first 300 relations in our corpus. We obtained high agreement results and Kappa value for the discourse relation type and top-level senses ($\\ge {0.9} $ ). However, what we did was more than this, and we also achieved great results on the second-level and third-level senses for the sake of our self-demand for high-quality, finally achieving agreement of 0.85 and Kappa value of 0.83 for these two deeper levels of senses."
                    ],
                    "highlighted_evidence": [
                        "However, what we did was more than this, and we also achieved great results on the second-level and third-level senses for the sake of our self-demand for high-quality, finally achieving agreement of 0.85 and Kappa value of 0.83 for these two deeper levels of senses."
                    ]
                }
            ]
        },
        {
            "question": "How are resources adapted to properties of Chinese text?",
            "answers": [
                {
                    "answer": "removing AltLexC and adding Progression into our sense hierarchy",
                    "type": "extractive"
                }
            ],
            "q_uid": "4d223225dbf84a80e2235448a4d7ba67bfb12490",
            "evidence": [
                {
                    "raw_evidence": [
                        "In this paper, we describe our scheme and process in annotating shallow discourse relations using PDTB-style. In view of the differences between English and Chinese, we made adaptations for the PDTB-3 scheme such as removing AltLexC and adding Progression into our sense hierarchy. To ensure the annotation quality, we formulated detailed annotation criteria and quality assurance strategies. After serious training, we annotated 3212 discourse relations, and we achieved a satisfactory consistency of labelling with a Kappa value of greater than 0.85 for most of the indicators. Finally, we display our annotation results in which the distribution of discourse relations and senses differ from that in other corpora which annotate news report or newspaper texts. Our corpus contains more Contingency, Temporal and Comparison relations instead of being governed by Expansion."
                    ],
                    "highlighted_evidence": [
                        "In view of the differences between English and Chinese, we made adaptations for the PDTB-3 scheme such as removing AltLexC and adding Progression into our sense hierarchy."
                    ]
                }
            ]
        }
    ],
    "1901.04085": [
        {
            "question": "What is the TREC-CAR dataset?",
            "answers": [
                {
                    "answer": "in this dataset, the input query is the concatenation of a Wikipedia article title with the title of one of its section. The relevant passages are the paragraphs within that section",
                    "type": "extractive"
                }
            ],
            "q_uid": "25c4fa78299481788a19d0c25ae07dfd8cb6315c",
            "evidence": [
                {
                    "raw_evidence": [
                        "Introduced by BIBREF16 , in this dataset, the input query is the concatenation of a Wikipedia article title with the title of one of its section. The relevant passages are the paragraphs within that section. The corpus consists of all of the English Wikipedia paragraphs, except the abstracts. The released dataset has five predefined folds, and we use the first four as a training set (approximately 3M queries), and the remaining as a validation set (approximately 700k queries). The test set is the same one used to evaluate the submissions to TREC-CAR 2017 (approx. 1,800 queries)."
                    ],
                    "highlighted_evidence": [
                        "Introduced by BIBREF16 , in this dataset, the input query is the concatenation of a Wikipedia article title with the title of one of its section. The relevant passages are the paragraphs within that section. The corpus consists of all of the English Wikipedia paragraphs, except the abstracts. The released dataset has five predefined folds, and we use the first four as a training set (approximately 3M queries), and the remaining as a validation set (approximately 700k queries). The test set is the same one used to evaluate the submissions to TREC-CAR 2017 (approx. 1,800 queries)."
                    ]
                }
            ]
        }
    ],
    "1809.09194": [
        {
            "question": "Do they use attention?",
            "answers": [
                {
                    "answer": "Yes",
                    "type": "boolean"
                },
                {
                    "answer": "Yes",
                    "type": "boolean"
                }
            ],
            "q_uid": "45e9533586199bde19313cd43b3d0ecadcaf7a33",
            "evidence": [
                {
                    "raw_evidence": [
                        "Memory Generation Layer. In this layer, we generate a working memory by fusing information from both passages INLINEFORM0 and questions INLINEFORM1 . The attention function BIBREF11 is used to compute the similarity score between passages and questions as: INLINEFORM2"
                    ],
                    "highlighted_evidence": [
                        "The attention function BIBREF11 is used to compute the similarity score between passages and questions as: INLINEFORM2"
                    ]
                },
                {
                    "raw_evidence": [
                        "Memory Generation Layer. In this layer, we generate a working memory by fusing information from both passages INLINEFORM0 and questions INLINEFORM1 . The attention function BIBREF11 is used to compute the similarity score between passages and questions as: INLINEFORM2"
                    ],
                    "highlighted_evidence": [
                        "Memory Generation Layer. In this layer, we generate a working memory by fusing information from both passages INLINEFORM0 and questions INLINEFORM1 . The attention function BIBREF11 is used to compute the similarity score between passages and questions as: INLINEFORM2"
                    ]
                }
            ]
        },
        {
            "question": "What is the architecture of the span detector?",
            "answers": [
                {
                    "answer": "adopt a multi-turn answer module for the span detector BIBREF1",
                    "type": "extractive"
                }
            ],
            "q_uid": "a5e49cdb91d9fd0ca625cc1ede236d3d4672403c",
            "evidence": [
                {
                    "raw_evidence": [
                        "Span detector. We adopt a multi-turn answer module for the span detector BIBREF1 . Formally, at time step INLINEFORM0 in the range of INLINEFORM1 , the state is defined by INLINEFORM2 . The initial state INLINEFORM3 is the summary of the INLINEFORM4 : INLINEFORM5 , where INLINEFORM6 . Here, INLINEFORM7 is computed from the previous state INLINEFORM8 and memory INLINEFORM9 : INLINEFORM10 and INLINEFORM11 . Finally, a bilinear function is used to find the begin and end point of answer spans at each reasoning step INLINEFORM12 : DISPLAYFORM0 DISPLAYFORM1",
                        "The final prediction is the average of each time step: INLINEFORM0 . We randomly apply dropout on the step level in each time step during training, as done in BIBREF1 ."
                    ],
                    "highlighted_evidence": [
                        "Span detector. We adopt a multi-turn answer module for the span detector BIBREF1 . Formally, at time step INLINEFORM0 in the range of INLINEFORM1 , the state is defined by INLINEFORM2 . The initial state INLINEFORM3 is the summary of the INLINEFORM4 : INLINEFORM5 , where INLINEFORM6 . Here, INLINEFORM7 is computed from the previous state INLINEFORM8 and memory INLINEFORM9 : INLINEFORM10 and INLINEFORM11 . Finally, a bilinear function is used to find the begin and end point of answer spans at each reasoning step INLINEFORM12 : DISPLAYFORM0 DISPLAYFORM1\n\nThe final prediction is the average of each time step: INLINEFORM0 . We randomly apply dropout on the step level in each time step during training, as done in BIBREF1 ."
                    ]
                }
            ]
        }
    ],
    "1811.00854": [
        {
            "question": "Which evaluation metric has been measured?",
            "answers": [
                {
                    "answer": "Mean Average Precision",
                    "type": "extractive"
                }
            ],
            "q_uid": "70c2dc170a73185c9d1a16953f85aca834ead6d3",
            "evidence": [
                {
                    "raw_evidence": [
                        "In order to evaluate the precision of the retrieved documents in each experiment, we used \"TREC_Eval\" tool [3]. TREC_Eval is a standard tool for evaluation of IR tasks and its name is a short form of Text REtrieval Conference (TREC) Evaluation tool. The Mean Average Precision (MAP) reported by TREC_Eval was 27.99% without query expansion and 37.10% with query expansion which shows more than 9 percent improvement."
                    ],
                    "highlighted_evidence": [
                        "In order to evaluate the precision of the retrieved documents in each experiment, we used \"TREC_Eval\" tool [3]. TREC_Eval is a standard tool for evaluation of IR tasks and its name is a short form of Text REtrieval Conference (TREC) Evaluation tool. The Mean Average Precision (MAP) reported by TREC_Eval was 27.99% without query expansion and 37.10% with query expansion which shows more than 9 percent improvement."
                    ]
                }
            ]
        },
        {
            "question": "What is the WordNet counterpart for Persian?",
            "answers": [
                {
                    "answer": "FarsNet",
                    "type": "extractive"
                }
            ],
            "q_uid": "38854255dbdf2f36eebefc0d9826aa76df9637c6",
            "evidence": [
                {
                    "raw_evidence": [
                        "FarsNet [20] [21] is the first WordNet for Persian, developed by the NLP Lab at Shahid Beheshti University and it follows the same structure as the original WordNet. The first version of FarsNet contained more than 10,000 synsets while version 2.0 and 2.5 contained 20,000 synsets. Currently, FarsNet version 3 is under release and contains more than 40,000 synsets [7]."
                    ],
                    "highlighted_evidence": [
                        "FarsNet [20] [21] is the first WordNet for Persian, developed by the NLP Lab at Shahid Beheshti University and it follows the same structure as the original WordNet."
                    ]
                }
            ]
        }
    ],
    "1705.01265": [
        {
            "question": "Do they report results only on English datasets?",
            "answers": [
                {
                    "answer": "Yes",
                    "type": "boolean"
                },
                {
                    "answer": "Yes",
                    "type": "boolean"
                }
            ],
            "q_uid": "92d1a6df3041667dc662376938bc65527a5a1c3c",
            "evidence": [
                {
                    "raw_evidence": [
                        "For all the tasks in our experimental study, we use 36 millions English tweets collected between August and September 2017. A pre-processing step has been applied to replace URLs with a placeholder and to pad punctuation. The final vocabulary size was around 1.6 millions words. Additionally to the in-domain corpus we collected, we use GloVe vectors trained on Wikipedia articles in order to investigate the impact of out-of-domain word-vectors."
                    ],
                    "highlighted_evidence": [
                        "For all the tasks in our experimental study, we use 36 millions English tweets collected between August and September 2017. "
                    ]
                },
                {
                    "raw_evidence": [
                        "For all the tasks in our experimental study, we use 36 millions English tweets collected between August and September 2017. A pre-processing step has been applied to replace URLs with a placeholder and to pad punctuation. The final vocabulary size was around 1.6 millions words. Additionally to the in-domain corpus we collected, we use GloVe vectors trained on Wikipedia articles in order to investigate the impact of out-of-domain word-vectors."
                    ],
                    "highlighted_evidence": [
                        "For all the tasks in our experimental study, we use 36 millions English tweets collected between August and September 2017. "
                    ]
                }
            ]
        },
        {
            "question": "Which other hyperparameters, other than number of clusters are typically evaluated in this type of research?",
            "answers": [
                {
                    "answer": "selection of word vectors",
                    "type": "extractive"
                }
            ],
            "q_uid": "a4a1fcef760b133e9aa876ac28145ad98a609927",
            "evidence": [
                {
                    "raw_evidence": [
                        "In this paper, we explore a hybrid approach, that uses text embeddings as a proxy to create features. Motivated by the argument that text embeddings manage to encode the semantics of text, we explore how clustering text embeddings can impact the performance of different NLP tasks. Although such an approach has been used in different studies during feature engineering, the selection of word vectors and the number of clusters remain a trial-end-error procedure. In this work we present an empirical evaluation across diverse tasks to verify whether and when such features are useful."
                    ],
                    "highlighted_evidence": [
                        "Although such an approach has been used in different studies during feature engineering, the selection of word vectors and the number of clusters remain a trial-end-error procedure. "
                    ]
                }
            ]
        },
        {
            "question": "How were the cluster extracted? ",
            "answers": [
                {
                    "answer": "Word clusters are extracted using k-means on word embeddings",
                    "type": "abstractive"
                }
            ],
            "q_uid": "63bb2040fa107c5296351c2b5f0312336dad2863",
            "evidence": [
                {
                    "raw_evidence": [
                        "We cluster the embeddings with INLINEFORM0 -Means. The k-means clusters are initialized using \u201ck-means++\u201d as proposed in BIBREF9 , while the algorithm is run for 300 iterations. We try different values for INLINEFORM1 . For each INLINEFORM2 , we repeat the clustering experiment with different seed initialization for 10 times and we select the clustering result that minimizes the cluster inertia."
                    ],
                    "highlighted_evidence": [
                        "We cluster the embeddings with INLINEFORM0 -Means. "
                    ]
                }
            ]
        }
    ],
    "1901.03438": [
        {
            "question": "Do they report results only on English data?",
            "answers": [
                {
                    "answer": "Yes",
                    "type": "boolean"
                }
            ],
            "q_uid": "0716b481b78d80b012bca17c897c62efbe7f3731",
            "evidence": [
                {
                    "raw_evidence": [
                        "We can see at least three reasons for these observed correlations. First, some correlations can be attributed to overlapping feature definitions. For instance, expletive arguments (e.g. There are birds singing) are, by definition, non-canonical arguments, and thus are a subset of add arg. However, some added arguments, such as benefactives (Bo baked Mo a cake), are not expletives. Second, some correlations can be attributed to grammatical properties of the relevant constructions. For instance, question and aux are correlated because main-clause questions in English require subject-aux inversion and in many cases the insertion of auxiliary do (Do lions meow?). Third, some correlations may be a consequence of the sources sampled in CoLA and the phenomena they focus on. For instance, the unusually high correlation of Emb-Q and ellipsis/anaphor can be attributed to BIBREF18 , which is an article about the sluicing construction involving ellipsis of an embedded interrogative (e.g. I saw someone, but I don't know who).",
                        "Expletives, or \u201cdummy\u201d arguments, are semantically inert arguments. The most common expletives in English are it and there, although not all occurrences of these items are expletives. Arguments are usually selected for by the head, and they are generally not optional. In this case, the expletive occupies a syntactic argument slot, but it is not semantically selected by the verb, and there is often a syntactic variation without the expletive. See [p.170-172]adger2003core and [p.82-83]kim2008syntax."
                    ],
                    "highlighted_evidence": [
                        "For instance, question and aux are correlated because main-clause questions in English require subject-aux inversion and in many cases the insertion of auxiliary do (Do lions meow?). ",
                        "Expletives, or \u201cdummy\u201d arguments, are semantically inert arguments. The most common expletives in English are it and there, although not all occurrences of these items are expletives."
                    ]
                }
            ]
        },
        {
            "question": "Do the authors have a hypothesis as to why morphological agreement is hardly learned by any model?",
            "answers": [
                {
                    "answer": "These models are likely to be deficient in encoding morphological features is that they are word level models, and do not have direct access sub-word information like inflectional endings, which indicates that these features are difficult to learn effectively purely from lexical distributions.",
                    "type": "extractive"
                }
            ],
            "q_uid": "fed0785d24375ebbde51fb0503b93f14da1d8583",
            "evidence": [
                {
                    "raw_evidence": [
                        "The results for the major features and minor features are shown in Figures FIGREF26 and FIGREF35 , respectively. For each feature, we measure the MCC of the sentences including that feature. We plot the mean of these results across the different restarts for each model, and error bars mark the mean INLINEFORM0 standard deviation. For the Violations features, MCC is technically undefined because these features only contain unacceptable sentences. We report MCC in these cases by including for each feature a single acceptable example that is correctly classified by all models.",
                        "The most challenging features are all related to Violations. Low performance on Infl/Agr Violations, which marks morphological violations (He washed yourself, This is happy), is especially striking because a relatively high proportion (29%) of these sentences are Simple. These models are likely to be deficient in encoding morphological features is that they are word level models, and do not have direct access sub-word information like inflectional endings, which indicates that these features are difficult to learn effectively purely from lexical distributions."
                    ],
                    "highlighted_evidence": [
                        "Violations",
                        "The most challenging features are all related to Violations. Low performance on Infl/Agr Violations, which marks morphological violations (He washed yourself, This is happy), is especially striking because a relatively high proportion (29%) of these sentences are Simple. These models are likely to be deficient in encoding morphological features is that they are word level models, and do not have direct access sub-word information like inflectional endings, which indicates that these features are difficult to learn effectively purely from lexical distributions."
                    ]
                }
            ]
        },
        {
            "question": "Which models are best for learning long-distance movement?",
            "answers": [
                {
                    "answer": "the transformer models",
                    "type": "extractive"
                }
            ],
            "q_uid": "675d7c48541b6368df135f71f9fc13a398f0c8c6",
            "evidence": [
                {
                    "raw_evidence": [
                        "We identify many specific syntactic features that make sentences harder to classify, and many that have little effect. For instance, sentences involving unusual or marked argument structures are no harder than the average sentence, while sentences with long distance dependencies are hard to learn. We also find features of sentences that accentuate or minimize the differences between models. Specifically, the transformer models seem to learn long-distance dependencies much better than the recurrent model, yet have no advantage on sentences with morphological violations."
                    ],
                    "highlighted_evidence": [
                        "We identify many specific syntactic features that make sentences harder to classify, and many that have little effect. For instance, sentences involving unusual or marked argument structures are no harder than the average sentence, while sentences with long distance dependencies are hard to learn. We also find features of sentences that accentuate or minimize the differences between models. Specifically, the transformer models seem to learn long-distance dependencies much better than the recurrent model, yet have no advantage on sentences with morphological violations.",
                        "We identify many specific syntactic features that make sentences harder to classify, and many that have little effect. For instance, sentences involving unusual or marked argument structures are no harder than the average sentence, while sentences with long distance dependencies are hard to learn. We also find features of sentences that accentuate or minimize the differences between models. Specifically, the transformer models seem to learn long-distance dependencies much better than the recurrent model, yet have no advantage on sentences with morphological violations."
                    ]
                }
            ]
        },
        {
            "question": "Where does the data in CoLA come from?",
            "answers": [
                {
                    "answer": " CoLA contains example sentences from linguistics publications labeled by experts",
                    "type": "extractive"
                }
            ],
            "q_uid": "868c69c8f623e30b96df5b5c8336070994469f60",
            "evidence": [
                {
                    "raw_evidence": [
                        "The effectiveness and ubiquity of pretrained sentence embeddings for natural language understanding has grown dramatically in recent years. Recent sentence encoders like OpenAI's Generative Pretrained Transformer BIBREF3 and BERT BIBREF2 achieve the state of the art on the GLUE benchmark BIBREF4 . Among the GLUE tasks, these state-of-the-art systems make their greatest gains on the acceptability task with the Corpus of Linguistic Acceptability BIBREF0 . CoLA contains example sentences from linguistics publications labeled by experts for grammatical acceptability, and written to show subtle grammatical features. Because minimal syntactic differences can separate acceptable sentences from unacceptable ones (What did Bo write a book about? / *What was a book about written by Bo?), and acceptability classifiers are more reliable when trained on GPT and BERT than on recurrent models, it stands to reason that GPT and BERT have better implicit knowledge of syntactic features relevant to acceptability."
                    ],
                    "highlighted_evidence": [
                        " Among the GLUE tasks, these state-of-the-art systems make their greatest gains on the acceptability task with the Corpus of Linguistic Acceptability BIBREF0 . CoLA contains example sentences from linguistics publications labeled by experts for grammatical acceptability, and written to show subtle grammatical features. Because minimal syntactic differences can separate acceptable sentences from unacceptable ones (What did Bo write a book about? / *What was a book about written by Bo?), and acceptability classifiers are more reliable when trained on GPT and BERT than on recurrent models, it stands to reason that GPT and BERT have better implicit knowledge of syntactic features relevant to acceptability."
                    ]
                }
            ]
        },
        {
            "question": "How is the CoLA grammatically annotated?",
            "answers": [
                {
                    "answer": "labeled by experts",
                    "type": "extractive"
                }
            ],
            "q_uid": "f809fd0d3acfaccbe6c8abb4a9d951a83eec9a32",
            "evidence": [
                {
                    "raw_evidence": [
                        "The effectiveness and ubiquity of pretrained sentence embeddings for natural language understanding has grown dramatically in recent years. Recent sentence encoders like OpenAI's Generative Pretrained Transformer BIBREF3 and BERT BIBREF2 achieve the state of the art on the GLUE benchmark BIBREF4 . Among the GLUE tasks, these state-of-the-art systems make their greatest gains on the acceptability task with the Corpus of Linguistic Acceptability BIBREF0 . CoLA contains example sentences from linguistics publications labeled by experts for grammatical acceptability, and written to show subtle grammatical features. Because minimal syntactic differences can separate acceptable sentences from unacceptable ones (What did Bo write a book about? / *What was a book about written by Bo?), and acceptability classifiers are more reliable when trained on GPT and BERT than on recurrent models, it stands to reason that GPT and BERT have better implicit knowledge of syntactic features relevant to acceptability."
                    ],
                    "highlighted_evidence": [
                        "Among the GLUE tasks, these state-of-the-art systems make their greatest gains on the acceptability task with the Corpus of Linguistic Acceptability BIBREF0 . CoLA contains example sentences from linguistics publications labeled by experts for grammatical acceptability, and written to show subtle grammatical features. Because minimal syntactic differences can separate acceptable sentences from unacceptable ones (What did Bo write a book about? / *What was a book about written by Bo?), and acceptability classifiers are more reliable when trained on GPT and BERT than on recurrent models, it stands to reason that GPT and BERT have better implicit knowledge of syntactic features relevant to acceptability."
                    ]
                }
            ]
        }
    ],
    "2001.02380": [
        {
            "question": "How is the delta-softmax calculated?",
            "answers": [
                {
                    "answer": "Answer with content missing: (Formula) Formula is the answer.",
                    "type": "abstractive"
                }
            ],
            "q_uid": "4059c6f395640a6acf20a0ed451d0ad8681bc59b",
            "evidence": [
                {
                    "raw_evidence": [
                        "We reason that, if a token is important for predicting the correct label, masking it will degrade the model's classification accuracy, or at least reduce its reported classification certainty. In SECREF36, it seems reasonable to assume that masking the word `To' has a greater impact on predicting the label purpose than masking the word `provide', and even less so, the following noun `information'. We therefore use reduction in softmax probability of the correct relation as our signaling strength metric for the model. We call this metric ${\\Delta }_s$ (for delta-softmax), which can be written as:",
                        "where $rel$ is the true relation of the EDU pair, $t_i$ represents the token at index $i$ of $N$ tokens, and $X_{mask=i}$ represents the input sequence with the masked position $i$ (for $i \\in 1 \\ldots N$ ignoring separators, or $\\phi $, the empty set)."
                    ],
                    "highlighted_evidence": [
                        "We therefore use reduction in softmax probability of the correct relation as our signaling strength metric for the model. We call this metric ${\\Delta }_s$ (for delta-softmax), which can be written as:\n\nwhere $rel$ is the true relation of the EDU pair, $t_i$ represents the token at index $i$ of $N$ tokens, and $X_{mask=i}$ represents the input sequence with the masked position $i$ (for $i \\in 1 \\ldots N$ ignoring separators, or $\\phi $, the empty set)."
                    ]
                }
            ]
        },
        {
            "question": "Are some models evaluated using this metric, what are the findings?",
            "answers": [
                {
                    "answer": "Yes",
                    "type": "boolean"
                }
            ],
            "q_uid": "99d7bef0ef395360b939a3f446eff67239551a9d",
            "evidence": [
                {
                    "raw_evidence": [
                        "As a framework, we use the sentence classifier configuration of FLAIR BIBREF46 with a biLSTM encoder/classifier architecture fed by character and word level representations composed of a concatenation of fixed 300 dimensional GloVe embeddings BIBREF47, pre-trained contextualized FLAIR word embeddings, and pre-trained contextualized character embeddings from AllenNLP BIBREF48 with FLAIR's default hyperparameters. The model's architecture is shown in Figure FIGREF30."
                    ],
                    "highlighted_evidence": [
                        "As a framework, we use the sentence classifier configuration of FLAIR BIBREF46 with a biLSTM encoder/classifier architecture fed by character and word level representations composed of a concatenation of fixed 300 dimensional GloVe embeddings BIBREF47, pre-trained contextualized FLAIR word embeddings, and pre-trained contextualized character embeddings from AllenNLP BIBREF48 with FLAIR's default hyperparameters."
                    ]
                }
            ]
        },
        {
            "question": "Where does proposed metric differ from juman judgement?",
            "answers": [
                {
                    "answer": "model points out plausible signals which were passed over by an annotator, it also picks up on a recurring tendency in how-to guides in which the second person pronoun referring to the reader is often the benefactee of some action",
                    "type": "extractive"
                }
            ],
            "q_uid": "a1097ce59270d6f521d92df8d2e3a279abee3e67",
            "evidence": [
                {
                    "raw_evidence": [
                        "In other cases, the model points out plausible signals which were passed over by an annotator, and may be considered errors in the gold standard. For example, the model easily notices that question marks indicate the solutionhood relation, even where these were skipped by annotators in favor of marking WH words instead:",
                        ". [RGB]230, 230, 230Which [RGB]230, 230, 230previous [RGB]230, 230, 230Virginia [RGB]230, 230, 230Governor(s) [RGB]230, 230, 230do [RGB]230, 230, 230you [RGB]230, 230, 230most [RGB]230, 230, 230admire [RGB]230, 230, 230and [RGB]230, 230, 230why [RGB]12, 12, 12? $\\xrightarrow[\\text{pred:solutionhood}]{\\text{gold:solutionhood}}$ [RGB]230, 230, 230Thomas [RGB]230, 230, 230Jefferson [RGB]183, 183, 183.",
                        "Unsurprisingly, the model sometimes make sporadic errors in signal detection for which good explanations are hard to find, especially when its predicted relation is incorrect, as in SECREF43. Here the evaluative adjective remarkable is missed in favor of neighboring words such as agreed and a subject pronoun, which are not indicative of the evaluation relation in this context but are part of several cohorts of high scoring words. However, the most interesting and interpretable errors arise when ${\\Delta }_s$ scores are high compared to an entire document, and not just among words in one EDU pair, in which most or even all words may be relatively weak signals. As an example of such a false positive with high confidence, we can consider SECREF43. In this example, the model correctly assigns the highest score to the DM so marking a purpose relation. However, it also picks up on a recurring tendency in how-to guides in which the second person pronoun referring to the reader is often the benefactee of some action, which contributes to the purpose reading and helps to disambiguate so, despite not being considered a signal by annotators."
                    ],
                    "highlighted_evidence": [
                        "In other cases, the model points out plausible signals which were passed over by an annotator, and may be considered errors in the gold standard. For example, the model easily notices that question marks indicate the solutionhood relation, even where these were skipped by annotators in favor of marking WH words instead:\n\n. [RGB]230, 230, 230Which [RGB]230, 230, 230previous [RGB]230, 230, 230Virginia [RGB]230, 230, 230Governor(s) [RGB]230, 230, 230do [RGB]230, 230, 230you [RGB]230, 230, 230most [RGB]230, 230, 230admire [RGB]230, 230, 230and [RGB]230, 230, 230why [RGB]12, 12, 12? $\\xrightarrow[\\text{pred:solutionhood}]{\\text{gold:solutionhood}}$ [RGB]230, 230, 230Thomas [RGB]230, 230, 230Jefferson [RGB]183, 183, 183.",
                        "However, it also picks up on a recurring tendency in how-to guides in which the second person pronoun referring to the reader is often the benefactee of some action, which contributes to the purpose reading and helps to disambiguate so, despite not being considered a signal by annotators."
                    ]
                }
            ]
        },
        {
            "question": "Where does proposed metric overlap with juman judgement?",
            "answers": [
                {
                    "answer": "influence of each word on the score of the correct relation, that impact should and does still correlate with human judgments",
                    "type": "extractive"
                }
            ],
            "q_uid": "56e58bdf0df76ad1599021801f6d4c7b77953e29",
            "evidence": [
                {
                    "raw_evidence": [
                        "Looking at the model's performance qualitatively, it is clear that it can detect not only DMs, but also morphological cues (e.g. gerunds as markers of elaboration, as in SECREF43), semantic classes and sentiment, such as positive and negative evaluatory terms in SECREF43, as well as multiple signals within the same EDU, as in SECREF43. In fact, only about 8.3% of the tokens correctly identified by the model in Table TABREF45 below are of the DM type, whereas about 7.2% of all tokens flagged by human annotators were DMs, meaning that the model frequently matches non-DM items to discourse relation signals (see Performance on Signal Types below). It should also be noted that signals can be recognized even when the model misclassifies relations, since ${\\Delta }_s$ does not rely on correct classification: it merely quantifies the contribution of a word in context toward the correct label's score. If we examine the influence of each word on the score of the correct relation, that impact should and does still correlate with human judgments based on what the system may tag as the second or third best class to choose."
                    ],
                    "highlighted_evidence": [
                        "It should also be noted that signals can be recognized even when the model misclassifies relations, since ${\\Delta }_s$ does not rely on correct classification: it merely quantifies the contribution of a word in context toward the correct label's score. If we examine the influence of each word on the score of the correct relation, that impact should and does still correlate with human judgments based on what the system may tag as the second or third best class to choose."
                    ]
                }
            ]
        }
    ],
    "1904.11942": [
        {
            "question": "What conclusions do the authors draw from their detailed analyses?",
            "answers": [
                {
                    "answer": "neural network-based models can outperform feature-based models with wide margins, contextualized representation learning can boost performance of NN models",
                    "type": "extractive"
                }
            ],
            "q_uid": "4266aacb575b4be7dbcdb8616766324f8790763c",
            "evidence": [
                {
                    "raw_evidence": [
                        "We established strong baselines for two story narrative understanding datasets: CaTeRS and RED. We have shown that neural network-based models can outperform feature-based models with wide margins, and we conducted an ablation study to show that contextualized representation learning can boost performance of NN models. Further research can focus on more systematic study or build stronger NN models over the same datasets used in this work. Exploring possibilities to directly apply temporal relation extraction to enhance performance of story generation systems is another promising research direction."
                    ],
                    "highlighted_evidence": [
                        "We have shown that neural network-based models can outperform feature-based models with wide margins, and we conducted an ablation study to show that contextualized representation learning can boost performance of NN models."
                    ]
                }
            ]
        },
        {
            "question": "Do the BERT-based embeddings improve results?",
            "answers": [
                {
                    "answer": "Yes",
                    "type": "boolean"
                }
            ],
            "q_uid": "191107cd112f7ee6d19c1dc43177e6899452a2c7",
            "evidence": [
                {
                    "raw_evidence": [
                        "Table TABREF25 contains the best hyper-parameters and Table TABREF26 contains micro-average F1 scores for both datasets on dev and test sets. We only consider positive pairs, i.e. correct predictions on NONE pairs are excluded for evaluation. In general, the baseline model CAEVO is outperformed by both NN models, and NN model with BERT embedding achieves the greatest performance. We now provide more detailed analysis and discussion for each dataset."
                    ],
                    "highlighted_evidence": [
                        "In general, the baseline model CAEVO is outperformed by both NN models, and NN model with BERT embedding achieves the greatest performance."
                    ]
                }
            ]
        },
        {
            "question": "What were the traditional linguistic feature-based models?",
            "answers": [
                {
                    "answer": "CAEVO",
                    "type": "extractive"
                }
            ],
            "q_uid": "b0dca7b74934f51ff3da0c074ad659c25d84174d",
            "evidence": [
                {
                    "raw_evidence": [
                        "The series of TempEval competitions BIBREF21 , BIBREF22 , BIBREF23 have attracted many research interests in predicting event temporal relations. Early attempts by BIBREF24 , BIBREF21 , BIBREF25 , BIBREF26 only use pair-wise classification models. State-of-the-art local methods, such as ClearTK BIBREF27 , UTTime BIBREF28 , and NavyTime BIBREF29 improve on earlier work by feature engineering with linguistic and syntactic rules. As we mention in the Section 2, CAEVO is the current state-of-the-art system for feature-based temporal event relation extraction BIBREF10 . It's widely used as the baseline for evaluating TB-Dense data. We adopt it as our baseline for evaluating CaTeRS and RED datasets. Additionally, several models BramsenDLB2006, ChambersJ2008, DoLuRo12, NingWuRo18, P18-1212 have successfully incorporated global inference to impose global prediction consistency such as temporal transitivity."
                    ],
                    "highlighted_evidence": [
                        "As we mention in the Section 2, CAEVO is the current state-of-the-art system for feature-based temporal event relation extraction BIBREF10 . It's widely used as the baseline for evaluating TB-Dense data. We adopt it as our baseline for evaluating CaTeRS and RED datasets."
                    ]
                }
            ]
        },
        {
            "question": "What type of baseline are established for the two datasets?",
            "answers": [
                {
                    "answer": "CAEVO",
                    "type": "extractive"
                }
            ],
            "q_uid": "601e58a3d2c03a0b4cd627c81c6228a714e43903",
            "evidence": [
                {
                    "raw_evidence": [
                        "The series of TempEval competitions BIBREF21 , BIBREF22 , BIBREF23 have attracted many research interests in predicting event temporal relations. Early attempts by BIBREF24 , BIBREF21 , BIBREF25 , BIBREF26 only use pair-wise classification models. State-of-the-art local methods, such as ClearTK BIBREF27 , UTTime BIBREF28 , and NavyTime BIBREF29 improve on earlier work by feature engineering with linguistic and syntactic rules. As we mention in the Section 2, CAEVO is the current state-of-the-art system for feature-based temporal event relation extraction BIBREF10 . It's widely used as the baseline for evaluating TB-Dense data. We adopt it as our baseline for evaluating CaTeRS and RED datasets. Additionally, several models BramsenDLB2006, ChambersJ2008, DoLuRo12, NingWuRo18, P18-1212 have successfully incorporated global inference to impose global prediction consistency such as temporal transitivity."
                    ],
                    "highlighted_evidence": [
                        "As we mention in the Section 2, CAEVO is the current state-of-the-art system for feature-based temporal event relation extraction BIBREF10 . It's widely used as the baseline for evaluating TB-Dense data. We adopt it as our baseline for evaluating CaTeRS and RED datasets."
                    ]
                }
            ]
        }
    ],
    "1907.05664": [
        {
            "question": "Which baselines did they compare?",
            "answers": [
                {
                    "answer": "The baseline model is a deep sequence-to-sequence encoder/decoder model with attention. The encoder is a bidirectional Long-Short Term Memory(LSTM) cell BIBREF14 and the decoder a single LSTM cell with attention mechanism. The attention mechanism is computed as in BIBREF9 and we use a greedy search for decoding. We train end-to-end including the words embeddings. The embedding size used is of 128 and the hidden state size of the LSTM cells is of 254.",
                    "type": "extractive"
                },
                {
                    "answer": "The baseline model is a deep sequence-to-sequence encoder/decoder model with attention. The encoder is a bidirectional Long-Short Term Memory(LSTM) cell BIBREF14 and the decoder a single LSTM cell with attention mechanism. The attention mechanism is computed as in BIBREF9 and we use a greedy search for decoding. We train end-to-end including the words embeddings. The embedding size used is of 128 and the hidden state size of the LSTM cells is of 254.",
                    "type": "extractive"
                }
            ],
            "q_uid": "6e2ad9ad88cceabb6977222f5e090ece36aa84ea",
            "evidence": [
                {
                    "raw_evidence": [
                        "We present in this section the baseline model from See et al. See2017 trained on the CNN/Daily Mail dataset. We reproduce the results from See et al. See2017 to then apply LRP on it.",
                        "The baseline model is a deep sequence-to-sequence encoder/decoder model with attention. The encoder is a bidirectional Long-Short Term Memory(LSTM) cell BIBREF14 and the decoder a single LSTM cell with attention mechanism. The attention mechanism is computed as in BIBREF9 and we use a greedy search for decoding. We train end-to-end including the words embeddings. The embedding size used is of 128 and the hidden state size of the LSTM cells is of 254."
                    ],
                    "highlighted_evidence": [
                        "We present in this section the baseline model from See et al. See2017 trained on the CNN/Daily Mail dataset.",
                        "The baseline model is a deep sequence-to-sequence encoder/decoder model with attention. The encoder is a bidirectional Long-Short Term Memory(LSTM) cell BIBREF14 and the decoder a single LSTM cell with attention mechanism. The attention mechanism is computed as in BIBREF9 and we use a greedy search for decoding. We train end-to-end including the words embeddings. The embedding size used is of 128 and the hidden state size of the LSTM cells is of 254."
                    ]
                },
                {
                    "raw_evidence": [
                        "The baseline model is a deep sequence-to-sequence encoder/decoder model with attention. The encoder is a bidirectional Long-Short Term Memory(LSTM) cell BIBREF14 and the decoder a single LSTM cell with attention mechanism. The attention mechanism is computed as in BIBREF9 and we use a greedy search for decoding. We train end-to-end including the words embeddings. The embedding size used is of 128 and the hidden state size of the LSTM cells is of 254."
                    ],
                    "highlighted_evidence": [
                        "The baseline model is a deep sequence-to-sequence encoder/decoder model with attention. The encoder is a bidirectional Long-Short Term Memory(LSTM) cell BIBREF14 and the decoder a single LSTM cell with attention mechanism. The attention mechanism is computed as in BIBREF9 and we use a greedy search for decoding. We train end-to-end including the words embeddings. The embedding size used is of 128 and the hidden state size of the LSTM cells is of 254."
                    ]
                }
            ]
        },
        {
            "question": "How many attention layers are there in their model?",
            "answers": [
                {
                    "answer": "one",
                    "type": "abstractive"
                }
            ],
            "q_uid": "aacb0b97aed6fc6a8b471b8c2e5c4ddb60988bf5",
            "evidence": [
                {
                    "raw_evidence": [
                        "The baseline model is a deep sequence-to-sequence encoder/decoder model with attention. The encoder is a bidirectional Long-Short Term Memory(LSTM) cell BIBREF14 and the decoder a single LSTM cell with attention mechanism. The attention mechanism is computed as in BIBREF9 and we use a greedy search for decoding. We train end-to-end including the words embeddings. The embedding size used is of 128 and the hidden state size of the LSTM cells is of 254."
                    ],
                    "highlighted_evidence": [
                        "The encoder is a bidirectional Long-Short Term Memory(LSTM) cell BIBREF14 and the decoder a single LSTM cell with attention mechanism. "
                    ]
                }
            ]
        },
        {
            "question": "Is the explanation from saliency map correct?",
            "answers": [
                {
                    "answer": "No",
                    "type": "boolean"
                }
            ],
            "q_uid": "710c1f8d4c137c8dad9972f5ceacdbf8004db208",
            "evidence": [
                {
                    "raw_evidence": [
                        "We showed that in some cases the saliency maps are truthful to the network's computation, meaning that they do highlight the input features that the network focused on. But we also showed that in some cases the saliency maps seem to not capture the important input features. This brought us to discuss the fact that these attributions are not sufficient by themselves, and that we need to define the counter-factual case and test it to measure how truthful the saliency maps are.",
                        "The second observation we can make is that the saliency map doesn't seem to highlight the right things in the input for the summary it generates. The saliency maps on Figure 3 correspond to the summary from Figure 1 , and we don't see the word \u201cvideo\" highlighted in the input text, which seems to be important for the output."
                    ],
                    "highlighted_evidence": [
                        "But we also showed that in some cases the saliency maps seem to not capture the important input features. ",
                        "The second observation we can make is that the saliency map doesn't seem to highlight the right things in the input for the summary it generates"
                    ]
                }
            ]
        }
    ],
    "1909.08752": [
        {
            "question": "What's the method used here?",
            "answers": [
                {
                    "answer": "Two neural networks: an extractor based on an encoder (BERT) and a decoder (LSTM Pointer Network BIBREF22) and an abstractor identical to the one proposed in BIBREF8.",
                    "type": "abstractive"
                }
            ],
            "q_uid": "0b411f942c6e2e34e3d81cc855332f815b6bc123",
            "evidence": [
                {
                    "raw_evidence": [
                        "Our model consists of two neural network modules, i.e. an extractor and abstractor. The extractor encodes a source document and chooses sentences from the document, and then the abstractor paraphrases the summary candidates. Formally, a single document consists of $n$ sentences $D=\\lbrace s_1,s_2,\\cdots ,s_n\\rbrace $. We denote $i$-th sentence as $s_i=\\lbrace w_{i1},w_{i2},\\cdots ,w_{im}\\rbrace $ where $w_{ij}$ is the $j$-th word in $s_i$. The extractor learns to pick out a subset of $D$ denoted as $\\hat{D}=\\lbrace \\hat{s}_1,\\hat{s}_2,\\cdots ,\\hat{s}_k|\\hat{s}_i\\in D\\rbrace $ where $k$ sentences are selected. The abstractor rewrites each of the selected sentences to form a summary $S=\\lbrace f(\\hat{s}_1),f(\\hat{s}_2),\\cdots ,f(\\hat{s}_k)\\rbrace $, where $f$ is an abstracting function. And a gold summary consists of $l$ sentences $A=\\lbrace a_1,a_2,\\cdots ,a_l\\rbrace $.",
                        "The extractor is based on the encoder-decoder framework. We adapt BERT for the encoder to exploit contextualized representations from pre-trained transformers. BERT as the encoder maps the input sequence $D$ to sentence representation vectors $H=\\lbrace h_1,h_2,\\cdots ,h_n\\rbrace $, where $h_i$ is for the $i$-th sentence in the document. Then, the decoder utilizes $H$ to extract $\\hat{D}$ from $D$.",
                        "We use LSTM Pointer Network BIBREF22 as the decoder to select the extracted sentences based on the above sentence representations. The decoder extracts sentences recurrently, producing a distribution over all of the remaining sentence representations excluding those already selected. Since we use the sequential model which selects one sentence at a time step, our decoder can consider the previously selected sentences. This property is needed to avoid selecting sentences that have overlapping information with the sentences extracted already.",
                        "The abstractor network approximates $f$, which compresses and paraphrases an extracted document sentence to a concise summary sentence. We use the standard attention based sequence-to-sequence (seq2seq) model BIBREF23, BIBREF24 with the copying mechanism BIBREF25 for handling out-of-vocabulary (OOV) words. Our abstractor is practically identical to the one proposed in BIBREF8."
                    ],
                    "highlighted_evidence": [
                        "Our model consists of two neural network modules, i.e. an extractor and abstractor. The extractor encodes a source document and chooses sentences from the document, and then the abstractor paraphrases the summary candidates. ",
                        "The extractor is based on the encoder-decoder framework. We adapt BERT for the encoder to exploit contextualized representations from pre-trained transformers.",
                        "We use LSTM Pointer Network BIBREF22 as the decoder to select the extracted sentences based on the above sentence representations.",
                        " Our abstractor is practically identical to the one proposed in BIBREF8."
                    ]
                }
            ]
        }
    ],
    "1902.00330": [
        {
            "question": "What datasets used for evaluation?",
            "answers": [
                {
                    "answer": "AIDA-B, ACE2004, MSNBC, AQUAINT, WNED-CWEB, WNED-WIKI",
                    "type": "extractive"
                },
                {
                    "answer": "AIDA-CoNLL, ACE2004, MSNBC, AQUAINT, WNED-CWEB, WNED-WIKI, OURSELF-WIKI",
                    "type": "extractive"
                }
            ],
            "q_uid": "dad8cc543a87534751f9f9e308787e1af06f0627",
            "evidence": [
                {
                    "raw_evidence": [
                        "We conduct experiments on several different types of public datasets including news and encyclopedia corpus. The training set is AIDA-Train and Wikipedia datasets, where AIDA-Train contains 18448 mentions and Wikipedia contains 25995 mentions. In order to compare with the previous methods, we evaluate our model on AIDA-B and other datasets. These datasets are well-known and have been used for the evaluation of most entity linking systems. The statistics of the datasets are shown in Table 1.",
                        "AIDA-CoNLL BIBREF14 is annotated on Reuters news articles. It contains training (AIDA-Train), validation (AIDA-A) and test (AIDA-B) sets.",
                        "ACE2004 BIBREF15 is a subset of the ACE2004 Coreference documents.",
                        "MSNBC BIBREF16 contains top two stories in the ten news categories(Politics, Business, Sports etc.)",
                        "AQUAINT BIBREF17 is a news corpus from the Xinhua News Service, the New York Times, and the Associated Press.",
                        "WNED-CWEB BIBREF18 is randomly picked from the FACC1 annotated ClueWeb 2012 dataset.",
                        "WNED-WIKI BIBREF18 is crawled from Wikipedia pages with its original hyperlink annotation."
                    ],
                    "highlighted_evidence": [
                        "In order to compare with the previous methods, we evaluate our model on AIDA-B and other datasets. ",
                        "AIDA-CoNLL BIBREF14 is annotated on Reuters news articles. It contains training (AIDA-Train), validation (AIDA-A) and test (AIDA-B) sets.\n\nACE2004 BIBREF15 is a subset of the ACE2004 Coreference documents.\n\nMSNBC BIBREF16 contains top two stories in the ten news categories(Politics, Business, Sports etc.)\n\nAQUAINT BIBREF17 is a news corpus from the Xinhua News Service, the New York Times, and the Associated Press.\n\nWNED-CWEB BIBREF18 is randomly picked from the FACC1 annotated ClueWeb 2012 dataset.\n\nWNED-WIKI BIBREF18 is crawled from Wikipedia pages with its original hyperlink annotation."
                    ]
                },
                {
                    "raw_evidence": [
                        "We conduct experiments on several different types of public datasets including news and encyclopedia corpus. The training set is AIDA-Train and Wikipedia datasets, where AIDA-Train contains 18448 mentions and Wikipedia contains 25995 mentions. In order to compare with the previous methods, we evaluate our model on AIDA-B and other datasets. These datasets are well-known and have been used for the evaluation of most entity linking systems. The statistics of the datasets are shown in Table 1.",
                        "AIDA-CoNLL BIBREF14 is annotated on Reuters news articles. It contains training (AIDA-Train), validation (AIDA-A) and test (AIDA-B) sets.",
                        "ACE2004 BIBREF15 is a subset of the ACE2004 Coreference documents.",
                        "MSNBC BIBREF16 contains top two stories in the ten news categories(Politics, Business, Sports etc.)",
                        "AQUAINT BIBREF17 is a news corpus from the Xinhua News Service, the New York Times, and the Associated Press.",
                        "WNED-CWEB BIBREF18 is randomly picked from the FACC1 annotated ClueWeb 2012 dataset.",
                        "WNED-WIKI BIBREF18 is crawled from Wikipedia pages with its original hyperlink annotation.",
                        "OURSELF-WIKI is crawled by ourselves from Wikipedia pages."
                    ],
                    "highlighted_evidence": [
                        "We conduct experiments on several different types of public datasets including news and encyclopedia corpus. ",
                        "AIDA-CoNLL BIBREF14 is annotated on Reuters news articles. It contains training (AIDA-Train), validation (AIDA-A) and test (AIDA-B) sets.\n\nACE2004 BIBREF15 is a subset of the ACE2004 Coreference documents.\n\nMSNBC BIBREF16 contains top two stories in the ten news categories(Politics, Business, Sports etc.)\n\nAQUAINT BIBREF17 is a news corpus from the Xinhua News Service, the New York Times, and the Associated Press.\n\nWNED-CWEB BIBREF18 is randomly picked from the FACC1 annotated ClueWeb 2012 dataset.\n\nWNED-WIKI BIBREF18 is crawled from Wikipedia pages with its original hyperlink annotation.\n\nOURSELF-WIKI is crawled by ourselves from Wikipedia pages."
                    ]
                }
            ]
        }
    ],
    "1709.05411": [
        {
            "question": "Why mixed initiative multi-turn dialogs are the greatest challenge in building open-domain conversational agents?",
            "answers": [
                {
                    "answer": "do not follow a particular plan or pursue a particular fixed information need,  integrating content found via search with content from structured data, at each system turn, there are a large number of conversational moves that are possible, most other domains do not have such high quality structured data available, live search may not be able to achieve the required speed and efficiency",
                    "type": "extractive"
                }
            ],
            "q_uid": "65e6a1cc2590b139729e7e44dce6d9af5dd2c3b5",
            "evidence": [
                {
                    "raw_evidence": [
                        "The Alexa Prize funded 12 international teams to compete to create a conversational agent that can discuss any topic for at least 20 minutes. UCSC's Slugbot was one of these funded teams. The greatest challenges with the competition arise directly from the potential for ongoing mixed-initiative multi-turn dialogues, which do not follow a particular plan or pursue a particular fixed information need. This paper describes some of the lessons we learned building SlugBot for the 2017 Alexa Prize, particularly focusing on the challenges of integrating content found via search with content from structured data in order to carry on an ongoing, coherent, open-domain, mixed-initiative conversation. SlugBot's conversations over the semi-finals user evaluation averaged 8:17 minutes.",
                        "More challenging is that at each system turn, there are a large number of conversational moves that are possible. Making good decisions about what to say next requires balancing a dialogue policy as to what dialogue acts might be good in this context, with real-time information as to what types of content might be possible to use in this context. Slugbot could offer an opinion as in turn S3, ask a follow-on question as in S3, take the initiative to provide unasked for information, as in S5, or decide, e.g. in the case of the user's request for plot information, to use search to retrieve some relevant content. Search cannot be used effectively here without constructing an appropriate query, or knowing in advance where plot information might be available. In a real-time system, live search may not be able to achieve the required speed and efficiency, so preprocessing or caching of relevant information may be necessary. Finally, most other domains do not have such high quality structured data available, leaving us to develop or try to rely on more general models of discourse coherence."
                    ],
                    "highlighted_evidence": [
                        "The greatest challenges with the competition arise directly from the potential for ongoing mixed-initiative multi-turn dialogues, which do not follow a particular plan or pursue a particular fixed information need. ",
                        "This paper describes some of the lessons we learned building SlugBot for the 2017 Alexa Prize, particularly focusing on the challenges of integrating content found via search with content from structured data in order to carry on an ongoing, coherent, open-domain, mixed-initiative conversation",
                        "More challenging is that at each system turn, there are a large number of conversational moves that are possible.",
                        " Finally, most other domains do not have such high quality structured data available, leaving us to develop or try to rely on more general models of discourse coherence.",
                        " Search cannot be used effectively here without constructing an appropriate query, or knowing in advance where plot information might be available. In a real-time system, live search may not be able to achieve the required speed and efficiency, so preprocessing or caching of relevant information may be necessary. "
                    ]
                }
            ]
        }
    ],
    "1804.09301": [
        {
            "question": "Which coreference resolution systems are tested?",
            "answers": [
                {
                    "answer": "the BIBREF5 sieve system from the rule-based paradigm (referred to as RULE), BIBREF6 from the statistical paradigm (STAT), and the BIBREF11 deep reinforcement system from the neural paradigm (NEURAL).",
                    "type": "extractive"
                }
            ],
            "q_uid": "c2553166463b7b5ae4d9786f0446eb06a90af458",
            "evidence": [
                {
                    "raw_evidence": [
                        "In this work, we evaluate three publicly-available off-the-shelf coreference resolution systems, representing three different machine learning paradigms: rule-based systems, feature-driven statistical systems, and neural systems.",
                        "We evaluate examples of each of the three coreference system architectures described in \"Coreference Systems\" : the BIBREF5 sieve system from the rule-based paradigm (referred to as RULE), BIBREF6 from the statistical paradigm (STAT), and the BIBREF11 deep reinforcement system from the neural paradigm (NEURAL)."
                    ],
                    "highlighted_evidence": [
                        "In this work, we evaluate three publicly-available off-the-shelf coreference resolution systems, representing three different machine learning paradigms: rule-based systems, feature-driven statistical systems, and neural systems.",
                        "We evaluate examples of each of the three coreference system architectures described in \"Coreference Systems\" : the BIBREF5 sieve system from the rule-based paradigm (referred to as RULE), BIBREF6 from the statistical paradigm (STAT), and the BIBREF11 deep reinforcement system from the neural paradigm (NEURAL)."
                    ]
                }
            ]
        }
    ],
    "1711.04457": [
        {
            "question": "Where does the vocabulary come from?",
            "answers": [
                {
                    "answer": "LDC corpus",
                    "type": "extractive"
                }
            ],
            "q_uid": "da495e2f99ee2d5db9cc17eca5517ddaa5ea8e42",
            "evidence": [
                {
                    "raw_evidence": [
                        "Our training data consists of 2.09M sentence pairs extracted from LDC corpus. Table 1 shows the detailed statistics of our training data. To test different approaches on Chinese-to-English translation task, we use NIST 2003(MT03) dataset as the validation set, and NIST 2004(MT04), NIST 2005(MT05), NIST 2006(MT06) datasets as our test sets. For English-to-Chinese translation task, we also use NIST 2003(MT03) dataset as the validation set, and NIST 2008(MT08) will be used as test set."
                    ],
                    "highlighted_evidence": [
                        "Our training data consists of 2.09M sentence pairs extracted from LDC corpus."
                    ]
                }
            ]
        },
        {
            "question": "What dataset did they use?",
            "answers": [
                {
                    "answer": "LDC corpus, NIST 2003(MT03), NIST 2004(MT04), NIST 2005(MT05), NIST 2006(MT06), NIST 2008(MT08)",
                    "type": "extractive"
                }
            ],
            "q_uid": "310e61b9dd4d75bc1bebbcb1dae578f55807cd04",
            "evidence": [
                {
                    "raw_evidence": [
                        "Our training data consists of 2.09M sentence pairs extracted from LDC corpus. Table 1 shows the detailed statistics of our training data. To test different approaches on Chinese-to-English translation task, we use NIST 2003(MT03) dataset as the validation set, and NIST 2004(MT04), NIST 2005(MT05), NIST 2006(MT06) datasets as our test sets. For English-to-Chinese translation task, we also use NIST 2003(MT03) dataset as the validation set, and NIST 2008(MT08) will be used as test set."
                    ],
                    "highlighted_evidence": [
                        "Our training data consists of 2.09M sentence pairs extracted from LDC corpus.",
                        "To test different approaches on Chinese-to-English translation task, we use NIST 2003(MT03) dataset as the validation set, and NIST 2004(MT04), NIST 2005(MT05), NIST 2006(MT06) datasets as our test sets. For English-to-Chinese translation task, we also use NIST 2003(MT03) dataset as the validation set, and NIST 2008(MT08) will be used as test set."
                    ]
                }
            ]
        }
    ],
    "1811.05711": [
        {
            "question": "Which text embedding methodologies are used?",
            "answers": [
                {
                    "answer": "Document to Vector (Doc2Vec)",
                    "type": "extractive"
                },
                {
                    "answer": "Doc2Vec, PV-DBOW model",
                    "type": "extractive"
                }
            ],
            "q_uid": "0689904db9b00a814e3109fb1698086370a28fa2",
            "evidence": [
                {
                    "raw_evidence": [
                        "Figure 1 shows a summary of our pipeline. First, we pre-process each document to transform text into consecutive word tokens, where words are in their most normalised forms, and some words are removed if they have no distinctive meaning when used out of context BIBREF5 , BIBREF6 . We then train a paragraph vector model using the Document to Vector (Doc2Vec) framework BIBREF7 on the whole set (13 million) of preprocessed text records, although training on smaller sets (1 million) also produces good results. This training step is only done once. This Doc2Vec model is subsequently used to infer high-dimensional vector descriptions for the text of each of the 3229 documents in our target analysis set. We then compute a matrix containing pairwise similarities between any pair of document vectors, as inferred with Doc2Vec. This matrix can be thought of as a full, weighted graph with documents as nodes and edges weighted by their similarity. We sparsify this graph to the union of a minimum spanning tree and a k-Nearest Neighbors (MST-kNN) graph BIBREF8 , a geometric construction that removes less important similarities but preserves global connectivity for the graph and, hence, for the dataset. The derived MST-kNN graph is analysed with Markov Stability BIBREF9 , BIBREF10 , BIBREF11 , BIBREF12 , a multi-resolution dynamics-based graph partitioning method that identifies relevant subgraphs (i.e., clusters of documents) at different levels of granularity. MS uses a diffusive process on the graph to reveal the multiscale organisation at different resolutions without the need for choosing a priori the number of clusters, scale or organisation. To analyse a posteriori the different partitions across levels of resolution, we use both visualisations and quantitative scores. The visualisations include word clouds to summarise the main content, graph layouts, as well as Sankey diagrams and contingency tables that capture the correspondences across levels of resolution and relationships to the hand-coded classifications. The partitions are also evaluated quantitatively to score: (i) their intrinsic topic coherence (using pairwise mutual information BIBREF13 , BIBREF14 ), and (ii) their similarity to the operator hand-coded categories (using normalised mutual information BIBREF15 ). We now expand on the steps of the computational framework."
                    ],
                    "highlighted_evidence": [
                        " We then train a paragraph vector model using the Document to Vector (Doc2Vec) framework BIBREF7 on the whole set (13 million) of preprocessed text records, although training on smaller sets (1 million) also produces good results."
                    ]
                },
                {
                    "raw_evidence": [
                        "Here, we use the Gensim Python library BIBREF23 to train the PV-DBOW model. The Doc2Vec training was repeated several times with a variety of training hyper-parameters to optimise the output based on our own numerical experiments and the general guidelines provided by BIBREF24 . We trained Doc2Vec models using text corpora of different sizes and content with different sets of hyper-parameters, in order to characterise the usability and quality of models. Specifically, we checked the effect of corpus size on model quality by training Doc2Vec models on the full 13 million NRLS records and on subsets of 1 million and 2 million randomly sampled records. (We note that our target subset of 3229 records has been excluded from these samples.) Furthermore, we checked the importance of the specificity of the text corpus by obtaining a Doc2Vec model from a generic, non-specific set of 5 million articles from Wikipedia representing standard English usage across a variety of topics."
                    ],
                    "highlighted_evidence": [
                        "Here, we use the Gensim Python library BIBREF23 to train the PV-DBOW model.",
                        "We trained Doc2Vec models using text corpora of different sizes and content with different sets of hyper-parameters, in order to characterise the usability and quality of models."
                    ]
                }
            ]
        }
    ],
    "1909.09534": [
        {
            "question": "Do they report results only on English data?",
            "answers": [
                {
                    "answer": "Yes",
                    "type": "boolean"
                },
                {
                    "answer": "Yes",
                    "type": "boolean"
                }
            ],
            "q_uid": "f20a389ace2267aa61eddcc235535452ccdae0e6",
            "evidence": [
                {
                    "raw_evidence": [
                        "We utilize AWD-LSTM BIBREF20 and TransformerXL BIBREF21 based language models. For model hyperparameters please to refer to Supplementary Section Table TABREF5. We use Adam optimizer BIBREF22 with $\\beta 1= 0.7$ and $\\beta 2= 0.8$ similar to BIBREF19 and use a batch size of 50. Other practices for LM training were the same as BIBREF21 and BIBREF20 for Transformer-XL and AWD-LSTM respectively. We refer to our proposed GAN as Creative-GAN and compare it to a baseline (a language model equivalent to our pre-trained generator) and a GumbelGAN model BIBREF14 across all proposed datasets. We use three creative English datasets with distinct linguistic characteristics: (1) A corpus of 740 classical and contemporary English poems, (2) a corpus of 14950 metaphor sentences retrieved from a metaphor database website and (3) a corpus of 1500 song lyrics ranging across genres. The mix of linguistic styles within this corpus offers the potential for interesting variation during the generation phase. We use the same pre-processing as in earlier work BIBREF19, BIBREF23. We reserve 10% of our data for test set and another 10% for our validation set."
                    ],
                    "highlighted_evidence": [
                        " We use three creative English datasets with distinct linguistic characteristics: (1) A corpus of 740 classical and contemporary English poems, (2) a corpus of 14950 metaphor sentences retrieved from a metaphor database website and (3) a corpus of 1500 song lyrics ranging across genres. "
                    ]
                },
                {
                    "raw_evidence": [
                        "We utilize AWD-LSTM BIBREF20 and TransformerXL BIBREF21 based language models. For model hyperparameters please to refer to Supplementary Section Table TABREF5. We use Adam optimizer BIBREF22 with $\\beta 1= 0.7$ and $\\beta 2= 0.8$ similar to BIBREF19 and use a batch size of 50. Other practices for LM training were the same as BIBREF21 and BIBREF20 for Transformer-XL and AWD-LSTM respectively. We refer to our proposed GAN as Creative-GAN and compare it to a baseline (a language model equivalent to our pre-trained generator) and a GumbelGAN model BIBREF14 across all proposed datasets. We use three creative English datasets with distinct linguistic characteristics: (1) A corpus of 740 classical and contemporary English poems, (2) a corpus of 14950 metaphor sentences retrieved from a metaphor database website and (3) a corpus of 1500 song lyrics ranging across genres. The mix of linguistic styles within this corpus offers the potential for interesting variation during the generation phase. We use the same pre-processing as in earlier work BIBREF19, BIBREF23. We reserve 10% of our data for test set and another 10% for our validation set."
                    ],
                    "highlighted_evidence": [
                        "We use three creative English datasets with distinct linguistic characteristics: (1) A corpus of 740 classical and contemporary English poems, (2) a corpus of 14950 metaphor sentences retrieved from a metaphor database website and (3) a corpus of 1500 song lyrics ranging across genres."
                    ]
                }
            ]
        },
        {
            "question": "What objective function is used in the GAN?",
            "answers": [
                {
                    "answer": "language modeling objective",
                    "type": "extractive"
                }
            ],
            "q_uid": "6411622cc8b2fbedbfa468859d453596d3bd2f03",
            "evidence": [
                {
                    "raw_evidence": [
                        "We first pre-train our generator on the Gutenberg dataset BIBREF24 for 20 epochs and then fine-tune BIBREF19 them to our target datasets with a language modeling objective. The discriminator's encoder is initialized to the same weights as our fine-tuned language model. Once we have our fine-tuned encoders for each target dataset, we train in an adversarial manner. The discriminator objective here is to score the quality of the creative text. The discriminator is trained for 3 iterations for every iteration of the generator, a practice seen in previous work BIBREF25. Creative-GAN relies on using the reward from the discriminator BIBREF12, BIBREF15 for backpropagation. We follow a similar training procedure for GumbelGAN. Outputs are generated through sampling over a multinomial distribution for all methods, instead of $argmax$ on the log-likelihood probabilities, as sampling has shown to produce better output quality BIBREF4. Please refer to Supplementary Section Table TABREF6 for training parameters of each dataset and Table TABREF5 for hyperparameters of each encoder. We pick these values after experimentation with our validation set. Training and output generation code can be found online."
                    ],
                    "highlighted_evidence": [
                        "We first pre-train our generator on the Gutenberg dataset BIBREF24 for 20 epochs and then fine-tune BIBREF19 them to our target datasets with a language modeling objective."
                    ]
                }
            ]
        },
        {
            "question": "Which datasets are used?",
            "answers": [
                {
                    "answer": "A corpus of 740 classical and contemporary English poems,  a corpus of 14950 metaphor sentences retrieved from a metaphor database website , a corpus of 1500 song lyrics ranging across genres, Gutenberg dataset ",
                    "type": "extractive"
                },
                {
                    "answer": "(1) A corpus of 740 classical and contemporary English poems, (2) a corpus of 14950 metaphor sentences retrieved from a metaphor database website, (3) a corpus of 1500 song lyrics ranging across genres, Gutenberg dataset BIBREF24",
                    "type": "extractive"
                }
            ],
            "q_uid": "fc77d70c305fa80447b191248aba93da63ac3704",
            "evidence": [
                {
                    "raw_evidence": [
                        "We utilize AWD-LSTM BIBREF20 and TransformerXL BIBREF21 based language models. For model hyperparameters please to refer to Supplementary Section Table TABREF5. We use Adam optimizer BIBREF22 with $\\beta 1= 0.7$ and $\\beta 2= 0.8$ similar to BIBREF19 and use a batch size of 50. Other practices for LM training were the same as BIBREF21 and BIBREF20 for Transformer-XL and AWD-LSTM respectively. We refer to our proposed GAN as Creative-GAN and compare it to a baseline (a language model equivalent to our pre-trained generator) and a GumbelGAN model BIBREF14 across all proposed datasets. We use three creative English datasets with distinct linguistic characteristics: (1) A corpus of 740 classical and contemporary English poems, (2) a corpus of 14950 metaphor sentences retrieved from a metaphor database website and (3) a corpus of 1500 song lyrics ranging across genres. The mix of linguistic styles within this corpus offers the potential for interesting variation during the generation phase. We use the same pre-processing as in earlier work BIBREF19, BIBREF23. We reserve 10% of our data for test set and another 10% for our validation set.",
                        "We first pre-train our generator on the Gutenberg dataset BIBREF24 for 20 epochs and then fine-tune BIBREF19 them to our target datasets with a language modeling objective. The discriminator's encoder is initialized to the same weights as our fine-tuned language model. Once we have our fine-tuned encoders for each target dataset, we train in an adversarial manner. The discriminator objective here is to score the quality of the creative text. The discriminator is trained for 3 iterations for every iteration of the generator, a practice seen in previous work BIBREF25. Creative-GAN relies on using the reward from the discriminator BIBREF12, BIBREF15 for backpropagation. We follow a similar training procedure for GumbelGAN. Outputs are generated through sampling over a multinomial distribution for all methods, instead of $argmax$ on the log-likelihood probabilities, as sampling has shown to produce better output quality BIBREF4. Please refer to Supplementary Section Table TABREF6 for training parameters of each dataset and Table TABREF5 for hyperparameters of each encoder. We pick these values after experimentation with our validation set. Training and output generation code can be found online."
                    ],
                    "highlighted_evidence": [
                        "We use three creative English datasets with distinct linguistic characteristics: (1) A corpus of 740 classical and contemporary English poems, (2) a corpus of 14950 metaphor sentences retrieved from a metaphor database website and (3) a corpus of 1500 song lyrics ranging across genres. ",
                        "We first pre-train our generator on the Gutenberg dataset BIBREF24 for 20 epochs and then fine-tune BIBREF19 them to our target datasets with a language modeling objective."
                    ]
                },
                {
                    "raw_evidence": [
                        "We utilize AWD-LSTM BIBREF20 and TransformerXL BIBREF21 based language models. For model hyperparameters please to refer to Supplementary Section Table TABREF5. We use Adam optimizer BIBREF22 with $\\beta 1= 0.7$ and $\\beta 2= 0.8$ similar to BIBREF19 and use a batch size of 50. Other practices for LM training were the same as BIBREF21 and BIBREF20 for Transformer-XL and AWD-LSTM respectively. We refer to our proposed GAN as Creative-GAN and compare it to a baseline (a language model equivalent to our pre-trained generator) and a GumbelGAN model BIBREF14 across all proposed datasets. We use three creative English datasets with distinct linguistic characteristics: (1) A corpus of 740 classical and contemporary English poems, (2) a corpus of 14950 metaphor sentences retrieved from a metaphor database website and (3) a corpus of 1500 song lyrics ranging across genres. The mix of linguistic styles within this corpus offers the potential for interesting variation during the generation phase. We use the same pre-processing as in earlier work BIBREF19, BIBREF23. We reserve 10% of our data for test set and another 10% for our validation set.",
                        "We first pre-train our generator on the Gutenberg dataset BIBREF24 for 20 epochs and then fine-tune BIBREF19 them to our target datasets with a language modeling objective. The discriminator's encoder is initialized to the same weights as our fine-tuned language model. Once we have our fine-tuned encoders for each target dataset, we train in an adversarial manner. The discriminator objective here is to score the quality of the creative text. The discriminator is trained for 3 iterations for every iteration of the generator, a practice seen in previous work BIBREF25. Creative-GAN relies on using the reward from the discriminator BIBREF12, BIBREF15 for backpropagation. We follow a similar training procedure for GumbelGAN. Outputs are generated through sampling over a multinomial distribution for all methods, instead of $argmax$ on the log-likelihood probabilities, as sampling has shown to produce better output quality BIBREF4. Please refer to Supplementary Section Table TABREF6 for training parameters of each dataset and Table TABREF5 for hyperparameters of each encoder. We pick these values after experimentation with our validation set. Training and output generation code can be found online."
                    ],
                    "highlighted_evidence": [
                        "We use three creative English datasets with distinct linguistic characteristics: (1) A corpus of 740 classical and contemporary English poems, (2) a corpus of 14950 metaphor sentences retrieved from a metaphor database website and (3) a corpus of 1500 song lyrics ranging across genres.",
                        "We first pre-train our generator on the Gutenberg dataset BIBREF24 for 20 epochs and then fine-tune BIBREF19 them to our target datasets with a language modeling objective."
                    ]
                }
            ]
        }
    ],
    "1910.02334": [
        {
            "question": "What is the source of memes?",
            "answers": [
                {
                    "answer": "Google Images, Reddit Memes Dataset",
                    "type": "extractive"
                }
            ],
            "q_uid": "5e997d4499b18f1ee1ef6fa145cadbc018b8dd87",
            "evidence": [
                {
                    "raw_evidence": [
                        "We built a dataset for the task of hate speech detection in memes with 5,020 images that were weakly labeled into hate or non-hate memes, depending on their source. Hate memes were retrieved from Google Images with a downloading tool. We used the following queries to collect a total of 1,695 hate memes: racist meme (643 memes), jew meme (551 memes), and muslim meme (501 Memes). Non-hate memes were obtained from the Reddit Memes Dataset . We assumed that all memes in the dataset do not contain any hate message, as we considered that average Reddit memes do not belong to this class. A total of 3,325 non-hate memes were collected. We split the dataset into train (4266 memes) and validation (754 memes) subsets. The splits were random and the distribution of classes in the two subsets is the same. We didn't split the dataset into three subsets because of the small amount of data we had and decided to rely on the validation set metrics."
                    ],
                    "highlighted_evidence": [
                        "Hate memes were retrieved from Google Images with a downloading tool. We used the following queries to collect a total of 1,695 hate memes: racist meme (643 memes), jew meme (551 memes), and muslim meme (501 Memes). Non-hate memes were obtained from the Reddit Memes Dataset ."
                    ]
                }
            ]
        },
        {
            "question": "Is the dataset multimodal?",
            "answers": [
                {
                    "answer": "Yes",
                    "type": "boolean"
                }
            ],
            "q_uid": "12c7d79d2a26af2d445229d0c8ba3ba1aab3f5b5",
            "evidence": [
                {
                    "raw_evidence": [
                        "The text and image encodings were combined by concatenation, which resulted in a feature vector of 4,864 dimensions. This multimodal representation was afterward fed as input into a multi-layer perceptron (MLP) with two hidden layer of 100 neurons with a ReLU activation function. The last single neuron with no activation function was added at the end to predict the hate speech detection score."
                    ],
                    "highlighted_evidence": [
                        "The text and image encodings were combined by concatenation, which resulted in a feature vector of 4,864 dimensions. This multimodal representation was afterward fed as input into a multi-layer perceptron (MLP) with two hidden layer of 100 neurons with a ReLU activation function."
                    ]
                }
            ]
        },
        {
            "question": "How is each instance of the dataset annotated?",
            "answers": [
                {
                    "answer": "weakly labeled into hate or non-hate memes, depending on their source",
                    "type": "extractive"
                }
            ],
            "q_uid": "98daaa9eaa1e1e574be336b8933b861bfd242e5e",
            "evidence": [
                {
                    "raw_evidence": [
                        "We built a dataset for the task of hate speech detection in memes with 5,020 images that were weakly labeled into hate or non-hate memes, depending on their source. Hate memes were retrieved from Google Images with a downloading tool. We used the following queries to collect a total of 1,695 hate memes: racist meme (643 memes), jew meme (551 memes), and muslim meme (501 Memes). Non-hate memes were obtained from the Reddit Memes Dataset . We assumed that all memes in the dataset do not contain any hate message, as we considered that average Reddit memes do not belong to this class. A total of 3,325 non-hate memes were collected. We split the dataset into train (4266 memes) and validation (754 memes) subsets. The splits were random and the distribution of classes in the two subsets is the same. We didn't split the dataset into three subsets because of the small amount of data we had and decided to rely on the validation set metrics."
                    ],
                    "highlighted_evidence": [
                        "We built a dataset for the task of hate speech detection in memes with 5,020 images that were weakly labeled into hate or non-hate memes, depending on their source."
                    ]
                }
            ]
        }
    ],
    "1908.09951": [
        {
            "question": "What is the baseline?",
            "answers": [
                {
                    "answer": "Majority Class baseline (MC) , Random selection baseline (RAN)",
                    "type": "extractive"
                }
            ],
            "q_uid": "1571e16063b53409f2d1bd6ec143fccc5b29ebb9",
            "evidence": [
                {
                    "raw_evidence": [
                        "Emotions have been used in many natural language processing tasks and they showed their efficiency BIBREF35. We aim at investigating their efficiency to detect false information. In addition to EIN, we created a model (Emotion-based Model) that uses emotional features only and compare it to two baselines. Our aim is to investigate if the emotional features independently can detect false news. The two baselines of this model are Majority Class baseline (MC) and the Random selection baseline (RAN)."
                    ],
                    "highlighted_evidence": [
                        " In addition to EIN, we created a model (Emotion-based Model) that uses emotional features only and compare it to two baselines. Our aim is to investigate if the emotional features independently can detect false news. The two baselines of this model are Majority Class baseline (MC) and the Random selection baseline (RAN)."
                    ]
                }
            ]
        },
        {
            "question": "What datasets did they use?",
            "answers": [
                {
                    "answer": "News Articles, Twitter",
                    "type": "extractive"
                }
            ],
            "q_uid": "d71937fa5da853f7529f767730547ccfb70e5908",
            "evidence": [
                {
                    "raw_evidence": [
                        "Evaluation Framework ::: Datasets ::: News Articles",
                        "Our dataset source of news articles is described in BIBREF2. This dataset was built from two different sources, for the trusted news (real news) they sampled news articles from the English Gigaword corpus. For the false news, they collected articles from seven different unreliable news sites. These news articles include satires, hoaxes, and propagandas but not clickbaits. Since we are interested also in analyzing clickbaits, we slice a sample from an available clickbait dataset BIBREF33 that was originally collected from two sources: Wikinews articles' headlines and other online sites that are known to publish clickbaits. The satire, hoax, and propaganda news articles are considerably long (some of them reach the length of 5,000 words). This length could affect the quality of the analysis as we mentioned before. We focus on analyzing the initial part of the article. Our intuition is that it is where emotion-bearing words will be more frequent. Therefore, we shorten long news articles into a maximum length of N words (N=300). We choose the value of N based on the length of the shortest articles. Moreover, we process the dataset by removing very short articles, redundant articles or articles that do not have a textual content.",
                        "With the complicated political and economic situations in many countries, some agendas are publishing suspicious news to affect public opinions regarding specific issues BIBREF0. The spreading of this phenomenon is increasing recently with the large usage of social media and online news sources. Many anonymous accounts in social media platforms start to appear, as well as new online news agencies without presenting a clear identity of the owner. Twitter has recently detected a campaign organized by agencies from two different countries to affect the results of the last U.S. presidential elections of 2016. The initial disclosures by Twitter have included 3,841 accounts. A similar attempt was done by Facebook, as they detected coordinated efforts to influence U.S. politics ahead of the 2018 midterm elections.",
                        "For this dataset, we rely on a list of several Twitter accounts for each type of false information from BIBREF6. This list was created based on public resources that annotated suspicious Twitter accounts. The authors in BIBREF6 have built a dataset by collecting tweets from these accounts and they made it available. For the real news, we merge this list with another 32 Twitter accounts from BIBREF34. In this work we could not use the previous dataset and we decide to collect tweets again. For each of these accounts, we collected the last M tweets posted (M=1000). By investigating these accounts manually, we found that many tweets just contain links without textual news. Therefore, to ensure of the quality of the crawled data, we chose a high value for M (also to have enough data). After the collecting process, we processed these tweets by removing duplicated, very short tweets, and tweets without textual content. Table TABREF35 shows a summary for both datasets."
                    ],
                    "highlighted_evidence": [
                        " News Articles\nOur dataset source of news articles is described in BIBREF2. This dataset was built from two different sources, for the trusted news (real news) they sampled news articles from the English Gigaword corpus. For the false news, they collected articles from seven different unreliable news sites.",
                        "Twitter\nFor this dataset, we rely on a list of several Twitter accounts for each type of false information from BIBREF6. This list was created based on public resources that annotated suspicious Twitter accounts. The authors in BIBREF6 have built a dataset by collecting tweets from these accounts and they made it available. "
                    ]
                }
            ]
        }
    ],
    "1805.03710": [
        {
            "question": "How do they evaluate their resulting word embeddings?",
            "answers": [
                {
                    "answer": "We also evaluate all five models on downstream tasks from the VecEval suite BIBREF13 , using only the tasks for which training and evaluation data is freely available: chunking, sentiment and question classification, and natural language identification (NLI). The default settings from the suite are used, but we run only the fixed settings, where the embeddings themselves are not tunable parameters of the models, forcing the system to use only the information already in the embeddings.",
                    "type": "extractive"
                }
            ],
            "q_uid": "b7381927764536bd97b099b6a172708125364954",
            "evidence": [
                {
                    "raw_evidence": [
                        "We also evaluate all five models on downstream tasks from the VecEval suite BIBREF13 , using only the tasks for which training and evaluation data is freely available: chunking, sentiment and question classification, and natural language identification (NLI). The default settings from the suite are used, but we run only the fixed settings, where the embeddings themselves are not tunable parameters of the models, forcing the system to use only the information already in the embeddings.",
                        "Finally, we use LV-N, LV-M, and FT to generate OOV word representations for the following words: 1) \u201chellooo\u201d: a greeting commonly used in instant messaging which emphasizes a syllable. 2) \u201cmarvelicious\u201d: a made-up word obtained by merging \u201cmarvelous\u201d and \u201cdelicious\u201d. 3) \u201clouisana\u201d: a misspelling of the proper name \u201cLouisiana\u201d. 4) \u201crereread\u201d: recursive use of prefix \u201cre\u201d. 5) \u201ctuzread\u201d: made-up prefix \u201ctuz\u201d."
                    ],
                    "highlighted_evidence": [
                        "We also evaluate all five models on downstream tasks from the VecEval suite BIBREF13 , using only the tasks for which training and evaluation data is freely available: chunking, sentiment and question classification, and natural language identification (NLI). The default settings from the suite are used, but we run only the fixed settings, where the embeddings themselves are not tunable parameters of the models, forcing the system to use only the information already in the embeddings.\n\nFinally, we use LV-N, LV-M, and FT to generate OOV word representations for the following words: 1) \u201chellooo\u201d: a greeting commonly used in instant messaging which emphasizes a syllable. 2) \u201cmarvelicious\u201d: a made-up word obtained by merging \u201cmarvelous\u201d and \u201cdelicious\u201d. 3) \u201clouisana\u201d: a misspelling of the proper name \u201cLouisiana\u201d. 4) \u201crereread\u201d: recursive use of prefix \u201cre\u201d. 5) \u201ctuzread\u201d: made-up prefix \u201ctuz\u201d."
                    ]
                }
            ]
        },
        {
            "question": "What types of subwords do they incorporate in their model?",
            "answers": [
                {
                    "answer": "n-gram subwords, unsupervised morphemes identified using Morfessor BIBREF11 to learn whether more linguistically motivated subwords ",
                    "type": "extractive"
                },
                {
                    "answer": "simple n-grams (like fastText) and unsupervised morphemes",
                    "type": "extractive"
                }
            ],
            "q_uid": "df95b3cb6aa0187655fd4856ae2b1f503d533583",
            "evidence": [
                {
                    "raw_evidence": [
                        "We compare 1) the use of n-gram subwords, like fastText, and 2) unsupervised morphemes identified using Morfessor BIBREF11 to learn whether more linguistically motivated subwords offer any advantage over simple n-grams."
                    ],
                    "highlighted_evidence": [
                        "INLINEFORM0 in the subsampled BIBREF2 training corpus and incrementing cell INLINEFORM1 for every context word INLINEFORM2 appearing within this window (forming a INLINEFORM3 pair). LexVec adjusts the PPMI matrix using context distribution smoothing BIBREF3 .",
                        "We compare 1) the use of n-gram subwords, like fastText, and 2) unsupervised morphemes identified using Morfessor BIBREF11 to learn whether more linguistically motivated subwords offer any advantage over simple n-grams."
                    ]
                },
                {
                    "raw_evidence": [
                        "We compare two types of subwords: simple n-grams (like fastText) and unsupervised morphemes. For example, given the word \u201ccat\u201d, we mark beginning and end with angled brackets and use all n-grams of length 3 to 6 as subwords, yielding $S_{\\textnormal {cat}} = \\lbrace \\textnormal {$ $ ca, at$ $, cat} \\rbrace $ . Morfessor BIBREF11 is used to probabilistically segment words into morphemes. The Morfessor model is trained using raw text so it is entirely unsupervised. For the word \u201csubsequent\u201d, we get $S_{\\textnormal {subsequent}} = \\lbrace \\textnormal {$ $ sub, sequent$ $} \\rbrace $ ."
                    ],
                    "highlighted_evidence": [
                        "We compare two types of subwords: simple n-grams (like fastText) and unsupervised morphemes. "
                    ]
                }
            ]
        },
        {
            "question": "Which matrix factorization methods do they use?",
            "answers": [
                {
                    "answer": "weighted factorization of a word-context co-occurrence matrix ",
                    "type": "extractive"
                },
                {
                    "answer": "The LexVec BIBREF7",
                    "type": "extractive"
                }
            ],
            "q_uid": "f7ed3b9ed469ed34f46acde86b8a066c52ecf430",
            "evidence": [
                {
                    "raw_evidence": [
                        "Word embeddings that leverage subword information were first introduced by BIBREF14 which represented a word of as the sum of four-gram vectors obtained running an SVD of a four-gram to four-gram co-occurrence matrix. Our model differs by learning the subword vectors and resulting representation jointly as weighted factorization of a word-context co-occurrence matrix is performed."
                    ],
                    "highlighted_evidence": [
                        "Our model differs by learning the subword vectors and resulting representation jointly as weighted factorization of a word-context co-occurrence matrix is performed."
                    ]
                },
                {
                    "raw_evidence": [],
                    "highlighted_evidence": [
                        "The LexVec BIBREF7 model factorizes the PPMI-weighted word-context co-occurrence matrix using stochastic gradient descent. DISPLAYFORM0"
                    ]
                }
            ]
        }
    ],
    "1906.11180": [
        {
            "question": "What is the reasoning method that is used?",
            "answers": [
                {
                    "answer": "SPARQL",
                    "type": "extractive"
                }
            ],
            "q_uid": "b6ffa18d49e188c454188669987b0a4807ca3018",
            "evidence": [
                {
                    "raw_evidence": [
                        "The DBpedia lookup service, which is based on the Spotlight index BIBREF18 , is used for entity lookup (retrieval). The DBpedia SPARQL endpoint is used for query answering and reasoning. The reported results are based on the following settings: the Adam optimizer together with cross-entropy loss are used for network training; $d_r$ and $d_a$ are set to 200 and 50 respectively; $N_0$ is set to 1200; word2vec trained with the latest Wikipedia article dump is adopted for word embedding; and ( $T_s$ , $T_p$ , $T_l$ ) are set to (12, 4, 12) for S-Lite and (12, 4, 15) for R-Lite. The experiments are run on a workstation with Intel(R) Xeon(R) CPU E5-2670 @2.60GHz, with programs implemented by Tensorflow."
                    ],
                    "highlighted_evidence": [
                        "The DBpedia SPARQL endpoint is used for query answering and reasoning."
                    ]
                }
            ]
        },
        {
            "question": "What KB is used in this work?",
            "answers": [
                {
                    "answer": "DBpedia",
                    "type": "extractive"
                }
            ],
            "q_uid": "2b61893b22ac190c94c2cb129e86086888347079",
            "evidence": [
                {
                    "raw_evidence": [
                        "In this study, we investigate KB literal canonicalization using a combination of RNN-based learning and semantic technologies. We first predict the semantic types of a literal by: (i) identifying candidate classes via lexical entity matching and KB queries; (ii) automatically generating positive and negative examples via KB sampling, with external semantics (e.g., from other KBs) injected for improved quality; (iii) training classifiers using relevant subject-predicate-literal triples embedded in an attentive bidirectional RNN (AttBiRNN); and (iv) using the trained classifiers and KB class hierarchy to predict candidate types. The novelty of our framework lies in its knowledge-based learning; this includes automatic candidate class extraction and sampling from the KB, triple embedding with different importance degrees suggesting different semantics, and using the predicted types to identify a potential canonical entity from the KB. We have evaluated our framework using a synthetic literal set (S-Lite) and a real literal set (R-Lite) from DBpedia BIBREF0 . The results are very promising, with significant improvements over several baselines, including the existing state-of-the-art."
                    ],
                    "highlighted_evidence": [
                        "We have evaluated our framework using a synthetic literal set (S-Lite) and a real literal set (R-Lite) from DBpedia BIBREF0 . "
                    ]
                }
            ]
        }
    ],
    "1912.01046": [
        {
            "question": "What evaluation metrics were used in the experiment?",
            "answers": [
                {
                    "answer": "For sentence-level prediction they used tolerance accuracy, for segment retrieval accuracy and MRR and for the pipeline approach they used overall accuracy",
                    "type": "abstractive"
                }
            ],
            "q_uid": "5bcc12680cf2eda2dd13ab763c42314a26f2d993",
            "evidence": [
                {
                    "raw_evidence": [
                        "Metrics. We use tolerance accuracy BIBREF16, which measures how far away the predicted span is from the gold standard span, as a metric. The rationale behind the metric is that, in practice, it suffices to recommend a rough span which contains the answer \u2013 a difference of a few seconds would not matter much to the user.",
                        "Metrics. We used accuracy and MRR (Mean Reciprocal Ranking) as metrics. The accuracy is",
                        "Metrics. To evaluate our pipeline approach we use overall accuracy after filtering and accuracy given that the segment is in the top 10 videos. While the first metric is similar to SECREF17, the second can indicate if initially searching on the video space can be used to improve our selection:"
                    ],
                    "highlighted_evidence": [
                        "Metrics. We use tolerance accuracy BIBREF16, which measures how far away the predicted span is from the gold standard span, as a metric. The rationale behind the metric is that, in practice, it suffices to recommend a rough span which contains the answer \u2013 a difference of a few seconds would not matter much to the user.",
                        "Metrics. We used accuracy and MRR (Mean Reciprocal Ranking) as metrics. ",
                        "Metrics. To evaluate our pipeline approach we use overall accuracy after filtering and accuracy given that the segment is in the top 10 videos. "
                    ]
                }
            ]
        },
        {
            "question": "What kind of instructional videos are in the dataset?",
            "answers": [
                {
                    "answer": "tutorial videos for a photo-editing software",
                    "type": "abstractive"
                }
            ],
            "q_uid": "7a53668cf2da4557735aec0ecf5f29868584ebcf",
            "evidence": [
                {
                    "raw_evidence": [
                        "The remainder of this paper is structured as follows. Section SECREF3 introduces TutorialVQA dataset as a case study of our proposed problem. The dataset includes about 6,000 triples, comprised of videos, questions, and answer spans manually collected from screencast tutorial videos with spoken narratives for a photo-editing software. Section SECREF4 presents the baseline models and their experiment details on the sentence-level prediction and video segment retrieval tasks on our dataset. Then, we discuss the experimental results in Section SECREF5 and conclude the paper in Section SECREF6."
                    ],
                    "highlighted_evidence": [
                        "The dataset includes about 6,000 triples, comprised of videos, questions, and answer spans manually collected from screencast tutorial videos with spoken narratives for a photo-editing software. "
                    ]
                }
            ]
        },
        {
            "question": "What baseline algorithms were presented?",
            "answers": [
                {
                    "answer": "a sentence-level prediction algorithm, a segment retrieval algorithm and a pipeline segment retrieval algorithm",
                    "type": "abstractive"
                }
            ],
            "q_uid": "8051927f914d730dfc61b2dc7a8580707b462e56",
            "evidence": [
                {
                    "raw_evidence": [
                        "Our video question answering task is novel and to our knowledge, no model has been designed specifically for this task. As a first step towards solving this problem, we evaluated the performance of state-of-the-art models developed for other QA tasks, including a sentence-level prediction task and two segment retrieval tasks. In this section, we report their results on the TutorialVQA dataset.",
                        "Baselines ::: Baseline1: Sentence-level prediction",
                        "Given a transcript (a sequence of sentences) and a question, Baseline1 predicts (starting sentence index, ending sentence index). The model is based on RaSor BIBREF13, which has been developed for the SQuAD QA task BIBREF6. RaSor concatenates the embedding vectors of the starting and the ending words to represent a span. Following this idea, Baseline1 represents a span of sentences by concatenating the vectors of the starting and ending sentences. The left diagram in Fig. FIGREF15 illustrates the Baseline1 model.",
                        "Model. The model takes two inputs, a transcript, $\\lbrace s_1, s_2, ... s_n\\rbrace $ where $s_i$ are individual sentences and a question, $q$. The output is the span scores, $y$, the scores over all possible spans. GLoVe BIBREF14 is used for the word representations in the transcript and the questions. We use two bi-LSTMs BIBREF15 to encode the transcript.",
                        "where n is the number of sentences . The output of Passage-level Encoding, $p$, is a sequence of vector, $p_i$, which represents the latent meaning of each sentence. Then, the model combines each pair of sentence embeddings ($p_i$, $p_j$) to generate a span embedding.",
                        "where [$\\cdot $,$\\cdot $] indicates the concatenation. Finally, we use a one-layer feed forward network to compute a score between each span and a question.",
                        "In training, we use cross-entropy as an objective function. In testing, the span with the highest score is picked as an answer.",
                        "Baselines ::: Baseline2: Segment retrieval",
                        "We also considered a simpler task by casting our problem as a retrieval task. Specifically, in addition to a plain transcript, we also provided the model with the segmentation information which was created during the data collection phrase (See Section. SECREF3). Note that each segments corresponds to a candidate answer. Then, the task is to pick the best segment for given a query. This task is easier than Baseline1's task in that the segmentation information is provided to the model. Unlike Baseline1, however, it is unable to return an answer span at various granularities. Baseline2 is based on the attentive LSTM BIBREF17, which has been developed for the InsuranceQA task. The right diagram in Fig. FIGREF15 illustrates the Baseline2 model.",
                        "Model. The two inputs, $s$ and $q$ represent the segment text and a question. The model first encodes the two inputs.",
                        "$h^s$ is then re-weighted using attention weights.",
                        "where $\\odot $ denotes the element-wise multiplication operation. The final score is computed using a one-layer feed-forward network.",
                        "During training, the model requires negative samples. For each positive example, (question, ground-truth segment), all the other segments in the same transcript are used as negative samples. Cross entropy is used as an objective function.",
                        "Baselines ::: Baseline3: Pipeline Segment retrieval",
                        "We construct a pipelined approach through another segment retrieval task, calculating the cosine similarities between the segment and question embeddings. In this task however, we want to test the accuracy of retrieving the segments given that we first retrieve the correct video from our 76 videos. First, we generate the TF-IDF embeddings for the whole video transcripts and questions. The next step involves retrieving the videos which have the lowest cosine distance between the video transcripts and question. We then filter and store the top ten videos, reducing the number of computations required in the next step. Finally, we calculate the cosine distances between the question and the segments which belong to the filtered top 10 videos, marking it as correct if found in these videos. While the task is less computationally expensive than the previous baseline, we do not learn the segment representations, as this task is a simple retrieval task based on TF-IDF embeddings.",
                        "Model. The first two inputs are are the question, q, and video transcript, v, encoded by their TF-IDF vectors: BIBREF18:",
                        "We then filter the top 10 video transcripts(out of 76) with the minimum cosine distance, and further compute the TF-IDF vectors for their segments, Stop10n, where n = 10. We repeat the process for the corresponding segments:",
                        "selecting the segment with the minimal cosine distance distance to the query."
                    ],
                    "highlighted_evidence": [
                        "Our video question answering task is novel and to our knowledge, no model has been designed specifically for this task. As a first step towards solving this problem, we evaluated the performance of state-of-the-art models developed for other QA tasks, including a sentence-level prediction task and two segment retrieval tasks.",
                        "Baselines ::: Baseline1: Sentence-level prediction\nGiven a transcript (a sequence of sentences) and a question, Baseline1 predicts (starting sentence index, ending sentence index). The model is based on RaSor BIBREF13, which has been developed for the SQuAD QA task BIBREF6. RaSor concatenates the embedding vectors of the starting and the ending words to represent a span. Following this idea, Baseline1 represents a span of sentences by concatenating the vectors of the starting and ending sentences. The left diagram in Fig. FIGREF15 illustrates the Baseline1 model.\n\nModel. The model takes two inputs, a transcript, $\\lbrace s_1, s_2, ... s_n\\rbrace $ where $s_i$ are individual sentences and a question, $q$. The output is the span scores, $y$, the scores over all possible spans. GLoVe BIBREF14 is used for the word representations in the transcript and the questions. We use two bi-LSTMs BIBREF15 to encode the transcript.\n\nwhere n is the number of sentences . The output of Passage-level Encoding, $p$, is a sequence of vector, $p_i$, which represents the latent meaning of each sentence. Then, the model combines each pair of sentence embeddings ($p_i$, $p_j$) to generate a span embedding.\n\nwhere [$\\cdot $,$\\cdot $] indicates the concatenation. Finally, we use a one-layer feed forward network to compute a score between each span and a question.\n\nIn training, we use cross-entropy as an objective function. In testing, the span with the highest score is picked as an answer.",
                        "Baselines ::: Baseline2: Segment retrieval\nWe also considered a simpler task by casting our problem as a retrieval task. Specifically, in addition to a plain transcript, we also provided the model with the segmentation information which was created during the data collection phrase (See Section. SECREF3). Note that each segments corresponds to a candidate answer. Then, the task is to pick the best segment for given a query. This task is easier than Baseline1's task in that the segmentation information is provided to the model. Unlike Baseline1, however, it is unable to return an answer span at various granularities. Baseline2 is based on the attentive LSTM BIBREF17, which has been developed for the InsuranceQA task. The right diagram in Fig. FIGREF15 illustrates the Baseline2 model.\n\nModel. The two inputs, $s$ and $q$ represent the segment text and a question. The model first encodes the two inputs.\n\n$h^s$ is then re-weighted using attention weights.\n\nwhere $\\odot $ denotes the element-wise multiplication operation. The final score is computed using a one-layer feed-forward network.\n\nDuring training, the model requires negative samples. For each positive example, (question, ground-truth segment), all the other segments in the same transcript are used as negative samples. Cross entropy is used as an objective function.\n\n",
                        "Baselines ::: Baseline3: Pipeline Segment retrieval\nWe construct a pipelined approach through another segment retrieval task, calculating the cosine similarities between the segment and question embeddings. In this task however, we want to test the accuracy of retrieving the segments given that we first retrieve the correct video from our 76 videos. First, we generate the TF-IDF embeddings for the whole video transcripts and questions. The next step involves retrieving the videos which have the lowest cosine distance between the video transcripts and question. We then filter and store the top ten videos, reducing the number of computations required in the next step. Finally, we calculate the cosine distances between the question and the segments which belong to the filtered top 10 videos, marking it as correct if found in these videos. While the task is less computationally expensive than the previous baseline, we do not learn the segment representations, as this task is a simple retrieval task based on TF-IDF embeddings.\n\nModel. The first two inputs are are the question, q, and video transcript, v, encoded by their TF-IDF vectors: BIBREF18:\n\nWe then filter the top 10 video transcripts(out of 76) with the minimum cosine distance, and further compute the TF-IDF vectors for their segments, Stop10n, where n = 10. We repeat the process for the corresponding segments:\n\nselecting the segment with the minimal cosine distance distance to the query."
                    ]
                }
            ]
        },
        {
            "question": "What is the source of the triples?",
            "answers": [
                {
                    "answer": "a tutorial website about an image editing program ",
                    "type": "extractive"
                }
            ],
            "q_uid": "09621c9cd762e1409f22d501513858d67dcd3c7c",
            "evidence": [
                {
                    "raw_evidence": [
                        "We downloaded 76 videos from a tutorial website about an image editing program . Each video is pre-processed to provide the transcripts and the time-stamp information for each sentence in the transcript. We then used Amazon Mechanical Turk to collect the question-answer pairs . One naive way of collecting the data is to prepare a question list and then, for each question, ask the workers to find the relevant parts in the video. However, this approach is not feasible and error-prone because the videos are typically long and finding a relevant part from a long video is difficult. Doing so might also cause us to miss questions which were relevant to the video segment. Instead, we took a reversed approach. First, for each video, we manually identified the sentence spans that can serve as answers. These candidates are of various granularity and may overlap. The segments are also complete in that they encompass the beginning and end of a task. In total, we identified 408 segments from the 76 videos. Second we asked AMT workers to provide question annotations for the videos."
                    ],
                    "highlighted_evidence": [
                        "We downloaded 76 videos from a tutorial website about an image editing program . "
                    ]
                }
            ]
        }
    ],
    "1707.00995": [
        {
            "question": "What misbehavior is identified?",
            "answers": [
                {
                    "answer": "if the attention loose track of the objects in the picture and \"gets lost\", the model still takes it into account and somehow overrides the information brought by the text-based annotations",
                    "type": "extractive"
                },
                {
                    "answer": "if the attention loose track of the objects in the picture and \"gets lost\", the model still takes it into account and somehow overrides the information brought by the text-based annotations",
                    "type": "extractive"
                }
            ],
            "q_uid": "f0317e48dafe117829e88e54ed2edab24b86edb1",
            "evidence": [
                {
                    "raw_evidence": [
                        "It is also worth to mention that we use a ResNet trained on 1.28 million images for a classification tasks. The features used by the attention mechanism are strongly object-oriented and the machine could miss important information for a multimodal translation task. We believe that the robust architecture of both encoders INLINEFORM0 combined with a GRU layer and word-embeddings took care of the right translation for relationships between objects and time-dependencies. Yet, we noticed a common misbehavior for all our multimodal models: if the attention loose track of the objects in the picture and \"gets lost\", the model still takes it into account and somehow overrides the information brought by the text-based annotations. The translation is then totally mislead. We illustrate with an example:"
                    ],
                    "highlighted_evidence": [
                        "Yet, we noticed a common misbehavior for all our multimodal models: if the attention loose track of the objects in the picture and \"gets lost\", the model still takes it into account and somehow overrides the information brought by the text-based annotations. The translation is then totally mislead. "
                    ]
                },
                {
                    "raw_evidence": [
                        "It is also worth to mention that we use a ResNet trained on 1.28 million images for a classification tasks. The features used by the attention mechanism are strongly object-oriented and the machine could miss important information for a multimodal translation task. We believe that the robust architecture of both encoders INLINEFORM0 combined with a GRU layer and word-embeddings took care of the right translation for relationships between objects and time-dependencies. Yet, we noticed a common misbehavior for all our multimodal models: if the attention loose track of the objects in the picture and \"gets lost\", the model still takes it into account and somehow overrides the information brought by the text-based annotations. The translation is then totally mislead. We illustrate with an example:"
                    ],
                    "highlighted_evidence": [
                        "Yet, we noticed a common misbehavior for all our multimodal models: if the attention loose track of the objects in the picture and \"gets lost\", the model still takes it into account and somehow overrides the information brought by the text-based annotations."
                    ]
                }
            ]
        },
        {
            "question": "Which attention mechanisms do they compare?",
            "answers": [
                {
                    "answer": "Soft attention, Hard Stochastic attention, Local Attention",
                    "type": "extractive"
                }
            ],
            "q_uid": "f129c97a81d81d32633c94111018880a7ffe16d1",
            "evidence": [
                {
                    "raw_evidence": [
                        "We evaluate three models of the image attention mechanism INLINEFORM0 of equation EQREF11 . They have in common the fact that at each time step INLINEFORM1 of the decoding phase, all approaches first take as input the annotation sequence INLINEFORM2 to derive a time-dependent context vector that contain relevant information in the image to help predict the current target word INLINEFORM3 . Even though these models differ in how the time-dependent context vector is derived, they share the same subsequent steps. For each mechanism, we propose two hand-picked illustrations showing where the attention is placed in an image.",
                        "Soft attention",
                        "Hard Stochastic attention",
                        "Local Attention"
                    ],
                    "highlighted_evidence": [
                        "We evaluate three models of the image attention mechanism INLINEFORM0 of equation EQREF11 .",
                        "Soft attention",
                        "Hard Stochastic attention",
                        "Local Attention"
                    ]
                }
            ]
        }
    ],
    "1908.04917": [
        {
            "question": "What was the previous state of the art model for this task?",
            "answers": [
                {
                    "answer": "WAS, LipCH-Net-seq, CSSMCM-w/o video",
                    "type": "extractive"
                }
            ],
            "q_uid": "8a7bd9579d2783bfa81e055a7a6ebc3935da9d20",
            "evidence": [
                {
                    "raw_evidence": [
                        "WAS: The architecture used in BIBREF3 without the audio input. The decoder output Chinese character at each timestep. Others keep unchanged to the original implementation.",
                        "LipCH-Net-seq: For a fair comparison, we use sequence-to-sequence with attention framework to replace the Connectionist temporal classification (CTC) loss BIBREF14 used in LipCH-Net BIBREF5 when converting picture to pinyin.",
                        "CSSMCM-w/o video: To evaluate the necessity of video information when predicting tone, the video stream is removed when predicting tone and Chinese characters. In other word, video is only used when predicting the pinyin sequence. The tone is predicted from the pinyin sequence. Tone information and pinyin information work together to predict Chinese character."
                    ],
                    "highlighted_evidence": [
                        "WAS: The architecture used in BIBREF3 without the audio input. The decoder output Chinese character at each timestep. Others keep unchanged to the original implementation.",
                        "LipCH-Net-seq: For a fair comparison, we use sequence-to-sequence with attention framework to replace the Connectionist temporal classification (CTC) loss BIBREF14 used in LipCH-Net BIBREF5 when converting picture to pinyin.",
                        "CSSMCM-w/o video: To evaluate the necessity of video information when predicting tone, the video stream is removed when predicting tone and Chinese characters. In other word, video is only used when predicting the pinyin sequence. The tone is predicted from the pinyin sequence. Tone information and pinyin information work together to predict Chinese character."
                    ]
                }
            ]
        },
        {
            "question": "What syntactic structure is used to model tones?",
            "answers": [
                {
                    "answer": "syllables",
                    "type": "extractive"
                }
            ],
            "q_uid": "27b01883ed947b457d3bab0c66de26c0736e4f90",
            "evidence": [
                {
                    "raw_evidence": [
                        "Based on the above considerations, in this paper, we present CSSMCM, a sentence-level Chinese Mandarin lip reading network, which contains three sub-networks. Same as BIBREF5 , in the first sub-network, pinyin sequence is predicted from the video. Different from BIBREF5 , which predicts pinyin characters from video, pinyin is taken as a whole in CSSMCM, also known as syllables. As we know, Mandarin Chinese is a syllable-based language and syllables are their logical unit of pronunciation. Compared with pinyin characters, syllables are a longer linguistic unit, and can reduce the difficulty of syllable choices in the decoder by sequence-to-sequence attention-based models BIBREF6 . Chen et al. BIBREF7 find that there might be a relationship between the production of lexical tones and the visible movements of the neck, head, and mouth. Motivated by this observation, in the second sub-network, both video and pinyin sequence is used as input to predict tone. Then in the third sub-network, video, pinyin, and tone sequence work together to predict the Chinese character sequence. At last, three sub-networks are jointly finetuned to improve overall performance."
                    ],
                    "highlighted_evidence": [
                        "Same as BIBREF5 , in the first sub-network, pinyin sequence is predicted from the video. Different from BIBREF5 , which predicts pinyin characters from video, pinyin is taken as a whole in CSSMCM, also known as syllables. As we know, Mandarin Chinese is a syllable-based language and syllables are their logical unit of pronunciation."
                    ]
                }
            ]
        },
        {
            "question": "What visual information characterizes tones?",
            "answers": [
                {
                    "answer": "video sequence is first fed into the VGG model BIBREF9 to extract visual feature",
                    "type": "extractive"
                }
            ],
            "q_uid": "9714cb7203c18a0c53805f6c889f2e20b4cab5dd",
            "evidence": [
                {
                    "raw_evidence": [
                        "As shown in Equation ( EQREF6 ), tone prediction sub-network ( INLINEFORM0 ) takes video and pinyin sequence as inputs and predict corresponding tone sequence. This problem is modeled as a sequence-to-sequence learning problem too. The corresponding model architecture is shown in Figure FIGREF8 .",
                        "In order to take both video and pinyin information into consideration when producing tone, a dual attention mechanism BIBREF3 is employed. Two independent attention mechanisms are used for video and pinyin sequence. Video context vectors INLINEFORM0 and pinyin context vectors INLINEFORM1 are fused when predicting a tone character at each decoder step.",
                        "The video encoder is the same as in Section SECREF7 and the pinyin encoder is: DISPLAYFORM0",
                        "The pinyin prediction sub-network transforms video sequence into pinyin sequence, which corresponds to INLINEFORM0 in Equation ( EQREF6 ). This sub-network is based on the sequence-to-sequence architecture with attention mechanism BIBREF8 . We name the encoder and decoder the video encoder and pinyin decoder, for the encoder process video sequence, and the decoder predicts pinyin sequence. The input video sequence is first fed into the VGG model BIBREF9 to extract visual feature. The output of conv5 of VGG is appended with global average pooling BIBREF10 to get the 512-dim feature vector. Then the 512-dim feature vector is fed into video encoder. The video encoder can be denoted as: DISPLAYFORM0"
                    ],
                    "highlighted_evidence": [
                        "As shown in Equation ( EQREF6 ), tone prediction sub-network ( INLINEFORM0 ) takes video and pinyin sequence as inputs and predict corresponding tone sequence.",
                        "Video context vectors INLINEFORM0 and pinyin context vectors INLINEFORM1 are fused when predicting a tone character at each decoder step.",
                        "The video encoder is the same as in Section SECREF7 and the pinyin encoder is: DISPLAYFORM0",
                        "The input video sequence is first fed into the VGG model BIBREF9 to extract visual feature. The output of conv5 of VGG is appended with global average pooling BIBREF10 to get the 512-dim feature vector. Then the 512-dim feature vector is fed into video encoder."
                    ]
                }
            ]
        }
    ],
    "1911.06964": [
        {
            "question": "How are models evaluated in this human-machine communication game?",
            "answers": [
                {
                    "answer": "by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews",
                    "type": "extractive"
                },
                {
                    "answer": "efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords, accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence",
                    "type": "extractive"
                }
            ],
            "q_uid": "1ff0fccf0dca95a6630380c84b0422bed854269a",
            "evidence": [
                {
                    "raw_evidence": [
                        "We evaluate our approach by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews BIBREF6 (see Appendix for details). We quantify the efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords. The accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence."
                    ],
                    "highlighted_evidence": [
                        "We evaluate our approach by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews BIBREF6 (see Appendix for details). We quantify the efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords. The accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence."
                    ]
                },
                {
                    "raw_evidence": [
                        "We evaluate our approach by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews BIBREF6 (see Appendix for details). We quantify the efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords. The accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence."
                    ],
                    "highlighted_evidence": [
                        "We quantify the efficiency of a communication scheme $(q_{\\alpha },p_{\\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords. The accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence."
                    ]
                }
            ]
        },
        {
            "question": "How many participants were trying this communication game?",
            "answers": [
                {
                    "answer": "100 ",
                    "type": "extractive"
                },
                {
                    "answer": "100 crowdworkers ",
                    "type": "extractive"
                }
            ],
            "q_uid": "3d7d865e905295d11f1e85af5fa89b210e3e9fdf",
            "evidence": [
                {
                    "raw_evidence": [
                        "We recruited 100 crowdworkers on Amazon Mechanical Turk (AMT) and measured completion times and accuracies for typing randomly sampled sentences from the Yelp corpus. Each user was shown alternating autocomplete and writing tasks across 50 sentences (see Appendix for user interface). For the autocomplete task, we gave users a target sentence and asked them to type a set of keywords into the system. The users were shown the top three suggestions from the autocomplete system, and were asked to mark whether each of these three suggestions was semantically equivalent to the target sentence. For the writing task, we gave users a target sentence and asked them to either type the sentence verbatim or a sentence that preserves the meaning of the target sentence."
                    ],
                    "highlighted_evidence": [
                        "We recruited 100 crowdworkers on Amazon Mechanical Turk (AMT) and measured completion times and accuracies for typing randomly sampled sentences from the Yelp corpus. "
                    ]
                },
                {
                    "raw_evidence": [
                        "We recruited 100 crowdworkers on Amazon Mechanical Turk (AMT) and measured completion times and accuracies for typing randomly sampled sentences from the Yelp corpus. Each user was shown alternating autocomplete and writing tasks across 50 sentences (see Appendix for user interface). For the autocomplete task, we gave users a target sentence and asked them to type a set of keywords into the system. The users were shown the top three suggestions from the autocomplete system, and were asked to mark whether each of these three suggestions was semantically equivalent to the target sentence. For the writing task, we gave users a target sentence and asked them to either type the sentence verbatim or a sentence that preserves the meaning of the target sentence."
                    ],
                    "highlighted_evidence": [
                        "We recruited 100 crowdworkers on Amazon Mechanical Turk (AMT) and measured completion times and accuracies for typing randomly sampled sentences from the Yelp corpus."
                    ]
                }
            ]
        },
        {
            "question": "What user variations have been tested?",
            "answers": [
                {
                    "answer": "completion times and accuracies ",
                    "type": "extractive"
                }
            ],
            "q_uid": "2ad4d3d222f5237ed97923640bc8e199409cbe52",
            "evidence": [
                {
                    "raw_evidence": [
                        "We recruited 100 crowdworkers on Amazon Mechanical Turk (AMT) and measured completion times and accuracies for typing randomly sampled sentences from the Yelp corpus. Each user was shown alternating autocomplete and writing tasks across 50 sentences (see Appendix for user interface). For the autocomplete task, we gave users a target sentence and asked them to type a set of keywords into the system. The users were shown the top three suggestions from the autocomplete system, and were asked to mark whether each of these three suggestions was semantically equivalent to the target sentence. For the writing task, we gave users a target sentence and asked them to either type the sentence verbatim or a sentence that preserves the meaning of the target sentence."
                    ],
                    "highlighted_evidence": [
                        "We recruited 100 crowdworkers on Amazon Mechanical Turk (AMT) and measured completion times and accuracies for typing randomly sampled sentences from the Yelp corpus. "
                    ]
                }
            ]
        },
        {
            "question": "What are the baselines used?",
            "answers": [
                {
                    "answer": "Unif and Stopword",
                    "type": "extractive"
                },
                {
                    "answer": "Unif and Stopword",
                    "type": "extractive"
                }
            ],
            "q_uid": "3fad42be0fb2052bb404b989cc7d58b440cd23a0",
            "evidence": [
                {
                    "raw_evidence": [
                        "We quantify the efficiency-accuracy tradeoff compared to two rule-based baselines: Unif and Stopword. The Unif encoder randomly keeps tokens to generate keywords with the probability $\\delta $. The Stopword encoder keeps all tokens but drops stop words (e.g. `the', `a', `or') all the time ($\\delta =0$) or half of the time ($\\delta =0.5$). The corresponding decoders for these encoders are optimized using gradient descent to minimize the reconstruction error (i.e. $\\mathrm {loss}(x, \\alpha , \\beta )$)."
                    ],
                    "highlighted_evidence": [
                        "We quantify the efficiency-accuracy tradeoff compared to two rule-based baselines: Unif and Stopword. "
                    ]
                },
                {
                    "raw_evidence": [
                        "We quantify the efficiency-accuracy tradeoff compared to two rule-based baselines: Unif and Stopword. The Unif encoder randomly keeps tokens to generate keywords with the probability $\\delta $. The Stopword encoder keeps all tokens but drops stop words (e.g. `the', `a', `or') all the time ($\\delta =0$) or half of the time ($\\delta =0.5$). The corresponding decoders for these encoders are optimized using gradient descent to minimize the reconstruction error (i.e. $\\mathrm {loss}(x, \\alpha , \\beta )$)."
                    ],
                    "highlighted_evidence": [
                        "We quantify the efficiency-accuracy tradeoff compared to two rule-based baselines: Unif and Stopword."
                    ]
                }
            ]
        }
    ],
    "1910.03467": [
        {
            "question": "Are synonymous relation taken into account in the Japanese-Vietnamese task?",
            "answers": [
                {
                    "answer": "Yes",
                    "type": "boolean"
                }
            ],
            "q_uid": "b367b823c5db4543ac421d0057b02f62ea16bf9f",
            "evidence": [
                {
                    "raw_evidence": [
                        "Due to the fact that Vietnamese WordNet is not available, we only exploit WordNet to tackle unknown words of Japanese texts in our Japanese$\\rightarrow $Vietnamese translation system. After using Kytea, Japanese texts are applied LSW algorithm to replace OOV words by their synonyms. We choose 1-best synonym for each OOV word. Table TABREF18 shows the number of OOV words replaced by their synonyms. The replaced texts are then BPEd and trained on the proposed architecture. The largest improvement is +0.92 between (1) and (3). We observed an improvement of +0.7 BLEU points between (3) and (5) without using data augmentation described in BIBREF21."
                    ],
                    "highlighted_evidence": [
                        "After using Kytea, Japanese texts are applied LSW algorithm to replace OOV words by their synonyms. "
                    ]
                }
            ]
        }
    ],
    "1808.09633": [
        {
            "question": "What text sequences are associated with each vertex?",
            "answers": [
                {
                    "answer": "abstracts, sentences",
                    "type": "extractive"
                }
            ],
            "q_uid": "9a4aa0e4096c73cd2c3b1eab437c1bf24ae7bf03",
            "evidence": [
                {
                    "raw_evidence": [
                        "Although these methods have demonstrated performance gains over structure-only network embeddings, the relationship between text sequences for a pair of vertices is accounted for solely by comparing their sentence embeddings. However, as shown in Figure 1 , to assess the similarity between two research papers, a more effective strategy would compare and align (via local-weighting) individual important words (keywords) within a pair of abstracts, while information from other words (e.g., stop words) that tend to be less relevant can be effectively ignored (down-weighted). This alignment mechanism is difficult to accomplish in models where text sequences are first embedded into a common space and then compared in pairs BIBREF14 , BIBREF15 , BIBREF16 , BIBREF17 , BIBREF18 .",
                        "We propose to learn a semantic-aware Network Embedding (NE) that incorporates word-level alignment features abstracted from text sequences associated with vertex pairs. Given a pair of sentences, our model first aligns each word within one sentence with keywords from the other sentence (adaptively up-weighted via an attention mechanism), producing a set of fine-grained matching vectors. These features are then accumulated via a simple but efficient aggregation function, obtaining the final representation for the sentence. As a result, the word-by-word alignment features (as illustrated in Figure 1 ) are explicitly and effectively captured by our model. Further, the learned network embeddings under our framework are adaptive to the specific (local) vertices that are considered, and thus are context-aware and especially suitable for downstream tasks, such as link prediction. Moreover, since the word-by-word matching procedure introduced here is highly parallelizable and does not require any complex encoding networks, such as Long Short-Term Memory (LSTM) or Convolutional Neural Networks (CNNs), our framework requires significantly less time for training, which is attractive for large-scale network applications."
                    ],
                    "highlighted_evidence": [
                        "However, as shown in Figure 1 , to assess the similarity between two research papers, a more effective strategy would compare and align (via local-weighting) individual important words (keywords) within a pair of abstracts, while information from other words (e.g., stop words) that tend to be less relevant can be effectively ignored (down-weighted). ",
                        "We propose to learn a semantic-aware Network Embedding (NE) that incorporates word-level alignment features abstracted from text sequences associated with vertex pairs. Given a pair of sentences, our model first aligns each word within one sentence with keywords from the other sentence (adaptively up-weighted via an attention mechanism), producing a set of fine-grained matching vectors"
                    ]
                }
            ]
        }
    ],
    "1808.05902": [
        {
            "question": "what are the advantages of the proposed model?",
            "answers": [
                {
                    "answer": "he proposed model outperforms all the baselines, being the svi version the one that performs best., the svi version converges much faster to higher values of the log marginal likelihood when compared to the batch version, which reflects the efficiency of the svi algorithm.",
                    "type": "extractive"
                }
            ],
            "q_uid": "330f2cdeab689670b68583fc4125f5c0b26615a8",
            "evidence": [
                {
                    "raw_evidence": [
                        "For all the experiments the hyper-parameters INLINEFORM0 , INLINEFORM1 and INLINEFORM2 were set using a simple grid search in the collection INLINEFORM3 . The same approach was used to optimize the hyper-parameters of the all the baselines. For the svi algorithm, different mini-batch sizes and forgetting rates INLINEFORM4 were tested. For the 20-Newsgroup dataset, the best results were obtained with a mini-batch size of 500 and INLINEFORM5 . The INLINEFORM6 was kept at 1. The results are shown in Fig. FIGREF87 for different numbers of topics, where we can see that the proposed model outperforms all the baselines, being the svi version the one that performs best.",
                        "In order to assess the computational advantages of the stochastic variational inference (svi) over the batch algorithm, the log marginal likelihood (or log evidence) was plotted against the number of iterations. Fig. FIGREF88 shows this comparison. Not surprisingly, the svi version converges much faster to higher values of the log marginal likelihood when compared to the batch version, which reflects the efficiency of the svi algorithm."
                    ],
                    "highlighted_evidence": [
                        "The results are shown in Fig. FIGREF87 for different numbers of topics, where we can see that the proposed model outperforms all the baselines, being the svi version the one that performs best.",
                        "In order to assess the computational advantages of the stochastic variational inference (svi) over the batch algorithm, the log marginal likelihood (or log evidence) was plotted against the number of iterations. Fig. FIGREF88 shows this comparison. Not surprisingly, the svi version converges much faster to higher values of the log marginal likelihood when compared to the batch version, which reflects the efficiency of the svi algorithm."
                    ]
                }
            ]
        },
        {
            "question": "what are the state of the art approaches?",
            "answers": [
                {
                    "answer": "Bosch 2006 (mv), LDA + LogReg (mv), LDA + Raykar, LDA + Rodrigues, Blei 2003 (mv), sLDA (mv)",
                    "type": "extractive"
                }
            ],
            "q_uid": "c87b2dd5c439d5e68841a705dd81323ec0d64c97",
            "evidence": [
                {
                    "raw_evidence": [
                        "With the purpose of comparing the proposed model with a popular state-of-the-art approach for image classification, for the LabelMe dataset, the following baseline was introduced:",
                        "Bosch 2006 (mv): This baseline is similar to one in BIBREF33 . The authors propose the use of pLSA to extract the latent topics, and the use of k-nearest neighbor (kNN) classifier using the documents' topics distributions. For this baseline, unsupervised LDA is used instead of pLSA, and the labels from the different annotators for kNN (with INLINEFORM0 ) are aggregated using majority voting (mv).",
                        "The results obtained by the different approaches for the LabelMe data are shown in Fig. FIGREF94 , where the svi version is using mini-batches of 200 documents.",
                        "Analyzing the results for the Reuters-21578 and LabelMe data, we can observe that MA-sLDAc outperforms all the baselines, with slightly better accuracies for the batch version, especially in the Reuters data. Interestingly, the second best results are consistently obtained by the multi-annotator approaches, which highlights the need for accounting for the noise and biases of the answers of the different annotators.",
                        "Both the batch and the stochastic variational inference (svi) versions of the proposed model (MA-sLDAc) are compared with the following baselines:",
                        "[itemsep=0.02cm]",
                        "LDA + LogReg (mv): This baseline corresponds to applying unsupervised LDA to the data, and learning a logistic regression classifier on the inferred topics distributions of the documents. The labels from the different annotators were aggregated using majority voting (mv). Notice that, when there is a single annotator label per instance, majority voting is equivalent to using that label for training. This is the case of the 20-Newsgroups' simulated annotators, but the same does not apply for the experiments in Section UID89 .",
                        "LDA + Raykar: For this baseline, the model of BIBREF21 was applied using the documents' topic distributions inferred by LDA as features.",
                        "LDA + Rodrigues: This baseline is similar to the previous one, but uses the model of BIBREF9 instead.",
                        "Blei 2003 (mv): The idea of this baseline is to replicate a popular state-of-the-art approach for document classification. Hence, the approach of BIBREF0 was used. It consists of applying LDA to extract the documents' topics distributions, which are then used to train a SVM. Similarly to the previous approach, the labels from the different annotators were aggregated using majority voting (mv).",
                        "sLDA (mv): This corresponds to using the classification version of sLDA BIBREF2 with the labels obtained by performing majority voting (mv) on the annotators' answers."
                    ],
                    "highlighted_evidence": [
                        "With the purpose of comparing the proposed model with a popular state-of-the-art approach for image classification, for the LabelMe dataset, the following baseline was introduced:\n\nBosch 2006 (mv): This baseline is similar to one in BIBREF33 . The authors propose the use of pLSA to extract the latent topics, and the use of k-nearest neighbor (kNN) classifier using the documents' topics distributions. For this baseline, unsupervised LDA is used instead of pLSA, and the labels from the different annotators for kNN (with INLINEFORM0 ) are aggregated using majority voting (mv).\n\nThe results obtained by the different approaches for the LabelMe data are shown in Fig. FIGREF94 , where the svi version is using mini-batches of 200 documents.\n\nAnalyzing the results for the Reuters-21578 and LabelMe data, we can observe that MA-sLDAc outperforms all the baselines, with slightly better accuracies for the batch version, especially in the Reuters data. Interestingly, the second best results are consistently obtained by the multi-annotator approaches, which highlights the need for accounting for the noise and biases of the answers of the different annotators.",
                        "Both the batch and the stochastic variational inference (svi) versions of the proposed model (MA-sLDAc) are compared with the following baselines:\n\n[itemsep=0.02cm]\n\nLDA + LogReg (mv): This baseline corresponds to applying unsupervised LDA to the data, and learning a logistic regression classifier on the inferred topics distributions of the documents. The labels from the different annotators were aggregated using majority voting (mv). Notice that, when there is a single annotator label per instance, majority voting is equivalent to using that label for training. This is the case of the 20-Newsgroups' simulated annotators, but the same does not apply for the experiments in Section UID89 .\n\nLDA + Raykar: For this baseline, the model of BIBREF21 was applied using the documents' topic distributions inferred by LDA as features.\n\nLDA + Rodrigues: This baseline is similar to the previous one, but uses the model of BIBREF9 instead.\n\nBlei 2003 (mv): The idea of this baseline is to replicate a popular state-of-the-art approach for document classification. Hence, the approach of BIBREF0 was used. It consists of applying LDA to extract the documents' topics distributions, which are then used to train a SVM. Similarly to the previous approach, the labels from the different annotators were aggregated using majority voting (mv).\n\nsLDA (mv): This corresponds to using the classification version of sLDA BIBREF2 with the labels obtained by performing majority voting (mv) on the annotators' answers."
                    ]
                }
            ]
        },
        {
            "question": "what datasets were used?",
            "answers": [
                {
                    "answer": "Reuters-21578 BIBREF30,  LabelMe BIBREF31, 20-Newsgroups benchmark corpus BIBREF29 ",
                    "type": "extractive"
                },
                {
                    "answer": " 20-Newsgroups benchmark corpus , Reuters-21578, LabelMe",
                    "type": "extractive"
                }
            ],
            "q_uid": "f7789313a804e41fcbca906a4e5cf69039eeef9f",
            "evidence": [
                {
                    "raw_evidence": [
                        "In order to validate the proposed classification model in real crowdsourcing settings, Amazon Mechanical Turk (AMT) was used to obtain labels from multiple annotators for two popular datasets: Reuters-21578 BIBREF30 and LabelMe BIBREF31 .",
                        "In order to first validate the proposed model for classification problems in a slightly more controlled environment, the well-known 20-Newsgroups benchmark corpus BIBREF29 was used by simulating multiple annotators with different levels of expertise. The 20-Newsgroups consists of twenty thousand messages taken from twenty newsgroups, and is divided in six super-classes, which are, in turn, partitioned in several sub-classes. For this first set of experiments, only the four most populated super-classes were used: \u201ccomputers\", \u201cscience\", \u201cpolitics\" and \u201crecreative\". The preprocessing of the documents consisted of stemming and stop-words removal. After that, 75% of the documents were randomly selected for training and the remaining 25% for testing."
                    ],
                    "highlighted_evidence": [
                        "In order to validate the proposed classification model in real crowdsourcing settings, Amazon Mechanical Turk (AMT) was used to obtain labels from multiple annotators for two popular datasets: Reuters-21578 BIBREF30 and LabelMe BIBREF31 .",
                        "In order to first validate the proposed model for classification problems in a slightly more controlled environment, the well-known 20-Newsgroups benchmark corpus BIBREF29 was used by simulating multiple annotators with different levels of expertise. "
                    ]
                },
                {
                    "raw_evidence": [
                        "In order to first validate the proposed model for classification problems in a slightly more controlled environment, the well-known 20-Newsgroups benchmark corpus BIBREF29 was used by simulating multiple annotators with different levels of expertise. The 20-Newsgroups consists of twenty thousand messages taken from twenty newsgroups, and is divided in six super-classes, which are, in turn, partitioned in several sub-classes. For this first set of experiments, only the four most populated super-classes were used: \u201ccomputers\", \u201cscience\", \u201cpolitics\" and \u201crecreative\". The preprocessing of the documents consisted of stemming and stop-words removal. After that, 75% of the documents were randomly selected for training and the remaining 25% for testing.",
                        "In order to validate the proposed classification model in real crowdsourcing settings, Amazon Mechanical Turk (AMT) was used to obtain labels from multiple annotators for two popular datasets: Reuters-21578 BIBREF30 and LabelMe BIBREF31 ."
                    ],
                    "highlighted_evidence": [
                        "In order to first validate the proposed model for classification problems in a slightly more controlled environment, the well-known 20-Newsgroups benchmark corpus BIBREF29 was used by simulating multiple annotators with different levels of expertise. ",
                        "In order to validate the proposed classification model in real crowdsourcing settings, Amazon Mechanical Turk (AMT) was used to obtain labels from multiple annotators for two popular datasets: Reuters-21578 BIBREF30 and LabelMe BIBREF31 ."
                    ]
                }
            ]
        }
    ],
    "1912.02761": [
        {
            "question": "Do they propose any solution to debias the embeddings?",
            "answers": [
                {
                    "answer": "No",
                    "type": "boolean"
                }
            ],
            "q_uid": "2cc42d14c8c927939a6b8d06f4fdee0913042416",
            "evidence": [
                {
                    "raw_evidence": [
                        "We have presented the first study on social bias in KG embeddings, and proposed a new metric for measuring such bias. We demonstrated that differences in the distributions of entities in real-world knowledge graphs (there are many more male bankers in Wikidata than female) translate into harmful biases related to professions being encoded in embeddings. Given that KGs are formed of real-world entities, we cannot simply equalize the counts; it is not possible to correct history by creating female US Presidents, etc. In light of this, we suggest that care is needed when applying graph embeddings in NLP pipelines, and work needed to develop robust methods to debias such embeddings."
                    ],
                    "highlighted_evidence": [
                        "In light of this, we suggest that care is needed when applying graph embeddings in NLP pipelines, and work needed to develop robust methods to debias such embeddings."
                    ]
                }
            ]
        }
    ],
    "1906.01183": [
        {
            "question": "Which translation system do they use to translate to English?",
            "answers": [
                {
                    "answer": "Attention-based translation model with convolution sequence to sequence model",
                    "type": "abstractive"
                }
            ],
            "q_uid": "c45feda62f23245f53e855706e2d8ea733b7fd03",
            "evidence": [
                {
                    "raw_evidence": [
                        "Attention-base translation model We use the system of BIBREF6 , a convolutional sequence to sequence model. It divides translation process into two steps. First, in the encoder step, given an input sentence INLINEFORM0 of length INLINEFORM1 , INLINEFORM2 represents each word as word embedding INLINEFORM3 . After that, we obtain the absolute position of input elements INLINEFORM4 . Both vectors are concatenated to get input sentence representations INLINEFORM5 . Similarly, output elements INLINEFORM6 generated from decoder network have the same structure. A convolutional neural network (CNN) is used to get the hidden state of the sentence representation from left to right. Second, in the decoder step, attention mechanism is used in each CNN layer. In order to acquire the attention value, we combine the current decoder state INLINEFORM7 with the embedding of previous decoder output value INLINEFORM8 : DISPLAYFORM0"
                    ],
                    "highlighted_evidence": [
                        "Attention-base translation model We use the system of BIBREF6 , a convolutional sequence to sequence model."
                    ]
                }
            ]
        },
        {
            "question": "Which languages do they work with?",
            "answers": [
                {
                    "answer": "German, Spanish, Chinese",
                    "type": "extractive"
                }
            ],
            "q_uid": "9785ecf1107090c84c57112d01a8e83418a913c1",
            "evidence": [
                {
                    "raw_evidence": [
                        "We use experiments to evaluate the effectiveness of our proposed method on NER task. On three different low-resource languages, we conducted an experimental evaluation to prove the effectiveness of our back attention mechanism on the NER task. Four datasets are used in our work, including CoNLL 2003 German BIBREF9 , CoNLL 2002 Spanish BIBREF10 , OntoNotes 4 BIBREF11 and Weibo NER BIBREF12 . All the annotations are mapped to the BIOES format. Table TABREF14 shows the detailed statistics of the datasets.",
                        "Table TABREF22 shows the results on Chinese OntoNotes 4.0. Adding BAN to baseline model leads to an increase from 63.25% to 72.15% F1-score. In order to further improve the performance, we use the BERT model BIBREF20 to produce word embeddings. With no segmentation, we surpass the previous state-of-the-art approach by 6.33% F1-score. For Weibo dataset, the experiment results are shown in Table TABREF23 , where NE, NM and Overall denote named entities, nominal entities and both. The baseline model gives a 33.18% F1-score. Using the transfer knowledge by BAN, the baseline model achieves an immense improvement in F1-score, rising by 10.39%. We find that BAN still gets consistent improvement on a strong model. With BAN, the F1-score of BERT+BiLSTM+CRF increases to 70.76%."
                    ],
                    "highlighted_evidence": [
                        "Four datasets are used in our work, including CoNLL 2003 German BIBREF9 , CoNLL 2002 Spanish BIBREF10 , OntoNotes 4 BIBREF11 and Weibo NER BIBREF12 . ",
                        "Table TABREF22 shows the results on Chinese OntoNotes 4.0. "
                    ]
                }
            ]
        },
        {
            "question": "Which pre-trained English NER model do they use?",
            "answers": [
                {
                    "answer": "Bidirectional LSTM based NER model of Flair",
                    "type": "abstractive"
                }
            ],
            "q_uid": "e051d68a7932f700e6c3f48da57d3e2519936c6d",
            "evidence": [
                {
                    "raw_evidence": [
                        "Pre-trained English NER model We construct the English NER system following BIBREF7 . This system uses a bidirectional LSTM as a character-level language model to take context information for word embedding generation. The hidden states of the character language model (CharLM) are used to create contextualized word embeddings. The final embedding INLINEFORM0 is concatenated by the CharLM embedding INLINEFORM1 and GLOVE embedding INLINEFORM2 BIBREF8 . A standard BiLSTM-CRF named entity recognition model BIBREF0 takes INLINEFORM3 to address the NER task.",
                        "We implement the basic BiLSTM-CRF model using PyTorch framework. FASTTEXT embeddings are used for generating word embeddings. Translation models are trained on United Nation Parallel Corpus. For pre-trained English NER system, we use the default NER model of Flair."
                    ],
                    "highlighted_evidence": [
                        "Pre-trained English NER model We construct the English NER system following BIBREF7 . This system uses a bidirectional LSTM as a character-level language model to take context information for word embedding generation. ",
                        "For pre-trained English NER system, we use the default NER model of Flair."
                    ]
                }
            ]
        }
    ],
    "1603.00968": [
        {
            "question": "How much faster is training time for MGNC-CNN over the baselines?",
            "answers": [
                {
                    "answer": "It is an order of magnitude more efficient in terms of training time., his model requires pre-training and mutual-learning and requires days of training time, whereas the simple architecture we propose requires on the order of an hour",
                    "type": "extractive"
                }
            ],
            "q_uid": "f7d67d6c6fbc62b2953ab74db6871b122b3c92cc",
            "evidence": [
                {
                    "raw_evidence": [
                        "Our approach enjoys the following advantages compared to the only existing comparable model BIBREF11 : (i) It can leverage diverse, readily available word embeddings with different dimensions, thus providing flexibility. (ii) It is comparatively simple, and does not, for example, require mutual learning or pre-training. (iii) It is an order of magnitude more efficient in terms of training time.",
                        "More similar to our work, Yin and Sch\u00fctze yin-schutze:2015:CoNLL proposed MVCNN for sentence classification. This CNN-based architecture accepts multiple word embeddings as inputs. These are then treated as separate `channels', analogous to RGB channels in images. Filters consider all channels simultaneously. MVCNN achieved state-of-the-art performance on multiple sentence classification tasks. However, this model has practical drawbacks. (i) MVCNN requires that input word embeddings have the same dimensionality. Thus to incorporate a second set of word vectors trained on a corpus (or using a model) of interest, one needs to either find embeddings that happen to have a set number of dimensions or to estimate embeddings from scratch. (ii) The model is complex, both in terms of implementation and run-time. Indeed, this model requires pre-training and mutual-learning and requires days of training time, whereas the simple architecture we propose requires on the order of an hour (and is easy to implement).",
                        "We can see that MGNC-CNN and MG-CNN always outperform baseline methods (including C-CNN), and MGNC-CNN is usually better than MG-CNN. And on the Subj dataset, MG-CNN actually achieves slightly better results than BIBREF11 , with far less complexity and required training time (MGNC-CNN performs comparably, although no better, here). On the TREC dataset, the best-ever accuracy we are aware of is 96.0% BIBREF21 , which falls within the range of the result of our MGNC-CNN model with three word embeddings. On the irony dataset, our model with three embeddings achieves 4% improvement (in terms of AUC) compared to the baseline model. On SST-1 and SST-2, our model performs slightly worse than BIBREF11 . However, we again note that their performance is achieved using a much more complex model which involves pre-training and mutual-learning steps. This model takes days to train, whereas our model requires on the order of an hour."
                    ],
                    "highlighted_evidence": [
                        "It is an order of magnitude more efficient in terms of training time.",
                        "The model is complex, both in terms of implementation and run-time. Indeed, this model requires pre-training and mutual-learning and requires days of training time, whereas the simple architecture we propose requires on the order of an hour (and is easy to implement).",
                        "MGNC-CNN is usually better than MG-CNN. And on the Subj dataset, MG-CNN actually achieves slightly better results than BIBREF11 , with far less complexity and required training time (MGNC-CNN performs comparably, although no better, here)."
                    ]
                }
            ]
        },
        {
            "question": "What dataset/corpus is this evaluated over?",
            "answers": [
                {
                    "answer": " SST-1, SST-2, Subj , TREC , Irony ",
                    "type": "extractive"
                }
            ],
            "q_uid": "a8e4a67dd67ae4a9ebf983a90b0d256f4b9ff6c6",
            "evidence": [
                {
                    "raw_evidence": [
                        "Stanford Sentiment Treebank Stanford Sentiment Treebank (SST) BIBREF14 . This concerns predicting movie review sentiment. Two datasets are derived from this corpus: (1) SST-1, containing five classes: very negative, negative, neutral, positive, and very positive. (2) SST-2, which has only two classes: negative and positive. For both, we remove phrases of length less than 4 from the training set.",
                        "Subj BIBREF15 . The aim here is to classify sentences as either subjective or objective. This comprises 5000 instances of each.",
                        "TREC BIBREF16 . A question classification dataset containing six classes: abbreviation, entity, description, human, location and numeric. There are 5500 training and 500 test instances.",
                        "Irony BIBREF17 . This dataset contains 16,006 sentences from reddit labeled as ironic (or not). The dataset is imbalanced (relatively few sentences are ironic). Thus before training, we under-sampled negative instances to make classes sizes equal. Note that for this dataset we report the Area Under Curve (AUC), rather than accuracy, because it is imbalanced."
                    ],
                    "highlighted_evidence": [
                        "Stanford Sentiment Treebank Stanford Sentiment Treebank (SST) BIBREF14 . This concerns predicting movie review sentiment. Two datasets are derived from this corpus: (1) SST-1, containing five classes: very negative, negative, neutral, positive, and very positive. (2) SST-2, which has only two classes: negative and positive. For both, we remove phrases of length less than 4 from the training set.",
                        "Subj BIBREF15 . The aim here is to classify sentences as either subjective or objective. This comprises 5000 instances of each.",
                        "TREC BIBREF16 . A question classification dataset containing six classes: abbreviation, entity, description, human, location and numeric. There are 5500 training and 500 test instances.",
                        "Irony BIBREF17 . This dataset contains 16,006 sentences from reddit labeled as ironic (or not). The dataset is imbalanced (relatively few sentences are ironic). Thus before training, we under-sampled negative instances to make classes sizes equal. Note that for this dataset we report the Area Under Curve (AUC), rather than accuracy, because it is imbalanced."
                    ]
                }
            ]
        }
    ],
    "1702.03274": [
        {
            "question": "Does the latent dialogue state heklp their model?",
            "answers": [
                {
                    "answer": "Yes",
                    "type": "boolean"
                }
            ],
            "q_uid": "ff814793387c8f3b61f09b88c73c00360a22a60e",
            "evidence": [
                {
                    "raw_evidence": [
                        "Recently, end-to-end approaches have trained recurrent neural networks (RNNs) directly on text transcripts of dialogs. A key benefit is that the RNN infers a latent representation of state, obviating the need for state labels. However, end-to-end methods lack a general mechanism for injecting domain knowledge and constraints. For example, simple operations like sorting a list of database results or updating a dictionary of entities can expressed in a few lines of software, yet may take thousands of dialogs to learn. Moreover, in some practical settings, programmed constraints are essential \u2013 for example, a banking dialog system would require that a user is logged in before they can retrieve account information."
                    ],
                    "highlighted_evidence": [
                        "A key benefit is that the RNN infers a latent representation of state, obviating the need for state labels."
                    ]
                }
            ]
        },
        {
            "question": "Do the authors test on datasets other than bAbl?",
            "answers": [
                {
                    "answer": "No",
                    "type": "boolean"
                }
            ],
            "q_uid": "059acc270062921ad27ee40a77fd50de6f02840a",
            "evidence": [
                {
                    "raw_evidence": [],
                    "highlighted_evidence": []
                }
            ]
        },
        {
            "question": "What is the reward model for the reinforcement learning appraoch?",
            "answers": [
                {
                    "answer": "reward 1 for successfully completing the task, with a discount by the number of turns, and reward 0 when fail",
                    "type": "abstractive"
                }
            ],
            "q_uid": "6a9eb407be6a459dc976ffeae17bdd8f71c8791c",
            "evidence": [
                {
                    "raw_evidence": [
                        "We defined the reward as being 1 for successfully completing the task, and 0 otherwise. A discount of $0.95$ was used to incentivize the system to complete dialogs faster rather than slower, yielding return 0 for failed dialogs, and $G = 0.95^{T-1}$ for successful dialogs, where $T$ is the number of system turns in the dialog. Finally, we created a set of 21 labeled dialogs, which will be used for supervised learning."
                    ],
                    "highlighted_evidence": [
                        "We defined the reward as being 1 for successfully completing the task, and 0 otherwise. A discount of $0.95$ was used to incentivize the system to complete dialogs faster rather than slower, yielding return 0 for failed dialogs, and $G = 0.95^{T-1}$ for successful dialogs, where $T$ is the number of system turns in the dialog. ",
                        " 0.95^{T-1}",
                        "reward  0.95^{T-1} "
                    ]
                }
            ]
        }
    ],
    "1909.09779": [
        {
            "question": "How were their results compared to state-of-the-art?",
            "answers": [
                {
                    "answer": "transformer model achieves higher BLEU score than both Attention encoder-decoder and sequence-sequence model",
                    "type": "extractive"
                }
            ],
            "q_uid": "e8e00b4c0673af5ab02ec82563105e4157cc54bb",
            "evidence": [
                {
                    "raw_evidence": [
                        "Table TABREF48 shows the BLEU score of all three models based on English-Hindi, Hindi-English on CFILT's test dataset respectively. From the results which we get, it is evident that the transformer model achieves higher BLEU score than both Attention encoder-decoder and sequence-sequence model. Attention encoder-decoder achieves better BLEU score and sequence-sequence model performs the worst out of the three which further consolidates the point that if we are dealing with long source and target sentences then attention mechanism is very much required to capture long term dependencies and we can solely rely on the attention mechanism, overthrowing recurrent cells completely for the machine translation task."
                    ],
                    "highlighted_evidence": [
                        "From the results which we get, it is evident that the transformer model achieves higher BLEU score than both Attention encoder-decoder and sequence-sequence model."
                    ]
                }
            ]
        }
    ],
    "1610.03112": [
        {
            "question": "Does this paper propose a new task that others can try to improve performance on?",
            "answers": [
                {
                    "answer": "No, there has been previous work on recognizing social norm violation.",
                    "type": "abstractive"
                }
            ],
            "q_uid": "cacb83e15e160d700db93c3f67c79a11281d20c5",
            "evidence": [
                {
                    "raw_evidence": [
                        "Interesting prior work on quantifying social norm violation has taken a heavily data-driven focus BIBREF8 , BIBREF9 . For instance, BIBREF8 trained a series of bigram language models to quantify the violation of social norms in users' posts on an online community by leveraging cross-entropy value, or the deviation of word sequences predicted by the language model and their usage by the user. However, their models were trained on written-language instead of natural face-face dialog corpus. Another kind of social norm violation was examined by BIBREF10 , who developed a classifier to identify specific types of sarcasm in tweets. They utilized a bootstrapping algorithm to automatically extract lists of positive sentiment phrases and negative situation phrases from given sarcastic tweets, which were in turn leveraged to recognize sarcasm in an SVM classifier. However, no contextual information was considered in this work. BIBREF11 understood the nature of social norm violation in dialog by correlating it with associated observable verbal, vocal and visual cues. By leveraging their findings and statistical machine learning techniques, they built a computational model for automatic recognition. While they preserved short-term temporal contextual information in the model, this study avoided dealing with sparsity of the social norm violation phenomena by under-sampling the negative-class instances to make a balanced dataset."
                    ],
                    "highlighted_evidence": [
                        "Interesting prior work on quantifying social norm violation has taken a heavily data-driven focus BIBREF8 , BIBREF9 . "
                    ]
                }
            ]
        }
    ],
    "1909.13717": [
        {
            "question": "what semantically conditioned models did they compare with?",
            "answers": [
                {
                    "answer": "Hierarchical Disentangled Self-Attention",
                    "type": "extractive"
                }
            ],
            "q_uid": "090fd9ce9a21438cdec1ea51ed216941d52eb3b6",
            "evidence": [
                {
                    "raw_evidence": [
                        "For goal-oriented generation, many of the models evaluated using the Inform/Request metrics have made use of structured data to semantically condition the generation model in order to generate better responses BIBREF35, BIBREF1, BIBREF0. BIBREF35 proposed to encode each individual dialog act as a unique vector and use it as an extra input feature, in order to influence the generated response. This method has been shown to work well when tested on single domains where the label space is limited to a few dialog acts. However, as the label space grows, using a one-hot encoding representation of the dialog act is not scalable. To deal with this problem, BIBREF1 introduced a semantically conditioned generation model using Hierarchical Disentangled Self-Attention (HDSA) . This model deals with the large label space by representing dialog acts using a multi-layer hierarchical graph that merges cross-branch nodes. For example, the distinct trees for hotel-recommend-area and attraction-recommend-area can be merged at the second and third levels sharing semantic information about actions and slots but maintaining specific information about the domains separate. This information can then be used as part of an attention mechanism when generating a response. This model achieves the state-of-the-art result for generation in both BLEU and Inform/Request metrics."
                    ],
                    "highlighted_evidence": [
                        "To deal with this problem, BIBREF1 introduced a semantically conditioned generation model using Hierarchical Disentangled Self-Attention (HDSA) ."
                    ]
                }
            ]
        }
    ],
    "1805.05581": [
        {
            "question": "How large is the dataset?",
            "answers": [
                {
                    "answer": "30M utterances",
                    "type": "extractive"
                }
            ],
            "q_uid": "77e57d19a0d48f46de8cbf857f5e5284bca0df2b",
            "evidence": [
                {
                    "raw_evidence": [
                        "We collected Japanese fictional stories from the Web to construct the dataset. The dataset contains approximately 30M utterances of fictional characters. We separated the data into a 99%\u20131% split for training and testing. In Japanese, the function words at the end of the sentence often exhibit style (e.g., desu+wa, desu+ze;) therefore, we used an existing lexicon of multi-word functional expressions BIBREF14 . Overall, the vocabulary size $\\vert \\mathcal {V} \\vert $ was 100K."
                    ],
                    "highlighted_evidence": [
                        "The dataset contains approximately 30M utterances of fictional characters."
                    ]
                }
            ]
        },
        {
            "question": "How is the dataset created?",
            "answers": [
                {
                    "answer": "We collected Japanese fictional stories from the Web",
                    "type": "extractive"
                }
            ],
            "q_uid": "50c8b821191339043306fd28e6cda2db400704f9",
            "evidence": [
                {
                    "raw_evidence": [
                        "We collected Japanese fictional stories from the Web to construct the dataset. The dataset contains approximately 30M utterances of fictional characters. We separated the data into a 99%\u20131% split for training and testing. In Japanese, the function words at the end of the sentence often exhibit style (e.g., desu+wa, desu+ze;) therefore, we used an existing lexicon of multi-word functional expressions BIBREF14 . Overall, the vocabulary size $\\vert \\mathcal {V} \\vert $ was 100K."
                    ],
                    "highlighted_evidence": [
                        "We collected Japanese fictional stories from the Web to construct the dataset."
                    ]
                }
            ]
        }
    ],
    "1910.05608": [
        {
            "question": "Is the data all in Vietnamese?",
            "answers": [
                {
                    "answer": "Yes",
                    "type": "boolean"
                }
            ],
            "q_uid": "284ea817fd79bc10b7a82c88d353e8f8a9d7e93c",
            "evidence": [
                {
                    "raw_evidence": [
                        "In the sixth international workshop on Vietnamese Language and Speech Processing (VLSP 2019), the Hate Speech Detection (HSD) task is proposed as one of the shared-tasks to handle the problem related to controlling content in SNSs. HSD is required to build a multi-class classification model that is capable of classifying an item to one of 3 classes (hate, offensive, clean). Hate speech (hate): an item is identified as hate speech if it (1) targets individuals or groups on the basis of their characteristics; (2) demonstrates a clear intention to incite harm, or to promote hatred; (3) may or may not use offensive or profane words. Offensive but not hate speech (offensive): an item (posts/comments) may contain offensive words but it does not target individuals or groups on the basis of their characteristics. Neither offensive nor hate speech (clean): normal item, it does not contain offensive language or hate speech.",
                        "The fundamental idea of this system is how to make a system that has the diversity of viewing an input. That because of the variety of the meaning in Vietnamese language especially with the acronym, teen code type. To make this diversity, after cleaning raw text input, we use multiple types of word tokenizers. Each one of these tokenizers, we combine with some types of representation methods, including word to vector methods such as continuous bag of words BIBREF5, pre-trained embedding as fasttext (trained on Wiki Vietnamese language) BIBREF6 and sonvx (trained on Vietnamese newspaper) BIBREF7. Each sentence has a set of words corresponding to a set of word vectors, and that set of word vectors is a representation of a sentence. We also make a sentence embedding by using RoBERTa architecture BIBREF8. CBOW and RoBERTa models trained on text from some resources including VLSP 2016 Sentiment Analysis, VLSP 2018 Sentiment Analysis, VLSP 2019 HSD and text crawled from Facebook. After having sentence representation, we use some classification models to classify input sentences. Those models will be described in detail in the section SECREF13. With the multiply output results, we will use an ensemble method to combine them and output the final result. Ensemble method we use here is Stacking method will be introduced in the section SECREF16."
                    ],
                    "highlighted_evidence": [
                        "In the sixth international workshop on Vietnamese Language and Speech Processing (VLSP 2019), the Hate Speech Detection (HSD) task is proposed as one of the shared-tasks to handle the problem related to controlling content in SNSs.",
                        "The fundamental idea of this system is how to make a system that has the diversity of viewing an input. That because of the variety of the meaning in Vietnamese language especially with the acronym, teen code type."
                    ]
                }
            ]
        },
        {
            "question": "What classifier do they use?",
            "answers": [
                {
                    "answer": "Stacking method, LSTMCNN, SARNN, simple LSTM bidirectional model, TextCNN",
                    "type": "extractive"
                }
            ],
            "q_uid": "c0122190119027dc3eb51f0d4b4483d2dbedc696",
            "evidence": [
                {
                    "raw_evidence": [
                        "The fundamental idea of this system is how to make a system that has the diversity of viewing an input. That because of the variety of the meaning in Vietnamese language especially with the acronym, teen code type. To make this diversity, after cleaning raw text input, we use multiple types of word tokenizers. Each one of these tokenizers, we combine with some types of representation methods, including word to vector methods such as continuous bag of words BIBREF5, pre-trained embedding as fasttext (trained on Wiki Vietnamese language) BIBREF6 and sonvx (trained on Vietnamese newspaper) BIBREF7. Each sentence has a set of words corresponding to a set of word vectors, and that set of word vectors is a representation of a sentence. We also make a sentence embedding by using RoBERTa architecture BIBREF8. CBOW and RoBERTa models trained on text from some resources including VLSP 2016 Sentiment Analysis, VLSP 2018 Sentiment Analysis, VLSP 2019 HSD and text crawled from Facebook. After having sentence representation, we use some classification models to classify input sentences. Those models will be described in detail in the section SECREF13. With the multiply output results, we will use an ensemble method to combine them and output the final result. Ensemble method we use here is Stacking method will be introduced in the section SECREF16.",
                        "The first model is TextCNN (figure FIGREF2) proposed in BIBREF11. It only contains CNN blocks following by some Dense layers. The output of multiple CNN blocks with different kernel sizes is connected to each other.",
                        "The second model is VDCNN (figure FIGREF5) inspired by the research in BIBREF12. Like the TextCNN model, it contains multiple CNN blocks. The addition in this model is its residual connection.",
                        "The third model is a simple LSTM bidirectional model (figure FIGREF15). It contains multiple LSTM bidirectional blocks stacked to each other.",
                        "The fourth model is LSTMCNN (figure FIGREF24). Before going through CNN blocks, series of word embedding will be transformed by LSTM bidirectional block.",
                        "The final model is the system named SARNN (figure FIGREF25). It adds an attention block between LTSM blocks.",
                        "Ensemble methods is a machine learning technique that combines several base models in order to produce one optimal predictive model. Have the main three types of ensemble methods including Bagging, Boosting and Stacking. In this system, we use the Stacking method. In this method, the output of each model is not only class id but also the probability of each class in the set of three classes. This probability will become a feature for the ensemble model. The stacking ensemble model here is a simple full-connection model with input is all of probability that output from sub-model. The output is the probability of each class."
                    ],
                    "highlighted_evidence": [
                        " After having sentence representation, we use some classification models to classify input sentences. Those models will be described in detail in the section SECREF13.",
                        "The first model is TextCNN (figure FIGREF2) proposed in BIBREF11. It only contains CNN blocks following by some Dense layers. The output of multiple CNN blocks with different kernel sizes is connected to each other.\n\nThe second model is VDCNN (figure FIGREF5) inspired by the research in BIBREF12. Like the TextCNN model, it contains multiple CNN blocks. The addition in this model is its residual connection.\n\nThe third model is a simple LSTM bidirectional model (figure FIGREF15). It contains multiple LSTM bidirectional blocks stacked to each other.\n\nThe fourth model is LSTMCNN (figure FIGREF24). Before going through CNN blocks, series of word embedding will be transformed by LSTM bidirectional block.",
                        "The final model is the system named SARNN (figure FIGREF25). It adds an attention block between LTSM blocks.",
                        "In this system, we use the Stacking method. In this method, the output of each model is not only class id but also the probability of each class in the set of three classes. This probability will become a feature for the ensemble model. The stacking ensemble model here is a simple full-connection model with input is all of probability that output from sub-model. The output is the probability of each class."
                    ]
                }
            ]
        },
        {
            "question": "What is private dashboard?",
            "answers": [
                {
                    "answer": "Private dashboard is leaderboard where competitors can see results after competition is finished - on hidden part of test set (private test set).",
                    "type": "abstractive"
                }
            ],
            "q_uid": "1ed6acb88954f31b78d2821bb230b722374792ed",
            "evidence": [
                {
                    "raw_evidence": [
                        "For each model having the best fit on the dev set, we export the probability distribution of classes for each sample in the dev set. In this case, we only use the result of model that has f1_macro score that larger than 0.67. The probability distribution of classes is then used as feature to input into a dense model with only one hidden layer (size 128). The training process of the ensemble model is done on samples of the dev set. The best fit result is 0.7356. The final result submitted in public leaderboard is 0.73019 and in private leaderboard is 0.58455. It is quite different in bad way. That maybe is the result of the model too overfit on train set tuning on public test set."
                    ],
                    "highlighted_evidence": [
                        "The final result submitted in public leaderboard is 0.73019 and in private leaderboard is 0.58455. It is quite different in bad way. That maybe is the result of the model too overfit on train set tuning on public test set."
                    ]
                }
            ]
        },
        {
            "question": "What is public dashboard?",
            "answers": [
                {
                    "answer": "Public dashboard where competitors can see their results during competition, on part of the test set (public test set).",
                    "type": "abstractive"
                }
            ],
            "q_uid": "5a33ec23b4341584a8079db459d89a4e23420494",
            "evidence": [
                {
                    "raw_evidence": [
                        "For each model having the best fit on the dev set, we export the probability distribution of classes for each sample in the dev set. In this case, we only use the result of model that has f1_macro score that larger than 0.67. The probability distribution of classes is then used as feature to input into a dense model with only one hidden layer (size 128). The training process of the ensemble model is done on samples of the dev set. The best fit result is 0.7356. The final result submitted in public leaderboard is 0.73019 and in private leaderboard is 0.58455. It is quite different in bad way. That maybe is the result of the model too overfit on train set tuning on public test set."
                    ],
                    "highlighted_evidence": [
                        "The final result submitted in public leaderboard is 0.73019 and in private leaderboard is 0.58455. It is quite different in bad way. That maybe is the result of the model too overfit on train set tuning on public test set."
                    ]
                }
            ]
        },
        {
            "question": "What dataset do they use?",
            "answers": [
                {
                    "answer": "They used Wiki Vietnamese language and Vietnamese newspapers to pretrain embeddings and dataset provided in HSD task to train model (details not mentioned in paper).",
                    "type": "abstractive"
                }
            ],
            "q_uid": "1b9119813ea637974d21862a8ace83bc1acbab8e",
            "evidence": [
                {
                    "raw_evidence": [
                        "The fundamental idea of this system is how to make a system that has the diversity of viewing an input. That because of the variety of the meaning in Vietnamese language especially with the acronym, teen code type. To make this diversity, after cleaning raw text input, we use multiple types of word tokenizers. Each one of these tokenizers, we combine with some types of representation methods, including word to vector methods such as continuous bag of words BIBREF5, pre-trained embedding as fasttext (trained on Wiki Vietnamese language) BIBREF6 and sonvx (trained on Vietnamese newspaper) BIBREF7. Each sentence has a set of words corresponding to a set of word vectors, and that set of word vectors is a representation of a sentence. We also make a sentence embedding by using RoBERTa architecture BIBREF8. CBOW and RoBERTa models trained on text from some resources including VLSP 2016 Sentiment Analysis, VLSP 2018 Sentiment Analysis, VLSP 2019 HSD and text crawled from Facebook. After having sentence representation, we use some classification models to classify input sentences. Those models will be described in detail in the section SECREF13. With the multiply output results, we will use an ensemble method to combine them and output the final result. Ensemble method we use here is Stacking method will be introduced in the section SECREF16.",
                        "The dataset in this HSD task is really imbalance. Clean class dominates with 91.5%, offensive class takes 5% and the rest belongs to hate class with 3.5%. To make model being able to learn with this imbalance data, we inject class weight to the loss function with the corresponding ratio (clean, offensive, hate) is $(0.09, 0.95, 0.96)$. Formular DISPLAY_FORM17 is the loss function apply for all models in our system. $w_i$ is the class weight, $y_i$ is the ground truth and $\\hat{y}_i$ is the output of the model. If the class weight is not set, we find that model cannot adjust parameters. The model tends to output all clean classes."
                    ],
                    "highlighted_evidence": [
                        "Each one of these tokenizers, we combine with some types of representation methods, including word to vector methods such as continuous bag of words BIBREF5, pre-trained embedding as fasttext (trained on Wiki Vietnamese language) BIBREF6 and sonvx (trained on Vietnamese newspaper) BIBREF7",
                        "The dataset in this HSD task is really imbalance. Clean class dominates with 91.5%, offensive class takes 5% and the rest belongs to hate class with 3.5%."
                    ]
                }
            ]
        }
    ],
    "1911.03059": [
        {
            "question": "what datasets did they use?",
            "answers": [
                {
                    "answer": "Dataset of total 3500 questions from the Internet and other sources such as books of general knowledge questions, history, etc.",
                    "type": "abstractive"
                },
                {
                    "answer": "3500 questions collected from the internet and books.",
                    "type": "abstractive"
                }
            ],
            "q_uid": "dc1cec824507fc85ac1ba87882fe1e422ff6cffb",
            "evidence": [
                {
                    "raw_evidence": [
                        "Though Bengali is the seventh most spoken language in terms of number of native speakers BIBREF23, there is no standard corpus of questions available BIBREF0. We have collected total 3500 questions from the Internet and other sources such as books of general knowledge questions, history etc. The corpus contains the questions and the classes each question belongs to."
                    ],
                    "highlighted_evidence": [
                        "We have collected total 3500 questions from the Internet and other sources such as books of general knowledge questions, history etc. The corpus contains the questions and the classes each question belongs to."
                    ]
                },
                {
                    "raw_evidence": [
                        "Though Bengali is the seventh most spoken language in terms of number of native speakers BIBREF23, there is no standard corpus of questions available BIBREF0. We have collected total 3500 questions from the Internet and other sources such as books of general knowledge questions, history etc. The corpus contains the questions and the classes each question belongs to."
                    ],
                    "highlighted_evidence": [
                        "We have collected total 3500 questions from the Internet and other sources such as books of general knowledge questions, history etc. "
                    ]
                }
            ]
        },
        {
            "question": "what ml based approaches were compared?",
            "answers": [
                {
                    "answer": "Multi-Layer Perceptron (MLP), Naive Bayes Classifier (NBC), Support Vector Machine (SVM), Gradient Boosting Classifier (GBC), Stochastic Gradient Descent (SGD), K Nearest Neighbour (K-NN) and Random Forest (RF)",
                    "type": "extractive"
                },
                {
                    "answer": "Multi-Layer Perceptron, Naive Bayes Classifier, Support Vector Machine, Gradient Boosting Classifier, Stochastic Gradient Descent, K Nearest Neighbour, Random Forest",
                    "type": "extractive"
                }
            ],
            "q_uid": "f428618ca9c017e0c9c2a23515dab30a7660f65f",
            "evidence": [
                {
                    "raw_evidence": [
                        "In this research, we briefly discuss the steps of QA system and compare the performance of seven machine learning based classifiers (Multi-Layer Perceptron (MLP), Naive Bayes Classifier (NBC), Support Vector Machine (SVM), Gradient Boosting Classifier (GBC), Stochastic Gradient Descent (SGD), K Nearest Neighbour (K-NN) and Random Forest (RF)) in classifying Bengali questions to classes based on their anticipated answers. Bengali questions have flexible inquiring ways, so there are many difficulties associated with Bengali QC BIBREF0. As there is no rich corpus of questions in Bengali Language available, collecting questions is an additional challenge. Different difficulties in building a QA System are mentioned in the literature BIBREF2 BIBREF3. The first work on a machine learning based approach towards Bengali question classification is presented in BIBREF0 that employ the Stochastic Gradient Descent (SGD)."
                    ],
                    "highlighted_evidence": [
                        "In this research, we briefly discuss the steps of QA system and compare the performance of seven machine learning based classifiers (Multi-Layer Perceptron (MLP), Naive Bayes Classifier (NBC), Support Vector Machine (SVM), Gradient Boosting Classifier (GBC), Stochastic Gradient Descent (SGD), K Nearest Neighbour (K-NN) and Random Forest (RF)) in classifying Bengali questions to classes based on their anticipated answers."
                    ]
                },
                {
                    "raw_evidence": [
                        "In this research, we briefly discuss the steps of QA system and compare the performance of seven machine learning based classifiers (Multi-Layer Perceptron (MLP), Naive Bayes Classifier (NBC), Support Vector Machine (SVM), Gradient Boosting Classifier (GBC), Stochastic Gradient Descent (SGD), K Nearest Neighbour (K-NN) and Random Forest (RF)) in classifying Bengali questions to classes based on their anticipated answers. Bengali questions have flexible inquiring ways, so there are many difficulties associated with Bengali QC BIBREF0. As there is no rich corpus of questions in Bengali Language available, collecting questions is an additional challenge. Different difficulties in building a QA System are mentioned in the literature BIBREF2 BIBREF3. The first work on a machine learning based approach towards Bengali question classification is presented in BIBREF0 that employ the Stochastic Gradient Descent (SGD)."
                    ],
                    "highlighted_evidence": [
                        "In this research, we briefly discuss the steps of QA system and compare the performance of seven machine learning based classifiers (Multi-Layer Perceptron (MLP), Naive Bayes Classifier (NBC), Support Vector Machine (SVM), Gradient Boosting Classifier (GBC), Stochastic Gradient Descent (SGD), K Nearest Neighbour (K-NN) and Random Forest (RF)) in classifying Bengali questions to classes based on their anticipated answers. "
                    ]
                }
            ]
        }
    ],
    "1910.03634": [
        {
            "question": "What models are used for painting embedding and what for language style transfer?",
            "answers": [
                {
                    "answer": "generating a poem from images we use an existing actor-critic architecture, various types of sequence to sequence models",
                    "type": "extractive"
                }
            ],
            "q_uid": "72e4e26d0dd79c590c28b10938952a9f9497ff1e",
            "evidence": [
                {
                    "raw_evidence": [
                        "For generating a poem from images we use an existing actor-critic architecture BIBREF1. This involves 3 parallel CNNs: an object CNN, sentiment CNN, and scene CNN, for feature extraction. These features are combined with a skip-thought model which provides poetic clues, which are then fed into a sequence-to-sequence model trained by policy gradient with 2 discriminator networks for rewards. This as a whole forms a pipeline that takes in an image and outputs a poem as shown on the top left of Figure FIGREF4. A CNN-RNN generative model acts as an agent. The parameters of this agent define a policy whose execution determines which word is selected as an action. When the agent selects all words in a poem, it receives a reward. Two discriminative networks, shown on the top right of Figure FIGREF4, are defined to serve as rewards concerning whether the generated poem properly describes the input image and whether the generated poem is poetic. The goal of the poem generation model is to generate a sequence of words as a poem for an image to maximize the expected return.",
                        "For Shakespearizing modern English texts, we experimented with various types of sequence to sequence models. Since the size of the parallel translation data available is small, we leverage a dictionary providing a mapping between Shakespearean words and modern English words to retrofit pre-trained word embeddings. Incorporating this extra information improves the translation task. The large number of shared word types between the source and target sentences indicates that sharing the representation between them is beneficial.",
                        "We use a sequence-to-sequence model which consists of a single layer unidrectional LSTM encoder and a single layer LSTM decoder and pre-trained retrofitted word embeddings shared between source and target sentences. We experimented with two different types of attention: global attention BIBREF9, in which the model makes use of the output from the encoder and decoder for the current time step only, and Bahdanau attention BIBREF10, where computing attention requires the output of the decoder from the prior time step. We found that global attention performs better in practice for our task of text style transfer.",
                        "Since a pair of corresponding Shakespeare and modern English sentences have significant vocabulary overlap we extend the sequence-to-sequence model mentioned above using pointer networks BIBREF11 that provide location based attention and have been used to enable copying of tokens directly from the input. Moreover, there are lot of proper nouns and rare words which might not be predicted by a vanilla sequence to sequence model."
                    ],
                    "highlighted_evidence": [
                        "For generating a poem from images we use an existing actor-critic architecture BIBREF1.",
                        "For Shakespearizing modern English texts, we experimented with various types of sequence to sequence models.",
                        "We use a sequence-to-sequence model which consists of a single layer unidrectional LSTM encoder and a single layer LSTM decoder and pre-trained retrofitted word embeddings shared between source and target sentences.",
                        "Since a pair of corresponding Shakespeare and modern English sentences have significant vocabulary overlap we extend the sequence-to-sequence model mentioned above using pointer networks BIBREF11 that provide location based attention and have been used to enable copying of tokens directly from the input."
                    ]
                }
            ]
        },
        {
            "question": "What limitations do the authors demnostrate of their model?",
            "answers": [
                {
                    "answer": "Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer",
                    "type": "extractive"
                },
                {
                    "answer": "we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score",
                    "type": "extractive"
                }
            ],
            "q_uid": "58ee0cbf1d8e3711c617b1cd3d7aca8620e26187",
            "evidence": [
                {
                    "raw_evidence": [
                        "Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score. This happens when the style transfer dataset does not have similar words in the training set of sentences. A solution would be to expand the style transfer dataset, for a better representation of the poem data."
                    ],
                    "highlighted_evidence": [
                        "Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score. This happens when the style transfer dataset does not have similar words in the training set of sentences. A solution would be to expand the style transfer dataset, for a better representation of the poem data."
                    ]
                },
                {
                    "raw_evidence": [
                        "Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score. This happens when the style transfer dataset does not have similar words in the training set of sentences. A solution would be to expand the style transfer dataset, for a better representation of the poem data."
                    ],
                    "highlighted_evidence": [
                        "Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for \"Starry Night\" with a low average content score. This happens when the style transfer dataset does not have similar words in the training set of sentences. A solution would be to expand the style transfer dataset, for a better representation of the poem data."
                    ]
                }
            ]
        },
        {
            "question": "How does final model rate on Likert scale?",
            "answers": [
                {
                    "answer": "average content score across the paintings is 3.7, average creativity score is 3.9, average style score is 3.9 ",
                    "type": "extractive"
                },
                {
                    "answer": "average content score across the paintings is 3.7, average creativity score is 3.9, average style score is 3.9",
                    "type": "extractive"
                }
            ],
            "q_uid": "f71b52e00e0be80c926f153b9fe0a06dd93af11e",
            "evidence": [
                {
                    "raw_evidence": [
                        "The average content score across the paintings is 3.7 which demonstrates that the prose generated is relevant to the painting. The average creativity score is 3.9 which demonstrates that the model captures more than basic objects in the painting successfully using poetic clues in the scene. The average style score is 3.9 which demonstrates that the prose generated is perceived to be in the style of Shakespeare."
                    ],
                    "highlighted_evidence": [
                        "The average content score across the paintings is 3.7 which demonstrates that the prose generated is relevant to the painting. The average creativity score is 3.9 which demonstrates that the model captures more than basic objects in the painting successfully using poetic clues in the scene. The average style score is 3.9 which demonstrates that the prose generated is perceived to be in the style of Shakespeare."
                    ]
                },
                {
                    "raw_evidence": [
                        "We perform a qualitative analysis of the Shakespearean prose generated for the input paintings. We conducted a survey, in which we presented famous paintings including those shown in Figures FIGREF1 and FIGREF10 and the corresponding Shakespearean prose generated by the model, and asked 32 students to rate them on the basis of content, creativity and similarity to Shakespearean style on a Likert scale of 1-5. Figure FIGREF12 shows the result of our human evaluation.",
                        "The average content score across the paintings is 3.7 which demonstrates that the prose generated is relevant to the painting. The average creativity score is 3.9 which demonstrates that the model captures more than basic objects in the painting successfully using poetic clues in the scene. The average style score is 3.9 which demonstrates that the prose generated is perceived to be in the style of Shakespeare."
                    ],
                    "highlighted_evidence": [
                        "We conducted a survey, in which we presented famous paintings including those shown in Figures FIGREF1 and FIGREF10 and the corresponding Shakespearean prose generated by the model, and asked 32 students to rate them on the basis of content, creativity and similarity to Shakespearean style on a Likert scale of 1-5. Figure FIGREF12 shows the result of our human evaluation.\n\nThe average content score across the paintings is 3.7 which demonstrates that the prose generated is relevant to the painting. The average creativity score is 3.9 which demonstrates that the model captures more than basic objects in the painting successfully using poetic clues in the scene. The average style score is 3.9 which demonstrates that the prose generated is perceived to be in the style of Shakespeare."
                    ]
                }
            ]
        },
        {
            "question": "What is best BLEU score of language style transfer authors got?",
            "answers": [
                {
                    "answer": "seq2seq model with global attention gives the best results with an average target BLEU score of 29.65",
                    "type": "extractive"
                },
                {
                    "answer": "average target BLEU score of 29.65",
                    "type": "extractive"
                }
            ],
            "q_uid": "54e945ea4b014e11ed4e1e61abc2aa9e68fea310",
            "evidence": [
                {
                    "raw_evidence": [
                        "For both seq2seq models, we use the attention matrices returned at each decoder time step during inference, to compute the next word in the translated sequence if the decoder output at the current time step is the UNK token. We replace the UNKs in the target output with the highest aligned, maximum attention, source word. The seq2seq model with global attention gives the best results with an average target BLEU score of 29.65 on the style transfer dataset, compared with an average target BLEU score of 26.97 using the seq2seq model with pointer networks."
                    ],
                    "highlighted_evidence": [
                        "The seq2seq model with global attention gives the best results with an average target BLEU score of 29.65 on the style transfer dataset, compared with an average target BLEU score of 26.97 using the seq2seq model with pointer networks."
                    ]
                },
                {
                    "raw_evidence": [
                        "For both seq2seq models, we use the attention matrices returned at each decoder time step during inference, to compute the next word in the translated sequence if the decoder output at the current time step is the UNK token. We replace the UNKs in the target output with the highest aligned, maximum attention, source word. The seq2seq model with global attention gives the best results with an average target BLEU score of 29.65 on the style transfer dataset, compared with an average target BLEU score of 26.97 using the seq2seq model with pointer networks."
                    ],
                    "highlighted_evidence": [
                        "The seq2seq model with global attention gives the best results with an average target BLEU score of 29.65 on the style transfer dataset, compared with an average target BLEU score of 26.97 using the seq2seq model with pointer networks."
                    ]
                }
            ]
        }
    ],
    "1703.07090": [
        {
            "question": "how small of a dataset did they train on?",
            "answers": [
                {
                    "answer": "1000 hours data",
                    "type": "extractive"
                },
                {
                    "answer": "23085 hours of data",
                    "type": "abstractive"
                }
            ],
            "q_uid": "8060a773f6a136944f7b59758d08cc6f2a59693b",
            "evidence": [
                {
                    "raw_evidence": [
                        "In this paper, we explore a entire deep LSTM RNN training framework, and employ it to real-time application. The deep learning systems benefit highly from a large quantity of labeled training data. Our first and basic speech recognition system is trained on 17000 hours of Shenma voice search dataset. It is a generic dataset sampled from diverse aspects of search queries. The requirement of speech recognition system also addressed by specific scenario, such as map and navigation task. The labeled dataset is too expensive, and training a new model with new large dataset from the beginning costs lots of time. Thus, it is natural to think of transferring the knowledge from basic model to new scenario's model. Transfer learning expends less data and less training time than full training. In this paper, we also introduce a novel transfer learning strategy with segmental Minimum Bayes-Risk (sMBR). As a result, transfer training with only 1000 hours data can match equivalent performance for full training with 7300 hours data."
                    ],
                    "highlighted_evidence": [
                        "As a result, transfer training with only 1000 hours data can match equivalent performance for full training with 7300 hours data."
                    ]
                },
                {
                    "raw_evidence": [
                        "Two dataset is divided into training set, validation set and test set separately, and the quantity of them is shown in Table TABREF10 . The three sets are split according to speakers, in order to avoid utterances of same speaker appearing in three sets simultaneously. The test sets of Shenma and Amap voice search are called Shenma Test and Amap Test."
                    ],
                    "highlighted_evidence": [
                        "Two dataset is divided into training set, validation set and test set separately, and the quantity of them is shown in Table TABREF10 . "
                    ]
                }
            ]
        }
    ],
    "1909.07863": [
        {
            "question": "What statistics on the VIST dataset are reported?",
            "answers": [
                {
                    "answer": "In the overall available data there are 40,071 training, 4,988 validation, and 5,050 usable testing stories.",
                    "type": "extractive"
                }
            ],
            "q_uid": "a9610cbcca813f4376fbfbf21cc14689c7fbd677",
            "evidence": [
                {
                    "raw_evidence": [
                        "We used the Visual storytelling (VIST) dataset comprising of image sequences obtained from Flickr albums and respective annotated descriptions collected through Amazon Mechanical Turk BIBREF1. Each sequence has 5 images with corresponding descriptions that together make up for a story. Furthermore, for each Flickr album there are 5 permutations of a selected set of its images. In the overall available data there are 40,071 training, 4,988 validation, and 5,050 usable testing stories."
                    ],
                    "highlighted_evidence": [
                        "We used the Visual storytelling (VIST) dataset comprising of image sequences obtained from Flickr albums and respective annotated descriptions collected through Amazon Mechanical Turk BIBREF1. Each sequence has 5 images with corresponding descriptions that together make up for a story. Furthermore, for each Flickr album there are 5 permutations of a selected set of its images. In the overall available data there are 40,071 training, 4,988 validation, and 5,050 usable testing stories."
                    ]
                }
            ]
        }
    ],
    "1909.04625": [
        {
            "question": "What is the size of the datasets employed?",
            "answers": [
                {
                    "answer": "(about 4 million sentences, 138 million word tokens), one trained on the Billion Word benchmark",
                    "type": "extractive"
                }
            ],
            "q_uid": "a1e07c7563ad038ee2a7de5093ea08efdd6077d4",
            "evidence": [
                {
                    "raw_evidence": [
                        "are trained to output the probability distribution of the upcoming word given a context, without explicitly representing the structure of the context BIBREF9, BIBREF10. We trained two two-layer recurrent neural language models with long short-term memory architecture BIBREF11 on a relatively small corpus. The first model, referred as `LSTM (PTB)' in the following sections, was trained on the sentences from Penn Treebank BIBREF12. The second model, referred as `LSTM (FTB)', was trained on the sentences from French Treebank BIBREF13. We set the size of input word embedding and LSTM hidden layer of both models as 256.",
                        "We also compare LSTM language models trained on large corpora. We incorporate two pretrained English language models: one trained on the Billion Word benchmark (referred as `LSTM (1B)') from BIBREF14, and the other trained on English Wikipedia (referred as `LSTM (enWiki)') from BIBREF3. For French, we trained a large LSTM language model (referred as `LSTM (frWaC)') on a random subset (about 4 million sentences, 138 million word tokens) of the frWaC dataset BIBREF15. We set the size of the input embeddings and hidden layers to 400 for the LSTM (frWaC) model since it is trained on a large dataset."
                    ],
                    "highlighted_evidence": [
                        "The first model, referred as `LSTM (PTB)' in the following sections, was trained on the sentences from Penn Treebank BIBREF12. The second model, referred as `LSTM (FTB)', was trained on the sentences from French Treebank BIBREF13.",
                        "We incorporate two pretrained English language models: one trained on the Billion Word benchmark (referred as `LSTM (1B)') from BIBREF14, and the other trained on English Wikipedia (referred as `LSTM (enWiki)') from BIBREF3. For French, we trained a large LSTM language model (referred as `LSTM (frWaC)') on a random subset (about 4 million sentences, 138 million word tokens) of the frWaC dataset BIBREF15."
                    ]
                }
            ]
        },
        {
            "question": "What are the baseline models?",
            "answers": [
                {
                    "answer": "Recurrent Neural Network (RNN), ActionLSTM, Generative Recurrent Neural Network Grammars (RNNG)",
                    "type": "extractive"
                }
            ],
            "q_uid": "a1c4f9e8661d4d488b8684f055e0ee0e2275f767",
            "evidence": [
                {
                    "raw_evidence": [
                        "Methods ::: Models Tested ::: Recurrent Neural Network (RNN) Language Models",
                        "are trained to output the probability distribution of the upcoming word given a context, without explicitly representing the structure of the context BIBREF9, BIBREF10. We trained two two-layer recurrent neural language models with long short-term memory architecture BIBREF11 on a relatively small corpus. The first model, referred as `LSTM (PTB)' in the following sections, was trained on the sentences from Penn Treebank BIBREF12. The second model, referred as `LSTM (FTB)', was trained on the sentences from French Treebank BIBREF13. We set the size of input word embedding and LSTM hidden layer of both models as 256.",
                        "Methods ::: Models Tested ::: ActionLSTM",
                        "models the linearized bracketed tree structure of a sentence by learning to predict the next action required to construct a phrase-structure parse BIBREF16. The action space consists of three possibilities: open a new non-terminal node and opening bracket; generate a terminal node; and close a bracket. To compute surprisal values for a given token, we approximate $P(w_i|w_{1\\cdots i-1)}$ by marginalizing over the most-likely partial parses found by word-synchronous beam search BIBREF17.",
                        "Methods ::: Models Tested ::: Generative Recurrent Neural Network Grammars (RNNG)",
                        "jointly model the word sequence as well as the underlying syntactic structure BIBREF18. Following BIBREF19, we estimate surprisal using word-synchronous beam search BIBREF17. We use the same hyper-parameter settings as BIBREF18."
                    ],
                    "highlighted_evidence": [
                        " Recurrent Neural Network (RNN) Language Models\nare trained to output the probability distribution of the upcoming word given a context, without explicitly representing the structure of the context BIBREF9, BIBREF10.",
                        "ActionLSTM\nmodels the linearized bracketed tree structure of a sentence by learning to predict the next action required to construct a phrase-structure parse BIBREF16.",
                        "Generative Recurrent Neural Network Grammars (RNNG)\njointly model the word sequence as well as the underlying syntactic structure BIBREF18."
                    ]
                }
            ]
        }
    ],
    "1908.06151": [
        {
            "question": "What was previous state of the art model for automatic post editing?",
            "answers": [
                {
                    "answer": "pal-EtAl:2018:WMT proposed an APE model that uses three self-attention-based encoders, tebbifakhr-EtAl:2018:WMT, the NMT-subtask winner of WMT 2018 ($wmt18^{nmt}_{best}$), employ sequence-level loss functions in order to avoid exposure bias during training and to be consistent with the automatic evaluation metrics., shin-lee:2018:WMT propose that each encoder has its own self-attention and feed-forward layer to process each input separately. , The APE PBSMT-subtask winner of WMT 2018 ($wmt18^{smt}_{best}$) BIBREF11 also presented another transformer-based multi-source APE which uses two encoders and stacks an additional cross-attention component for $src \\rightarrow pe$ above the previous cross-attention for $mt \\rightarrow pe$.",
                    "type": "extractive"
                }
            ],
            "q_uid": "7889ec45b996be0b8bf7360d08f84daf3644f115",
            "evidence": [
                {
                    "raw_evidence": [
                        "Recently, in the WMT 2018 APE shared task, several adaptations of the transformer architecture have been presented for multi-source APE. pal-EtAl:2018:WMT proposed an APE model that uses three self-attention-based encoders. They introduce an additional joint encoder that attends over a combination of the two encoded sequences from $mt$ and $src$. tebbifakhr-EtAl:2018:WMT, the NMT-subtask winner of WMT 2018 ($wmt18^{nmt}_{best}$), employ sequence-level loss functions in order to avoid exposure bias during training and to be consistent with the automatic evaluation metrics. shin-lee:2018:WMT propose that each encoder has its own self-attention and feed-forward layer to process each input separately. On the decoder side, they add two additional multi-head attention layers, one for $src \\rightarrow mt$ and another for $src \\rightarrow pe$. Thereafter another multi-head attention between the output of those attention layers helps the decoder to capture common words in $mt$ which should remain in $pe$. The APE PBSMT-subtask winner of WMT 2018 ($wmt18^{smt}_{best}$) BIBREF11 also presented another transformer-based multi-source APE which uses two encoders and stacks an additional cross-attention component for $src \\rightarrow pe$ above the previous cross-attention for $mt \\rightarrow pe$. Comparing shin-lee:2018:WMT's approach with the winner system, there are only two differences in the architecture: (i) the cross-attention order of $src \\rightarrow mt$ and $src \\rightarrow pe$ in the decoder, and (ii) $wmt18^{smt}_{best}$ additionally shares parameters between two encoders."
                    ],
                    "highlighted_evidence": [
                        "Recently, in the WMT 2018 APE shared task, several adaptations of the transformer architecture have been presented for multi-source APE. pal-EtAl:2018:WMT proposed an APE model that uses three self-attention-based encoders. They introduce an additional joint encoder that attends over a combination of the two encoded sequences from $mt$ and $src$. tebbifakhr-EtAl:2018:WMT, the NMT-subtask winner of WMT 2018 ($wmt18^{nmt}_{best}$), employ sequence-level loss functions in order to avoid exposure bias during training and to be consistent with the automatic evaluation metrics. shin-lee:2018:WMT propose that each encoder has its own self-attention and feed-forward layer to process each input separately. On the decoder side, they add two additional multi-head attention layers, one for $src \\rightarrow mt$ and another for $src \\rightarrow pe$. Thereafter another multi-head attention between the output of those attention layers helps the decoder to capture common words in $mt$ which should remain in $pe$. The APE PBSMT-subtask winner of WMT 2018 ($wmt18^{smt}_{best}$) BIBREF11 also presented another transformer-based multi-source APE which uses two encoders and stacks an additional cross-attention component for $src \\rightarrow pe$ above the previous cross-attention for $mt \\rightarrow pe$. Comparing shin-lee:2018:WMT's approach with the winner system, there are only two differences in the architecture: (i) the cross-attention order of $src \\rightarrow mt$ and $src \\rightarrow pe$ in the decoder, and (ii) $wmt18^{smt}_{best}$ additionally shares parameters between two encoders.",
                        "Recently, in the WMT 2018 APE shared task, several adaptations of the transformer architecture have been presented for multi-source APE. pal-EtAl:2018:WMT proposed an APE model that uses three self-attention-based encoders. They introduce an additional joint encoder that attends over a combination of the two encoded sequences from $mt$ and $src$. tebbifakhr-EtAl:2018:WMT, the NMT-subtask winner of WMT 2018 ($wmt18^{nmt}_{best}$), employ sequence-level loss functions in order to avoid exposure bias during training and to be consistent with the automatic evaluation metrics. shin-lee:2018:WMT propose that each encoder has its own self-attention and feed-forward layer to process each input separately. On the decoder side, they add two additional multi-head attention layers, one for $src \\rightarrow mt$ and another for $src \\rightarrow pe$. Thereafter another multi-head attention between the output of those attention layers helps the decoder to capture common words in $mt$ which should remain in $pe$. The APE PBSMT-subtask winner of WMT 2018 ($wmt18^{smt}_{best}$) BIBREF11 also presented another transformer-based multi-source APE which uses two encoders and stacks an additional cross-attention component for $src \\rightarrow pe$ above the previous cross-attention for $mt \\rightarrow pe$. Comparing shin-lee:2018:WMT's approach with the winner system, there are only two differences in the architecture: (i) the cross-attention order of $src \\rightarrow mt$ and $src \\rightarrow pe$ in the decoder, and (ii) $wmt18^{smt}_{best}$ additionally shares parameters between two encoders."
                    ]
                }
            ]
        }
    ],
    "1909.06200": [
        {
            "question": "What experiments are conducted?",
            "answers": [
                {
                    "answer": "Irony Classifier, Sentiment Classifier for Irony, Sentiment Classifier for Non-irony, transformation from ironic sentences to non-ironic sentences",
                    "type": "extractive"
                }
            ],
            "q_uid": "5c3e98e3cebaecd5d3e75ec2c9fc3dd267ac3c83",
            "evidence": [
                {
                    "raw_evidence": [
                        "Irony Classifier: We implement a CNN classifier trained with our irony dataset. All the CNN classifiers we utilize in this paper use the same parameters as BIBREF20 .",
                        "Sentiment Classifier for Irony: We first implement a one-layer LSTM network to classify ironic sentences in our dataset into positive and negative ironies. The LSTM network is trained with the dataset of Semeval 2015 Task 11 BIBREF0 which is used for the sentiment analysis of figurative language in twitter. Then, we use the positive ironies and negative ironies to train the CNN sentiment classifier for irony.",
                        "Sentiment Classifier for Non-irony: Similar to the training process of the sentiment classifier for irony, we first implement a one-layer LSTM network trained with the dataset for the sentiment analysis of common twitters to classify the non-ironies into positive and negative non-ironies. Then we use the positive and negative non-ironies to train the sentiment classifier for non-irony.",
                        "In this section, we describe some additional experiments on the transformation from ironic sentences to non-ironic sentences. Sometimes ironies are hard to understand and may cause misunderstanding, for which our task also explores the transformation from ironic sentences to non-ironic sentences."
                    ],
                    "highlighted_evidence": [
                        "Irony Classifier: We implement a CNN classifier trained with our irony dataset.",
                        "Sentiment Classifier for Irony: We first implement a one-layer LSTM network to classify ironic sentences in our dataset into positive and negative ironies.",
                        "Sentiment Classifier for Non-irony: Similar to the training process of the sentiment classifier for irony, we first implement a one-layer LSTM network trained with the dataset for the sentiment analysis of common twitters to classify the non-ironies into positive and negative non-ironies.",
                        "In this section, we describe some additional experiments on the transformation from ironic sentences to non-ironic sentences."
                    ]
                }
            ]
        },
        {
            "question": "What is the combination of rewards for reinforcement learning?",
            "answers": [
                {
                    "answer": "irony accuracy, sentiment preservation",
                    "type": "extractive"
                },
                {
                    "answer": " irony accuracy and sentiment preservation",
                    "type": "extractive"
                }
            ],
            "q_uid": "3f0ae9b772eeddfbfd239b7e3196dc6dfa21365f",
            "evidence": [
                {
                    "raw_evidence": [
                        "Since the gold transferred result of input is unavailable, we cannot evaluate the quality of the generated sentence directly. Therefore, we implement reinforcement learning and elaborately design two rewards to describe the irony accuracy and sentiment preservation, respectively."
                    ],
                    "highlighted_evidence": [
                        "Therefore, we implement reinforcement learning and elaborately design two rewards to describe the irony accuracy and sentiment preservation, respectively."
                    ]
                },
                {
                    "raw_evidence": [
                        "Since the gold transferred result of input is unavailable, we cannot evaluate the quality of the generated sentence directly. Therefore, we implement reinforcement learning and elaborately design two rewards to describe the irony accuracy and sentiment preservation, respectively."
                    ],
                    "highlighted_evidence": [
                        "Since the gold transferred result of input is unavailable, we cannot evaluate the quality of the generated sentence directly. Therefore, we implement reinforcement learning and elaborately design two rewards to describe the irony accuracy and sentiment preservation, respectively."
                    ]
                }
            ]
        },
        {
            "question": "What are the difficulties in modelling the ironic pattern?",
            "answers": [
                {
                    "answer": "obscure and hard to understand,  lack of previous work and baselines on irony generation",
                    "type": "extractive"
                },
                {
                    "answer": "ironies are often obscure and hard to understand",
                    "type": "abstractive"
                }
            ],
            "q_uid": "14b8ae5656e7d4ee02237288372d9e682b24fdb8",
            "evidence": [
                {
                    "raw_evidence": [
                        "Although some previous studies focus on irony detection, little attention is paid to irony generation. As ironies can strengthen sentiments and express stronger emotions, we mainly focus on generating ironic sentences. Given a non-ironic sentence, we implement a neural network to transfer it to an ironic sentence and constrain the sentiment polarity of the two sentences to be the same. For example, the input is \u201cI hate it when my plans get ruined\" which is negative in sentiment polarity and the output should be ironic and negative in sentiment as well, such as \u201cI like it when my plans get ruined\". The speaker uses \u201clike\" to be ironic and express his or her negative sentiment. At the same time, our model can preserve contents which are irrelevant to sentiment polarity and irony. According to the categories mentioned in BIBREF5 , irony can be classified into 3 classes: verbal irony by means of a polarity contrast, the sentences containing expression whose polarity is inverted between the intended and the literal evaluation; other types of verbal irony, the sentences that show no polarity contrast between the literal and intended meaning but are still ironic; and situational irony, the sentences that describe situations that fail to meet some expectations. As ironies in the latter two categories are obscure and hard to understand, we decide to only focus on ironies in the first category in this work. For example, our work can be specifically described as: given a sentence \u201cI hate to be ignored\", we train our model to generate an ironic sentence such as \u201cI love to be ignored\". Although there is \u201clove\" in the generated sentence, the speaker still expresses his or her negative sentiment by irony. We also make some explorations in the transformation from ironic sentences to non-ironic sentences at the end of our work. Because of the lack of previous work and baselines on irony generation, we implement our model based on style transfer. Our work will not only provide the first large-scale irony dataset but also make our model as a benchmark for the irony generation."
                    ],
                    "highlighted_evidence": [
                        "According to the categories mentioned in BIBREF5 , irony can be classified into 3 classes: verbal irony by means of a polarity contrast, the sentences containing expression whose polarity is inverted between the intended and the literal evaluation; other types of verbal irony, the sentences that show no polarity contrast between the literal and intended meaning but are still ironic; and situational irony, the sentences that describe situations that fail to meet some expectations. As ironies in the latter two categories are obscure and hard to understand, we decide to only focus on ironies in the first category in this work. ",
                        " Because of the lack of previous work and baselines on irony generation, we implement our model based on style transfer. "
                    ]
                },
                {
                    "raw_evidence": [
                        "Although some previous studies focus on irony detection, little attention is paid to irony generation. As ironies can strengthen sentiments and express stronger emotions, we mainly focus on generating ironic sentences. Given a non-ironic sentence, we implement a neural network to transfer it to an ironic sentence and constrain the sentiment polarity of the two sentences to be the same. For example, the input is \u201cI hate it when my plans get ruined\" which is negative in sentiment polarity and the output should be ironic and negative in sentiment as well, such as \u201cI like it when my plans get ruined\". The speaker uses \u201clike\" to be ironic and express his or her negative sentiment. At the same time, our model can preserve contents which are irrelevant to sentiment polarity and irony. According to the categories mentioned in BIBREF5 , irony can be classified into 3 classes: verbal irony by means of a polarity contrast, the sentences containing expression whose polarity is inverted between the intended and the literal evaluation; other types of verbal irony, the sentences that show no polarity contrast between the literal and intended meaning but are still ironic; and situational irony, the sentences that describe situations that fail to meet some expectations. As ironies in the latter two categories are obscure and hard to understand, we decide to only focus on ironies in the first category in this work. For example, our work can be specifically described as: given a sentence \u201cI hate to be ignored\", we train our model to generate an ironic sentence such as \u201cI love to be ignored\". Although there is \u201clove\" in the generated sentence, the speaker still expresses his or her negative sentiment by irony. We also make some explorations in the transformation from ironic sentences to non-ironic sentences at the end of our work. Because of the lack of previous work and baselines on irony generation, we implement our model based on style transfer. Our work will not only provide the first large-scale irony dataset but also make our model as a benchmark for the irony generation."
                    ],
                    "highlighted_evidence": [
                        "According to the categories mentioned in BIBREF5 , irony can be classified into 3 classes: verbal irony by means of a polarity contrast, the sentences containing expression whose polarity is inverted between the intended and the literal evaluation; other types of verbal irony, the sentences that show no polarity contrast between the literal and intended meaning but are still ironic; and situational irony, the sentences that describe situations that fail to meet some expectations. As ironies in the latter two categories are obscure and hard to understand, we decide to only focus on ironies in the first category in this work."
                    ]
                }
            ]
        },
        {
            "question": "How did the authors find ironic data on twitter?",
            "answers": [
                {
                    "answer": "They developed a classifier to find ironic sentences in twitter data",
                    "type": "abstractive"
                },
                {
                    "answer": "by crawling",
                    "type": "abstractive"
                }
            ],
            "q_uid": "e3a2d8886f03e78ed5e138df870f48635875727e",
            "evidence": [
                {
                    "raw_evidence": [
                        "As neural networks are proved effective in irony detection, we decide to implement a neural classifier in order to classify the sentences into ironic and non-ironic sentences. However, the only high-quality irony dataset we can obtain is the dataset of Semeval-2018 Task 3 and the dataset is pretty small, which will cause overfitting to complex models. Therefore, we just implement a simple one-layer RNN with LSTM cell to classify pre-processed sentences into ironic sentences and non-ironic sentences because LSTM networks are widely used in irony detection. We train the model with the dataset of Semeval-2018 Task 3. After classification, we get 262,755 ironic sentences and 399,775 non-ironic sentences. According to our observation, not all non-ironic sentences are suitable to be transferred into ironic sentences. For example, \u201cjust hanging out . watching . is it monday yet\" is hard to transfer because it does not have an explicit sentiment polarity. So we remove all interrogative sentences from the non-ironic sentences and only obtain the sentences which have words expressing strong sentiments. We evaluate the sentiment polarity of each word with TextBlob and we view those words with sentiment scores greater than 0.5 or less than -0.5 as words expressing strong sentiments. Finally, we build our irony dataset with 262,755 ironic sentences and 102,330 non-ironic sentences."
                    ],
                    "highlighted_evidence": [
                        "Therefore, we just implement a simple one-layer RNN with LSTM cell to classify pre-processed sentences into ironic sentences and non-ironic sentences because LSTM networks are widely used in irony detection."
                    ]
                },
                {
                    "raw_evidence": [
                        "In this paper, in order to address the lack of irony data, we first crawl over 2M tweets from twitter to build a dataset with 262,755 ironic and 112,330 non-ironic tweets. Then, due to the lack of parallel data, we propose a novel model to transfer non-ironic sentences to ironic sentences in an unsupervised way. As ironic style is hard to model and describe, we implement our model with the control of classifiers and reinforcement learning. Different from other studies in style transfer, the transformation from non-ironic to ironic sentences has to preserve sentiment polarity as mentioned above. Therefore, we not only design an irony reward to control the irony accuracy and implement denoising auto-encoder and back-translation to control content preservation but also design a sentiment reward to control sentiment preservation."
                    ],
                    "highlighted_evidence": [
                        "In this paper, in order to address the lack of irony data, we first crawl over 2M tweets from twitter to build a dataset with 262,755 ironic and 112,330 non-ironic tweets. "
                    ]
                }
            ]
        },
        {
            "question": "Who judged the irony accuracy, sentiment preservation and content preservation?",
            "answers": [
                {
                    "answer": "Irony accuracy is judged only by human ; senriment preservation and content preservation are judged  both by human and using automatic metrics (ACC and BLEU).",
                    "type": "abstractive"
                },
                {
                    "answer": "four annotators who are proficient in English",
                    "type": "extractive"
                }
            ],
            "q_uid": "62f27fe08ddb67f16857fab2a8a721926ecbb6fb",
            "evidence": [
                {
                    "raw_evidence": [
                        "In order to evaluate sentiment preservation, we use the absolute value of the difference between the standardized sentiment score of the input sentence and that of the generated sentence. We call the value as sentiment delta (senti delta). Besides, we report the sentiment accuracy (Senti ACC) which measures whether the output sentence has the same sentiment polarity as the input sentence based on our standardized sentiment classifiers. The BLEU score BIBREF25 between the input sentences and the output sentences is calculated to evaluate the content preservation performance. In order to evaluate the overall performance of different models, we also report the geometric mean (G2) and harmonic mean (H2) of the sentiment accuracy and the BLEU score. As for the irony accuracy, we only report it in human evaluation results because it is more accurate for the human to evaluate the quality of irony as it is very complicated."
                    ],
                    "highlighted_evidence": [
                        "Besides, we report the sentiment accuracy (Senti ACC) which measures whether the output sentence has the same sentiment polarity as the input sentence based on our standardized sentiment classifiers. The BLEU score BIBREF25 between the input sentences and the output sentences is calculated to evaluate the content preservation performance. In order to evaluate the overall performance of different models, we also report the geometric mean (G2) and harmonic mean (H2) of the sentiment accuracy and the BLEU score. As for the irony accuracy, we only report it in human evaluation results because it is more accurate for the human to evaluate the quality of irony as it is very complicated."
                    ]
                },
                {
                    "raw_evidence": [
                        "We first sample 50 non-ironic input sentences and their corresponding output sentences of different models. Then, we ask four annotators who are proficient in English to evaluate the qualities of the generated sentences of different models. They are required to rank the output sentences of our model and baselines from the best to the worst in terms of irony accuracy (Irony), Sentiment preservation (Senti) and content preservation (Content). The best output is ranked with 1 and the worst output is ranked with 6. That means that the smaller our human evaluation value is, the better the corresponding model is."
                    ],
                    "highlighted_evidence": [
                        "Then, we ask four annotators who are proficient in English to evaluate the qualities of the generated sentences of different models. They are required to rank the output sentences of our model and baselines from the best to the worst in terms of irony accuracy (Irony), Sentiment preservation (Senti) and content preservation (Content)."
                    ]
                }
            ]
        }
    ],
    "1902.11049": [
        {
            "question": "What human evaluation metrics were used in the paper?",
            "answers": [
                {
                    "answer": "rating questions on a scale of 1-5 based on fluency of language used and relevance of the question to the context",
                    "type": "abstractive"
                }
            ],
            "q_uid": "bfce2afe7a4b71f9127d4f9ef479a0bfb16eaf76",
            "evidence": [
                {
                    "raw_evidence": [
                        "For the human evaluation, we follow the standard approach in evaluating machine translation systems BIBREF23 , as used for question generation by BIBREF9 . We asked three workers to rate 300 generated questions between 1 (poor) and 5 (good) on two separate criteria: the fluency of the language used, and the relevance of the question to the context document and answer."
                    ],
                    "highlighted_evidence": [
                        "For the human evaluation, we follow the standard approach in evaluating machine translation systems BIBREF23 , as used for question generation by BIBREF9 . We asked three workers to rate 300 generated questions between 1 (poor) and 5 (good) on two separate criteria: the fluency of the language used, and the relevance of the question to the context document and answer."
                    ]
                }
            ]
        }
    ],
    "1909.13375": [
        {
            "question": "What approach did previous models use for multi-span questions?",
            "answers": [
                {
                    "answer": "Only MTMSM specifically tried to tackle the multi-span questions. Their approach consisted of two parts: first train a dedicated categorical variable to predict the number of spans to extract and the second was to generalize the single-span head method of extracting a span",
                    "type": "abstractive"
                }
            ],
            "q_uid": "9ab43f941c11a4b09a0e4aea61b4a5b4612e7933",
            "evidence": [
                {
                    "raw_evidence": [
                        "MTMSN BIBREF4 is the first, and only model so far, that specifically tried to tackle the multi-span questions of DROP. Their approach consisted of two parts. The first was to train a dedicated categorical variable to predict the number of spans to extract. The second was to generalize the single-span head method of extracting a span, by utilizing the non-maximum suppression (NMS) algorithm BIBREF7 to find the most probable set of non-overlapping spans. The number of spans to extract was determined by the aforementioned categorical variable."
                    ],
                    "highlighted_evidence": [
                        "MTMSN BIBREF4 is the first, and only model so far, that specifically tried to tackle the multi-span questions of DROP. Their approach consisted of two parts. The first was to train a dedicated categorical variable to predict the number of spans to extract. The second was to generalize the single-span head method of extracting a span, by utilizing the non-maximum suppression (NMS) algorithm BIBREF7 to find the most probable set of non-overlapping spans. The number of spans to extract was determined by the aforementioned categorical variable"
                    ]
                }
            ]
        },
        {
            "question": "How they use sequence tagging to answer multi-span questions?",
            "answers": [
                {
                    "answer": "To model an answer which is a collection of spans, the multi-span head uses the $\\mathtt {BIO}$ tagging format BIBREF8: $\\mathtt {B}$ is used to mark the beginning of a span, $\\mathtt {I}$ is used to mark the inside of a span and $\\mathtt {O}$ is used to mark tokens not included in a span",
                    "type": "extractive"
                }
            ],
            "q_uid": "5a02a3dd26485a4e4a77411b50b902d2bda3731b",
            "evidence": [
                {
                    "raw_evidence": [
                        "To model an answer which is a collection of spans, the multi-span head uses the $\\mathtt {BIO}$ tagging format BIBREF8: $\\mathtt {B}$ is used to mark the beginning of a span, $\\mathtt {I}$ is used to mark the inside of a span and $\\mathtt {O}$ is used to mark tokens not included in a span. In this way, we get a sequence of chunks that can be decoded to a final answer - a collection of spans."
                    ],
                    "highlighted_evidence": [
                        "To model an answer which is a collection of spans, the multi-span head uses the $\\mathtt {BIO}$ tagging format BIBREF8: $\\mathtt {B}$ is used to mark the beginning of a span, $\\mathtt {I}$ is used to mark the inside of a span and $\\mathtt {O}$ is used to mark tokens not included in a span. In this way, we get a sequence of chunks that can be decoded to a final answer - a collection of spans."
                    ]
                }
            ]
        },
        {
            "question": "What is the previous model that attempted to tackle multi-span questions as a part of its design?",
            "answers": [
                {
                    "answer": "MTMSN BIBREF4",
                    "type": "extractive"
                }
            ],
            "q_uid": "a9def7958eac7b9a780403d4f136927f756bab83",
            "evidence": [
                {
                    "raw_evidence": [
                        "MTMSN BIBREF4 is the first, and only model so far, that specifically tried to tackle the multi-span questions of DROP. Their approach consisted of two parts. The first was to train a dedicated categorical variable to predict the number of spans to extract. The second was to generalize the single-span head method of extracting a span, by utilizing the non-maximum suppression (NMS) algorithm BIBREF7 to find the most probable set of non-overlapping spans. The number of spans to extract was determined by the aforementioned categorical variable."
                    ],
                    "highlighted_evidence": [
                        "MTMSN BIBREF4 is the first, and only model so far, that specifically tried to tackle the multi-span questions of DROP. "
                    ]
                }
            ]
        }
    ],
    "1708.09609": [
        {
            "question": "Who annotated the data?",
            "answers": [
                {
                    "answer": "annotators who were not security experts, researchers in either NLP or computer security",
                    "type": "extractive"
                }
            ],
            "q_uid": "fc06502fa62803b62f6fd84265bfcfb207c1113b",
            "evidence": [
                {
                    "raw_evidence": [
                        "We developed our annotation guidelines through six preliminary rounds of annotation, covering 560 posts. Each round was followed by discussion and resolution of every post with disagreements. We benefited from members of our team who brought extensive domain expertise to the task. As well as refining the annotation guidelines, the development process trained annotators who were not security experts. The data annotated during this process is not included in Table TABREF3 .",
                        "Once we had defined the annotation standard, we annotated datasets from Darkode, Hack Forums, Blackhat, and Nulled as described in Table TABREF3 . Three people annotated every post in the Darkode training, Hack Forums training, Blackhat test, and Nulled test sets; these annotations were then merged into a final annotation by majority vote. The development and test sets for Darkode and Hack Forums were annotated by additional team members (five for Darkode, one for Hack Forums), and then every disagreement was discussed and resolved to produce a final annotation. The authors, who are researchers in either NLP or computer security, did all of the annotation."
                    ],
                    "highlighted_evidence": [
                        " As well as refining the annotation guidelines, the development process trained annotators who were not security experts.",
                        "The development and test sets for Darkode and Hack Forums were annotated by additional team members (five for Darkode, one for Hack Forums), and then every disagreement was discussed and resolved to produce a final annotation. The authors, who are researchers in either NLP or computer security, did all of the annotation."
                    ]
                }
            ]
        }
    ],
    "1805.07513": [
        {
            "question": "Do they compare with the MAML algorithm?",
            "answers": [
                {
                    "answer": "No",
                    "type": "boolean"
                }
            ],
            "q_uid": "5c4a2a3d6e02bcbeae784e439441524535916e85",
            "evidence": [
                {
                    "raw_evidence": [
                        "We compare our method to the following baselines: (1) Single-task CNN: training a CNN model for each task individually; (2) Single-task FastText: training one FastText model BIBREF23 with fixed embeddings for each individual task; (3) Fine-tuned the holistic MTL-CNN: a standard transfer-learning approach, which trains one MTL-CNN model on all the training tasks offline, then fine-tunes the classifier layer (i.e. $\\mathrm {M}^{(cls)}$ Figure 1 (a)) on each target task; (4) Matching Network: a metric-learning based few-shot learning model trained on all training tasks; (5) Prototypical Network: a variation of matching network with different prediction function as Eq. 9 ; (6) Convex combining all single-task models: training one CNN classifier on each meta-training task individually and taking the encoder, then for each target task training a linear combination of all the above single-task encoders with Eq. ( 24 ). This baseline can be viewed as a variation of our method without task clustering. We initialize all models with pre-trained 100-dim Glove embeddings (trained on 6B corpus) BIBREF24 ."
                    ],
                    "highlighted_evidence": [
                        "We compare our method to the following baselines: (1) Single-task CNN: training a CNN model for each task individually; (2) Single-task FastText: training one FastText model BIBREF23 with fixed embeddings for each individual task; (3) Fine-tuned the holistic MTL-CNN: a standard transfer-learning approach, which trains one MTL-CNN model on all the training tasks offline, then fine-tunes the classifier layer (i.e. $\\mathrm {M}^{(cls)}$ Figure 1 (a)) on each target task; (4) Matching Network: a metric-learning based few-shot learning model trained on all training tasks; (5) Prototypical Network: a variation of matching network with different prediction function as Eq. 9 ; (6) Convex combining all single-task models: training one CNN classifier on each meta-training task individually and taking the encoder, then for each target task training a linear combination of all the above single-task encoders with Eq. ( 24 ). "
                    ]
                }
            ]
        }
    ],
    "1702.01517": [
        {
            "question": "Does they focus on any specific product/service domain?",
            "answers": [
                {
                    "answer": "local businesses (i.e. restaurants)",
                    "type": "extractive"
                }
            ],
            "q_uid": "33d1f53cf25a7701db605b6b7ac36946af588bb7",
            "evidence": [
                {
                    "raw_evidence": [
                        "Our data are collected from the yelp academic dataset, provided by Yelp.com, a popular restaurant review website. The data set contains three types of objects: business, user, and review, where business objects contain basic information about local businesses (i.e. restaurants), review objects contain review texts and star rating, and user objects contain aggregate information about a single user across all of Yelp. Table TABREF31 illustrates the general statistics of the dataset."
                    ],
                    "highlighted_evidence": [
                        "Our data are collected from the yelp academic dataset, provided by Yelp.com, a popular restaurant review website. The data set contains three types of objects: business, user, and review, where business objects contain basic information about local businesses (i.e. restaurants), review objects contain review texts and star rating, and user objects contain aggregate information about a single user across all of Yelp. Table TABREF31 illustrates the general statistics of the dataset."
                    ]
                }
            ]
        },
        {
            "question": "What are the baselines?",
            "answers": [
                {
                    "answer": "RS-Average , RS-Linear, RS-Item, RS-MF, Sum-Opinosis, Sum-LSTM-Att",
                    "type": "extractive"
                }
            ],
            "q_uid": "0aa46c132515d8830a72f263812cdf7cbd5627c6",
            "evidence": [
                {
                    "raw_evidence": [
                        "We show the final results for opinion recommendation, comparing our proposed model with the following state-of-the-art baseline systems:",
                        "RS-Average is the widely-adopted baseline (e.g., by Yelp.com), using the averaged review scores as the final score.",
                        "RS-Linear estimates the rating score that a user would give by INLINEFORM0 BIBREF49 , where INLINEFORM1 and INLINEFORM2 are the the training deviations of the user INLINEFORM3 and the product INLINEFORM4 , respectively.",
                        "RS-Item applies INLINEFORM0 NN to estimate the rating score BIBREF50 . We choose the cosine similarity between INLINEFORM1 to measure the distance between product.",
                        "RS-MF is a state-of-the-art recommendation model, which uses matrix factorisation to predict rating score BIBREF8 , BIBREF41 , BIBREF25 .",
                        "Sum-Opinosis uses a graph-based framework to generate abstractive summarisation given redundant opinions BIBREF51 .",
                        "Sum-LSTM-Att is a state-of-the-art neural abstractive summariser, which uses an attentional neural model to consolidate information from multiple text sources, generating summaries using LSTM decoding BIBREF44 , BIBREF3 .",
                        "All the baseline models are single-task models, without considering rating and summarisation prediction jointly. The results are shown in Table TABREF46 . Our model (\u201c Joint\u201d) significantly outperforms both \u201cRS-Average\u201d and \u201cRS-Linear\u201d ( INLINEFORM0 using INLINEFORM1 -test), which demonstrates the strength of opinion recommendation, which leverages user characteristics for calculating a rating score for the user."
                    ],
                    "highlighted_evidence": [
                        "We show the final results for opinion recommendation, comparing our proposed model with the following state-of-the-art baseline systems:\n\nRS-Average is the widely-adopted baseline (e.g., by Yelp.com), using the averaged review scores as the final score.\n\nRS-Linear estimates the rating score that a user would give by INLINEFORM0 BIBREF49 , where INLINEFORM1 and INLINEFORM2 are the the training deviations of the user INLINEFORM3 and the product INLINEFORM4 , respectively.\n\nRS-Item applies INLINEFORM0 NN to estimate the rating score BIBREF50 . We choose the cosine similarity between INLINEFORM1 to measure the distance between product.\n\nRS-MF is a state-of-the-art recommendation model, which uses matrix factorisation to predict rating score BIBREF8 , BIBREF41 , BIBREF25 .\n\nSum-Opinosis uses a graph-based framework to generate abstractive summarisation given redundant opinions BIBREF51 .\n\nSum-LSTM-Att is a state-of-the-art neural abstractive summariser, which uses an attentional neural model to consolidate information from multiple text sources, generating summaries using LSTM decoding BIBREF44 , BIBREF3 .\n\nAll the baseline models are single-task models, without considering rating and summarisation prediction jointly. The results are shown in Table TABREF46 . "
                    ]
                }
            ]
        }
    ],
    "1804.11346": [
        {
            "question": "Are the annotations automatic or manually created?",
            "answers": [
                {
                    "answer": "Automatic",
                    "type": "abstractive"
                },
                {
                    "answer": "We performed the annotation with freely available tools for the Portuguese language.",
                    "type": "extractive"
                }
            ],
            "q_uid": "7793805982354947ea9fc742411bec314a6998f6",
            "evidence": [
                {
                    "raw_evidence": [
                        "We annotated the dataset at two levels: Part of Speech (POS) and syntax. We performed the annotation with freely available tools for the Portuguese language. For POS we added a simple POS, that is, only type of word, and a fine-grained POS, which is the type of word plus its morphological features. We used the LX Parser BIBREF14 , for the simple POS and the Portuguese morphological module of Freeling BIBREF15 , for detailed POS. Concerning syntactic annotations, we included constituency and dependency annotations. For constituency parsing, we used the LX Parser, and for dependency, the DepPattern toolkit BIBREF16 ."
                    ],
                    "highlighted_evidence": [
                        " We performed the annotation with freely available tools for the Portuguese language."
                    ]
                },
                {
                    "raw_evidence": [
                        "We annotated the dataset at two levels: Part of Speech (POS) and syntax. We performed the annotation with freely available tools for the Portuguese language. For POS we added a simple POS, that is, only type of word, and a fine-grained POS, which is the type of word plus its morphological features. We used the LX Parser BIBREF14 , for the simple POS and the Portuguese morphological module of Freeling BIBREF15 , for detailed POS. Concerning syntactic annotations, we included constituency and dependency annotations. For constituency parsing, we used the LX Parser, and for dependency, the DepPattern toolkit BIBREF16 ."
                    ],
                    "highlighted_evidence": [
                        "We annotated the dataset at two levels: Part of Speech (POS) and syntax. We performed the annotation with freely available tools for the Portuguese language. For POS we added a simple POS, that is, only type of word, and a fine-grained POS, which is the type of word plus its morphological features. We used the LX Parser BIBREF14 , for the simple POS and the Portuguese morphological module of Freeling BIBREF15 , for detailed POS. Concerning syntactic annotations, we included constituency and dependency annotations. For constituency parsing, we used the LX Parser, and for dependency, the DepPattern toolkit BIBREF16 ."
                    ]
                }
            ]
        }
    ],
    "1910.00194": [
        {
            "question": "Which language(s) are found in the WSD datasets?",
            "answers": [
                {
                    "answer": " WSD is predominantly evaluated on English, we are also interested in evaluating our approach on Chinese",
                    "type": "extractive"
                }
            ],
            "q_uid": "496e81769a8d9992dae187ed60639ff2eec531f3",
            "evidence": [
                {
                    "raw_evidence": [
                        "While WSD is predominantly evaluated on English, we are also interested in evaluating our approach on Chinese, to evaluate the effectiveness of our approach in a different language. We use OntoNotes Release 5.0, which contains a number of annotations including word senses for Chinese. We follow the data setup of BIBREF26 and conduct an evaluation on four genres, i.e., broadcast conversation (BC), broadcast news (BN), magazine (MZ), and newswire (NW), as well as the concatenation of all genres. While the training and development datasets are divided into genres, we train on the concatenation of all genres and test on each individual genre."
                    ],
                    "highlighted_evidence": [
                        "While WSD is predominantly evaluated on English, we are also interested in evaluating our approach on Chinese, to evaluate the effectiveness of our approach in a different language."
                    ]
                }
            ]
        },
        {
            "question": "What datasets are used for testing?",
            "answers": [
                {
                    "answer": "Senseval-2 (SE2), Senseval-3 (SE3), SemEval 2013 task 12 (SE13), and SemEval 2015 task 13 (SE15), OntoNotes Release 5.0",
                    "type": "extractive"
                }
            ],
            "q_uid": "f103789b85b00ec973076652c639bd31c605381e",
            "evidence": [
                {
                    "raw_evidence": [
                        "For English all-words WSD, we train our WSD model on SemCor BIBREF24, and test it on Senseval-2 (SE2), Senseval-3 (SE3), SemEval 2013 task 12 (SE13), and SemEval 2015 task 13 (SE15). This common benchmark, which has been annotated with WordNet-3.0 senses BIBREF25, has recently been adopted in English all-words WSD. Following BIBREF9, we choose SemEval 2007 Task 17 (SE07) as our development data to pick the best model parameters after a number of neural network updates, for models that require back-propagation training.",
                        "While WSD is predominantly evaluated on English, we are also interested in evaluating our approach on Chinese, to evaluate the effectiveness of our approach in a different language. We use OntoNotes Release 5.0, which contains a number of annotations including word senses for Chinese. We follow the data setup of BIBREF26 and conduct an evaluation on four genres, i.e., broadcast conversation (BC), broadcast news (BN), magazine (MZ), and newswire (NW), as well as the concatenation of all genres. While the training and development datasets are divided into genres, we train on the concatenation of all genres and test on each individual genre."
                    ],
                    "highlighted_evidence": [
                        "For English all-words WSD, we train our WSD model on SemCor BIBREF24, and test it on Senseval-2 (SE2), Senseval-3 (SE3), SemEval 2013 task 12 (SE13), and SemEval 2015 task 13 (SE15).",
                        "We use OntoNotes Release 5.0, which contains a number of annotations including word senses for Chinese."
                    ]
                }
            ]
        }
    ],
    "1806.03191": [
        {
            "question": "Which distributional methods did they consider?",
            "answers": [
                {
                    "answer": "WeedsPrec BIBREF8, invCL BIBREF11, SLQS model, cosine similarity",
                    "type": "extractive"
                }
            ],
            "q_uid": "cdf7e60150a166d41baed9dad539e3b93b544624",
            "evidence": [
                {
                    "raw_evidence": [
                        "Most unsupervised distributional approaches for hypernymy detection are based on variants of the Distributional Inclusion Hypothesis BIBREF8 , BIBREF9 , BIBREF10 , BIBREF11 , BIBREF4 . Here, we compare to two methods with strong empirical results. As with most DIH measures, they are only defined for large, sparse, positively-valued distributional spaces. First, we consider WeedsPrec BIBREF8 which captures the features of INLINEFORM0 which are included in the set of a broader term's features, INLINEFORM1 : DISPLAYFORM0",
                        "Second, we consider invCL BIBREF11 which introduces a notion of distributional exclusion by also measuring the degree to which the broader term contains contexts not used by the narrower term. In particular, let INLINEFORM0",
                        "Although most unsupervised distributional approaches are based on the DIH, we also consider the distributional SLQS model based on on an alternative informativeness hypothesis BIBREF10 , BIBREF4 . Intuitively, the SLQS model presupposes that general words appear mostly in uninformative contexts, as measured by entropy. Specifically, SLQS depends on the median entropy of a term's top INLINEFORM0 contexts, defined as INLINEFORM1",
                        "For completeness, we also include cosine similarity as a baseline in our evaluation."
                    ],
                    "highlighted_evidence": [
                        "First, we consider WeedsPrec BIBREF8 which captures the features of INLINEFORM0 which are included in the set of a broader term's features, INLINEFORM1 : DISPLAYFORM0\n\nSecond, we consider invCL BIBREF11 which introduces a notion of distributional exclusion by also measuring the degree to which the broader term contains contexts not used by the narrower term.",
                        "Although most unsupervised distributional approaches are based on the DIH, we also consider the distributional SLQS model based on on an alternative informativeness hypothesis BIBREF10 , BIBREF4 .",
                        "For completeness, we also include cosine similarity as a baseline in our evaluation."
                    ]
                }
            ]
        },
        {
            "question": "Which benchmark datasets are used?",
            "answers": [
                {
                    "answer": "noun-noun subset of bless, leds BIBREF13, bless, wbless, bibless, hyperlex BIBREF20",
                    "type": "extractive"
                }
            ],
            "q_uid": "c06b5623c35b6fa7938340fa340269dc81d061e1",
            "evidence": [
                {
                    "raw_evidence": [
                        "Detection: In hypernymy detection, the task is to classify whether pairs of words are in a hypernymy relation. For this task, we evaluate all models on five benchmark datasets: First, we employ the noun-noun subset of bless, which contains hypernymy annotations for 200 concrete, mostly unambiguous nouns. Negative pairs contain a mixture of co-hyponymy, meronymy, and random pairs. This version contains 14,542 total pairs with 1,337 positive examples. Second, we evaluate on leds BIBREF13 , which consists of 2,770 noun pairs balanced between positive hypernymy examples, and randomly shuffled negative pairs. We also consider eval BIBREF14 , containing 7,378 pairs in a mixture of hypernymy, synonymy, antonymy, meronymy, and adjectival relations. eval is notable for its absence of random pairs. The largest dataset is shwartz BIBREF2 , which was collected from a mixture of WordNet, DBPedia, and other resources. We limit ourselves to a 52,578 pair subset excluding multiword expressions. Finally, we evaluate on wbless BIBREF15 , a 1,668 pair subset of bless, with negative pairs being selected from co-hyponymy, random, and hyponymy relations. Previous work has used different metrics for evaluating on BLESS BIBREF11 , BIBREF5 , BIBREF6 . We chose to evaluate the global ranking using Average Precision. This allowed us to use the same metric on all detection benchmarks, and is consistent with evaluations in BIBREF4 .",
                        "Direction: In direction prediction, the task is to identify which term is broader in a given pair of words. For this task, we evaluate all models on three datasets described by BIBREF16 : On bless, the task is to predict the direction for all 1337 positive pairs in the dataset. Pairs are only counted correct if the hypernymy direction scores higher than the reverse direction, i.e. INLINEFORM0 . We reserve 10% of the data for validation, and test on the remaining 90%. On wbless, we follow prior work BIBREF17 , BIBREF18 and perform 1000 random iterations in which 2% of the data is used as a validation set to learn a classification threshold, and test on the remainder of the data. We report average accuracy across all iterations. Finally, we evaluate on bibless BIBREF16 , a variant of wbless with hypernymy and hyponymy pairs explicitly annotated for their direction. Since this task requires three-way classification (hypernymy, hyponymy, and other), we perform two-stage classification. First, a threshold is tuned using 2% of the data, identifying whether a pair exhibits hypernymy in either direction. Second, the relative comparison of scores determines which direction is predicted. As with wbless, we report the average accuracy over 1000 iterations.",
                        "Graded Entailment: In graded entailment, the task is to quantify the degree to which a hypernymy relation holds. For this task, we follow prior work BIBREF19 , BIBREF18 and use the noun part of hyperlex BIBREF20 , consisting of 2,163 noun pairs which are annotated to what degree INLINEFORM0 is-a INLINEFORM1 holds on a scale of INLINEFORM2 . For all models, we report Spearman's rank correlation INLINEFORM3 . We handle out-of-vocabulary (OOV) words by assigning the median of the scores (computed across the training set) to pairs with OOV words."
                    ],
                    "highlighted_evidence": [
                        "Detection: In hypernymy detection, the task is to classify whether pairs of words are in a hypernymy relation. For this task, we evaluate all models on five benchmark datasets: First, we employ the noun-noun subset of bless, which contains hypernymy annotations for 200 concrete, mostly unambiguous nouns.",
                        "Second, we evaluate on leds BIBREF13 , which consists of 2,770 noun pairs balanced between positive hypernymy examples, and randomly shuffled negative pairs.",
                        "Direction: In direction prediction, the task is to identify which term is broader in a given pair of words. For this task, we evaluate all models on three datasets described by BIBREF16 : On bless, the task is to predict the direction for all 1337 positive pairs in the dataset. Pairs are only counted correct if the hypernymy direction scores higher than the reverse direction, i.e. INLINEFORM0 . We reserve 10% of the data for validation, and test on the remaining 90%. On wbless, we follow prior work BIBREF17 , BIBREF18 and perform 1000 random iterations in which 2% of the data is used as a validation set to learn a classification threshold, and test on the remainder of the data. We report average accuracy across all iterations. Finally, we evaluate on bibless BIBREF16 , a variant of wbless with hypernymy and hyponymy pairs explicitly annotated for their direction.",
                        "Graded Entailment: In graded entailment, the task is to quantify the degree to which a hypernymy relation holds. For this task, we follow prior work BIBREF19 , BIBREF18 and use the noun part of hyperlex BIBREF20 , consisting of 2,163 noun pairs which are annotated to what degree INLINEFORM0 is-a INLINEFORM1 holds on a scale of INLINEFORM2 ."
                    ]
                }
            ]
        },
        {
            "question": "What hypernymy tasks do they study?",
            "answers": [
                {
                    "answer": "Detection, Direction, Graded Entailment",
                    "type": "extractive"
                }
            ],
            "q_uid": "d325a3c21660dbc481b4e839ff1a2d37dcc7ca46",
            "evidence": [
                {
                    "raw_evidence": [
                        "Detection: In hypernymy detection, the task is to classify whether pairs of words are in a hypernymy relation. For this task, we evaluate all models on five benchmark datasets: First, we employ the noun-noun subset of bless, which contains hypernymy annotations for 200 concrete, mostly unambiguous nouns. Negative pairs contain a mixture of co-hyponymy, meronymy, and random pairs. This version contains 14,542 total pairs with 1,337 positive examples. Second, we evaluate on leds BIBREF13 , which consists of 2,770 noun pairs balanced between positive hypernymy examples, and randomly shuffled negative pairs. We also consider eval BIBREF14 , containing 7,378 pairs in a mixture of hypernymy, synonymy, antonymy, meronymy, and adjectival relations. eval is notable for its absence of random pairs. The largest dataset is shwartz BIBREF2 , which was collected from a mixture of WordNet, DBPedia, and other resources. We limit ourselves to a 52,578 pair subset excluding multiword expressions. Finally, we evaluate on wbless BIBREF15 , a 1,668 pair subset of bless, with negative pairs being selected from co-hyponymy, random, and hyponymy relations. Previous work has used different metrics for evaluating on BLESS BIBREF11 , BIBREF5 , BIBREF6 . We chose to evaluate the global ranking using Average Precision. This allowed us to use the same metric on all detection benchmarks, and is consistent with evaluations in BIBREF4 .",
                        "Direction: In direction prediction, the task is to identify which term is broader in a given pair of words. For this task, we evaluate all models on three datasets described by BIBREF16 : On bless, the task is to predict the direction for all 1337 positive pairs in the dataset. Pairs are only counted correct if the hypernymy direction scores higher than the reverse direction, i.e. INLINEFORM0 . We reserve 10% of the data for validation, and test on the remaining 90%. On wbless, we follow prior work BIBREF17 , BIBREF18 and perform 1000 random iterations in which 2% of the data is used as a validation set to learn a classification threshold, and test on the remainder of the data. We report average accuracy across all iterations. Finally, we evaluate on bibless BIBREF16 , a variant of wbless with hypernymy and hyponymy pairs explicitly annotated for their direction. Since this task requires three-way classification (hypernymy, hyponymy, and other), we perform two-stage classification. First, a threshold is tuned using 2% of the data, identifying whether a pair exhibits hypernymy in either direction. Second, the relative comparison of scores determines which direction is predicted. As with wbless, we report the average accuracy over 1000 iterations.",
                        "Graded Entailment: In graded entailment, the task is to quantify the degree to which a hypernymy relation holds. For this task, we follow prior work BIBREF19 , BIBREF18 and use the noun part of hyperlex BIBREF20 , consisting of 2,163 noun pairs which are annotated to what degree INLINEFORM0 is-a INLINEFORM1 holds on a scale of INLINEFORM2 . For all models, we report Spearman's rank correlation INLINEFORM3 . We handle out-of-vocabulary (OOV) words by assigning the median of the scores (computed across the training set) to pairs with OOV words."
                    ],
                    "highlighted_evidence": [
                        "Detection: In hypernymy detection, the task is to classify whether pairs of words are in a hypernymy relation.",
                        "Direction: In direction prediction, the task is to identify which term is broader in a given pair of words.",
                        "Graded Entailment: In graded entailment, the task is to quantify the degree to which a hypernymy relation holds."
                    ]
                }
            ]
        }
    ],
    "2004.02083": [
        {
            "question": "How does morphological analysis differ from morphological inflection?",
            "answers": [
                {
                    "answer": "Morphological analysis is the task of creating a morphosyntactic description for a given word,  inflectional realization is framed as a mapping from the pairing of a lemma with a set of morphological tags to the corresponding word form",
                    "type": "extractive"
                }
            ],
            "q_uid": "99760276cfd699e55b827ceeb653b31b043b9ceb",
            "evidence": [
                {
                    "raw_evidence": [
                        "Inflectional realization defines the inflected forms of a lexeme/lemma. As a computational task, often referred to as simply \u201cmorphological inflection,\" inflectional realization is framed as a mapping from the pairing of a lemma with a set of morphological tags to the corresponding word form. For example, the inflectional realization of SJQ Chatino verb forms entails a mapping of the pairing of the lemma lyu1 `fall' with the tag-set 1;SG;PROG to the word form nlyon32.",
                        "Morphological analysis is the task of creating a morphosyntactic description for a given word. It can be framed in a context-agnostic manner (as in our case) or within a given context, as for instance for the SIGMORPHON 2019 second shared task BIBREF11. We trained an encoder-decoder model that receives the form as character-level input, encodes it with a BiLSTM encoder, and then an attention enhanced decoder BIBREF14 outputs the corresponding sequence of morphological tags, implemented in DyNet. The baseline results are shown in Table . The exact-match accuracy of 67% is lower than the average accuracy that context-aware systems can achieve, and it highlights the challenge that the complexity of the tonal system of SJQ Chatino can pose."
                    ],
                    "highlighted_evidence": [
                        "Inflectional realization defines the inflected forms of a lexeme/lemma. As a computational task, often referred to as simply \u201cmorphological inflection,\" inflectional realization is framed as a mapping from the pairing of a lemma with a set of morphological tags to the corresponding word form. For example, the inflectional realization of SJQ Chatino verb forms entails a mapping of the pairing of the lemma lyu1 `fall' with the tag-set 1;SG;PROG to the word form nlyon32.",
                        "Morphological analysis is the task of creating a morphosyntactic description for a given word. It can be framed in a context-agnostic manner (as in our case) or within a given context, as for instance for the SIGMORPHON 2019 second shared task BIBREF11."
                    ]
                }
            ]
        },
        {
            "question": "What are the architectures used for the three tasks?",
            "answers": [
                {
                    "answer": "DyNet",
                    "type": "extractive"
                }
            ],
            "q_uid": "79cfd1b82c72d18e2279792c66a042c0e9dfa6b7",
            "evidence": [
                {
                    "raw_evidence": [
                        "Morphological inflection has been thoroughly studied in monolingual high resource settings, especially through the recent SIGMORPHON challenges BIBREF8, BIBREF9, BIBREF10, with the latest iteration focusing more on low-resource settings, utilizing cross-lingual transfer BIBREF11. We use the guidelines of the state-of-the-art approach of BIBREF12 that achieved the highest inflection accuracy in the latest SIGMORPHON 2019 morphological inflection shared task. Our models are implemented in DyNet BIBREF13.",
                        "Morphological analysis is the task of creating a morphosyntactic description for a given word. It can be framed in a context-agnostic manner (as in our case) or within a given context, as for instance for the SIGMORPHON 2019 second shared task BIBREF11. We trained an encoder-decoder model that receives the form as character-level input, encodes it with a BiLSTM encoder, and then an attention enhanced decoder BIBREF14 outputs the corresponding sequence of morphological tags, implemented in DyNet. The baseline results are shown in Table . The exact-match accuracy of 67% is lower than the average accuracy that context-aware systems can achieve, and it highlights the challenge that the complexity of the tonal system of SJQ Chatino can pose.",
                        "Lemmatization is the task of retrieving the underlying lemma from which an inflected form was derived. Although in some languages the lemma is distinct from all forms, in SJQ Chatino the lemma is defined as the completive third-person singular form. As a computational task, lemmatization entails producing the lemma given an inflected form (and possibly, given a set of morphological tags describing the input form). Popular approaches tackle it as a character-level edit sequence generation task BIBREF15, or as a character-level sequence-to-sequence task BIBREF16. For our baseline lemmatization systems we follow the latter approach. We trained a character level encoder-decoder model, similar to the above-mentioned inflection system, implemented in DyNet."
                    ],
                    "highlighted_evidence": [
                        "We use the guidelines of the state-of-the-art approach of BIBREF12 that achieved the highest inflection accuracy in the latest SIGMORPHON 2019 morphological inflection shared task. Our models are implemented in DyNet BIBREF13.",
                        "We trained an encoder-decoder model that receives the form as character-level input, encodes it with a BiLSTM encoder, and then an attention enhanced decoder BIBREF14 outputs the corresponding sequence of morphological tags, implemented in DyNet.",
                        "We trained a character level encoder-decoder model, similar to the above-mentioned inflection system, implemented in DyNet."
                    ]
                }
            ]
        },
        {
            "question": "Which language family does Chatino belong to?",
            "answers": [
                {
                    "answer": "the Otomanguean language family",
                    "type": "extractive"
                }
            ],
            "q_uid": "9e1bf306658ef2972159643fdaf149c569db524b",
            "evidence": [
                {
                    "raw_evidence": [
                        "Chatino is a group of languages spoken in Oaxaca, Mexico. Together with the Zapotec language group, the Chatino languages form the Zapotecan branch of the Otomanguean language family. There are three main Chatino languages: Zenzontepec Chatino (ZEN, ISO 639-2 code czn), Tataltepec Chatino (TAT, cta), and Eastern Chatino (ISO 639-2 ctp, cya, ctz, and cly) (E.Cruz 2011 and Campbell 2011). San Juan Quiahije Chatino (SJQ), the language of the focus of this study, belongs to Eastern Chatino, and is used by about 3000 speakers."
                    ],
                    "highlighted_evidence": [
                        "Chatino is a group of languages spoken in Oaxaca, Mexico. Together with the Zapotec language group, the Chatino languages form the Zapotecan branch of the Otomanguean language family. "
                    ]
                }
            ]
        },
        {
            "question": "What system is used as baseline?",
            "answers": [
                {
                    "answer": "DyNet",
                    "type": "extractive"
                }
            ],
            "q_uid": "25b24ab1248f14a621686a57555189acc1afd49c",
            "evidence": [
                {
                    "raw_evidence": [
                        "Morphological inflection has been thoroughly studied in monolingual high resource settings, especially through the recent SIGMORPHON challenges BIBREF8, BIBREF9, BIBREF10, with the latest iteration focusing more on low-resource settings, utilizing cross-lingual transfer BIBREF11. We use the guidelines of the state-of-the-art approach of BIBREF12 that achieved the highest inflection accuracy in the latest SIGMORPHON 2019 morphological inflection shared task. Our models are implemented in DyNet BIBREF13.",
                        "Morphological analysis is the task of creating a morphosyntactic description for a given word. It can be framed in a context-agnostic manner (as in our case) or within a given context, as for instance for the SIGMORPHON 2019 second shared task BIBREF11. We trained an encoder-decoder model that receives the form as character-level input, encodes it with a BiLSTM encoder, and then an attention enhanced decoder BIBREF14 outputs the corresponding sequence of morphological tags, implemented in DyNet. The baseline results are shown in Table . The exact-match accuracy of 67% is lower than the average accuracy that context-aware systems can achieve, and it highlights the challenge that the complexity of the tonal system of SJQ Chatino can pose.",
                        "Lemmatization is the task of retrieving the underlying lemma from which an inflected form was derived. Although in some languages the lemma is distinct from all forms, in SJQ Chatino the lemma is defined as the completive third-person singular form. As a computational task, lemmatization entails producing the lemma given an inflected form (and possibly, given a set of morphological tags describing the input form). Popular approaches tackle it as a character-level edit sequence generation task BIBREF15, or as a character-level sequence-to-sequence task BIBREF16. For our baseline lemmatization systems we follow the latter approach. We trained a character level encoder-decoder model, similar to the above-mentioned inflection system, implemented in DyNet."
                    ],
                    "highlighted_evidence": [
                        "We use the guidelines of the state-of-the-art approach of BIBREF12 that achieved the highest inflection accuracy in the latest SIGMORPHON 2019 morphological inflection shared task. Our models are implemented in DyNet BIBREF13.",
                        "We trained an encoder-decoder model that receives the form as character-level input, encodes it with a BiLSTM encoder, and then an attention enhanced decoder BIBREF14 outputs the corresponding sequence of morphological tags, implemented in DyNet.",
                        "We trained a character level encoder-decoder model, similar to the above-mentioned inflection system, implemented in DyNet."
                    ]
                }
            ]
        },
        {
            "question": "How was annotation done?",
            "answers": [
                {
                    "answer": " hand-curated collection of complete inflection tables for 198 lemmata",
                    "type": "extractive"
                }
            ],
            "q_uid": "8486e06c03f82ebd48c7cfbaffaa76e8b899eea5",
            "evidence": [
                {
                    "raw_evidence": [
                        "We provide a hand-curated collection of complete inflection tables for 198 lemmata. The morphological tags follow the guidelines of the UniMorph schema BIBREF6, BIBREF7, in order to allow for the potential of cross-lingual transfer learning, and they are tagged with respect to:",
                        "Person: first (1), second (2), and third (3)",
                        "Number: singular (SG) ad plural (PL)",
                        "Inclusivity (only applicable to first person plural verbs: inclusive (INCL) and exclusive (EXCL)",
                        "Aspect/mood: completive (CPL), progressive (PROG), potential (POT), and habitual (HAB)."
                    ],
                    "highlighted_evidence": [
                        "We provide a hand-curated collection of complete inflection tables for 198 lemmata. The morphological tags follow the guidelines of the UniMorph schema BIBREF6, BIBREF7, in order to allow for the potential of cross-lingual transfer learning, and they are tagged with respect to:\n\nPerson: first (1), second (2), and third (3)\n\nNumber: singular (SG) ad plural (PL)\n\nInclusivity (only applicable to first person plural verbs: inclusive (INCL) and exclusive (EXCL)\n\nAspect/mood: completive (CPL), progressive (PROG), potential (POT), and habitual (HAB)."
                    ]
                }
            ]
        }
    ],
    "1910.10869": [
        {
            "question": "What they use as a metric of finding hot spots in meeting?",
            "answers": [
                {
                    "answer": "unweighted average recall (UAR) metric",
                    "type": "extractive"
                }
            ],
            "q_uid": "bdc1f37c8b5e96e3c29cc02dae4ce80087d83284",
            "evidence": [
                {
                    "raw_evidence": [
                        "In spite of the windowing approach, the class distribution is still skewed, and an accuracy metric would reflect the particular class distribution in our data set. Therefore, we adopt the unweighted average recall (UAR) metric commonly used in emotion classification research. UAR is a reweighted accuracy where the samples of both classes are weighted equally in aggregate. UAR thus simulates a uniform class distribution. To match the objective, our classifiers are trained on appropriately weighted training data. Note that chance performance for UAR is by definition 50%, making results more comparable across different data sets."
                    ],
                    "highlighted_evidence": [
                        "In spite of the windowing approach, the class distribution is still skewed, and an accuracy metric would reflect the particular class distribution in our data set. Therefore, we adopt the unweighted average recall (UAR) metric commonly used in emotion classification research. UAR is a reweighted accuracy where the samples of both classes are weighted equally in aggregate. UAR thus simulates a uniform class distribution. To match the objective, our classifiers are trained on appropriately weighted training data. Note that chance performance for UAR is by definition 50%, making results more comparable across different data sets."
                    ]
                }
            ]
        },
        {
            "question": "How big is ICSI meeting corpus?",
            "answers": [
                {
                    "answer": " 75 meetings and about 70 hours of real-time audio duration",
                    "type": "extractive"
                }
            ],
            "q_uid": "fdd9dea06550a2fd0df7a1e6a5109facf3601d76",
            "evidence": [
                {
                    "raw_evidence": [
                        "The ICSI Meeting Corpus BIBREF11 is a collection of meeting recordings that has been thoroughly annotated, including annotations for involvement hot spots BIBREF12, linguistic utterance units, and word time boundaries based on forced alignment. The dataset is comprised of 75 meetings and about 70 hours of real-time audio duration, with 6 speakers per meeting on average. Most of the participants are well-acquainted and friendly with each other. Hot spots were originally annotated with 8 levels and degrees, ranging from `not hot' to `luke warm' to `hot +'. Every utterance was labeled with one of these discrete labels by a single annotator. Hightened involvement is rare, being marked on only 1% of utterances."
                    ],
                    "highlighted_evidence": [
                        "The ICSI Meeting Corpus BIBREF11 is a collection of meeting recordings that has been thoroughly annotated, including annotations for involvement hot spots BIBREF12, linguistic utterance units, and word time boundaries based on forced alignment. The dataset is comprised of 75 meetings and about 70 hours of real-time audio duration, with 6 speakers per meeting on average. Most of the participants are well-acquainted and friendly with each other. Hot spots were originally annotated with 8 levels and degrees, ranging from `not hot' to `luke warm' to `hot +'. Every utterance was labeled with one of these discrete labels by a single annotator. Hightened involvement is rare, being marked on only 1% of utterances."
                    ]
                }
            ]
        },
        {
            "question": "What annotations are available in ICSI meeting corpus?",
            "answers": [
                {
                    "answer": "8 levels and degrees, ranging from `not hot' to `luke warm' to `hot +'. Every utterance was labeled with one of these discrete labels by a single annotator",
                    "type": "extractive"
                }
            ],
            "q_uid": "3786164eaf3965c11c9969c4463b8c3223627067",
            "evidence": [
                {
                    "raw_evidence": [
                        "The ICSI Meeting Corpus BIBREF11 is a collection of meeting recordings that has been thoroughly annotated, including annotations for involvement hot spots BIBREF12, linguistic utterance units, and word time boundaries based on forced alignment. The dataset is comprised of 75 meetings and about 70 hours of real-time audio duration, with 6 speakers per meeting on average. Most of the participants are well-acquainted and friendly with each other. Hot spots were originally annotated with 8 levels and degrees, ranging from `not hot' to `luke warm' to `hot +'. Every utterance was labeled with one of these discrete labels by a single annotator. Hightened involvement is rare, being marked on only 1% of utterances."
                    ],
                    "highlighted_evidence": [
                        "Hot spots were originally annotated with 8 levels and degrees, ranging from `not hot' to `luke warm' to `hot +'. Every utterance was labeled with one of these discrete labels by a single annotator. Hightened involvement is rare, being marked on only 1% of utterances."
                    ]
                }
            ]
        }
    ],
    "1912.13072": [
        {
            "question": "Did they experiment on all the tasks?",
            "answers": [
                {
                    "answer": "Yes",
                    "type": "boolean"
                }
            ],
            "q_uid": "2419b38624201d678c530eba877c0c016cccd49f",
            "evidence": [
                {
                    "raw_evidence": [
                        "Implementation & Models Parameters. For all our tasks, we use the BERT-Base Multilingual Cased model released by the authors . The model is trained on 104 languages (including Arabic) with 12 layer, 768 hidden units each, 12 attention heads, and has 110M parameters in entire model. The model has 119,547 shared WordPieces vocabulary, and was pre-trained on the entire Wikipedia for each language. For fine-tuning, we use a maximum sequence size of 50 tokens and a batch size of 32. We set the learning rate to $2e-5$ and train for 15 epochs and choose the best model based on performance on a development set. We use the same hyper-parameters in all of our BERT models. We fine-tune BERT on each respective labeled dataset for each task. For BERT input, we apply WordPiece tokenization, setting the maximal sequence length to 50 words/WordPieces. For all tasks, we use a TensorFlow implementation. An exception is the sentiment analysis task, where we used a PyTorch implementation with the same hyper-parameters but with a learning rate $2e-6$.",
                        "We presented AraNet, a deep learning toolkit for a host of Arabic social media processing. AraNet predicts age, dialect, gender, emotion, irony, and sentiment from social media posts. It delivers state-of-the-art and competitive performance on these tasks and has the advantage of using a unified, simple framework based on the recently-developed BERT model. AraNet has the potential to alleviate issues related to comparing across different Arabic social media NLP tasks, by providing one way to test new models against AraNet predictions. Our toolkit can be used to make important discoveries on the wide region of the Arab world, and can enhance our understating of Arab online communication. AraNet will be publicly available upon acceptance."
                    ],
                    "highlighted_evidence": [
                        "For all tasks, we use a TensorFlow implementation.",
                        "AraNet predicts age, dialect, gender, emotion, irony, and sentiment from social media posts. It delivers state-of-the-art and competitive performance on these tasks and has the advantage of using a unified, simple framework based on the recently-developed BERT model. "
                    ]
                }
            ]
        },
        {
            "question": "What models did they compare to?",
            "answers": [
                {
                    "answer": " we do not explicitly compare to previous research since most existing works either exploit smaller data (and so it will not be a fair comparison), use methods pre-dating BERT (and so will likely be outperformed by our models)",
                    "type": "extractive"
                }
            ],
            "q_uid": "b99d100d17e2a121c3c8ff789971ce66d1d40a4d",
            "evidence": [
                {
                    "raw_evidence": [
                        "Although we create new models for tasks such as sentiment analysis and gender detection as part of AraNet, our focus is more on putting together the toolkit itself and providing strong baselines that can be compared to. Hence, although we provide some baseline models for some of the tasks, we do not explicitly compare to previous research since most existing works either exploit smaller data (and so it will not be a fair comparison), use methods pre-dating BERT (and so will likely be outperformed by our models) . For many of the tasks we model, there have not been standard benchmarks for comparisons across models. This makes it difficult to measure progress and identify areas worthy of allocating efforts and budgets. As such, by publishing our toolkit models, we believe model-based comparisons will be one way to relieve this bottleneck. For these reasons, we also package models from our recent works on dialect BIBREF12 and irony BIBREF14 as part of AraNet ."
                    ],
                    "highlighted_evidence": [
                        "Although we create new models for tasks such as sentiment analysis and gender detection as part of AraNet, our focus is more on putting together the toolkit itself and providing strong baselines that can be compared to. Hence, although we provide some baseline models for some of the tasks, we do not explicitly compare to previous research since most existing works either exploit smaller data (and so it will not be a fair comparison), use methods pre-dating BERT (and so will likely be outperformed by our models) . For many of the tasks we model, there have not been standard benchmarks for comparisons across models. This makes it difficult to measure progress and identify areas worthy of allocating efforts and budgets."
                    ]
                }
            ]
        },
        {
            "question": "What datasets are used in training?",
            "answers": [
                {
                    "answer": "Arap-Tweet BIBREF19 , an in-house Twitter dataset for gender, the MADAR shared task 2 BIBREF20, the LAMA-DINA dataset from BIBREF22, LAMA-DIST, Arabic tweets released by IDAT@FIRE2019 shared-task BIBREF24, BIBREF25, BIBREF26, BIBREF27, BIBREF1, BIBREF28, BIBREF29, BIBREF30, BIBREF31, BIBREF32, BIBREF33, BIBREF34",
                    "type": "extractive"
                },
                {
                    "answer": " Arap-Tweet , UBC Twitter Gender Dataset, MADAR , LAMA-DINA , IDAT@FIRE2019, 15 datasets related to sentiment analysis of Arabic, including MSA and dialects",
                    "type": "extractive"
                }
            ],
            "q_uid": "578d0b23cb983b445b1a256a34f969b34d332075",
            "evidence": [
                {
                    "raw_evidence": [
                        "Arab-Tweet. For modeling age and gender, we use Arap-Tweet BIBREF19 , which we will refer to as Arab-Tweet. Arab-tweet is a tweet dataset of 11 Arabic regions from 17 different countries. For each region, data from 100 Twitter users were crawled. Users needed to have posted at least 2,000 and were selected based on an initial list of seed words characteristic of each region. The seed list included words such as <\u0628\u0631\u0634\u0629> /barsha/ \u2018many\u2019 for Tunisian Arabic and <\u0648\u0627\u064a\u062f> /wayed/ \u2018many\u2019 for Gulf Arabic. BIBREF19 employed human annotators to verify that users do belong to each respective region. Annotators also assigned gender labels from the set male, female and age group labels from the set under-25, 25-to34, above-35 at the user-level, which in turn is assigned at tweet level. Tweets with less than 3 words and re-tweets were removed. Refer to BIBREF19 for details about how annotation was carried out. We provide a description of the data in Table TABREF10. Table TABREF10 also provides class breakdown across our splits.We note that BIBREF19 do not report classification models exploiting the data.",
                        "UBC Twitter Gender Dataset. We also develop an in-house Twitter dataset for gender. We manually labeled 1,989 users from each of the 21 Arab countries. The data had 1,246 \u201cmale\", 528 \u201cfemale\", and 215 unknown users. We remove the \u201cunknown\" category and balance the dataset to have 528 from each of the two `male\" and \u201cfemale\" categories. We ended with 69,509 tweets for `male\" and 67,511 tweets for \u201cfemale\". We split the users into 80% TRAIN set (110,750 tweets for 845 users), 10% DEV set (14,158 tweets for 106 users), and 10% TEST set (12,112 tweets for 105 users). We, then, model this dataset with BERT-Base, Multilingual Cased model and evaluate on development and test sets. Table TABREF15 shows that fine-tuned model obtains 62.42% acc on DEV and 60.54% acc on TEST.",
                        "The dialect identification model in AraNet is based on our winning system in the MADAR shared task 2 BIBREF20 as described in BIBREF12. The corpus is divided into train, dev and test, and the organizers masked test set labels. We lost some tweets from training data when we crawled using tweet ids, ultimately acquiring 2,036 (TRAIN-A), 281 (DEV) and 466 (TEST). We also make use of the task 1 corpus (95,000 sentences BIBREF21). More specifically, we concatenate the task 1 data to the training data of task 2, to create TRAIN-B. Again, note that TEST labels were only released to participants after the official task evaluation. Table TABREF17 shows statistics of the data. We used tweets from 21 Arab countries as distributed by task organizers, except that we lost some tweets when we crawled using tweet ids. We had 2,036 (TRAIN-A), 281 (DEV) and 466 (TEST). For our experiments, we also make use of the task 1 corpus (95,000 sentences BIBREF21). More specifically, we concatenate the task 1 data to the training data of task 2, to create TRAIN-B. Note that both DEV and TEST across our experiments are exclusively the data released in task 2, as described above. TEST labels were only released to participants after the official task evaluation. Table TABREF17 shows statistics of the data. More information about the data is in BIBREF21. We use TRAIN-A to perform supervised modeling with BERT and TRAIN-B for self training, under various conditions. We refer the reader to BIBREF12 for more information about our different experimental settings on dialect id. We acquire our best results with self-training, with a classification accuracy of 49.39% and F1 score at 35.44. This is the winning system model in the MADAR shared task and we showed in BIBREF12 that our tweet-level predictions can be ported to user-level prediction. On user-level detection, our models perform superbly, with 77.40% acc and 71.70% F1 score on unseen MADAR blind test data.",
                        "We make use of two datasets, the LAMA-DINA dataset from BIBREF22, a Twitter dataset with a combination of gold labels from BIBREF23 and distant supervision labels. The tweets are labeled with the Plutchik 8 primary emotions from the set: {anger, anticipation, disgust, fear, joy, sadness, surprise, trust}. The distant supervision approach depends on use of seed phrases with the Arabic first person pronoun \u0627\u0646\u0627> (Eng. \u201cI\") + a seed word expressing an emotion, e.g., \u0641\u0631\u062d\u0627\u0646> (Eng. \u201chappy\"). The manually labeled part of the data comprises tweets carrying the seed phrases verified by human annotators $9,064$ tweets for inclusion of the respective emotion. The rest of the dataset is only labeled using distant supervision (LAMA-DIST) ($182,605$ tweets) . For more information about the dataset, readers are referred to BIBREF22. The data distribution over the emotion classes is in Table TABREF20. We combine LAMA+DINA and LAMA-DIST training set and refer to this new training set as LAMA-D2 (189,903 tweets). We fine-tune BERT-Based, Multilingual Cased on the LAMA-D2 and evaluate the model with same DEV and TEST sets from LAMA+DINA. On DEV set, the fine-tuned BERT model obtains 61.43% on accuracy and 58.83 on $F_1$ score. On TEST set, we acquire 62.38% acc and 60.32% $F_1$ score.",
                        "We use the dataset for irony identification on Arabic tweets released by IDAT@FIRE2019 shared-task BIBREF24. The shared task dataset contains 5,030 tweets related to different political issues and events in the Middle East taking place between 2011 and 2018. Tweets are collected using pre-defined keywords (i.e., targeted political figures or events) and the positive class involves ironic hashtags such as #sokhria, #tahakoum, and #maskhara (Arabic variants for \u201cirony\"). Duplicates, retweets, and non-intelligible tweets are removed by organizers. Tweets involve both MSA as well as dialects at various degrees of granularity such as Egyptian, Gulf, and Levantine.",
                        "We collect 15 datasets related to sentiment analysis of Arabic, including MSA and dialects BIBREF25, BIBREF26, BIBREF27, BIBREF1, BIBREF28, BIBREF29, BIBREF30, BIBREF31, BIBREF32, BIBREF33, BIBREF34. Table TABREF28 shows all the corpora we use. These datasets involve different types of sentiment analysis tasks such as binary classification (i.e., negative or positive), 3-way classification (i.e., negative, neutral, or positive), and subjective language detection. To combine these datasets for binary sentiment classification, we normalize different types of label to binary labels in the set $\\lbrace `positive^{\\prime }, `negative^{\\prime }\\rbrace $ by following rules:"
                    ],
                    "highlighted_evidence": [
                        "Arab-Tweet. For modeling age and gender, we use Arap-Tweet BIBREF19 , which we will refer to as Arab-Tweet.",
                        "UBC Twitter Gender Dataset. We also develop an in-house Twitter dataset for gender. We manually labeled 1,989 users from each of the 21 Arab countries.",
                        "The dialect identification model in AraNet is based on our winning system in the MADAR shared task 2 BIBREF20 as described in BIBREF12. The corpus is divided into train, dev and test, and the organizers masked test set labels.",
                        "We make use of two datasets, the LAMA-DINA dataset from BIBREF22, a Twitter dataset with a combination of gold labels from BIBREF23 and distant supervision labels.",
                        "The rest of the dataset is only labeled using distant supervision (LAMA-DIST) ($182,605$ tweets) . For more information about the dataset, readers are referred to BIBREF22. ",
                        "We use the dataset for irony identification on Arabic tweets released by IDAT@FIRE2019 shared-task BIBREF24.",
                        "We collect 15 datasets related to sentiment analysis of Arabic, including MSA and dialects BIBREF25, BIBREF26, BIBREF27, BIBREF1, BIBREF28, BIBREF29, BIBREF30, BIBREF31, BIBREF32, BIBREF33, BIBREF34. Table TABREF28 shows all the corpora we use. "
                    ]
                },
                {
                    "raw_evidence": [
                        "Arab-Tweet. For modeling age and gender, we use Arap-Tweet BIBREF19 , which we will refer to as Arab-Tweet. Arab-tweet is a tweet dataset of 11 Arabic regions from 17 different countries. For each region, data from 100 Twitter users were crawled. Users needed to have posted at least 2,000 and were selected based on an initial list of seed words characteristic of each region. The seed list included words such as <\u0628\u0631\u0634\u0629> /barsha/ \u2018many\u2019 for Tunisian Arabic and <\u0648\u0627\u064a\u062f> /wayed/ \u2018many\u2019 for Gulf Arabic. BIBREF19 employed human annotators to verify that users do belong to each respective region. Annotators also assigned gender labels from the set male, female and age group labels from the set under-25, 25-to34, above-35 at the user-level, which in turn is assigned at tweet level. Tweets with less than 3 words and re-tweets were removed. Refer to BIBREF19 for details about how annotation was carried out. We provide a description of the data in Table TABREF10. Table TABREF10 also provides class breakdown across our splits.We note that BIBREF19 do not report classification models exploiting the data.",
                        "UBC Twitter Gender Dataset. We also develop an in-house Twitter dataset for gender. We manually labeled 1,989 users from each of the 21 Arab countries. The data had 1,246 \u201cmale\", 528 \u201cfemale\", and 215 unknown users. We remove the \u201cunknown\" category and balance the dataset to have 528 from each of the two `male\" and \u201cfemale\" categories. We ended with 69,509 tweets for `male\" and 67,511 tweets for \u201cfemale\". We split the users into 80% TRAIN set (110,750 tweets for 845 users), 10% DEV set (14,158 tweets for 106 users), and 10% TEST set (12,112 tweets for 105 users). We, then, model this dataset with BERT-Base, Multilingual Cased model and evaluate on development and test sets. Table TABREF15 shows that fine-tuned model obtains 62.42% acc on DEV and 60.54% acc on TEST.",
                        "The dialect identification model in AraNet is based on our winning system in the MADAR shared task 2 BIBREF20 as described in BIBREF12. The corpus is divided into train, dev and test, and the organizers masked test set labels. We lost some tweets from training data when we crawled using tweet ids, ultimately acquiring 2,036 (TRAIN-A), 281 (DEV) and 466 (TEST). We also make use of the task 1 corpus (95,000 sentences BIBREF21). More specifically, we concatenate the task 1 data to the training data of task 2, to create TRAIN-B. Again, note that TEST labels were only released to participants after the official task evaluation. Table TABREF17 shows statistics of the data. We used tweets from 21 Arab countries as distributed by task organizers, except that we lost some tweets when we crawled using tweet ids. We had 2,036 (TRAIN-A), 281 (DEV) and 466 (TEST). For our experiments, we also make use of the task 1 corpus (95,000 sentences BIBREF21). More specifically, we concatenate the task 1 data to the training data of task 2, to create TRAIN-B. Note that both DEV and TEST across our experiments are exclusively the data released in task 2, as described above. TEST labels were only released to participants after the official task evaluation. Table TABREF17 shows statistics of the data. More information about the data is in BIBREF21. We use TRAIN-A to perform supervised modeling with BERT and TRAIN-B for self training, under various conditions. We refer the reader to BIBREF12 for more information about our different experimental settings on dialect id. We acquire our best results with self-training, with a classification accuracy of 49.39% and F1 score at 35.44. This is the winning system model in the MADAR shared task and we showed in BIBREF12 that our tweet-level predictions can be ported to user-level prediction. On user-level detection, our models perform superbly, with 77.40% acc and 71.70% F1 score on unseen MADAR blind test data.",
                        "Data and Models ::: Emotion",
                        "We make use of two datasets, the LAMA-DINA dataset from BIBREF22, a Twitter dataset with a combination of gold labels from BIBREF23 and distant supervision labels. The tweets are labeled with the Plutchik 8 primary emotions from the set: {anger, anticipation, disgust, fear, joy, sadness, surprise, trust}. The distant supervision approach depends on use of seed phrases with the Arabic first person pronoun \u0627\u0646\u0627> (Eng. \u201cI\") + a seed word expressing an emotion, e.g., \u0641\u0631\u062d\u0627\u0646> (Eng. \u201chappy\"). The manually labeled part of the data comprises tweets carrying the seed phrases verified by human annotators $9,064$ tweets for inclusion of the respective emotion. The rest of the dataset is only labeled using distant supervision (LAMA-DIST) ($182,605$ tweets) . For more information about the dataset, readers are referred to BIBREF22. The data distribution over the emotion classes is in Table TABREF20. We combine LAMA+DINA and LAMA-DIST training set and refer to this new training set as LAMA-D2 (189,903 tweets). We fine-tune BERT-Based, Multilingual Cased on the LAMA-D2 and evaluate the model with same DEV and TEST sets from LAMA+DINA. On DEV set, the fine-tuned BERT model obtains 61.43% on accuracy and 58.83 on $F_1$ score. On TEST set, we acquire 62.38% acc and 60.32% $F_1$ score.",
                        "We use the dataset for irony identification on Arabic tweets released by IDAT@FIRE2019 shared-task BIBREF24. The shared task dataset contains 5,030 tweets related to different political issues and events in the Middle East taking place between 2011 and 2018. Tweets are collected using pre-defined keywords (i.e., targeted political figures or events) and the positive class involves ironic hashtags such as #sokhria, #tahakoum, and #maskhara (Arabic variants for \u201cirony\"). Duplicates, retweets, and non-intelligible tweets are removed by organizers. Tweets involve both MSA as well as dialects at various degrees of granularity such as Egyptian, Gulf, and Levantine.",
                        "We collect 15 datasets related to sentiment analysis of Arabic, including MSA and dialects BIBREF25, BIBREF26, BIBREF27, BIBREF1, BIBREF28, BIBREF29, BIBREF30, BIBREF31, BIBREF32, BIBREF33, BIBREF34. Table TABREF28 shows all the corpora we use. These datasets involve different types of sentiment analysis tasks such as binary classification (i.e., negative or positive), 3-way classification (i.e., negative, neutral, or positive), and subjective language detection. To combine these datasets for binary sentiment classification, we normalize different types of label to binary labels in the set $\\lbrace `positive^{\\prime }, `negative^{\\prime }\\rbrace $ by following rules:"
                    ],
                    "highlighted_evidence": [
                        "For modeling age and gender, we use Arap-Tweet BIBREF19 , which we will refer to as Arab-Tweet.",
                        "UBC Twitter Gender Dataset. We also develop an in-house Twitter dataset for gender. ",
                        "The dialect identification model in AraNet is based on our winning system in the MADAR shared task 2 BIBREF20 as described in BIBREF12. ",
                        "Emotion\nWe make use of two datasets, the LAMA-DINA dataset from BIBREF22, a Twitter dataset with a combination of gold labels from BIBREF23 and distant supervision labels. ",
                        "We use the dataset for irony identification on Arabic tweets released by IDAT@FIRE2019 shared-task BIBREF24.",
                        "We collect 15 datasets related to sentiment analysis of Arabic, including MSA and dialects BIBREF25, BIBREF26, BIBREF27, BIBREF1, BIBREF28, BIBREF29, BIBREF30, BIBREF31, BIBREF32, BIBREF33, BIBREF34."
                    ]
                }
            ]
        }
    ],
    "1911.06118": [
        {
            "question": "What are the qualitative experiments performed on benchmark datasets?",
            "answers": [
                {
                    "answer": "Spearman correlation values of GM_KL model evaluated on the benchmark word similarity datasets.\nEvaluation results of GM_KL model on the entailment datasets such as entailment pairs dataset created from WordNet, crowdsourced dataset of 79 semantic relations labelled as entailed or not and annotated distributionally similar nouns dataset.",
                    "type": "abstractive"
                },
                {
                    "answer": "Given a query word and component id, the set of nearest neighbours along with their respective component ids are listed",
                    "type": "extractive"
                }
            ],
            "q_uid": "26126068d72408555bcb52977cd669faf660bdf7",
            "evidence": [
                {
                    "raw_evidence": [
                        "Table TABREF18 shows the Spearman correlation values of GM$\\_$KL model evaluated on the benchmark word similarity datasets: SL BIBREF20, WS, WS-R, WS-S BIBREF21, MEN BIBREF22, MC BIBREF23, RG BIBREF24, YP BIBREF25, MTurk-287 and MTurk-771 BIBREF26, BIBREF27, and RW BIBREF28. The metric used for comparison is 'AvgCos'. It can be seen that for most of the datasets, GM$\\_$KL achieves significantly better correlation score than w2g and w2gm approaches. Other datasets such as MC and RW consist of only a single sense, and hence w2g model performs better and GM$\\_$KL achieves next better performance. The YP dataset have multiple senses but does not contain entailed data and hence could not make use of entailment benefits of GM$\\_$KL.",
                        "Table TABREF19 shows the evaluation results of GM$\\_$KL model on the entailment datasets such as entailment pairs dataset BIBREF29 created from WordNet with both positive and negative labels, a crowdsourced dataset BIBREF30 of 79 semantic relations labelled as entailed or not and annotated distributionally similar nouns dataset BIBREF31. The 'MaxCos' similarity metric is used for evaluation and the best precision and best F1-score is shown, by picking the optimal threshold. Overall, GM$\\_$KL performs better than both w2g and w2gm approaches."
                    ],
                    "highlighted_evidence": [
                        "Table TABREF18 shows the Spearman correlation values of GM$\\_$KL model evaluated on the benchmark word similarity datasets: SL BIBREF20, WS, WS-R, WS-S BIBREF21, MEN BIBREF22, MC BIBREF23, RG BIBREF24, YP BIBREF25, MTurk-287 and MTurk-771 BIBREF26, BIBREF27, and RW BIBREF28. ",
                        "Table TABREF19 shows the evaluation results of GM$\\_$KL model on the entailment datasets such as entailment pairs dataset BIBREF29 created from WordNet with both positive and negative labels, a crowdsourced dataset BIBREF30 of 79 semantic relations labelled as entailed or not and annotated distributionally similar nouns dataset BIBREF31"
                    ]
                },
                {
                    "raw_evidence": [
                        "Table TABREF9 shows the qualitative results of GM$\\_$KL. Given a query word and component id, the set of nearest neighbours along with their respective component ids are listed. For eg., the word `plane' in its 0th component captures the `geometry' sense and so are its neighbours, and its 1st component captures `vehicle' sense and so are its corresponding neighbours. Other words such as `rock' captures both the `metal' and `music' senses, `star' captures `celebrity' and `astronomical' senses, and `phone' captures `telephony' and `internet' senses."
                    ],
                    "highlighted_evidence": [
                        "Table TABREF9 shows the qualitative results of GM$\\_$KL. Given a query word and component id, the set of nearest neighbours along with their respective component ids are listed. For eg., the word `plane' in its 0th component captures the `geometry' sense and so are its neighbours, and its 1st component captures `vehicle' sense and so are its corresponding neighbours. Other words such as `rock' captures both the `metal' and `music' senses, `star' captures `celebrity' and `astronomical' senses, and `phone' captures `telephony' and `internet' senses."
                    ]
                }
            ]
        },
        {
            "question": "How does this approach compare to other WSD approaches employing word embeddings?",
            "answers": [
                {
                    "answer": "GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset.",
                    "type": "extractive"
                }
            ],
            "q_uid": "660284b0a21fe3801e64dc9e0e51da5400223fe3",
            "evidence": [
                {
                    "raw_evidence": [
                        "Table TABREF17 compares the performance of the approaches on the SCWS dataset. It is evident from Table TABREF17 that GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset."
                    ],
                    "highlighted_evidence": [
                        ". It is evident from Table TABREF17 that GM$\\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset."
                    ]
                }
            ]
        }
    ],
    "1808.10113": [
        {
            "question": "Did they compare to Transformer based large language models?",
            "answers": [
                {
                    "answer": "No",
                    "type": "boolean"
                }
            ],
            "q_uid": "e28a6e3d8f3aa303e1e0daff26b659a842aba97b",
            "evidence": [
                {
                    "raw_evidence": [
                        "We compared our models with the following state-of-the-art baselines:",
                        "Sequence to Sequence (Seq2Seq): A simple encoder-decoder model which concatenates four sentences to a long sentence with an attention mechanism BIBREF31 .",
                        "Hierarchical LSTM (HLSTM): The story context is represented by a hierarchical LSTM: a word-level LSTM for each sentence and a sentence-level LSTM connecting the four sentences BIBREF29 . A hierarchical attention mechanism is applied, which attends to the states of the two LSTMs respectively.",
                        "HLSTM+Copy: The copy mechanism BIBREF32 is applied to hierarchical states to copy the words in the story context for generation.",
                        "HLSTM+Graph Attention(GA): We applied multi-source attention HLSTM where commonsense knowledge is encoded by graph attention.",
                        "HLSTM+Contextual Attention(CA): Contextual attention is applied to represent commonsense knowledge."
                    ],
                    "highlighted_evidence": [
                        "We compared our models with the following state-of-the-art baselines:\n\nSequence to Sequence (Seq2Seq): A simple encoder-decoder model which concatenates four sentences to a long sentence with an attention mechanism BIBREF31 .\n\nHierarchical LSTM (HLSTM): The story context is represented by a hierarchical LSTM: a word-level LSTM for each sentence and a sentence-level LSTM connecting the four sentences BIBREF29 . A hierarchical attention mechanism is applied, which attends to the states of the two LSTMs respectively.\n\nHLSTM+Copy: The copy mechanism BIBREF32 is applied to hierarchical states to copy the words in the story context for generation.\n\nHLSTM+Graph Attention(GA): We applied multi-source attention HLSTM where commonsense knowledge is encoded by graph attention.\n\nHLSTM+Contextual Attention(CA): Contextual attention is applied to represent commonsense knowledge."
                    ]
                }
            ]
        },
        {
            "question": "Which baselines are they using?",
            "answers": [
                {
                    "answer": "Seq2Seq, HLSTM, HLSTM+Copy, HLSTM+Graph Attention, HLSTM+Contextual Attention",
                    "type": "extractive"
                }
            ],
            "q_uid": "0fce128b8aaa327ac0d58ec30cd2ecbea2019baa",
            "evidence": [
                {
                    "raw_evidence": [
                        "We compared our models with the following state-of-the-art baselines:",
                        "Sequence to Sequence (Seq2Seq): A simple encoder-decoder model which concatenates four sentences to a long sentence with an attention mechanism BIBREF31 .",
                        "Hierarchical LSTM (HLSTM): The story context is represented by a hierarchical LSTM: a word-level LSTM for each sentence and a sentence-level LSTM connecting the four sentences BIBREF29 . A hierarchical attention mechanism is applied, which attends to the states of the two LSTMs respectively.",
                        "HLSTM+Copy: The copy mechanism BIBREF32 is applied to hierarchical states to copy the words in the story context for generation.",
                        "HLSTM+Graph Attention(GA): We applied multi-source attention HLSTM where commonsense knowledge is encoded by graph attention.",
                        "HLSTM+Contextual Attention(CA): Contextual attention is applied to represent commonsense knowledge."
                    ],
                    "highlighted_evidence": [
                        "We compared our models with the following state-of-the-art baselines:\n\nSequence to Sequence (Seq2Seq): A simple encoder-decoder model which concatenates four sentences to a long sentence with an attention mechanism BIBREF31 .\n\nHierarchical LSTM (HLSTM): The story context is represented by a hierarchical LSTM: a word-level LSTM for each sentence and a sentence-level LSTM connecting the four sentences BIBREF29 . A hierarchical attention mechanism is applied, which attends to the states of the two LSTMs respectively.\n\nHLSTM+Copy: The copy mechanism BIBREF32 is applied to hierarchical states to copy the words in the story context for generation.\n\nHLSTM+Graph Attention(GA): We applied multi-source attention HLSTM where commonsense knowledge is encoded by graph attention.\n\nHLSTM+Contextual Attention(CA): Contextual attention is applied to represent commonsense knowledge."
                    ]
                }
            ]
        }
    ],
    "1911.06171": [
        {
            "question": "Which future direction in NLG are discussed?",
            "answers": [
                {
                    "answer": "1) How to introduce unsupervised pre-training into NLG tasks with cross-modal context?, 2) How to design a generic pre-training algorithm to fit a wide range of NLG tasks?, 3) How to reduce the computing resources required for large-scale pre-training?, 4) What aspect of knowledge do the pre-trained models provide for better language generation?",
                    "type": "extractive"
                }
            ],
            "q_uid": "dbf606cb6fc1d070418cc25e38ae57bbbb7087a0",
            "evidence": [
                {
                    "raw_evidence": [
                        "The diversity of NLG applications poses challenges on the employment of unsupervised pre-training, yet it also raises more scientific questions for us to explore. In terms of the future development of this technology, we emphasize the importance of answering four questions: 1) How to introduce unsupervised pre-training into NLG tasks with cross-modal context? 2) How to design a generic pre-training algorithm to fit a wide range of NLG tasks? 3) How to reduce the computing resources required for large-scale pre-training? 4) What aspect of knowledge do the pre-trained models provide for better language generation?"
                    ],
                    "highlighted_evidence": [
                        "In terms of the future development of this technology, we emphasize the importance of answering four questions: 1) How to introduce unsupervised pre-training into NLG tasks with cross-modal context? 2) How to design a generic pre-training algorithm to fit a wide range of NLG tasks? 3) How to reduce the computing resources required for large-scale pre-training? 4) What aspect of knowledge do the pre-trained models provide for better language generation?"
                    ]
                }
            ]
        },
        {
            "question": "What experimental phenomena are presented?",
            "answers": [
                {
                    "answer": "The advantage of pre-training gradually diminishes with the increase of labeled data, Fixed representations yield better results than fine-tuning in some cases, pre-training the Seq2Seq encoder outperforms pre-training the decoder",
                    "type": "extractive"
                }
            ],
            "q_uid": "9651fbd887439bf12590244c75e714f15f50f73d",
            "evidence": [
                {
                    "raw_evidence": [
                        "Existing researches on unsupervised pre-training for NLG are conducted on various tasks for different purposes. Probing into the assorted empirical results may help us discover some interesting phenomenons:",
                        "The advantage of pre-training gradually diminishes with the increase of labeled data BIBREF14, BIBREF17, BIBREF18.",
                        "Fixed representations yield better results than fine-tuning in some cases BIBREF24.",
                        "Overall, pre-training the Seq2Seq encoder outperforms pre-training the decoder BIBREF24, BIBREF17, BIBREF15, BIBREF16."
                    ],
                    "highlighted_evidence": [
                        "Probing into the assorted empirical results may help us discover some interesting phenomenons:\n\nThe advantage of pre-training gradually diminishes with the increase of labeled data BIBREF14, BIBREF17, BIBREF18.\n\nFixed representations yield better results than fine-tuning in some cases BIBREF24.\n\nOverall, pre-training the Seq2Seq encoder outperforms pre-training the decoder BIBREF24, BIBREF17, BIBREF15, BIBREF16."
                    ]
                }
            ]
        },
        {
            "question": "How strategy-based methods handle obstacles in NLG?",
            "answers": [
                {
                    "answer": "fine-tuning schedules that elaborately design the control of learning rates for optimization, proxy tasks that leverage labeled data to help the pre-trained model better fit the target data distribution, knowledge distillation approaches that ditch the paradigm of initialization with pre-trained parameters by adopting the pre-trained model as a teacher network",
                    "type": "extractive"
                }
            ],
            "q_uid": "1fd969f53bc714d9b5e6604a7780cbd6b12fd616",
            "evidence": [
                {
                    "raw_evidence": [
                        "In response to the above challenges, two lines of work are proposed by resorting to architecture-based and strategy-based solutions, respectively. Architecture-based methods either try to induce task-specific architecture during pre-training (task-specific methods), or aim at building a general pre-training architecture to fit all downstream tasks (task-agnostic methods). Strategy-based methods depart from the pre-training stage, seeking to take advantage of the pre-trained models during the process of target task learning. The approaches include fine-tuning schedules that elaborately design the control of learning rates for optimization, proxy tasks that leverage labeled data to help the pre-trained model better fit the target data distribution, and knowledge distillation approaches that ditch the paradigm of initialization with pre-trained parameters by adopting the pre-trained model as a teacher network."
                    ],
                    "highlighted_evidence": [
                        "Strategy-based methods depart from the pre-training stage, seeking to take advantage of the pre-trained models during the process of target task learning. The approaches include fine-tuning schedules that elaborately design the control of learning rates for optimization, proxy tasks that leverage labeled data to help the pre-trained model better fit the target data distribution, and knowledge distillation approaches that ditch the paradigm of initialization with pre-trained parameters by adopting the pre-trained model as a teacher network."
                    ]
                }
            ]
        },
        {
            "question": "How architecture-based method handle obstacles in NLG?",
            "answers": [
                {
                    "answer": "task-specific architecture during pre-training (task-specific methods), aim at building a general pre-training architecture to fit all downstream tasks (task-agnostic methods)",
                    "type": "extractive"
                }
            ],
            "q_uid": "cd37ad149d500e1c7d2de9de1f4bae8dcc443a72",
            "evidence": [
                {
                    "raw_evidence": [
                        "In response to the above challenges, two lines of work are proposed by resorting to architecture-based and strategy-based solutions, respectively. Architecture-based methods either try to induce task-specific architecture during pre-training (task-specific methods), or aim at building a general pre-training architecture to fit all downstream tasks (task-agnostic methods). Strategy-based methods depart from the pre-training stage, seeking to take advantage of the pre-trained models during the process of target task learning. The approaches include fine-tuning schedules that elaborately design the control of learning rates for optimization, proxy tasks that leverage labeled data to help the pre-trained model better fit the target data distribution, and knowledge distillation approaches that ditch the paradigm of initialization with pre-trained parameters by adopting the pre-trained model as a teacher network."
                    ],
                    "highlighted_evidence": [
                        "Architecture-based methods either try to induce task-specific architecture during pre-training (task-specific methods), or aim at building a general pre-training architecture to fit all downstream tasks (task-agnostic methods)"
                    ]
                }
            ]
        }
    ],
    "2003.09586": [
        {
            "question": "How much is decoding speed increased by increasing encoder and decreasing decoder depth?",
            "answers": [
                {
                    "answer": "the Transformer with 10 encoder layers and 2 decoder layers is $2.32$ times as fast as the 6-layer Transformer",
                    "type": "extractive"
                }
            ],
            "q_uid": "e75f5bd7cc7107f10412d61e3202a74b082b0934",
            "evidence": [
                {
                    "raw_evidence": [
                        "Table TABREF24 shows that while the acceleration of trading decoder layers for encoding layers in training is small, in decoding is significant. Specifically, the Transformer with 10 encoder layers and 2 decoder layers is $2.32$ times as fast as the 6-layer Transformer while achieving a slightly higher BLEU."
                    ],
                    "highlighted_evidence": [
                        "Table TABREF24 shows that while the acceleration of trading decoder layers for encoding layers in training is small, in decoding is significant. Specifically, the Transformer with 10 encoder layers and 2 decoder layers is $2.32$ times as fast as the 6-layer Transformer while achieving a slightly higher BLEU."
                    ]
                }
            ]
        }
    ],
    "1803.05223": [
        {
            "question": "what dataset statistics are provided?",
            "answers": [
                {
                    "answer": "More than 2,100 texts were paired with 15 questions each, resulting in a total number of approx. 32,000 annotated questions. 13% of the questions are not answerable.  Out of the answerable questions, 10,160 could be answered from the text directly (text-based) and 3,914 questions required the use of commonsense knowledge (script-based).  The final dataset comprises 13,939 questions, 3,827 of which require commonsense knowledge (i.e. 27.4%).",
                    "type": "abstractive"
                },
                {
                    "answer": "Distribution of category labels, number of answerable-not answerable questions, number of text-based and script-based questions, average text, question, and answer length, number of questions per text",
                    "type": "abstractive"
                }
            ],
            "q_uid": "ad1be65c4f0655ac5c902d17f05454c0d4c4a15d",
            "evidence": [
                {
                    "raw_evidence": [
                        "More than 2,100 texts were paired with 15 questions each, resulting in a total number of approx. 32,000 annotated questions. For 13% of the questions, the workers did not agree on one of the 4 categories with a 3 out of 5 majority, so we did not include these questions in our dataset.",
                        "The distribution of category labels on the remaining 87% is shown in Table TABREF10 . 14,074 (52%) questions could be answered. Out of the answerable questions, 10,160 could be answered from the text directly (text-based) and 3,914 questions required the use of commonsense knowledge (script-based). After removing 135 questions during the validation, the final dataset comprises 13,939 questions, 3,827 of which require commonsense knowledge (i.e. 27.4%). This ratio was manually verified based on a random sample of questions."
                    ],
                    "highlighted_evidence": [
                        "More than 2,100 texts were paired with 15 questions each, resulting in a total number of approx. 32,000 annotated questions. For 13% of the questions, the workers did not agree on one of the 4 categories with a 3 out of 5 majority, so we did not include these questions in our dataset.\n\nThe distribution of category labels on the remaining 87% is shown in Table TABREF10 . 14,074 (52%) questions could be answered. Out of the answerable questions, 10,160 could be answered from the text directly (text-based) and 3,914 questions required the use of commonsense knowledge (script-based). After removing 135 questions during the validation, the final dataset comprises 13,939 questions, 3,827 of which require commonsense knowledge (i.e. 27.4%). This ratio was manually verified based on a random sample of questions."
                    ]
                },
                {
                    "raw_evidence": [
                        "The distribution of category labels on the remaining 87% is shown in Table TABREF10 . 14,074 (52%) questions could be answered. Out of the answerable questions, 10,160 could be answered from the text directly (text-based) and 3,914 questions required the use of commonsense knowledge (script-based). After removing 135 questions during the validation, the final dataset comprises 13,939 questions, 3,827 of which require commonsense knowledge (i.e. 27.4%). This ratio was manually verified based on a random sample of questions.",
                        "We split the dataset into training (9,731 questions on 1,470 texts), development (1,411 questions on 219 texts), and test set (2,797 questions on 430 texts). Each text appears only in one of the three sets. The complete set of texts for 5 scenarios was held out for the test set. The average text, question, and answer length is 196.0 words, 7.8 words, and 3.6 words, respectively. On average, there are 6.7 questions per text."
                    ],
                    "highlighted_evidence": [
                        "The distribution of category labels on the remaining 87% is shown in Table TABREF10 . 14,074 (52%) questions could be answered. Out of the answerable questions, 10,160 could be answered from the text directly (text-based) and 3,914 questions required the use of commonsense knowledge (script-based). After removing 135 questions during the validation, the final dataset comprises 13,939 questions, 3,827 of which require commonsense knowledge (i.e. 27.4%). ",
                        "The average text, question, and answer length is 196.0 words, 7.8 words, and 3.6 words, respectively. On average, there are 6.7 questions per text."
                    ]
                }
            ]
        },
        {
            "question": "what is the size of their dataset?",
            "answers": [
                {
                    "answer": "13,939",
                    "type": "extractive"
                }
            ],
            "q_uid": "2eb9280d72cde9de3aabbed993009a98a5fe0990",
            "evidence": [
                {
                    "raw_evidence": [
                        "The distribution of category labels on the remaining 87% is shown in Table TABREF10 . 14,074 (52%) questions could be answered. Out of the answerable questions, 10,160 could be answered from the text directly (text-based) and 3,914 questions required the use of commonsense knowledge (script-based). After removing 135 questions during the validation, the final dataset comprises 13,939 questions, 3,827 of which require commonsense knowledge (i.e. 27.4%). This ratio was manually verified based on a random sample of questions."
                    ],
                    "highlighted_evidence": [
                        "After removing 135 questions during the validation, the final dataset comprises 13,939 questions, 3,827 of which require commonsense knowledge (i.e. 27.4%). "
                    ]
                }
            ]
        },
        {
            "question": "what crowdsourcing platform was used?",
            "answers": [
                {
                    "answer": "Amazon Mechanical Turk",
                    "type": "extractive"
                },
                {
                    "answer": "Amazon Mechanical Turk",
                    "type": "extractive"
                }
            ],
            "q_uid": "154a721ccc1d425688942e22e75af711b423e086",
            "evidence": [
                {
                    "raw_evidence": [
                        "Machine comprehension datasets consist of three main components: texts, questions and answers. In this section, we describe our data collection for these 3 components. We first describe a series of pilot studies that we conducted in order to collect commonsense inference questions (Section SECREF4 ). In Section SECREF5 , we discuss the resulting data collection of questions, texts and answers via crowdsourcing on Amazon Mechanical Turk (henceforth MTurk). Section SECREF17 gives information about some necessary postprocessing steps and the dataset validation. Lastly, Section SECREF19 gives statistics about the final dataset."
                    ],
                    "highlighted_evidence": [
                        "In Section SECREF5 , we discuss the resulting data collection of questions, texts and answers via crowdsourcing on Amazon Mechanical Turk (henceforth MTurk)."
                    ]
                },
                {
                    "raw_evidence": [
                        "Machine comprehension datasets consist of three main components: texts, questions and answers. In this section, we describe our data collection for these 3 components. We first describe a series of pilot studies that we conducted in order to collect commonsense inference questions (Section SECREF4 ). In Section SECREF5 , we discuss the resulting data collection of questions, texts and answers via crowdsourcing on Amazon Mechanical Turk (henceforth MTurk). Section SECREF17 gives information about some necessary postprocessing steps and the dataset validation. Lastly, Section SECREF19 gives statistics about the final dataset."
                    ],
                    "highlighted_evidence": [
                        " In Section SECREF5 , we discuss the resulting data collection of questions, texts and answers via crowdsourcing on Amazon Mechanical Turk (henceforth MTurk)."
                    ]
                }
            ]
        },
        {
            "question": "how was the data collected?",
            "answers": [
                {
                    "answer": "The data was collected using 3 components: describe a series of pilot studies that were conducted to collect commonsense inference questions, then discuss the resulting data collection of questions, texts and answers via crowdsourcing on Amazon Mechanical Turk and gives information about some necessary postprocessing steps and the dataset validation.",
                    "type": "abstractive"
                }
            ],
            "q_uid": "84bad9a821917cb96584cf5383c6d2a035358d7c",
            "evidence": [
                {
                    "raw_evidence": [
                        "Machine comprehension datasets consist of three main components: texts, questions and answers. In this section, we describe our data collection for these 3 components. We first describe a series of pilot studies that we conducted in order to collect commonsense inference questions (Section SECREF4 ). In Section SECREF5 , we discuss the resulting data collection of questions, texts and answers via crowdsourcing on Amazon Mechanical Turk (henceforth MTurk). Section SECREF17 gives information about some necessary postprocessing steps and the dataset validation. Lastly, Section SECREF19 gives statistics about the final dataset."
                    ],
                    "highlighted_evidence": [
                        "In this section, we describe our data collection for these 3 components. We first describe a series of pilot studies that we conducted in order to collect commonsense inference questions (Section SECREF4 ). In Section SECREF5 , we discuss the resulting data collection of questions, texts and answers via crowdsourcing on Amazon Mechanical Turk (henceforth MTurk). Section SECREF17 gives information about some necessary postprocessing steps and the dataset validation. "
                    ]
                }
            ]
        }
    ],
    "1809.01341": [
        {
            "question": "What other multimodal knowledge base embedding methods are there?",
            "answers": [
                {
                    "answer": "merging, concatenating, or averaging the entity and its features to compute its embeddings, graph embedding approaches, matrix factorization to jointly embed KB and textual relations",
                    "type": "extractive"
                }
            ],
            "q_uid": "29e5e055e01fdbf7b90d5907158676dd3169732d",
            "evidence": [
                {
                    "raw_evidence": [
                        "A number of methods utilize an extra type of information as the observed features for entities, by either merging, concatenating, or averaging the entity and its features to compute its embeddings, such as numerical values BIBREF26 (we use KBLN from this work to compare it with our approach using only numerical as extra attributes), images BIBREF27 , BIBREF28 (we use IKRL from the first work to compare it with our approach using only images as extra attributes), text BIBREF29 , BIBREF30 , BIBREF31 , BIBREF32 , BIBREF33 , BIBREF34 , and a combination of text and image BIBREF35 . Further, BIBREF7 address the multilingual relation extraction task to attain a universal schema by considering raw text with no annotation as extra feature and using matrix factorization to jointly embed KB and textual relations BIBREF36 . In addition to treating the extra information as features, graph embedding approaches BIBREF37 , BIBREF38 consider observed attributes while encoding to achieve more accurate embeddings."
                    ],
                    "highlighted_evidence": [
                        "A number of methods utilize an extra type of information as the observed features for entities, by either merging, concatenating, or averaging the entity and its features to compute its embeddings, such as numerical values BIBREF26 (we use KBLN from this work to compare it with our approach using only numerical as extra attributes), images BIBREF27 , BIBREF28 (we use IKRL from the first work to compare it with our approach using only images as extra attributes), text BIBREF29 , BIBREF30 , BIBREF31 , BIBREF32 , BIBREF33 , BIBREF34 , and a combination of text and image BIBREF35 . Further, BIBREF7 address the multilingual relation extraction task to attain a universal schema by considering raw text with no annotation as extra feature and using matrix factorization to jointly embed KB and textual relations BIBREF36 . In addition to treating the extra information as features, graph embedding approaches BIBREF37 , BIBREF38 consider observed attributes while encoding to achieve more accurate embeddings."
                    ]
                }
            ]
        }
    ],
    "1909.04181": [
        {
            "question": "Does the paper report F1-scores for the age and language variety tasks?",
            "answers": [
                {
                    "answer": "No",
                    "type": "boolean"
                }
            ],
            "q_uid": "e9cfe3f15735e2b0d5c59a54c9940ed1d00401a2",
            "evidence": [
                {
                    "raw_evidence": [],
                    "highlighted_evidence": []
                }
            ]
        },
        {
            "question": "Are the models compared to some baseline models?",
            "answers": [
                {
                    "answer": "Yes",
                    "type": "boolean"
                }
            ],
            "q_uid": "52ed2eb6f4d1f74ebdc4dcddcae201786d4c0463",
            "evidence": [
                {
                    "raw_evidence": [
                        "Our baseline is a GRU network for each of the three tasks. We use the same network architecture across the 3 tasks. For each network, the network contains a layer unidirectional GRU, with 500 units and an output linear layer. The network is trained end-to-end. Our input embedding layer is initialized with a standard normal distribution, with $\\mu =0$, and $\\sigma =1$, i.e., $W \\sim N(0,1)$. We use a maximum sequence length of 50 tokens, and choose an arbitrary vocabulary size of 100,000 types, where we use the 100,000 most frequent words in TRAIN. To avoid over-fitting, we use dropout BIBREF2 with a rate of 0.5 on the hidden layer. For the training, we use the Adam BIBREF3 optimizer with a fixed learning rate of $1e-3$. We employ batch training with a batch size of 32 for this model. We train the network for 15 epochs and save the model at the end of each epoch, choosing the model that performs highest accuracy on DEV as our best model. We present our best result on DEV in Table TABREF7. We report all our results using accuracy. Our best model obtains 42.48% for age, 37.50% for dialect, and 57.81% for gender. All models obtains best results with 2 epochs."
                    ],
                    "highlighted_evidence": [
                        "Our baseline is a GRU network for each of the three tasks."
                    ]
                }
            ]
        },
        {
            "question": "What are the in-house data employed?",
            "answers": [
                {
                    "answer": "we manually label an in-house dataset of 1,100 users with gender tags, we randomly sample 20,000 tweets for each class from an in-house dataset gold labeled with the same 15 classes as the shared task",
                    "type": "extractive"
                }
            ],
            "q_uid": "2c576072e494ab5598667cd6b40bc97fdd7d92d7",
            "evidence": [
                {
                    "raw_evidence": [
                        "To further improve the performance of our models, we introduce in-house labeled data that we use to fine-tune BERT. For the gender classification task, we manually label an in-house dataset of 1,100 users with gender tags, including 550 female users, 550 male users. We obtain 162,829 tweets by crawling the 1,100 users' timelines. We combine this new gender dataset with the gender TRAIN data (from shared task) to obtain an extended dataset, to which we refer as EXTENDED_Gender. For the dialect identification task, we randomly sample 20,000 tweets for each class from an in-house dataset gold labeled with the same 15 classes as the shared task. In this way, we obtain 298,929 tweets (Sudan only has 18,929 tweets). We combine this new dialect data with the shared task dialect TRAIN data to form EXTENDED_Dialect. For both the dialect and gender tasks, we fine-tune BERT on EXTENDED_Dialect and EXTENDED_Gender independently and report performance on DEV. We refer to this iteration of experiments as BERT_EXT. As Table TABREF7 shows, BERT_EXT is 2.18% better than BERT for dialect and 0.75% better than BERT for gender."
                    ],
                    "highlighted_evidence": [
                        "To further improve the performance of our models, we introduce in-house labeled data that we use to fine-tune BERT. For the gender classification task, we manually label an in-house dataset of 1,100 users with gender tags, including 550 female users, 550 male users. We obtain 162,829 tweets by crawling the 1,100 users' timelines.",
                        "For the dialect identification task, we randomly sample 20,000 tweets for each class from an in-house dataset gold labeled with the same 15 classes as the shared task."
                    ]
                }
            ]
        },
        {
            "question": "What are the three datasets used in the paper?",
            "answers": [
                {
                    "answer": "Data released for APDA shared task contains 3 datasets.",
                    "type": "abstractive"
                }
            ],
            "q_uid": "8602160e98e4b2c9c702440da395df5261f55b1f",
            "evidence": [
                {
                    "raw_evidence": [
                        "For the purpose of our experiments, we use data released by the APDA shared task organizers. The dataset is divided into train and test by organizers. The training set is distributed with labels for the three tasks of age, dialect, and gender. Following the standard shared tasks set up, the test set is distributed without labels and participants were expected to submit their predictions on test. The shared task predictions are expected by organizers at the level of users. The distribution has 100 tweets for each user, and so each tweet is distributed with a corresponding user id. As such, in total, the distributed training data has 2,250 users, contributing a total of 225,000 tweets. The official task test set contains 720,00 tweets posted by 720 users. For our experiments, we split the training data released by organizers into 90% TRAIN set (202,500 tweets from 2,025 users) and 10% DEV set (22,500 tweets from 225 users). The age task labels come from the tagset {under-25, between-25 and 34, above-35}. For dialects, the data are labeled with 15 classes, from the set {Algeria, Egypt, Iraq, Kuwait, Lebanon-Syria, Lybia, Morocco, Oman, Palestine-Jordan, Qatar, Saudi Arabia, Sudan, Tunisia, UAE, Yemen}. The gender task involves binary labels from the set {male, female}."
                    ],
                    "highlighted_evidence": [
                        "For the purpose of our experiments, we use data released by the APDA shared task organizers. The dataset is divided into train and test by organizers. The training set is distributed with labels for the three tasks of age, dialect, and gender."
                    ]
                }
            ]
        }
    ],
    "1806.02908": [
        {
            "question": "Do the authors offer any hypothesis as to why the transformations sometimes disimproved performance?",
            "answers": [
                {
                    "answer": "No",
                    "type": "boolean"
                }
            ],
            "q_uid": "d0c636fa9ef99c4f44ab39e837a680217b140269",
            "evidence": [
                {
                    "raw_evidence": [],
                    "highlighted_evidence": []
                }
            ]
        },
        {
            "question": "What preprocessing techniques are used in the experiments?",
            "answers": [
                {
                    "answer": "See Figure FIGREF3",
                    "type": "extractive"
                }
            ],
            "q_uid": "c47f593a5b92abc2e3c536fe2baaca226913688b",
            "evidence": [
                {
                    "raw_evidence": [
                        "To explore how helpful these transformations are, we incorporated 20 simple transformations and 15 additional sequences of transformations in our experiment to see their effect on different type of metrics on four different ML models (See Figure FIGREF3 ).",
                        "FLOAT SELECTED: Fig. 1: List of transformations."
                    ],
                    "highlighted_evidence": [
                        "To explore how helpful these transformations are, we incorporated 20 simple transformations and 15 additional sequences of transformations in our experiment to see their effect on different type of metrics on four different ML models (See Figure FIGREF3 ).",
                        "FLOAT SELECTED: Fig. 1: List of transformations."
                    ]
                }
            ]
        },
        {
            "question": "What state of the art models are used in the experiments?",
            "answers": [
                {
                    "answer": "2) Na\u00efve Bayes with SVM (NBSVM), 3) Extreme Gradient Boosting (XGBoost), 4) FastText algorithm with Bidirectional LSTM (FastText-BiLSTM)",
                    "type": "extractive"
                }
            ],
            "q_uid": "c3a9732599849ba4a9f07170ce1e50867cf7d7bf",
            "evidence": [
                {
                    "raw_evidence": [
                        "We used four classification algorithms: 1) Logistic regression, which is conventionally used in sentiment classification. Other three algorithms which are relatively new and has shown great results on sentiment classification types of problems are: 2) Na\u00efve Bayes with SVM (NBSVM), 3) Extreme Gradient Boosting (XGBoost) and 4) FastText algorithm with Bidirectional LSTM (FastText-BiLSTM)."
                    ],
                    "highlighted_evidence": [
                        "We used four classification algorithms: 1) Logistic regression, which is conventionally used in sentiment classification. Other three algorithms which are relatively new and has shown great results on sentiment classification types of problems are: 2) Na\u00efve Bayes with SVM (NBSVM), 3) Extreme Gradient Boosting (XGBoost) and 4) FastText algorithm with Bidirectional LSTM (FastText-BiLSTM)."
                    ]
                }
            ]
        }
    ],
    "1907.01413": [
        {
            "question": "Do they propose any further additions that could be made to improve generalisation to unseen speakers?",
            "answers": [
                {
                    "answer": "Yes",
                    "type": "boolean"
                }
            ],
            "q_uid": "abad9beb7295d809d7e5e1407cbf673c9ffffd19",
            "evidence": [
                {
                    "raw_evidence": [
                        "There are various possible extensions for this work. For example, using all frames assigned to a phone, rather than using only the middle frame. Recurrent architectures are natural candidates for such systems. Additionally, if using these techniques for speech therapy, the audio signal will be available. An extension of these analyses should not be limited to the ultrasound signal, but instead evaluate whether audio and ultrasound can be complementary. Further work should aim to extend the four classes to more a fine-grained place of articulation, possibly based on phonological processes. Similarly, investigating which classes lead to classification errors might help explain some of the observed results. Although we have looked at variables such as age, gender, or amount of data to explain speaker variation, there may be additional factors involved, such as the general quality of the ultrasound image. Image quality could be affected by probe placement, dry mouths, or other factors. Automatically identifying or measuring such cases could be beneficial for speech therapy, for example, by signalling the therapist that the data being collected is sub-optimal."
                    ],
                    "highlighted_evidence": [
                        "There are various possible extensions for this work. For example, using all frames assigned to a phone, rather than using only the middle frame."
                    ]
                }
            ]
        },
        {
            "question": "What are the characteristics of the dataset?",
            "answers": [
                {
                    "answer": "synchronized acoustic and ultrasound data from 58 typically developing children, aged 5-12 years old (31 female, 27 male), data was aligned at the phone-level, 121fps with a 135 field of view, single ultrasound frame consists of 412 echo returns from each of the 63 scan lines (63x412 raw frames)",
                    "type": "extractive"
                }
            ],
            "q_uid": "265c9b733e4dfffb76acfbade4c0c9b14d3ccde1",
            "evidence": [
                {
                    "raw_evidence": [
                        "We use the Ultrax Typically Developing dataset (UXTD) from the publicly available UltraSuite repository BIBREF19 . This dataset contains synchronized acoustic and ultrasound data from 58 typically developing children, aged 5-12 years old (31 female, 27 male). The data was aligned at the phone-level, according to the methods described in BIBREF19 , BIBREF25 . For this work, we discarded the acoustic data and focused only on the B-Mode ultrasound images capturing a midsaggital view of the tongue. The data was recorded using an Ultrasonix SonixRP machine using Articulate Assistant Advanced (AAA) software at INLINEFORM0 121fps with a 135 field of view. A single ultrasound frame consists of 412 echo returns from each of the 63 scan lines (63x412 raw frames). For this work, we only use UXTD type A (semantically unrelated words, such as pack, tap, peak, tea, oak, toe) and type B (non-words designed to elicit the articulation of target phones, such as apa, eepee, opo) utterances."
                    ],
                    "highlighted_evidence": [
                        "We use the Ultrax Typically Developing dataset (UXTD) from the publicly available UltraSuite repository BIBREF19 . This dataset contains synchronized acoustic and ultrasound data from 58 typically developing children, aged 5-12 years old (31 female, 27 male). The data was aligned at the phone-level, according to the methods described in BIBREF19 , BIBREF25 .",
                        "The data was recorded using an Ultrasonix SonixRP machine using Articulate Assistant Advanced (AAA) software at INLINEFORM0 121fps with a 135 field of view. A single ultrasound frame consists of 412 echo returns from each of the 63 scan lines (63x412 raw frames)."
                    ]
                }
            ]
        },
        {
            "question": "What type of models are used for classification?",
            "answers": [
                {
                    "answer": "feedforward neural networks (DNNs), convolutional neural networks (CNNs)",
                    "type": "extractive"
                }
            ],
            "q_uid": "0f928732f226185c76ad5960402e9342c0619310",
            "evidence": [
                {
                    "raw_evidence": [
                        "The first type of classifier we evaluate in this work are feedforward neural networks (DNNs) consisting of 3 hidden layers, each with 512 rectified linear units (ReLUs) with a softmax activation function. The networks are optimized for 40 epochs with a mini-batch of 32 samples using stochastic gradient descent. Based on preliminary experiments on the validation set, hyperparameters such learning rate, decay rate, and L2 weight vary depending on the input format (Raw, PCA, or DCT). Generally, Raw inputs work better with smaller learning rates and heavier regularization to prevent overfitting to the high-dimensional data. As a second classifier to evaluate, we use convolutional neural networks (CNNs) with 2 convolutional and max pooling layers, followed by 2 fully-connected ReLU layers with 512 nodes. The convolutional layers use 16 filters, 8x8 and 4x4 kernels respectively, and rectified units. The fully-connected layers use dropout with a drop probability of 0.2. Because CNN systems take longer to converge, they are optimized over 200 epochs. For all systems, at the end of every epoch, the model is evaluated on the development set, and the best model across all epochs is kept."
                    ],
                    "highlighted_evidence": [
                        "The first type of classifier we evaluate in this work are feedforward neural networks (DNNs) consisting of 3 hidden layers, each with 512 rectified linear units (ReLUs) with a softmax activation function.",
                        "As a second classifier to evaluate, we use convolutional neural networks (CNNs) with 2 convolutional and max pooling layers, followed by 2 fully-connected ReLU layers with 512 nodes."
                    ]
                }
            ]
        },
        {
            "question": "Do they compare to previous work?",
            "answers": [
                {
                    "answer": "No",
                    "type": "boolean"
                }
            ],
            "q_uid": "11c5b12e675cfd8d1113724f019d8476275bd700",
            "evidence": [
                {
                    "raw_evidence": [],
                    "highlighted_evidence": []
                }
            ]
        },
        {
            "question": "How many instances does their dataset have?",
            "answers": [
                {
                    "answer": "10700",
                    "type": "extractive"
                }
            ],
            "q_uid": "d24acc567ebaec1efee52826b7eaadddc0a89e8b",
            "evidence": [
                {
                    "raw_evidence": [
                        "For each speaker, we divide all available utterances into disjoint train, development, and test sets. Using the force-aligned phone boundaries, we extract the mid-phone frame for each example across the four classes, which leads to a data imbalance. Therefore, for all utterances in the training set, we randomly sample additional examples within a window of 5 frames around the center phone, to at least 50 training examples per class per speaker. It is not always possible to reach the target of 50 examples, however, if no more data is available to sample from. This process gives a total of INLINEFORM0 10700 training examples with roughly 2000 to 3000 examples per class, with each speaker having an average of 185 examples. Because the amount of data varies per speaker, we compute a sampling score, which denotes the proportion of sampled examples to the speaker's total training examples. We expect speakers with high sampling scores (less unique data overall) to underperform when compared with speakers with more varied training examples."
                    ],
                    "highlighted_evidence": [
                        "This process gives a total of INLINEFORM0 10700 training examples with roughly 2000 to 3000 examples per class, with each speaker having an average of 185 examples."
                    ]
                }
            ]
        },
        {
            "question": "What model do they use to classify phonetic segments? ",
            "answers": [
                {
                    "answer": "feedforward neural networks, convolutional neural networks",
                    "type": "extractive"
                }
            ],
            "q_uid": "2d62a75af409835e4c123a615b06235a352a67fe",
            "evidence": [
                {
                    "raw_evidence": [
                        "The first type of classifier we evaluate in this work are feedforward neural networks (DNNs) consisting of 3 hidden layers, each with 512 rectified linear units (ReLUs) with a softmax activation function. The networks are optimized for 40 epochs with a mini-batch of 32 samples using stochastic gradient descent. Based on preliminary experiments on the validation set, hyperparameters such learning rate, decay rate, and L2 weight vary depending on the input format (Raw, PCA, or DCT). Generally, Raw inputs work better with smaller learning rates and heavier regularization to prevent overfitting to the high-dimensional data. As a second classifier to evaluate, we use convolutional neural networks (CNNs) with 2 convolutional and max pooling layers, followed by 2 fully-connected ReLU layers with 512 nodes. The convolutional layers use 16 filters, 8x8 and 4x4 kernels respectively, and rectified units. The fully-connected layers use dropout with a drop probability of 0.2. Because CNN systems take longer to converge, they are optimized over 200 epochs. For all systems, at the end of every epoch, the model is evaluated on the development set, and the best model across all epochs is kept."
                    ],
                    "highlighted_evidence": [
                        "The first type of classifier we evaluate in this work are feedforward neural networks (DNNs) consisting of 3 hidden layers, each with 512 rectified linear units (ReLUs) with a softmax activation function. ",
                        "As a second classifier to evaluate, we use convolutional neural networks (CNNs) with 2 convolutional and max pooling layers, followed by 2 fully-connected ReLU layers with 512 nodes. "
                    ]
                }
            ]
        },
        {
            "question": "How many speakers do they have in the dataset?",
            "answers": [
                {
                    "answer": "58",
                    "type": "extractive"
                }
            ],
            "q_uid": "fffbd6cafef96eeeee2f9fa5d8ab2b325ec528e6",
            "evidence": [
                {
                    "raw_evidence": [
                        "We use the Ultrax Typically Developing dataset (UXTD) from the publicly available UltraSuite repository BIBREF19 . This dataset contains synchronized acoustic and ultrasound data from 58 typically developing children, aged 5-12 years old (31 female, 27 male). The data was aligned at the phone-level, according to the methods described in BIBREF19 , BIBREF25 . For this work, we discarded the acoustic data and focused only on the B-Mode ultrasound images capturing a midsaggital view of the tongue. The data was recorded using an Ultrasonix SonixRP machine using Articulate Assistant Advanced (AAA) software at INLINEFORM0 121fps with a 135 field of view. A single ultrasound frame consists of 412 echo returns from each of the 63 scan lines (63x412 raw frames). For this work, we only use UXTD type A (semantically unrelated words, such as pack, tap, peak, tea, oak, toe) and type B (non-words designed to elicit the articulation of target phones, such as apa, eepee, opo) utterances."
                    ],
                    "highlighted_evidence": [
                        "We use the Ultrax Typically Developing dataset (UXTD) from the publicly available UltraSuite repository BIBREF19 . This dataset contains synchronized acoustic and ultrasound data from 58 typically developing children, aged 5-12 years old (31 female, 27 male). "
                    ]
                }
            ]
        }
    ],
    "1903.09588": [
        {
            "question": "How do they generate the auxiliary sentence?",
            "answers": [
                {
                    "answer": "The sentence we want to generate from the target-aspect pair is a question, and the format needs to be the same., For the NLI task, the conditions we set when generating sentences are less strict, and the form is much simpler., For QA-B, we add the label information and temporarily convert TABSA into a binary classification problem ( INLINEFORM0 ) to obtain the probability distribution, auxiliary sentence changes from a question to a pseudo-sentence",
                    "type": "extractive"
                }
            ],
            "q_uid": "3554ac92d4f2d00dbf58f7b4ff2b36a852854e95",
            "evidence": [
                {
                    "raw_evidence": [
                        "Construction of the auxiliary sentence",
                        "For simplicity, we mainly describe our method with TABSA as an example.",
                        "We consider the following four methods to convert the TABSA task into a sentence pair classification task:",
                        "The sentence we want to generate from the target-aspect pair is a question, and the format needs to be the same. For example, for the set of a target-aspect pair (LOCATION1, safety), the sentence we generate is \u201cwhat do you think of the safety of location - 1 ?\"",
                        "For the NLI task, the conditions we set when generating sentences are less strict, and the form is much simpler. The sentence created at this time is not a standard sentence, but a simple pseudo-sentence, with (LOCATION1, safety) pair as an example: the auxiliary sentence is: \u201clocation - 1 - safety\".",
                        "For QA-B, we add the label information and temporarily convert TABSA into a binary classification problem ( INLINEFORM0 ) to obtain the probability distribution. At this time, each target-aspect pair will generate three sequences such as \u201cthe polarity of the aspect safety of location - 1 is positive\", \u201cthe polarity of the aspect safety of location - 1 is negative\", \u201cthe polarity of the aspect safety of location - 1 is none\". We use the probability value of INLINEFORM1 as the matching score. For a target-aspect pair which generates three sequences ( INLINEFORM2 ), we take the class of the sequence with the highest matching score for the predicted category.",
                        "The difference between NLI-B and QA-B is that the auxiliary sentence changes from a question to a pseudo-sentence. The auxiliary sentences are: \u201clocation - 1 - safety - positive\", \u201clocation - 1 - safety - negative\", and \u201clocation - 1 - safety - none\".",
                        "After we construct the auxiliary sentence, we can transform the TABSA task from a single sentence classification task to a sentence pair classification task. As shown in Table TABREF19 , this is a necessary operation that can significantly improve the experimental results of the TABSA task."
                    ],
                    "highlighted_evidence": [
                        "Construction of the auxiliary sentence\nFor simplicity, we mainly describe our method with TABSA as an example.\n\nWe consider the following four methods to convert the TABSA task into a sentence pair classification task:\n\nThe sentence we want to generate from the target-aspect pair is a question, and the format needs to be the same. For example, for the set of a target-aspect pair (LOCATION1, safety), the sentence we generate is \u201cwhat do you think of the safety of location - 1 ?\"\n\nFor the NLI task, the conditions we set when generating sentences are less strict, and the form is much simpler. The sentence created at this time is not a standard sentence, but a simple pseudo-sentence, with (LOCATION1, safety) pair as an example: the auxiliary sentence is: \u201clocation - 1 - safety\".\n\nFor QA-B, we add the label information and temporarily convert TABSA into a binary classification problem ( INLINEFORM0 ) to obtain the probability distribution. At this time, each target-aspect pair will generate three sequences such as \u201cthe polarity of the aspect safety of location - 1 is positive\", \u201cthe polarity of the aspect safety of location - 1 is negative\", \u201cthe polarity of the aspect safety of location - 1 is none\". We use the probability value of INLINEFORM1 as the matching score. For a target-aspect pair which generates three sequences ( INLINEFORM2 ), we take the class of the sequence with the highest matching score for the predicted category.\n\nThe difference between NLI-B and QA-B is that the auxiliary sentence changes from a question to a pseudo-sentence. The auxiliary sentences are: \u201clocation - 1 - safety - positive\", \u201clocation - 1 - safety - negative\", and \u201clocation - 1 - safety - none\".\n\nAfter we construct the auxiliary sentence, we can transform the TABSA task from a single sentence classification task to a sentence pair classification task. As shown in Table TABREF19 , this is a necessary operation that can significantly improve the experimental results of the TABSA task."
                    ]
                }
            ]
        }
    ],
    "1902.06843": [
        {
            "question": "What insights into the relationship between demographics and mental health are provided?",
            "answers": [
                {
                    "answer": "either likely depressed-user population is younger, or depressed youngsters are more likely to disclose their age, more women than men were given a diagnosis of depression",
                    "type": "extractive"
                }
            ],
            "q_uid": "97dac7092cf8082a6238aaa35f4b185343b914af",
            "evidence": [
                {
                    "raw_evidence": [
                        "Age Enabled Ground-truth Dataset: We extract user's age by applying regular expression patterns to profile descriptions (such as \"17 years old, self-harm, anxiety, depression\") BIBREF41 . We compile \"age prefixes\" and \"age suffixes\", and use three age-extraction rules: 1. I am X years old 2. Born in X 3. X years old, where X is a \"date\" or age (e.g., 1994). We selected a subset of 1061 users among INLINEFORM0 as gold standard dataset INLINEFORM1 who disclose their age. From these 1061 users, 822 belong to depressed class and 239 belong to control class. From 3981 depressed users, 20.6% disclose their age in contrast with only 4% (239/4789) among control group. So self-disclosure of age is more prevalent among vulnerable users. Figure FIGREF18 depicts the age distribution in INLINEFORM2 . The general trend, consistent with the results in BIBREF42 , BIBREF49 , is biased toward young people. Indeed, according to Pew, 47% of Twitter users are younger than 30 years old BIBREF50 . Similar data collection procedure with comparable distribution have been used in many prior efforts BIBREF51 , BIBREF49 , BIBREF42 . We discuss our approach to mitigate the impact of the bias in Section 4.1. The median age is 17 for depressed class versus 19 for control class suggesting either likely depressed-user population is younger, or depressed youngsters are more likely to disclose their age for connecting to their peers (social homophily.) BIBREF51",
                        "Gender Enabled Ground-truth Dataset: We selected a subset of 1464 users INLINEFORM0 from INLINEFORM1 who disclose their gender in their profile description. From 1464 users 64% belonged to the depressed group, and the rest (36%) to the control group. 23% of the likely depressed users disclose their gender which is considerably higher (12%) than that for the control class. Once again, gender disclosure varies among the two gender groups. For statistical significance, we performed chi-square test (null hypothesis: gender and depression are two independent variables). Figure FIGREF19 illustrates gender association with each of the two classes. Blue circles (positive residuals, see Figure FIGREF19 -A,D) show positive association among corresponding row and column variables while red circles (negative residuals, see Figure FIGREF19 -B,C) imply a repulsion. Our findings are consistent with the medical literature BIBREF10 as according to BIBREF52 more women than men were given a diagnosis of depression. In particular, the female-to-male ratio is 2.1 and 1.9 for Major Depressive Disorder and Dysthymic Disorder respectively. Our findings from Twitter data indicate there is a strong association (Chi-square: 32.75, p-value:1.04e-08) between being female and showing depressive behavior on Twitter."
                    ],
                    "highlighted_evidence": [
                        "The median age is 17 for depressed class versus 19 for control class suggesting either likely depressed-user population is younger, or depressed youngsters are more likely to disclose their age for connecting to their peers (social homophily.)",
                        "Our findings are consistent with the medical literature BIBREF10 as according to BIBREF52 more women than men were given a diagnosis of depression."
                    ]
                }
            ]
        },
        {
            "question": "What model is used to achieve 5% improvement on F1 for identifying depressed individuals on Twitter?",
            "answers": [
                {
                    "answer": "Random Forest classifier",
                    "type": "extractive"
                }
            ],
            "q_uid": "195611926760d1ceec00bd043dfdc8eba2df5ad1",
            "evidence": [
                {
                    "raw_evidence": [
                        "We use the above findings for predicting depressive behavior. Our model exploits early fusion BIBREF32 technique in feature space and requires modeling each user INLINEFORM0 in INLINEFORM1 as vector concatenation of individual modality features. As opposed to computationally expensive late fusion scheme where each modality requires a separate supervised modeling, this model reduces the learning effort and shows promising results BIBREF75 . To develop a generalizable model that avoids overfitting, we perform feature selection using statistical tests and all relevant ensemble learning models. It adds randomness to the data by creating shuffled copies of all features (shadow feature), and then trains Random Forest classifier on the extended data. Iteratively, it checks whether the actual feature has a higher Z-score than its shadow feature (See Algorithm SECREF6 and Figure FIGREF45 ) BIBREF76 ."
                    ],
                    "highlighted_evidence": [
                        "To develop a generalizable model that avoids overfitting, we perform feature selection using statistical tests and all relevant ensemble learning models. It adds randomness to the data by creating shuffled copies of all features (shadow feature), and then trains Random Forest classifier on the extended data."
                    ]
                }
            ]
        },
        {
            "question": "How do this framework facilitate demographic inference from social media?",
            "answers": [
                {
                    "answer": "Demographic information is predicted using weighted lexicon of terms.",
                    "type": "abstractive"
                }
            ],
            "q_uid": "445e792ce7e699e960e2cb4fe217aeacdd88d392",
            "evidence": [
                {
                    "raw_evidence": [
                        "We employ BIBREF73 's weighted lexicon of terms that uses the dataset of 75,394 Facebook users who shared their status, age and gender. The predictive power of this lexica was evaluated on Twitter, blog, and Facebook, showing promising results BIBREF73 . Utilizing these two weighted lexicon of terms, we are predicting the demographic information (age or gender) of INLINEFORM0 (denoted by INLINEFORM1 ) using following equation: INLINEFORM2",
                        "where INLINEFORM0 is the lexicon weight of the term, and INLINEFORM1 represents the frequency of the term in the user generated INLINEFORM2 , and INLINEFORM3 measures total word count in INLINEFORM4 . As our data is biased toward young people, we report age prediction performance for each age group separately (Table TABREF42 ). Moreover, to measure the average accuracy of this model, we build a balanced dataset (keeping all the users above 23 -416 users), and then randomly sampling the same number of users from the age ranges (11,19] and (19,23]. The average accuracy of this model is 0.63 for depressed users and 0.64 for control class. Table TABREF44 illustrates the performance of gender prediction for each class. The average accuracy is 0.82 on INLINEFORM5 ground-truth dataset."
                    ],
                    "highlighted_evidence": [
                        "We employ BIBREF73 's weighted lexicon of terms that uses the dataset of 75,394 Facebook users who shared their status, age and gender.",
                        "Utilizing these two weighted lexicon of terms, we are predicting the demographic information (age or gender) of INLINEFORM0 (denoted by INLINEFORM1 ) using following equation: INLINEFORM2\n\nwhere INLINEFORM0 is the lexicon weight of the term, and INLINEFORM1 represents the frequency of the term in the user generated INLINEFORM2 , and INLINEFORM3 measures total word count in INLINEFORM4 ."
                    ]
                }
            ]
        },
        {
            "question": "What types of features are used from each data type?",
            "answers": [
                {
                    "answer": "facial presence, Facial Expression, General Image Features,  textual content, analytical thinking, clout, authenticity, emotional tone, Sixltr,  informal language markers, 1st person singular pronouns",
                    "type": "extractive"
                }
            ],
            "q_uid": "a3b1520e3da29d64af2b6e22ff15d330026d0b36",
            "evidence": [
                {
                    "raw_evidence": [
                        "For capturing facial presence, we rely on BIBREF56 's approach that uses multilevel convolutional coarse-to-fine network cascade to tackle facial landmark localization. We identify facial presentation, emotion from facial expression, and demographic features from profile/posted images . Table TABREF21 illustrates facial presentation differences in both profile and posted images (media) for depressed and control users in INLINEFORM0 . With control class showing significantly higher in both profile and media (8%, 9% respectively) compared to that for the depressed class. In contrast with age and gender disclosure, vulnerable users are less likely to disclose their facial identity, possibly due to lack of confidence or fear of stigma.",
                        "Facial Expression:",
                        "Following BIBREF8 's approach, we adopt Ekman's model of six emotions: anger, disgust, fear, joy, sadness and surprise, and use the Face++ API to automatically capture them from the shared images. Positive emotions are joy and surprise, and negative emotions are anger, disgust, fear, and sadness. In general, for each user u in INLINEFORM0 , we process profile/shared images for both the depressed and the control groups with at least one face from the shared images (Table TABREF23 ). For the photos that contain multiple faces, we measure the average emotion.",
                        "General Image Features:",
                        "The importance of interpretable computational aesthetic features for studying users' online behavior has been highlighted by several efforts BIBREF55 , BIBREF8 , BIBREF57 . Color, as a pillar of the human vision system, has a strong association with conceptual ideas like emotion BIBREF58 , BIBREF59 . We measured the normalized red, green, blue and the mean of original colors, and brightness and contrast relative to variations of luminance. We represent images in Hue-Saturation-Value color space that seems intuitive for humans, and measure mean and variance for saturation and hue. Saturation is defined as the difference in the intensities of the different light wavelengths that compose the color. Although hue is not interpretable, high saturation indicates vividness and chromatic purity which are more appealing to the human eye BIBREF8 . Colorfulness is measured as a difference against gray background BIBREF60 . Naturalness is a measure of the degree of correspondence between images and the human perception of reality BIBREF60 . In color reproduction, naturalness is measured from the mental recollection of the colors of familiar objects. Additionally, there is a tendency among vulnerable users to share sentimental quotes bearing negative emotions. We performed optical character recognition (OCR) with python-tesseract to extract text and their sentiment score. As illustrated in Table TABREF26 , vulnerable users tend to use less colorful (higher grayscale) profile as well as shared images to convey their negative feelings, and share images that are less natural (Figure FIGREF15 ). With respect to the aesthetic quality of images (saturation, brightness, and hue), depressed users use images that are less appealing to the human eye. We employ independent t-test, while adopting Bonferroni Correction as a conservative approach to adjust the confidence intervals. Overall, we have 223 features, and choose Bonferroni-corrected INLINEFORM0 level of INLINEFORM1 (*** INLINEFORM2 , ** INLINEFORM3 ).",
                        "Qualitative Language Analysis: The recent LIWC version summarizes textual content in terms of language variables such as analytical thinking, clout, authenticity, and emotional tone. It also measures other linguistic dimensions such as descriptors categories (e.g., percent of target words gleaned by dictionary, or longer than six letters - Sixltr) and informal language markers (e.g., swear words, netspeak), and other linguistic aspects (e.g., 1st person singular pronouns.)"
                    ],
                    "highlighted_evidence": [
                        "For capturing facial presence, we rely on BIBREF56 's approach that uses multilevel convolutional coarse-to-fine network cascade to tackle facial landmark localization.",
                        "Facial Expression:\n\nFollowing BIBREF8 's approach, we adopt Ekman's model of six emotions: anger, disgust, fear, joy, sadness and surprise, and use the Face++ API to automatically capture them from the shared images.",
                        "General Image Features:\n\nThe importance of interpretable computational aesthetic features for studying users' online behavior has been highlighted by several efforts BIBREF55 , BIBREF8 , BIBREF57 . ",
                        "Qualitative Language Analysis: The recent LIWC version summarizes textual content in terms of language variables such as analytical thinking, clout, authenticity, and emotional tone. ",
                        "It also measures other linguistic dimensions such as descriptors categories (e.g., percent of target words gleaned by dictionary, or longer than six letters - Sixltr) and informal language markers (e.g., swear words, netspeak), and other linguistic aspects (e.g., 1st person singular pronouns.)"
                    ]
                }
            ]
        },
        {
            "question": "How is the data annotated?",
            "answers": [
                {
                    "answer": "The data are self-reported by Twitter users and then verified by two human experts.",
                    "type": "abstractive"
                }
            ],
            "q_uid": "2cf8825639164a842c3172af039ff079a8448592",
            "evidence": [
                {
                    "raw_evidence": [
                        "Self-disclosure clues have been extensively utilized for creating ground-truth data for numerous social media analytic studies e.g., for predicting demographics BIBREF36 , BIBREF41 , and user's depressive behavior BIBREF46 , BIBREF47 , BIBREF48 . For instance, vulnerable individuals may employ depressive-indicative terms in their Twitter profile descriptions. Others may share their age and gender, e.g., \"16 years old suicidal girl\"(see Figure FIGREF15 ). We employ a huge dataset of 45,000 self-reported depressed users introduced in BIBREF46 where a lexicon of depression symptoms consisting of 1500 depression-indicative terms was created with the help of psychologist clinician and employed for collecting self-declared depressed individual's profiles. A subset of 8,770 users (24 million time-stamped tweets) containing 3981 depressed and 4789 control users (that do not show any depressive behavior) were verified by two human judges BIBREF46 . This dataset INLINEFORM0 contains the metadata values of each user such as profile descriptions, followers_count, created_at, and profile_image_url."
                    ],
                    "highlighted_evidence": [
                        "We employ a huge dataset of 45,000 self-reported depressed users introduced in BIBREF46 where a lexicon of depression symptoms consisting of 1500 depression-indicative terms was created with the help of psychologist clinician and employed for collecting self-declared depressed individual's profiles. A subset of 8,770 users (24 million time-stamped tweets) containing 3981 depressed and 4789 control users (that do not show any depressive behavior) were verified by two human judges BIBREF46 ."
                    ]
                }
            ]
        },
        {
            "question": "Where does the information on individual-level demographics come from?",
            "answers": [
                {
                    "answer": "From Twitter profile descriptions of the users.",
                    "type": "abstractive"
                }
            ],
            "q_uid": "36b25021464a9574bf449e52ae50810c4ac7b642",
            "evidence": [
                {
                    "raw_evidence": [
                        "Age Enabled Ground-truth Dataset: We extract user's age by applying regular expression patterns to profile descriptions (such as \"17 years old, self-harm, anxiety, depression\") BIBREF41 . We compile \"age prefixes\" and \"age suffixes\", and use three age-extraction rules: 1. I am X years old 2. Born in X 3. X years old, where X is a \"date\" or age (e.g., 1994). We selected a subset of 1061 users among INLINEFORM0 as gold standard dataset INLINEFORM1 who disclose their age. From these 1061 users, 822 belong to depressed class and 239 belong to control class. From 3981 depressed users, 20.6% disclose their age in contrast with only 4% (239/4789) among control group. So self-disclosure of age is more prevalent among vulnerable users. Figure FIGREF18 depicts the age distribution in INLINEFORM2 . The general trend, consistent with the results in BIBREF42 , BIBREF49 , is biased toward young people. Indeed, according to Pew, 47% of Twitter users are younger than 30 years old BIBREF50 . Similar data collection procedure with comparable distribution have been used in many prior efforts BIBREF51 , BIBREF49 , BIBREF42 . We discuss our approach to mitigate the impact of the bias in Section 4.1. The median age is 17 for depressed class versus 19 for control class suggesting either likely depressed-user population is younger, or depressed youngsters are more likely to disclose their age for connecting to their peers (social homophily.) BIBREF51",
                        "Gender Enabled Ground-truth Dataset: We selected a subset of 1464 users INLINEFORM0 from INLINEFORM1 who disclose their gender in their profile description. From 1464 users 64% belonged to the depressed group, and the rest (36%) to the control group. 23% of the likely depressed users disclose their gender which is considerably higher (12%) than that for the control class. Once again, gender disclosure varies among the two gender groups. For statistical significance, we performed chi-square test (null hypothesis: gender and depression are two independent variables). Figure FIGREF19 illustrates gender association with each of the two classes. Blue circles (positive residuals, see Figure FIGREF19 -A,D) show positive association among corresponding row and column variables while red circles (negative residuals, see Figure FIGREF19 -B,C) imply a repulsion. Our findings are consistent with the medical literature BIBREF10 as according to BIBREF52 more women than men were given a diagnosis of depression. In particular, the female-to-male ratio is 2.1 and 1.9 for Major Depressive Disorder and Dysthymic Disorder respectively. Our findings from Twitter data indicate there is a strong association (Chi-square: 32.75, p-value:1.04e-08) between being female and showing depressive behavior on Twitter."
                    ],
                    "highlighted_evidence": [
                        "We extract user's age by applying regular expression patterns to profile descriptions (such as \"17 years old, self-harm, anxiety, depression\") BIBREF41 . We compile \"age prefixes\" and \"age suffixes\", and use three age-extraction rules: 1. I am X years old 2. Born in X 3. X years old, where X is a \"date\" or age (e.g., 1994).",
                        "We selected a subset of 1464 users INLINEFORM0 from INLINEFORM1 who disclose their gender in their profile description."
                    ]
                }
            ]
        },
        {
            "question": "What is the source of the user interaction data? ",
            "answers": [
                {
                    "answer": "Sociability from ego-network on Twitter",
                    "type": "abstractive"
                }
            ],
            "q_uid": "98515bd97e4fae6bfce2d164659cd75e87a9fc89",
            "evidence": [
                {
                    "raw_evidence": [
                        "The recent advancements in deep neural networks, specifically for image analysis task, can lead to determining demographic features such as age and gender BIBREF13 . We show that by determining and integrating heterogeneous set of features from different modalities \u2013 aesthetic features from posted images (colorfulness, hue variance, sharpness, brightness, blurriness, naturalness), choice of profile picture (for gender, age, and facial expression), the screen name, the language features from both textual content and profile's description (n-gram, emotion, sentiment), and finally sociability from ego-network, and user engagement \u2013 we can reliably detect likely depressed individuals in a data set of 8,770 human-annotated Twitter users."
                    ],
                    "highlighted_evidence": [
                        "We show that by determining and integrating heterogeneous set of features from different modalities \u2013 aesthetic features from posted images (colorfulness, hue variance, sharpness, brightness, blurriness, naturalness), choice of profile picture (for gender, age, and facial expression), the screen name, the language features from both textual content and profile's description (n-gram, emotion, sentiment), and finally sociability from ego-network, and user engagement \u2013 we can reliably detect likely depressed individuals in a data set of 8,770 human-annotated Twitter users."
                    ]
                }
            ]
        },
        {
            "question": "What is the source of the textual data? ",
            "answers": [
                {
                    "answer": "Users' tweets",
                    "type": "abstractive"
                }
            ],
            "q_uid": "53bf6238baa29a10f4ff91656c470609c16320e1",
            "evidence": [
                {
                    "raw_evidence": [
                        "The recent advancements in deep neural networks, specifically for image analysis task, can lead to determining demographic features such as age and gender BIBREF13 . We show that by determining and integrating heterogeneous set of features from different modalities \u2013 aesthetic features from posted images (colorfulness, hue variance, sharpness, brightness, blurriness, naturalness), choice of profile picture (for gender, age, and facial expression), the screen name, the language features from both textual content and profile's description (n-gram, emotion, sentiment), and finally sociability from ego-network, and user engagement \u2013 we can reliably detect likely depressed individuals in a data set of 8,770 human-annotated Twitter users."
                    ],
                    "highlighted_evidence": [
                        "We show that by determining and integrating heterogeneous set of features from different modalities \u2013 aesthetic features from posted images (colorfulness, hue variance, sharpness, brightness, blurriness, naturalness), choice of profile picture (for gender, age, and facial expression), the screen name, the language features from both textual content and profile's description (n-gram, emotion, sentiment), and finally sociability from ego-network, and user engagement \u2013 we can reliably detect likely depressed individuals in a data set of 8,770 human-annotated Twitter users."
                    ]
                }
            ]
        },
        {
            "question": "What is the source of the visual data? ",
            "answers": [
                {
                    "answer": "Profile pictures from the Twitter users' profiles.",
                    "type": "abstractive"
                }
            ],
            "q_uid": "b27f7993b1fe7804c5660d1a33655e424cea8d10",
            "evidence": [
                {
                    "raw_evidence": [
                        "The recent advancements in deep neural networks, specifically for image analysis task, can lead to determining demographic features such as age and gender BIBREF13 . We show that by determining and integrating heterogeneous set of features from different modalities \u2013 aesthetic features from posted images (colorfulness, hue variance, sharpness, brightness, blurriness, naturalness), choice of profile picture (for gender, age, and facial expression), the screen name, the language features from both textual content and profile's description (n-gram, emotion, sentiment), and finally sociability from ego-network, and user engagement \u2013 we can reliably detect likely depressed individuals in a data set of 8,770 human-annotated Twitter users."
                    ],
                    "highlighted_evidence": [
                        "We show that by determining and integrating heterogeneous set of features from different modalities \u2013 aesthetic features from posted images (colorfulness, hue variance, sharpness, brightness, blurriness, naturalness), choice of profile picture (for gender, age, and facial expression), the screen name, the language features from both textual content and profile's description (n-gram, emotion, sentiment), and finally sociability from ego-network, and user engagement \u2013 we can reliably detect likely depressed individuals in a data set of 8,770 human-annotated Twitter users."
                    ]
                }
            ]
        }
    ]
}