doc_name|q_uid|question|answer_1|answer_2|answer_3
1912.01214|b6f15fb6279b82e34a5bf4828b7b5ddabfdf1d54|which multilingual approaches do they compare with?|BIBREF19, BIBREF20|multilingual NMT (MNMT) BIBREF19| 
1912.01214|f5e6f43454332e0521a778db0b769481e23e7682|what are the pivot-based baselines?|pivoting, pivoting$_{\rm m}$|firstly translates a source language into the pivot language which is later translated to the target language| 
1912.01214|9a05a5f4351db75da371f7ac12eb0b03607c4b87|which datasets did they experiment with?|Europarl, MultiUN|Europarl BIBREF31, MultiUN BIBREF32| 
1810.08699|18c5d366b1da8447b5404eab71f4cc658ba12e6f|what ner models were evaluated?|Stanford NER, spaCy 2.0 , recurrent model with a CRF top layer|Stanford NER, spaCy 2.0, recurrent model with a CRF top layer| 
1810.08699|b5e4866f0685299f1d7af267bbcc4afe2aab806f|what is the source of the news sentences?|ilur.am|links between Wikipedia articles to generate sequences of named-entity annotated tokens| 
1810.08699|1f085b9bb7bfd0d6c8cba1a9d73f08fcf2da7590|did they use a crowdsourcing platform for manual annotations?|No|No| 
1609.00425|b6ae8e10c6a0d34c834f18f66ab730b670fb528c|what are the topics pulled from Reddit?|politics, business, science, and AskReddit, and 1000 additional posts from the Reddit frontpage. |training data has posts from politics, business, science and other popular topics; the trained model is applied to millions of unannotated posts on all of Reddit| 
1609.00425|a87a009c242d57c51fc94fe312af5e02070f898b|What predictive model do they build?|logistic regression models|logistic regression models based on unigram bag-of-words features (BOW), sentiment signals (SENT), the linguistic features from our earlier analyses (LING), and combinations of these features.| 
1801.05147|2df4a045a9cd7b44874340b6fdf9308d3c55327a|What crowdsourcing platform is used?|They did not use any platform, instead they hired undergraduate students to do the annotation.| | 
1811.00383|a313e98994fc039a82aa2447c411dda92c65a470|How do they match words before reordering them?|CFILT-preorder system| | 
1811.00383|37861be6aecd9242c4fdccdfcd06e48f3f1f8f81|On how many language pairs do they show that preordering assisting language sentences helps translation quality?|5|Bengali, Gujarati, Marathi, Malayalam and Tamil are the primary source languages, and translation from these to Hindi constitute the child tasks.| 
1811.00383|7e62a53823aba08bc26b2812db016f5ce6159565|Which dataset(s) do they experiment with?|IITB English-Hindi parallel corpus BIBREF22, ILCI English-Hindi parallel corpus|IITB English-Hindi parallel corpus, ILCI English-Hindi parallel corpus| 
1909.09067|9eabb54c2408dac24f00f92cf1061258c7ea2e1a|Which information about text structure is included in the corpus?|paragraphs, lines, Information on physical page segmentation (for PDFs only), paragraph segmentation, and line segmentation|paragraph, lines, textspan element (paragraph segmentation, line segmentation, Information on physical page segmentation(for PDF only))| 
1909.09067|3d013f15796ae7fed5272183a166c45f16e24e39|Which information about typography is included in the corpus?|font type, font style, Information on the font type and font style (e.g., italics, bold print) of a token and its position on the physical page|font type and font style (e.g., italics, bold print) of a token and its position on the physical page (for PDFs only) was specified as attributes to the token elements of the tokens layer, A separate fonts layer was introduced to preserve detailed information on the font configurations referenced in the tokens layer| 
1704.06194|d3aa0449708cc861a51551b128d73e11d62207d2|What they use in their propsoed framework?|break the relation names into word sequences,  relation-level and word-level relation representations, bidirectional LSTMs (BiLSTMs),  residual learning method|break the relation names into word sequences for question-relation matching, build both relation-level and word-level relation representations, use deep bidirectional LSTMs (BiLSTMs) to learn different levels of question representations, residual learning method for sequence matching, a simple KBQA implementation composed of two-step relation detection| 
1704.06194|cfbec1ef032ac968560a7c76dec70faf1269b27c|What does KBQA abbreviate for|Knowledge Base Question Answering|Knowledge Base Question Answering | 
1704.06194|c0e341c4d2253eb42c8840381b082aae274eddad|What is te core component for KBQA?|answer questions by obtaining information from KB tuples |hierarchical matching between questions and relations with residual learning| 
1909.00512|1ec152119cf756b16191b236c85522afeed11f59|What experiments are proposed to test that upper layers produce context-specific embeddings?|They measure self-similarity, intra-sentence similarity and maximum explainable variance of the embeddings in the upper layers.|"They plot the average cosine similarity between uniformly random words increases exponentially from layers 8 through 12.  
They plot the average self-similarity of uniformly randomly sampled words in each layer of BERT, ELMo, and GPT-2 and shown that the higher layer produces more context-specific embeddings.
They plot that word representations in a sentence become more context-specific in upper layers, they drift away from one another."| 
2003.03106|6b53e1f46ae4ba9b75117fc6e593abded89366be|What are the other algorithms tested?|NER model, CRF classifier trained with sklearn-crfsuite, classifier has been developed that consists of regular-expressions and dictionary look-up|As the simplest baseline, a sensitive data recogniser and classifier, Conditional Random Fields (CRF), spaCy | 
2003.03106|c0bee6539eb6956a7347daa9d2419b367bd02064|Does BERT reach the best performance among all the algorithms compared?|No|No| 
2003.03106|3de0487276bb5961586acc6e9f82934ef8cb668c|What are the clinical datasets used in the paper?|MEDDOCAN, NUBes-PHI|MEDDOCAN, NUBes | 
1708.01464|113d791df6fcfc9cecfb7b1bebaf32cc2e4402ab|how is model compactness measured?|Using file size on disk|15.4 MB| 
1708.01464|0752d71a0a1f73b3482a888313622ce9e9870d6e|what was the baseline?|system presented by deri2016grapheme|wFST| 
1708.01464|55c8f7acbfd4f5cde634aaecd775b3bb32e9ffa3|what evaluation metrics were used?|Phoneme Error Rate (PER), Word Error Rate (WER), Word Error Rate 100 (WER 100)|PER, WER, WER 100| 
1708.01464|4eaf9787f51cd7cdc45eb85cf223d752328c6ee4|what datasets did they use?|the Carnegie Mellon Pronouncing Dictionary BIBREF12, the multilingual pronunciation corpus collected by deri2016grapheme , ranscriptions extracted from Wiktionary|multilingual pronunciation corpus collected by deri2016grapheme| 
2002.03407|31735ec3d83c40b79d11df5c34154849aeb3fb47|Who were the human evaluators used?|20 evaluators were recruited from our institution and asked to each perform 20 annotations|20 annotatos from author's institution| 
2002.03407|10d450960907091f13e0be55f40bcb96f44dd074|Is the template-based model realistic?  |Yes|Yes| 
2002.03407|b5608076d91450b0d295ad14c3e3a90d7e168d0e|Is the student reflection data very different from the newspaper data?  |Yes|Yes| 
2002.03407|c21b87c97d1afac85ece2450ee76d01c946de668|What is the recent abstractive summarization method in this paper?|pointer networks with coverage mechanism (PG-net)| pointer networks with coverage mechanism (PG-net)BIBREF0| 
1909.11687|d087539e6a38c42f0a521ff2173ef42c0733878e|Why are prior knowledge distillation techniques models are ineffective in producing student models with vocabularies different from the original teacher models?  |While there has been existing work on reducing NLP model vocabulary sizes BIBREF15, distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes.|distillation techniques cannot utilize these, since they require the student and teacher models to share the same vocabulary and output space. This profoundly limits their potential to further reduce model sizes.| 
1605.06083|7561a968470a8936d10e1ba722d2f38b5a9a4d38|What is the size of the dataset?|30,000|collection of over 30,000 images with 5 crowdsourced descriptions each| 
1605.06083|6d4400f45bd97b812e946b8a682b018826e841f1|Which methods are considered to find examples of biases and unwarranted inferences??|spot patterns by just looking at a collection of images, tag all descriptions with part-of-speech information, I applied Louvain clustering|"Looking for adjectives marking the noun ""baby"" and also looking for most-common adjectives related to certain nouns using POS-tagging"| 
1605.06083|26c2e1eb12143d985e4fb50543cf0d1eb4395e67|What biases are found in the dataset?|Ethnic bias|adjectives are used to create “more narrow labels [or subtypes] for individuals who do not fit with general social category expectations”| 
1804.05918|bd5bd1765362c2d972a762ca12675108754aa437|How much does this model improve state-of-the-art?|the basic model yields good performance for recognizing explicit discourse relations as well, which is comparable with previous best result (92.05% macro F1-score and 93.09% accuracy as reported in BIBREF11 )., full paragraph-level neural network model achieves the best macro-average F1-score of 48.82% in predicting implicit discourse relations, which outperforms previous neural tensor network models (e.g., BIBREF18 ) by more than 2 percents and outperforms the best previous system BIBREF19 by 1 percent., Then we also created ensemble models by applying majority voting to combine results of ten runs. From table 5 , each ensemble model obtains performance improvements compared with single model. The full model achieves performance boosting of (51.84 - 48.82 = 3.02) and (94.17 - 93.21 = 0.96) in macro F1-scores for predicting implicit and explicit discourse relations respectively. |1 percent| 
1809.04267|d9b6c61fc6d29ad399d27b931b6cb7b1117b314a|Where is a question generation model used?|The question generation model provides each candidate answer with a score by measuring semantic relevance between the question and the generated question based on the semantics of the candidate answer. |framework consisting of both a question answering model and a question generation model| 
1901.05287|d27438b11bc70e706431dda0af2b1c0b0d209f96|Were any of these tasks evaluated in any previous work?|Yes|Yes| 
1612.06685|8d4ac4afbf5b14f412171729ceb5e822afcfa3f4|Do they build a model to automatically detect demographic, lingustic or psycological dimensons of people?|No|No| 
1612.06685|3c93894c4baf49deacc6ed2a14ef5e0f13b7d96f|Which demographic dimensions of people do they obtain?|occupation, industry, profile information, language use, gender |density of users, gender distribution| 
1612.06685|07d15501a599bae7eb4a9ead63e9df3d55b3dc35|How do they obtain psychological dimensions of people?|using the Meaning Extraction Method| | 
1912.04961|99e78c390932594bd833be0f5c890af5c605d808|What is the baseline?|QA PGNet, Multi-decoder QA PGNet with lookup table embedding|QA PGNet and Multi-decoder QA PGNet| 
1912.04961|861187338c5ad445b9acddba8f2c7688785667b1|Is the data de-identified?|Yes|Yes| 
1912.04961|f161e6d5aecf8fae3a26374dcb3e4e1b40530c95|What embeddings are used?| simple lookup table embeddings learned from scratch, using high-performance contextual embeddings, which are ELMo BIBREF11, BERT BIBREF16 and ClinicalBERT BIBREF13|ELMO BIBREF11, BERT BIBREF12 and ClinicalBERT BIBREF13| 
1910.10781|12c50dea84f9a8845795fa8b8c1679328bd66246|What datasets did they use for evaluation?|CSAT dataset, 20 newsgroups, Fisher Phase 1 corpus|CSAT dataset , 20 newsgroups, Fisher Phase 1 corpus| 
1910.10781|0810b43404686ddfe4ca84783477ae300fdd2ea4|On top of BERT does the RNN layer work better or the transformer layer?|Transformer over BERT (ToBERT)|The transformer layer| 
1603.09631|455d4ef8611f62b1361be4f6387b222858bb5e56|How was this data collected?|CrowdFlower|The crowdsourcing platform CrowdFlower was used to obtain natural dialog data that prompted the user to paraphrase, explain, and/or answer a question from a Simple questions BIBREF7 dataset. The CrowdFlower users were restricted to English-speaking countries to avoid dialogs  with poor English.| 
1603.09631|bc16ce6e9c61ae13d46970ebe6c4728a47f8f425|What is the average length of dialog?|4.49 turns|4.5 turns per dialog (8533 turns / 1900 dialogs)| 
1911.06964|1ff0fccf0dca95a6630380c84b0422bed854269a|How are models evaluated in this human-machine communication game?|by training an autocomplete system on 500K randomly sampled sentences from Yelp reviews|efficiency of a communication scheme $(q_{\alpha },p_{\beta })$ by the retention rate of tokens, which is measured as the fraction of tokens that are kept in the keywords, accuracy of a scheme is measured as the fraction of sentences generated by greedily decoding the model that exactly matches the target sentence| 
1911.06964|3d7d865e905295d11f1e85af5fa89b210e3e9fdf|How many participants were trying this communication game?|100 |100 crowdworkers | 
1911.06964|2ad4d3d222f5237ed97923640bc8e199409cbe52|What user variations have been tested?|completion times and accuracies | | 
1911.06964|3fad42be0fb2052bb404b989cc7d58b440cd23a0|What are the baselines used?|Unif and Stopword|Unif and Stopword| 
2001.02284|ee417fea65f9b1029455797671da0840c8c1abbe|Do they use off-the-shelf NLP systems to build their assitant?|No|No| 
2001.02284|ca5a82b54cb707c9b947aa8445aac51ea218b23a|How does the IPA label data after interacting with users?|It defined a sequence labeling task to extract custom entities from user input and label the next action (out of 13  custom actions defined).|Plain dialogues with unique dialogue indexes, Plain Information Dictionary information (e.g., extracted entities) collected for the whole dialogue, Pairs of questions (i.e., user requests) and responses (i.e., bot responses), Triples in the form of (User Request, Next Action, Response)| 
2001.02284|da55bd769721b878dd17f07f124a37a0a165db02|What kind of repetitive and time-consuming activities does their assistant handle?| What kind of topic (or sub-topic) a student has a problem with, At which examination mode (i.e., quiz, chapter level training or exercise, section level training or exercise, or final examination) the student is working right now,  the exact question number and exact problem formulation| | 
2002.01664|feb448860918ef5b905bb25d7b855ba389117c1f|How was the audio data gathered?|Through the All India Radio new channel where actors read news.| $\textbf {All India Radio}$ news channel| 
2002.01664|4bc2784be43d599000cb71d31928908250d4cef3|What is the GhostVLAD approach?|extension of the NetVLAD, adds Ghost clusters along with the NetVLAD clusters|An extension of NetVLAD which replaces hard assignment-based clustering with soft assignment-based clustering with the additon o fusing Ghost clusters to deal with noisy content.| 
1808.09111|6424e442b34a576f904d9649d63acf1e4fdefdfc|What datasets do they evaluate on?| Wall Street Journal (WSJ) portion of the Penn Treebank| | 
1808.09111|5eabfc6cc8aa8a99e6e42514ef9584569cb75dec|Do they evaluate only on English datasets?|Yes| | 
1808.09111|887c6727e9f25ade61b4853a869fe712fe0b703d|What is the invertibility condition?|The neural projector must be invertible.|we constrain our neural projector with two requirements: (1) INLINEFORM0 and (2) INLINEFORM1 exists| 
1906.08593|6236762b5631d9e395f81e1ebccc4bf3ab9b24ac|Do they show on which examples how conflict works better than attention?|Yes|Yes| 
1906.08593|31d695ba855d821d3e5cdb7bea638c7dbb7c87c7|Which neural architecture do they use as a base for their attention conflict mechanisms?|GRU-based encoder, interaction block, and classifier consisting of stacked fully-connected layers.|two stacked GRU layers, attention for one model while for the another one it consists of attention and conflict combined, fully-connected layers| 
1906.08593|b14217978ad9c3c9b6b1ce393b1b5c6e7f49ecab|On which tasks do they test their conflict method?|Task 1: Quora Duplicate Question Pair Detection, Task 2: Ranking questions|Quora Duplicate Question Pair Detection, Ranking questions in Bing's People Also Ask| 
1809.00540|2c78993524ca62bf1f525b60f2220a374d0e3535|What are the sources of the datasets?|rupnik2016news|rupnik2016news, Deutsche Welle's news website| 
2004.03354|16535db1d73a9373ffe9d6eedaa2369cefd91ac4|What in-domain text did they use?|PubMed+PMC|PubMed+PMC (the data used for BioBERTv1.0) and/or CORD-19 (Covid-19 Open Research Dataset)| 
1611.04798|41ac23e32bf208b69414f4b687c4f324c6132464|Which languages do they test on for the under-resourced scenario?|English, German|small portion of the large parallel corpus for English-German is used as a simulation| 
1912.13337|e97186c51d4af490dba6faaf833d269c8256426c|Are the automatically constructed datasets subject to quality control?|No|No| 
1912.13337|5bb3c27606c59d73fd6944ba7382096de4fa58d8|Do they focus on Reading Comprehension or multiple choice question answering?|MULTIPLE CHOICE QUESTION ANSWERING|multiple-choice| 
1912.13337|8de9f14c7c4f37ab103bc8a639d6d80ade1bc27b|After how many hops does accuracy decrease?|1-hop links to 2-hops|one additional hop| 
1912.13337|85590bb26fed01a802241bc537d85ba5ef1c6dc2|How do they control for annotation artificats?| we use several of the MCQA baseline models first introduced in BIBREF0|Choice-Only model, which is a variant of the well-known hypothesis-only baseline, Choice-to-choice model, tries to single out a given answer choice relative to other choices, Question-to-choice model, in contrast, uses the contextual representations for each question and individual choice and an attention model Att model to get a score| 
1912.13337|75ff6e425ce304a35f18c0230c0d13d3913a31a9|Is WordNet useful for taxonomic reasoning for this task?|Yes| | 
1809.01541|5cb610d3d5d7d447b4cd5736d6a7d8262140af58|How do they perform multilingual training?|Multilingual training is performed by randomly alternating between languages for every new minibatch|by randomly alternating between languages for every new minibatch| 
1809.01541|b9d168da5321a7d7b812c52bb102a05210fe45bd|Does the model have attention?|Yes|Yes| 
1809.01541|0c234db3b380c27c4c70579a5d6948e1e3b24ff1|What architecture does the decoder have?|LSTM|LSTM| 
1809.01541|fa527becb8e2551f4fd2ae840dbd4a68971349e0|What architecture does the encoder have?|LSTM|LSTM| 
1809.09194|45e9533586199bde19313cd43b3d0ecadcaf7a33|Do they use attention?|Yes|Yes| 
1809.09194|a5e49cdb91d9fd0ca625cc1ede236d3d4672403c|What is the architecture of the span detector?|adopt a multi-turn answer module for the span detector BIBREF1| | 
1604.05372|aefa333b2cf0a4000cd40566149816f5b36135e7|What evaluation metric do they use?|Accuracy|ratio of correct `translations'| 
2002.08795|eb2d5edcdfe18bd708348283f92a32294bb193a5|What are the baselines?|a score of 40|KG-A2C, A2C, A2C-chained, A2C-Explore| 
2002.08795|88ab7811662157680144ed3fdd00939e36552672|What are the two new strategies?|a method that detects bottlenecks in text-games using the overall reward gained and the knowledge graph state, to leverage knowledge graphs to improve existing exploration algorithms for dealing with combinatorial action-space|KG-A2C-chained, KG-A2C-Explore| 
1802.06024|8f16dc7d7be0d284069841e456ebb2c69575b32b|What baseline is used in the experiments?|versions of LiLi|various versions of LiLi as baselines, Single, Sep, F-th, BG, w/o PTS| 
1802.06024|a7d020120a45c39bee624f65443e09b895c10533|In what way does LiLi imitate how humans acquire knowledge and perform inference during an interactive conversation?|newly acquired facts are retained in the KB and used in inference for future queries, and that the accumulated knowledge in addition to the updated KB including past inference performances are leveraged to guide future interaction and learning|Whenever we encounter an unknown concept or relation while answering a query, we perform inference using our existing knowledge. If our knowledge does not allow us to draw a conclusion, we typically ask questions to others to acquire related knowledge and use it in inference. | 
1802.06024|585626d18a20d304ae7df228c2128da542d248ff|What metrics are used to establish that this makes chatbots more knowledgeable and better at learning and conversation? |Coverage, Avg. MCC and avg. +ve F1 score|strategy formulation ability, we introduce a measure called Coverage( INLINEFORM0 ), To evaluate the predictive performance, we use Avg. MCC and avg. +ve F1 score| 
1802.06024|bfc2dc913e7b78f3bd45e5449d71383d0aa4a890|What are the components of the general knowledge learning engine?|"Answer with content missing: (list)
LiLi should have the following capabilities:
1. to formulate an inference strategy for a given query that embeds processing and interactive actions.
2. to learn interaction behaviors (deciding what to ask and when to ask the user).
3. to leverage the acquired knowledge in the current and future inference process.
4. to perform 1, 2 and 3 in a lifelong manner for continuous knowledge learning."|Knowledge Store (KS) , Knowledge Graph ( INLINEFORM0 ),  Relation-Entity Matrix ( INLINEFORM2 ), Task Experience Store ( INLINEFORM15 ), Incomplete Feature DB ( INLINEFORM29 )| 
1809.00530|b46c0015a122ee5fb95c2a45691cb97f80de1bb6|What is the architecture of the model?|one-layer CNN structure from previous works BIBREF22 , BIBREF4| one-layer CNN| 
1809.00530|5b7a4994bfdbf8882f391adf1cd2218dbc2255a0|What are the baseline methods?|(1) Naive, (2) mSDA BIBREF7, (3) NaiveNN, (4) AuxNN BIBREF4, (5) ADAN BIBREF16, (6) MMD|non-domain-adaptive baseline with bag-of-words representations and SVM classifier, mSDA, non-domain-adaptive CNN trained on source domain, neural model that exploits auxiliary tasks, adversarial training to reduce representation difference between domains, variants of deep CNNs are used for encoding images and the MMDs of multiple layers are jointly minimized| 
1710.07960|0ba3ea93eef5660a79ea3c26c6a270eac32dfa4c|Did they use a crowdsourcing platform for annotations?|No| | 
1710.07960|5e324846a99a5573cd2e843d1657e87f4eb22fa6|How do they deal with unknown distribution senses?|The Näive-Bayes classifier is corrected so it is not biased to most frequent classes|Bayesian classifier has been modified, removing the bias towards frequent labels in the training data| 
1912.03804|2ccc26e11df4eb26fcccdd1f446dc749aff5d572|Do they report results only on English data?|Yes|Yes| 
1912.03804|f318a2851d7061f05a5b32b94251f943480fbd15|What conclusions do the authors draw from their finding that the emotional appeal of ISIS and Catholic materials are similar?|both corpuses used words that aim to inspire readers while avoiding fear, actual words that lead to these effects are very different in the two contexts, our findings indicate that, using proper methods, automated analysis of large bodies of textual data can provide novel insight insight into extremist propaganda|By comparing scores for each word calculated using Depechemood dictionary and normalize emotional score for each article, they found Catholic and ISIS materials show similar scores| 
1912.03804|6bbbb9933aab97ce2342200447c6322527427061|How id Depechemood trained?|By multiplying crowd-annotated document-emotion matrix with emotion-word matrix. |researchers asked subjects to report their emotions after reading each article, multiplied the document-emotion matrix and word-document matrix to derive emotion-word matrix for these words, Depechemood simply creates dictionaries of words where each word has scores between 0 and 1 for all of these 8 emotion categories| 
1912.03804|2007bfb8f66e88a235c3a8d8c0a3b3dd88734706|How are similarities and differences between the texts from violent and non-violent religious groups analyzed?|By using topic modeling and unsupervised emotion detection on ISIS materials and articles from Catholic women forum|A comparison of common words, We aggregated the score for each word and normalized each article by emotions. To better compare the result, we added a baseline of 100 random articles from a Reuters news dataset as a non-religious general resource| 
1912.03804|d859cc37799a508bbbe4270ed291ca6394afce2c|How are prominent topics idenified in Dabiq and Rumiyah?|LDA, non-negative matrix factorization (NMF)|Using NMF based topic modeling and their coherence prominent topics are identified| 
1912.08960|50e80cfa84200717921840fddcf3b051a9216ad8|Are the images from a specific domain?|Yes|Yes| 
1912.08960|63a1cbe66fd58ff0ead895a8bac1198c38c008aa|Which existing models are evaluated?|Show&Tell and LRCN1u|Show&Tell model, LRCN1u| 
1912.08960|509af1f11bd6f3db59284258e18fdfebe86cae47|How is diversity measured?|diversity score as the ratio of observed number versus optimal number| we look at language constructions used and compute the corresponding diversity score as the ratio of observed number versus optimal number| 
2002.11910|23e16c1173b7def2c5cb56053b57047c9971e3bb|What state-of-the-art deep neural network is used?|LSTM model|BIBREF15, BIBREF19, BIBREF20 | 
2002.11910|d78f7f84a76a07b777d4092cb58161528ca3803c|What boundary assembling method is used?|This motivates us to carry out a backward greedy search over each sentence's label sequence to identify word boundaries. If two words segmented in a sentence are identified as nouns, and one word is immediately before the other, we assemble their boundaries, creating a new word candidate for entity recognition.|backward greedy search over each sentence's label sequence to identify word boundaries| 
1909.09587|009ce6f2bea67e7df911b3f93443b23467c9f4a1|What model is used as a baseline?  |pre-trained multi-BERT|QANet , BIBREF14,  fine-tuned a BERT model| 
1909.09587|55569d0a4586d20c01268a80a7e31a17a18198e2|what does the model learn in zero-shot setting?|we simply adopted the official training script of BERT, with default hyperparameters, to fine-tune each model until training loss converged| | 
1802.07862|7cd22ca9e107d2b13a7cc94252aaa9007976b338|Do they inspect their model to see if their model learned to associate image parts with words related to entities?|Yes|Yes| 
1802.07862|adbf33c6144b2f5c40d0c6a328a92687a476f371|Does their NER model learn NER from both text and images?|Yes|Yes| 
1802.07862|f7a89b9cd2792f23f2cb43d50a01b8218a6fbb24|Which types of named entities do they recognize?|PER, LOC, ORG, MISC|PER, LOC, ORG, MISC| 
1802.07862|a0543b4afda15ea47c1e623c7f00d4aaca045be0|Can named entities in SnapCaptions be discontigious?|No| | 
1802.07862|1591068b747c94f45b948e12edafe74b5e721047|How large is their MNER SnapCaptions dataset?|10K user-generated image (snap) and textual caption pairs|10000| 
2004.01853|193ee49ae0f8827a6e67388a10da59e137e7769f|What is masked document generation?|A task for seq2seq model pra-training that recovers a masked document to its original form.|recovers a masked document to its original form| 
2004.01853|ed2eb4e54b641b7670ab5a7060c7b16c628699ab|Which of the three pretraining tasks is the most helpful?|SR|SR| 
1710.03348|beac555c4aea76c88f19db7cc901fa638765c250|What useful information does attention capture?|it captures other information rather than only the translational equivalent in the case of verbs|Alignment points of the POS tags.| 
1710.03348|91e326fde8b0a538bc34d419541b5990d8aae14b|What datasets are used?|WMT15 German-to-English, RWTH German-English dataset|RWTH German-English dataset| 
1612.03226|f94b53db307685d572aefad52cd55f53d23769c2|How do they calculate variance from the model outputs?|reducing the variance of an estimator,  EGL method in BIBREF3 is almost the same as Eq. ( EQREF8 ), except the gradient's norm is not squared in BIBREF3| Fisher Information Ratio| 
1612.03226|aa7d327ef98f9f9847b447d4def04889b4508d7a|How much data samples do they start with before obtaining the initial model labels?|1,700-hour ( INLINEFORM2 1.1M instances) unlabeled dataset|INLINEFORM2 is queried for the “most informative” instance(s) INLINEFORM3| 
1612.03226|b8d7d055ddb94f5826a9aad7479b4a92a9c8a2f0|Which model do they use for end-to-end speech recognition?|RNN| Recurrent Neural Network (RNN)| 
1612.03226|551457ed34ca7fc0878c85bc664b135c21059b58|Which dataset do they use?|190 hours ( INLINEFORM1 100K instances)|trained on 190 hours ( INLINEFORM1 100K instances) of transcribed speech data, selects a subset of a 1,700-hour ( INLINEFORM2 1.1M instances) unlabeled dataset| 
1809.01202|a4d115220438c0ded06a91ad62337061389a6747|What types of social media did they consider?|Facebook status update messages|Facebook status update messages| 
1909.02027|2c7e94a65f5f532aa31d3e538dcab0468a43b264|How was the dataset annotated?|intents are annotated manually with guidance from queries collected using a scoping crowdsourcing task|manually | 
1909.02027|149da739b1c19a157880d9d4827f0b692006aa2c|Which classifiers are evaluated?|SVM, MLP, FastText, CNN, BERT, Google's DialogFlow, Rasa NLU|SVM, MLP, FastText, CNN, BERT, DialogFlow, Rasa NLU| 
1909.02027|27de1d499348e17fec324d0ef00361a490659988|What is the size of this dataset?|23,700 | 23,700 queries, including 22,500 in-scope queries covering 150 intents, which can be grouped into 10 general domains and 1,200 out-of-scope queries.| 
1909.02027|cfcdd73e712caf552ba44d0aa264d8dace65a589|Where does the data come from?|crowsourcing platform|"For ins scope data collection:crowd workers which provide questions and commands related to topic domains and additional data the rephrase and scenario crowdsourcing tasks proposed by BIBREF2 is used. 
For out of scope data collection:  from workers mistakes-queries written for one of the 150 intents that did not actually match any of the intents and using scoping and scenario tasks with prompts based on topic areas found on Quora, Wikipedia, and elsewhere."| 
1911.02855|23b2901264bda91045258b5d4120879ae292e950|What are method improvements of F1 for paraphrase identification?|Using DSC loss improves the F1 score by +0.58 for MRPC and +0.73 for QQP|+0.58| 
1911.02855|b5bc34e1e381dbf972d0b594fe8c66ff75305d71|What are method's improvements of F1 for NER task for English and Chinese datasets?|English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively, Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively|For English datasets including CoNLL2003 and OntoNotes5.0, our proposed method outperforms BERT-MRCBIBREF38 by +0.29 and +0.96 respectively., huge performance boosts on Chinese datasets, achieving F1 improvements by +0.97 and +2.36 on MSRA and OntoNotes4.0, respectively| 
1911.02855|72f7ef55e150e16dcf97fe443aff9971a32414ef|What are method's improvements of F1 w.r.t. baseline BERT tagger for Chinese POS datasets?|+1.86 in terms of F1 score on CTB5, +1.80 on CTB6, +2.19 on UD1.4| +1.86| 
1911.02855|20e38438471266ce021817c6364f6a46d01564f2|How are weights dynamically adjusted?|One can think $(1-p_{i1})$ as a weight associated with each example, which changes as training proceeds. The intuition of changing $p_{i1}$ to $(1-p_{i1}) p_{i1}$ is to push down the weight of easy examples. For easy examples whose probability are approaching 0 or 1, $(1-p_{i1}) p_{i1}$ makes the model attach significantly less focus to them. Figure FIGREF23 gives gives an explanation from the perspective in derivative: the derivative of $\frac{(1-p)p}{1+(1-p)p}$ with respect to $p$ approaches 0 immediately after $p$ approaches 0, which means the model attends less to examples once they are correctly classified.|associates each training example with a weight in proportion to $(1-p)$, and this weight dynamically changes as training proceeds| 
1906.01081|28067da818e3f61f8b5152c0d42a531bf0f987d4|Ngrams of which length are aligned using PARENT?|Answer with content missing: (Parent subsections) combine precisions for n-gram orders 1-4| | 
1906.01081|bf3b27a4f4be1f9ae31319877fd0c75c03126fd5|How many people participated in their evaluation study of table-to-text models?|about 500| | 
1611.01576|2f901dab6b757e12763b23ae8b37ae2e517a2271|What languages pairs are used in machine translation?|German–English|German–English| 
1611.01576|b591853e938984e6069d738371500ebdec50d256|What sentiment classification dataset is used?|the IMDb movie review dataset BIBREF17|IMDb movie review| 
1611.01576|a130306c6662ff489df13fb3f8faa7cba8c52a21|What pooling function is used?|dynamic average pooling| f-pooling, fo-pooling, and ifo-pooling | 
1904.09535|b1cf5739467ba90059add58d11b73d075a11ec86|Do they report results only on English?|Yes| | 
1904.09535|2ea4347f1992b0b3958c4844681ff0fe4d0dd1dd|What neural network modules are included in NeuronBlocks?|Embedding Layer, Neural Network Layers, Loss Function, Metrics|Embedding Layer, Neural Network Layers, Loss Function, Metrics| 
1904.09535|4f253dfced6a749bf57a1b4984dc962ce9550184|How do the authors evidence the claim that many engineers find it a big overhead to choose from multiple frameworks, models and optimization techniques?|By conducting a survey among engineers| | 
1911.03059|dc1cec824507fc85ac1ba87882fe1e422ff6cffb|what datasets did they use?|Dataset of total 3500 questions from the Internet and other sources such as books of general knowledge questions, history, etc.|3500 questions collected from the internet and books.| 
1911.03059|f428618ca9c017e0c9c2a23515dab30a7660f65f|what ml based approaches were compared?|Multi-Layer Perceptron (MLP), Naive Bayes Classifier (NBC), Support Vector Machine (SVM), Gradient Boosting Classifier (GBC), Stochastic Gradient Descent (SGD), K Nearest Neighbour (K-NN) and Random Forest (RF)|Multi-Layer Perceptron, Naive Bayes Classifier, Support Vector Machine, Gradient Boosting Classifier, Stochastic Gradient Descent, K Nearest Neighbour, Random Forest| 
1706.08198|8ce11515634236165cdb06ba80b9a36a8b9099a2|Is pre-training effective in their evaluation?|Yes|Yes| 
1706.08198|6024039bbd1118c5dab86c41cce1175d99f10a25|What parallel corpus did they use?|Asian Scientific Paper Excerpt Corpus (ASPEC) BIBREF0, NTCIR PatentMT Parallel Corpus BIBREF1|Asian Scientific Paper Excerpt Corpus, NTCIR PatentMT Parallel Corpus | 
1909.08089|b66c9a4021b6c8529cac1a2b54dacd8ec79afa5f|What do they mean by global and local context?|global (the whole document), local context (e.g., the section/topic)|global (the whole document) and the local context (e.g., the section/topic) | 
1910.09982|6bfba3ddca5101ed15256fca75fcdc95a53cece7|What are the 18 propaganda techniques?|Loaded language, Name calling or labeling, Repetition, Exaggeration or minimization, Doubt, Appeal to fear/prejudice, Flag-waving, Causal oversimplification, Slogans,  Appeal to authority, Black-and-white fallacy, dictatorship, Thought-terminating cliché, Whataboutism, Reductio ad Hitlerum, Red herring, Bandwagon, Obfuscation, intentional vagueness, confusion, Straw man|1. Loaded language, 2. Name calling or labeling, 3. Repetition, 4. Exaggeration or minimization, 5. Doubt, 6. Appeal to fear/prejudice, 7. Flag-waving, 8. Causal oversimplification, 9. Slogans, 10. Appeal to authority, 11. Black-and-white fallacy, dictatorship, 12. Thought-terminating cliché, 13. Whataboutism, 14. Reductio ad Hitlerum, 15. Red herring, 16. Bandwagon, 17. Obfuscation, intentional vagueness, confusion, 18. Straw man| 
1910.09982|df5a4505edccc0ee11349ed6e7958cf6b84c9ed4|What dataset was used?| news articles in free-text format|collected from 36 propagandist and 12 non-propagandist news outlets and then annotated by professional annotators| 
1910.09982|fd753ab5177d7bd27db0e0afc12411876ee607df|What was the baseline for this task?|The baseline system for the SLC task is a very simple logistic regression classifier with default parameters. The baseline for the FLC task generates spans and selects one of the 18 techniques randomly.|SLC task is a very simple logistic regression classifier, FLC task generates spans and selects one of the 18 techniques randomly| 
1609.00559|88e62ea7a4d1d2921624b8480b5c6b50cfa5ad42|What is a second order co-ocurrence matrix?|frequencies of the other words which occur with both of them (i.e., second order co–occurrences)|The matrix containing co-occurrences of the words which occur with the both words of every given pair of words.| 
1609.00559|4dcf67b5e7bd1422e7e70c657f6eacccd8de06d3|How many humans participated?|16| | 
1604.00727|784ce5a983c5f2cc95a2c60ce66f2a8a50f3636f|Do the authors also try the model on other datasets?|No|No| 
1604.00727|7705dd04acedaefee30d8b2c9978537afb2040dc|What word level and character level model baselines are used?|None|Word-level Memory Neural Networks (MemNNs) proposed in Bordes et al. (2015)| 
1612.02482|0ee73909ac638903da4a0e5565c8571fc794ab96|How were the human judgements assembled?|50 human annotators ranked a random sample of 100 translations by Adequacy, Fluency and overall ranking on a 5-point scale.|adequacy, precision and ranking values| 
1608.01084|1f07e837574519f2b696f3d6fa3230af0b931e5d|Did they only experiment with one language pair?|Yes|Yes| 
1904.10503|729694a9fe1e05d329b7a4078a596fe606bc5a95|What results do they achieve using their proposed approach?|F-1 score on the OntoNotes is 88%, and it is 53% on Wiki (gold).| total F-1 score on the OntoNotes dataset is 88%, total F-1 cross-validation score on the 112 class Wiki(gold) dataset is 53%| 
1904.10503|1c997c268c68149ae6fb43d83ffcd53f0e7fe57e|How do they combine a deep learning model with a knowledge base?|Entities from a deep learning model are linked to the related entities from a knowledge base by a lookup.|ELMo embeddings are then used with a residual LSTM to learn informative morphological representations from the character sequence of each token| 
1912.01772|5cc2daca2a84ddccba9cdd9449e51bb3f64b3dde|What are the models used for the baseline of the three NLP tasks?|state-of-the-art Transformer architecture, Kaldi, speech clustergen statistical speech synthesizer|For speech synthesis, they build a speech clustergen statistical speech synthesizer BIBREF9. For speech recognition, they use Kaldi BIBREF11. For Machine Translation, they use a Transformer architecture from BIBREF15.| 
1603.01987|6a633811019e9323dc8549ad540550d27aa6d972|Is it valid to presume a bad medical wikipedia article should not contain much domain-specific jargon?|No| | 
1908.06941|6b9b9e5d154cb963f6d921093539490daa5ebbae|What novel PMI variants are introduced?|clipped PMI; NNEGPMI|clipped $\mathit {PMI}$, $\mathit {NNEGPMI}$| 
1908.06941|bc4dca3e1e83f3b4bbb53a31557fc5d8971603b2|What semantic and syntactic tasks are used as probes?|Word Content (WC) probing task, Depth (Dep) and Top Constituent (TopC) (of the input sentence's constituent parse tree) probing tasks|SimLex, Rare Word, Google Semantic, Semantic Textual Similarity, Word Content (WC) probing, Google Syntactic analogies, Depth, Top Constituent, part-of-speech (POS) tagging| 
1908.06941|d46c0ea1ba68c649cc64d2ebb6af20202a74a3c7|What are the disadvantages to clipping negative PMI?|It may lead to poor rare word representations and word analogies.| | 
1908.06941|6844683935d0d8f588fa06530f5068bf3e1ed0c0|Why are statistics from finite corpora unreliable?|$\mathit {PMI}(w,c)$ goes to negative infinity when the word-context pair $(w,c)$ does not appear in the training corpus|A finite corpora may entirely omit rare word combinations| 
1702.03856|8acab64ba72831633e8cc174d5469afecccf3ae9|what is the domain of the corpus?|telephone calls| | 
1702.03856|53aa07cc4cc4e7107789ae637dbda8c9f6c1e6aa|what challenges are identified?|Assigning wrong words to a cluster, Splitting words across different clusters, sparse, giving low coverage|low coverage of audio, difficulty in cross-speaker clustering| 
1702.03856|72755c2d79210857cfff60bfbcb55f83c71ada51|what is the size of the speech corpus?|104 telephone calls, transcripts contain 168,195 Spanish word tokens,  translations contain 159,777 English word tokens|104 telephone calls, which pair 11 hours of audio| 
1904.01548|7d2f812cb345bb3ab91eb8cbbdeefd4b58f65569|Which two pairs of ERPs from the literature benefit from joint training?|"Answer with content missing: (Whole Method and Results sections) Self-paced reading times widely benefit ERP prediction, while eye-tracking data seems to have more limited benefit to just the ELAN, LAN, and PNP ERP components.
Select:
- ELAN, LAN
- PNP ERP"| | 
1904.01548|bd6dc38a9ac8d329114172194b0820766458dacc|What datasets are used?|"Answer with content missing: (Whole Method and Results sections) The primary dataset we use is the ERP data collected and computed by Frank et al. (2015), and we also use behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013) which were collected on the same set of 205 sentences.
Select:
- ERP data collected and computed by Frank et al. (2015)
- behavioral data (eye-tracking data and self-paced reading times) from Frank et al. (2013)"|the ERP data: BIBREF0| 
1606.03676|3ddff6b707767c3dd54d7104fe88b628765cae58|which datasets did they experiment with?|"Universal Dependencies v1.2 treebanks for the following 16 languages: Bulgarian, Croatian, Czech, Danish, English, French, German,
Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish, and Swedish"|Universal Dependencies v1.2 treebanks BIBREF21 , hereafter UD1.2| 
1606.03676|0a5ffe4697913a57fda1fd5a188cd5ed59bdc5c7|which languages are explored?|Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish and Swedish|Bulgarian, Croatian, Czech, Danish, English, French, German, Indonesian, Italian, Norwegian, Persian, Polish, Portuguese, Slovenian, Spanish , Swedish| 
1911.03343|78292bc57ee68fdb93ed45430d80acca25a9e916|How did they extend LAMA evaluation framework to focus on negation?|To this end, we introduce the negated LAMA dataset. We construct it by simply inserting negation elements (e.g., “not”) in LAMA cloze statement|Create the negated LAMA dataset and  query the pretrained language models with both original LAMA and negated LAMA statements and compare their predictions.| 
1712.00991|aa6d956c2860f58fc9baea74c353c9d985b05605|What evaluation metrics were used for the summarization task?|ROUGE BIBREF22 unigram score|ROUGE| 
1712.00991|4c18081ae3b676cc7831403d11bc070c10120f8e|What clustering algorithms were used?|CLUTO, Carrot2 Lingo|simple clustering algorithm which uses the cosine similarity between word embeddings| 
1712.00991|e025061e199b121f2ac8f3d9637d9bf987d65cd5|What is the average length of the sentences?|15.5|average:15.5| 
1712.00991|61652a3da85196564401d616d251084a25ab4596|What is the size of the real-life dataset?|26972|26972 sentences| 
1805.08241|14b74ad5a6f5b0506511c9b454e9c464371ef8c4|What are the language pairs explored in this paper?|De-En, Ja-En, Ro-En|De-En, Ja-En, Ro-En| 
1907.04433|5f2bade0881c719ab026bc2e2962e2ada96cdb25|Do they experiment with the toolkits?|Yes|Yes| 
2003.04642|5c88d601e8fca96bffebfa9ef22331ecf31c6d75|Have they made any attempt to correct MRC gold standards according to their findings? |Yes|No| 
2003.04642|71bd5db79635d48a0730163a9f2e8ef19a86cd66|What features are absent from MRC gold standards that can result in potential lexical ambiguity?|Restrictivity , Factivity , Coreference |semantics-altering grammatical modifiers| 
2001.09215|bcc0cd4e262f2db4270429ab520971bcf39414cf|How many tweets were collected?|$19,300$, added 2500 randomly sampled tweets|$19,300$ tweets| 
2001.09215|f641f561ad2ea2794a52e4e4bdd62e1f353ab797|What language is explored in this paper?|English language| | 
1909.07575|af34051bf3e628c1e2a00b110bb84e5f018b419f|What are the baselines?|Vanilla ST baseline, encoder pre-training, in which the ST encoder is initialized from an ASR model, decoder pre-training, in which the ST decoder is initialized from an MT model, encoder-decoder pre-training, where both the encoder and decoder are pre-trained, many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models, Triangle+pre-train: BIBREF18 DBLP:conf/naacl/AnastasopoulosC18 proposed a triangle multi-task strategy for speech translation|Vanilla ST baseline, Pre-training baselines, Multi-task baselines, Many-to-many+pre-training, Triangle+pre-train|"Vanilla ST baseline: The vanilla ST BIBREF9 has only a speech encoder and a decoder. It is trained from scratch on the ST-TED corpus.

Pre-training baselines: We conduct three pre-training baseline experiments: 1) encoder pre-training, in which the ST encoder is initialized from an ASR model; 2) decoder pre-training, in which the ST decoder is initialized from an MT model; and 3) encoder-decoder pre-training, where both the encoder and decoder are pre-trained. The ASR model has the same architecture with vanilla ST model, trained on the mixture of ST-TED and TED-LIUM2 corpus. The MT model has a text encoder and decoder with the same architecture of which in TCEN. It is first trained on WMT data (out-of-domain) and then fine-tuned on in-domain data.

Multi-task baselines: We also conduct three multi-task baseline experiments including one-to-many setting, many-to-one setting, and many-to-many setting. In the first two settings, we train the model with $\alpha _{st}=0.75$ while $\alpha _{asr}=0.25$ or $\alpha _{mt}=0.25$. For many-to-many setting, we use $\alpha _{st}=0.6, \alpha _{asr}=0.2$ and $\alpha _{mt}=0.2$.. For MT task, we use only in-domain data.

Many-to-many+pre-training: We train a many-to-many multi-task model where the encoders and decoders are derived from pre-trained ASR and MT models. "
1909.07575|022c365a14fdec406c7a945a1a18e7e79df37f08|What is the attention module pretrained on?|the model is pre-trained on CTC-based ASR task and MT task in the pre-training stage.| | 
1701.04056|5260cb56b7d127772425583c5c28958c37cb9bea|How long of dialog history is captured?|two previous turns|160| 
1904.07904|9b97805a0c093df405391a85e4d3ab447671c86a|What evaluation metrics were used?|Exact Match (EM), Macro-averaged F1 scores (F1)|Exact Match (EM) and Macro-averaged F1 scores (F1) | 
1904.07904|7ee5c45b127fb284a4a9e72bb9b980a602f7445a|What was the previous best model?|(c) previous best model trained on S-SQuAD BIBREF5 by using Dr.QA BIBREF20 | | 
1904.07904|ddf5e1f600b9ce2e8f63213982ef4209bab01fd8|Which datasets did they use for evaluation?|Spoken-SQuAD testing set|Spoken-SQuAD| 
2003.11645|ef3567ce7301b28e34377e7b62c4ec9b496f00bf|What Named Entity Recognition dataset is used?|Groningen Meaning Bank|Groningen Meaning Bank (GMB)| 
2003.11645|7595260c5747aede0b32b7414e13899869209506|What sentiment analysis dataset is used?|IMDb dataset of movie reviews|IMDb| 
1909.02304|5a22293b055f5775081d6acdc0450f7bd5f5de04|What is the state-of-the-art model for the task?|OpATT BIBREF6, Neural Content Planning with conditional copy (NCP+CC) BIBREF4| | 
1909.02304|03c967763e51ef2537793db7902e2c9c17e43e95|What is the strong baseline?|Conditional Copy (CC) model |delayed copy model (DEL),  template system (TEM), conditional copy (CC), NCP+CC (NCP)| 
1604.03114|26327ccebc620a73ba37a95aabe968864e3392b2|what aspects of conversation flow do they look at?|The time devoted to self-coverage, opponent-coverage, and the number of adopted discussion points.|—promoting one's own points and attacking the opponents' points| 
1604.03114|ababb79dd3c301f4541beafa181f6a6726839a10|what debates dataset was used?|Intelligence Squared Debates|“Intelligence Squared Debates” (IQ2 for short)| 
1608.06757|8eefa116e3c3d3db751423cc4095d1c4153d3a5f|what standard dataset were used?|The GENIA Corpus , CoNLL2003|GENIA Corpus BIBREF3, CoNLL2003 BIBREF14, KORE50 BIBREF21 , ACE2004 BIBREF22 and MSNBC|CoNLL2003-testA, GENIA
2001.03131|133eb4aa4394758be5f41744c60c99901b2bc01c|Do they perform error analysis?|No|No| 
2001.03131|a778b8204a415b295f73b93623d09599f242f202|What is the Random Kitchen Sink approach?|Random Kitchen Sink method uses a kernel function to map data vectors to a space where linear separation is possible.|explicitly maps data vectors to a space where linear separation is possible, RKS method provides an approximate kernel function via explicit mapping| 
1606.02891|642e8cf1d39faa1cd985d16750cdc6696c52db2f|what are the baseline systems?|attentional encoder-decoder networks BIBREF0| the dl4mt-tutorial| 
1803.09123|493e971ee3f57a821ef1f67ef3cd47ade154e7c4|What word embeddings do they test?|Bernoulli embeddings (b-emb) BIBREF1 , continuous bag-of-words (CBOW) BIBREF5 , Distributed Memory version of Paragraph Vector (PV-DM) BIBREF11 and the Global Vectors (GloVe) BIBREF6 model|Bernoulli embeddings, continuous bag-of-words, Distributed Memory version of Paragraph Vector, Global Vectors, equation embeddings, equation unit embeddings| 
1803.09123|8dd8e5599fc56562f2acbc16dd8544689cddd938|How do they define similar equations?|By using Euclidean distance computed between the context vector representations of the equations|Similar words were ranked by computing Cosine distance between the embedding vector ( INLINEFORM0 ) representation of the query equation and the context vector representation of the words ( INLINEFORM1 ). Similar equations were discovered using Euclidean distance computed between the context vector representations of the equations ( INLINEFORM2 ). We give additional example results in Appendix B.| 
1910.01863|abe2393415e533cb06311e74ed1c5674cff8571f|What evaluation criteria and metrics were used to evaluate the generated text?|BLEU , NIST , METEOR , ROUGE-L, CIDEr , evaluation script, automatic evaluation, human evaluation, minimum edit evaluation, word error rate (WER), factual errors and their types, fluency issues, acceptability of the output for production use in a news agency|BLEU, NIST, METEOR, ROUGE-L, CIDEr| 
1701.08229|00c57e45ac6afbdfa67350a57e81b4fad0ed2885|Do they evaluate only on English datasets?|Yes|Yes| 
1701.08229|22714f6cad2d5c54c28823e7285dc85e8d6bc109|What are the three steps to feature elimination?|Reduction, Selection, Rank|reduced the dataset by eliminating features, apply feature selection to select highest ranked features to train and test the model and rank the performance of incrementally adding features.| 
1701.08229|82642d3111287abf736b781043d49536fe48c350|How is the dataset annotated?|no evidence of depression, depressed mood, disturbed sleep, fatigue or loss of energy|The annotations are based on evidence of depression and further annotated by the depressive symptom if there is evidence of depression| 
1701.08229|5a81732d52f64e81f1f83e8fd3514251227efbc7|What dataset is used for this study?|BIBREF12 , BIBREF13|an existing, annotated Twitter dataset that was constructed based on a hierarchical model of depression-related symptoms BIBREF12 , BIBREF13| 
1912.06262|9a8b9ea3176d30da2453cac6e9347737c729a538|what were their performance results?| the hybrid NER model achieved a F1 score of $0.995$ on synthesized queries and $0.948$ on clinical notes while the i2b2 NER model achieved a F1 score of $0.441$ on synthesized queries and $0.927$ on clinical notes|hybrid NER model achieved a F1 score of $0.995$ on synthesized queries and $0.948$ on clinical notes| 
1912.06262|4477bb513d56e57732fba126944073d414d1f75f|where did they obtain the annotated clinical notes from?|clinical notes from the CE task in 2010 i2b2/VA|clinical notes from the CE task in 2010 i2b2/VA | 
1709.07814|1b23c4535a6c10eb70bbc95313c465e4a547db5e|Which architecture do they use for the encoder and decoder?|we construct an encoder with several convolutional layers BIBREF14 followed by NIN layers BIBREF15 as the lower part in the encoder and integrate them with deep bidirectional long short-term memory (Bi-LSTM) BIBREF16 at the higher part, On the decoder side, we use a standard deep unidirectional LSTM with global attention BIBREF13 that is calculated by a multi-layer perceptron (MLP)|In encoder they use convolutional, NIN and bidirectional LSTM layers and in decoder they use unidirectional LSTM | 
1709.07814|0a75a52450ed866df3a304077769e1725a995bb7|How does their decoder generate text?|decoder task, which predicts the target sequence probability at time INLINEFORM3 based on previous output and context information|Decoder predicts the sequence of phoneme or grapheme at each time based on the previous output and context information with a beam search strategy| 
1709.07814|fd0a3e9c210163a55d3ed791e95ae3875184b8f8|Which dataset do they use?|WSJ|WSJ-SI84, WSJ-SI284| 
1806.00738|c37f65c9f0d543a35c784263b79236ccf1c44fac|What model is used to encode the images?|a Convolutional Neural Network (CNN)|LSTM| 
1806.00738|584af673429c7f8621c6bf83362a37048daa0e5d|How is the sequential nature of the story captured?|we provide the decoder with the context of the whole sequence and the content of the current image (i.e. global and local information) to generate the corresponding text that will contribute to the overall story|The encoder takes the images in order, one at every timestep $t$ . At time $t=5$ , we obtain the context vector through $h_e^{(t)}$ (represented by $\mathbf {Z}$ ). This vector is used to initialize each decoder's hidden state while the first input to each decoder is its corresponding image embedding $e(I_i)$ . Each decoder generates a sequence of words $\lbrace p_1,...,p_{n}\rbrace $ for each image in the sequence. | 
1806.00738|1be54c5b3ea67d837ffba2290a40c1e720d9587f|Is the position in the sequence part of the input?|No|Yes| 
1806.00738|b08f88d1facefceb87e134ba2c1fa90035018e83|Do the decoder LSTMs all have the same weights?|No|No| 
1811.00147|b06512c17d99f9339ffdab12cedbc63501ff527e|Is fine-tuning required to incorporate these embeddings into existing models?|No|No| 
1811.00147|fd8e23947095fe2230ffe1a478945829b09c8c95|How are meaningful chains in the graph selected?|No|utilize the machinery of language modeling using deep neural networks to learn Dolores embeddings.| 
1610.09225|3611a72f754de1e256fbd25b012197e1c24e8470|Do they remove seasonality from the time series?|No| | 
1610.09225|4c07c33dfaf4f3e6db55e377da6fa69825d0ba15|What is the dimension of the embeddings?|300|300| 
1610.09225|b1ce129678e37070e69f01332f1a8587e18e06b0|What dataset is used to train the model?|2,50,000 tweets, Stock opening and closing prices of Microsoft from August 31st, 2015 to August 25th, 2016|Collected tweets and opening and closing stock prices of Microsoft.| 
2003.08380|7fb27d8d5a8bb351f97236a1f6dcd8b2613b16f1|What is the previous state of the art?|RoBERTa|RoBERTa| 
1811.05711|0689904db9b00a814e3109fb1698086370a28fa2|Which text embedding methodologies are used?|Document to Vector (Doc2Vec)|Doc2Vec, PV-DBOW model| 
1805.04508|cc354c952b5aaed2d4d1e932175e008ff2d801dd|Which race and gender are given higher sentiment intensity predictions?|"Females are given higher sentiment intensity when predicting anger, joy or valence, but males are given higher sentiment intensity when predicting  fear.
African American names are given higher score on the tasks of anger, fear, and sadness intensity prediction,  but European American names are given higher scores on joy and valence task."| the number of systems consistently giving higher scores to sentences with female noun phrases, higher scores to sentences with African American names on the tasks of anger, fear, and sadness,  joy and valence tasks, most submissions tended to assign higher scores to sentences with European American names| 
1805.04508|0f12dc077fe8e5b95ca9163cea1dd17195c96929|What criteria are used to select the 8,640 English sentences?|Sentences involving at least one race- or gender-associated word,  sentence  have to be short and grammatically simple,  sentence have to  include expressions of sentiment and emotion.|generated with the various combinations of INLINEFORM4 person INLINEFORM5 and INLINEFORM6 emotion word INLINEFORM7 values across the eleven templates, differ only in one word corresponding to gender or race| 
1909.13714|5563a3538d311c979c2fb83c1cc9afc66ff6fffc|Is collected multimodal in cabin dataset public?|No| | 
1910.05603|91bc8c0bc1634045177065536dd311f89134630b|Is the model tested against any baseline?|No|No| 
1910.05603|fe1dcd6ef1f8618bbceee418f07cafe63a8efe08|What is the language model combination technique used in the paper?|system combination on the decoding lattice level, combination weights|system combination on the decoding lattice level| 
1910.05603|53f74250948015c394e7b8438a2041fdeb330911|What are the deep learning architectures used in the task?|DNN-based acoustic model BIBREF0| | 
1909.03405|7b4fb6da74e6bd1baea556788a02969134cf0800|Do they train their model starting from a checkpoint?|No|No| 
1909.03405|bc31a3d2f7c608df8c019a64d64cb0ccc5669210|What BERT model do they test?|BERTbase|BERTbase| 
1801.07887|f67b9bda14ec70feba2e0d10c400b2b2025a0a6a|What downstream tasks are evaluated?|text classification| | 
1801.07887|1cfed6b0c9b5a079a51166209649a987e7553e4e|What is active learning?|A process of training a model when selected unlabeled samples are annotated on each iteration.|Active learning is a process that selectively determines which unlabeled samples for a machine learning model should be annotated.| 
2002.04095|f8da63df16c4c42093e5778c01a8e7e9b270142e|How is segmentation quality evaluated?|Segmentation quality is evaluated by calculating the precision, recall, and F-score of the automatic segmentations in comparison to the segmentations made by expert annotators from the ANNODIS subcorpus.|we compare the Annodis segmentation with the automatically produced segmentation| 
1710.04203|c09a92e25e6a81369fcc4ae6045491f2690ccc10|How do they compare lexicons?|Human evaluators were asked to evaluate on a scale from 1 to 5 the validity of the lexicon annotations made by the experts and crowd contributors.|1000 term groups based on the number of total annotations(200 term groups with 2 total annotations, 200 term groups with 3 total annotations, and so on up to term groups with 6 total annotations)| 
1911.01371|051df74dc643498e95d16e58851701628fdfd43e|How did they obtain the OSG dataset?|crawling and pre-processing an OSG web forum|data has been developed by crawling and pre-processing an OSG web forum| 
1911.01371|33554065284110859a8ea3ca7346474ab2cab100|How large is the Twitter dataset?|1,873 Twitter conversation threads, roughly 14k tweets|1,873 Twitter conversation threads, roughly 14k tweets| 
1909.09551|54830abe73fef4e629a36866ceeeca10214bd2c8|How they utilize LDA and Gibbs sampling to evaluate ISWC and WWW publications?|the LDA approaches to recommendation systems and given the importance of research, we have studied recent impressive articles on this subject and presented a taxonomy of recommendation systems based on LDA of the recent research, we evaluated ISWC and WWW conferences articles from DBLP website and used the Gibbs sampling algorithm as an evaluation parameter|discover the trends of the topics and find relationship between LDA topics and paper features and generate trust tags,  learn a LDA model with 100 topics; $\alpha =0.01$, $\beta = 0.01$ and using Gibbs sampling as a parameter estimation| 
1701.02962|2fbb6322e485e7743ec3fb4bb02d44bf4b5ea8a6|What dataset do they use to evaluate their method?|antonym and synonym pairs, collected from WordNet BIBREF9 and Wordnik|English Wikipedia dump from June 2016| 
1806.05504|ef7212075e80bf35b7889dc8dd52fcbae0d1400a|Why are current ELS's not sufficiently effective?|Linked entities may be ambiguous or too common|linked entities extracted from ELS's have issues because of low precision rates BIBREF11 and design challenges in training datasets BIBREF12 . These issues can be summarized into two parts: ambiguity and coarseness., the linked entities may also be too common to be considered an entity.| 
1908.05828|567dc9bad8428ea9a2658c88203a0ed0f8da0dc3|What is the best model?|BiLSTM+CNN(grapheme-level) and BiLSTM+CNN(G)+POS | | 
1908.05828|d8627ba08b7342e473b8a2b560baa8cdbae3c7fd|Do the authors train a Naive Bayes classifier on their dataset?|No|No| 
1908.05828|8a7615fc6ff1de287d36ab21bf2c6a3b2914f73d|Which machine learning models do they explore?|BiLSTM, BiLSTM-CNN, BiLSTM-CRF, BiLSTM-CNN-CRF|BiLSTMBIBREF14, BiLSTM+CNNBIBREF20, BiLSTM+CRFBIBREF1, BiLSTM+CNN+CRFBIBREF2, CNN modelBIBREF0 and Stanford CRF modelBIBREF21| 
1908.05828|bb2de20ee5937da7e3e6230e942bec7b6e8f61ee|What is the source of their dataset?|daily newspaper of the year 2015-2016|daily newspaper of the year 2015-2016| 
1908.05828|1170e4ee76fa202cabac9f621e8fbeb4a6c5f094|Do they try to use byte-pair encoding representations?|No|No| 
1908.05828|6d1217b3d9cfb04be7fcd2238666fa02855ce9c5|Which models are used to solve NER for Nepali?|BiLSTMBIBREF14, BiLSTM+CNNBIBREF20, BiLSTM+CRFBIBREF1, BiLSTM+CNN+CRFBIBREF2, CNN modelBIBREF0 and Stanford CRF modelBIBREF21|BiLSTM, BiLSTM+CNN, BiLSTM+CRF, BiLSTM+CNN+CRF, CNN, Stanford CRF| 
1909.13104|1e775cf30784e6b1c2b573294a82e145a3f959bb|What language(s) is/are represented in the dataset?|english|english| 
1909.13104|392fb87564c4f45d0d8d491a9bb217c4fce87f03|What baseline model is used?| LastStateRNN, AvgRNN, AttentionRNN|LastStateRNN, AvgRNN, AttentionRNN | 
1909.13104|203337c15bd1ee05763c748391d295a1f6415b9b|Which variation provides the best results on this dataset?|the model with multi-attention mechanism and a projected layer|Projected Layer| 
1909.13104|d004ca2e999940ac5c1576046e30efa3059832fa|What are the different variations of the attention-based approach which are examined?|classic RNN model, avgRNN model, attentionRNN model and multiattention RNN model with and without a projected layer| four attention mechanisms instead of one, a projection layer for the word embeddings| 
1909.13104|21548433abd21346659505296fb0576e78287a74|What dataset is used for this work?|Twitter dataset provided by the organizers|The dataset from the SociaL Media And Harassment Competition of the ECML PKDD 2019 Conference.| 
1909.13104|f0b2289cb887740f9255909018f400f028b1ef26|What types of online harassment are studied?|indirect harassment, sexual and physical harassment|indirect, physical, sexual| 
1909.13104|51b1142c1d23420dbf6d49446730b0e82b32137c|What was the baseline?|LastStateRNN, AvgRNN, AttentionRNN| | 
1909.13104|58355e2a782bf145b61ee2a3e0e426119985c179|What were the datasets used in this paper?|The dataset from the SociaL Media And Harassment Competition of the ECML PKDD 2019 Conference. |Twitter dataset provided by organizers containing harassment and non-harassment tweets| 
2002.02070|25c1c4a91f5dedd4e06d14121af3b5921db125e9|Is car-speak language collection of abstract features that classifier is later trained on?|No|No| 
2002.02070|f88036174b4a0dbf4fe70ddad884d16082c5748d|"Is order of ""words"" important in car speak language?"|No| | 
2002.02070|a267d620af319b48e56c191aa4c433ea3870f6fb|What are labels in car speak language dataset?|car |the car| 
2002.02070|899ed05c460bf2aa0aa65101cad1986d4f622652|How big is dataset of car-speak language?|$3,209$ reviews |$3,209$ reviews about 553 different cars from 49 different car manufacturers| 
2002.02070|6bf93968110c6e3e3640360440607744007a5228|How does car speak pertains to a car's physical attributes?|we do not know exactly| | 
1611.03599|37a79be0148e1751ffb2daabe4c8ec6680036106|What topic is covered in the Chinese Facebook data? |anti-nuclear-power|anti-nuclear-power| 
1611.03599|518dae6f936882152c162058895db4eca815e649|How many layers does the UTCNN model have?|eight layers| | 
1611.03599|e44a6bf67ce3fde0c6608b150030e44d87eb25e3|What topics are included in the debate data?|abortion, gay rights, Obama, marijuana|abortion (ABO), gay rights (GAY), Obama (OBA), and marijuana (MAR)| 
1611.03599|6a31db1aca57a818f36bba9002561724655372a7|What is the size of the Chinese data?|32,595 posts|32,595| 
1611.03599|e330e162ec29722f5ec9f83853d129c9e0693d65|Did they collected the two datasets?|No|No| 
1611.03599|d3093062aebff475b4deab90815004051e802aa6|What are the baselines?|SVM with unigram, bigram, and trigram features, SVM with average word embedding, SVM with average transformed word embeddings, CNN, ecurrent Convolutional Neural Networks, SVM and deep learning models with comment information|SVM with unigram, bigram, trigram features, with average word embedding, with average transformed word embeddings, CNN and RCNN, SVM, CNN, RCNN with comment information| 
1908.10084|4944cd597b836b62616a4e37c045ce48de8c82ca|What transfer learning tasks are evaluated?|MR, CR, SUBJ, MPQA, SST, TREC, MRPC|"MR: Sentiment prediction for movie reviews snippets on a five start scale BIBREF25.

CR: Sentiment prediction of customer product reviews BIBREF26.

SUBJ: Subjectivity prediction of sentences from movie reviews and plot summaries BIBREF27.

MPQA: Phrase level opinion polarity classification from newswire BIBREF28.

SST: Stanford Sentiment Treebank with binary labels BIBREF29.

TREC: Fine grained question-type classification from TREC BIBREF30.

MRPC: Microsoft Research Paraphrase Corpus from parallel news sources BIBREF31."|Semantic Textual Similarity, sentiment prediction, subjectivity prediction, phrase level opinion polarity classification, Stanford Sentiment Treebank, fine grained question-type classification.
1908.10084|a29c071065d26e5ee3c3bcd877e7f215c59d1d33|What metrics are used for the STS tasks?| Spearman's rank correlation between the cosine-similarity of the sentence embeddings and the gold labels|Spearman's rank correlation between the cosine-similarity of the sentence embeddings and the gold labels| 
1908.10084|7f207549c75f5c4388efc15ed28822672b845663|How much time takes its training?|20 minutes| | 
1908.10084|2e89ebd2e4008c67bb2413699589ee55f59c4f36|How are the siamese networks trained?|update the weights such that the produced sentence embeddings are semantically meaningful and can be compared with cosine-similarity., Classification Objective Function, Regression Objective Function, Triplet Objective Function| | 
1707.06806|ed67359889cf61fa11ee291d6c378cccf83d599d|Which pretrained word vectors did they use?| pre-trained GloVe word vectors |GloVe word vectors BIBREF16 pre-trained on two datasets: Wikipedia 2014 with Gigaword5 (W+G5) and Common Crawl (CC)| 
1707.06806|425bd2ccfd95ead91d8f2b1b1c8ab9fc3446cb82|What evaluation metrics are used?|standard accuracy metric|accuracy| 
1707.06806|955de9f7412ba98a0c91998919fa048d339b1d48|Which shallow approaches did they experiment with?|SVM|SVM with linear kernel using bag-of-words features| 
1707.06806|3b371ea554fa6639c76a364060258454e4b931d4|Where do they obtain the news videos from?|NowThisNews Facebook page|NowThisNews Facebook page| 
1707.06806|ddb23a71113cbc092cbc158066d891cae261e2c6|What is the source of the news articles?|main news channels, such as Yahoo News, The Guardian or The Washington Post|The BreakingNews dataset| 
1806.04511|c7486d039304ca9d50d0571236429f4f6fbcfcf7|which non-english language was the had the worst results?|Turkish| | 
1806.04511|f1f1dcc67b3e4d554bfeb508226cdadb3c32d2e9|what datasets were used in evaluation?|SemEval-2016 Challenge Task 5 BIBREF27 , BIBREF28| English reviews ,  restaurant reviews from four different languages (Spanish, Turkish, Dutch, Russian)| 
1806.04511|a103636c8d1dbfa53341133aeb751ffec269415c|what are the baselines?|majority baseline, lexicon-based approach|majority baseline corresponds to a model's accuracy if it always predicts the majority class in the dataset, lexicon-based approach| 
1806.04511|55139fcfe04ce90aad407e2e5a0067a45f31e07e|how did the authors translate the reviews to other languages?|Using Google translation API.|Google translation API| 
1806.04511|fbaf060004f196a286fef67593d2d76826f0304e|what dataset was used for training?|Amazon reviews, Yelp restaurant reviews, restaurant reviews|Amazon reviews BIBREF23 , BIBREF24, Yelp restaurant reviews dataset,  restaurant reviews dataset as part of a Kaggle competition BIBREF26| 
1904.04358|7ae38f51243cb80b16a1df14872b72a1f8a2048f|How do they demonstrate that this type of EEG has discriminative information about the intended articulatory movements responsible for speech?|we plot T-distributed Stochastic Neighbor Embedding (tSNE) corresponding to INLINEFORM0 and V/C classification tasks in Fig. FIGREF8 .| | 
1904.04358|deb89bca0925657e0f91ab5daca78b9e548de2bd|What are the five different binary classification tasks?| presence/absence of consonants, phonemic nasal, bilabial, high-front vowels and high-back vowels.|presence/absence of consonants, presence/absence of phonemic nasal, presence/absence of bilabial, presence/absence of high-front vowels, and presence/absence of high-back vowels| 
1904.04358|9c33b340aefbc1f15b6eb6fb3e23ee615ce5b570|How was the spatial aspect of the EEG signal computed?|we use a CNN BIBREF19 , in particular a four-layered 2D CNN stacking two convolutional and two fully connected hidden layers.|They use four-layered 2D CNN and two fully connected hidden layers on the channel covariance matrix to compute the spatial aspect.| 
1904.04358|e6583c60b13b87fc37af75ffc975e7e316d4f4e0|What data was presented to the subjects to elicit event-related responses?|7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)|KARA ONE BIBREF17 , composed of multimodal data for stimulus-based, imagined and articulated speech state corresponding to 7 phonemic/syllabic ( /iy/, /piy/, /tiy/, /diy/, /uw/, /m/, /n/ ) as well as 4 words(pat, pot, knew and gnaw)| 
1904.04358|c7b6e6cb997de1660fd24d31759fe6bb21c7863f|How many electrodes were used on the subject in EEG sessions?|1913 signals| | 
1904.04358|f9f59c171531c452bd2767dc332dc74cadee5120|How many subjects does the EEG data come from?|14|14 participants| 
1912.00667|4ac2c3c259024d7cd8e449600b499f93332dab60|Do they report results only on English data?|Yes| | 
1912.00667|bc730e4d964b6a66656078e2da130310142ab641|What type of classifiers are used?|probabilistic model|Logistic Regression, Multilayer Perceptron| 
1912.00667|3941401a182a3d6234894a5c8a75d48c6116c45c|Which real-world datasets are used?|Tweets related to CyberAttack and tweets related to PoliticianDeath|cyber security (CyberAttack), death of politicians (PoliticianDeath)| 
1912.00667|67e9e147b2cab5ba43572ce8a17fc863690172f0|How are the interpretability merits of the approach demonstrated?|By involving humans for post-hoc evaluation of model's interpretability|directly solicits informative keywords from the crowd for model training, thereby providing human-understandable explanations for the improved model| 
1912.00667|a74190189a6ced2a2d5b781e445e36f4e527e82a|How are the accuracy merits of the approach demonstrated?|significant improvements clearly demonstrate that our approach is effective at improving model performance|By evaluating the performance of the approach using accuracy and AUC| 
1912.00667|43f074bacabd0a355b4e0f91a1afd538c0a6244f|How is the keyword specific expectation elicited from the crowd?|workers are first asked to find those microposts where the model predictions are deemed correct,  asked to find the keyword that best indicates the class of the microposts| | 
1912.08904|58ef2442450c392bfc55c4dc35f216542f5f2dbb|Does the paper provide any case studies to illustrate how one can use Macaw for CIS research?|No|No| 
1912.08904|78a5546e87d4d88e3d9638a0a8cd0b7debf1f09d|What functionality does Macaw provide?|Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation|conversational search, conversational question answering, conversational recommendation, conversational natural language interface to structured and semi-structured data| 
1912.08904|375b281e7441547ba284068326dd834216e55c07|What is a wizard of oz setup?|seeker interacts with a real conversational interface, intermediary (or the wizard) receives the seeker's message and performs different information seeking actions|a setup where the seeker interacts with a real conversational interface and the wizard, an intermediary, performs actions related to the seeker's message| 
1912.08904|05c49b9f84772e6df41f530d86c1f7a1da6aa489|What interface does Macaw currently have?|File IO, Standard IO, Telegram|The current implementation of Macaw supports a command line interface as well as mobile, desktop, and web apps.| 
1912.08904|6ecb69360449bb9915ac73c0a816c8ac479cbbfc|What modalities are supported by Macaw?|text, speech, image, click, etc| | 
1912.08904|68df324e5fa697baed25c761d0be4c528f7f5cf7|What are the different modules in Macaw?|Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation|Co-Reference Resolution, Query Generation, Retrieval Model, Result Generation| 
1703.10344|2ee715c7c6289669f11a79743a6b2b696073805d|What baseline model is used?|"For Article-Entity placement, they consider two baselines: the first one using only salience-based features, and the second baseline checks if the entity appears in the title of the article. 

For Article-Section Placement, they consider two baselines: the first picks the section with the highest lexical similarity to the article, and the second one picks the most frequent section."|"B1. The first baseline uses only the salience-based features by Dunietz and Gillick BIBREF11 ., B2. The second baseline assigns the value relevant to a pair INLINEFORM0 , if and only if INLINEFORM1 appears in the title of INLINEFORM2 .

, S1: Pick the section from template INLINEFORM0 with the highest lexical similarity to INLINEFORM1 : S1 INLINEFORM2, S2: Place the news into the most frequent section in INLINEFORM0"| 
1703.10344|61a9ea36ddc37c60d1a51dabcfff9445a2225725|What news article sources are used?| the news external references in Wikipedia| | 
1703.10344|cc850bc8245a7ae790e1f59014371d4f35cd46d7|How do they determine the exact section to use the input article?|They use a multi-class classifier to determine the section it should be cited| | 
1703.10344|984fc3e726848f8f13dfe72b89e3770d00c3a1af|What features are used to represent the novelty of news articles to entity pages?|KL-divergences of language models for the news article and the already added news references|KL divergence between the language model of INLINEFORM5 and articles in INLINEFORM6| 
1703.10344|fb1227b3681c69f60eb0539e16c5a8cd784177a7|What features are used to represent the salience and relative authority of entities?|"Salience features positional features, occurrence frequency and the internal POS structure of the entity and the sentence it occurs in.
The relative authority of entity features:   comparative relevance of the news article to the different entities occurring in it."|positional features, occurrence frequency, internal POS structure of the entity and the sentence it occurs in, relative entity frequency, centrality measures like PageRank | 
2003.13032|8df35c24af9efc3348d3b8d746df116480dfe661|Do they experiment with other tasks?|No| | 
2003.13032|277a7e916e65dfefd44d2d05774f95257ac946ae|What baselines do they introduce?|"Conditional Random Fields, BiLSTM-CRF, Multi-Task Learning, BioBERT
"|Conditional Random Fields, BiLSTM-CRF, Multi-Task Learning, BioBERT| 
2003.13032|2916bbdb95ef31ab26527ba67961cf5ec94d6afe|How large is the corpus?|8,275 sentences and 167,739 words in total|The corpus comprises 8,275 sentences and 167,739 words in total.| 
2003.13032|f2e8497aa16327aa297a7f9f7d156e485fe33945|How was annotation performed?|Experienced medical doctors used a linguistic annotation tool to annotate entities.|WebAnno| 
2003.13032|9b76f428b7c8c9fc930aa88ee585a03478bff9b3|How many documents are in the new corpus?|53 documents|53 documents| 
2003.13032|dd6b378d89c05058e8f49e48fd48f5c458ea2ebc|What baseline systems are proposed?|Conditional Random Fields, BiLSTM-CRF, Multi-Task Learning, BioBERT|Conditional Random Fields, BiLSTM-CRF, Multi-Task Learning, BioBERT| 
1910.06592|e35c2fa99d5c84d8cb5d83fca2b434dcd83f3851|How did they obtain the dataset?|For the non-factual accounts, we rely on a list of 180 Twitter accounts from BIBREF1, we use a list with another 32 Twitter accounts from BIBREF19 that are considered trustworthy|public resources where suspicious Twitter accounts were annotated, list with another 32 Twitter accounts from BIBREF19 that are considered trustworthy| 
1910.06592|c00ce1e3be14610fb4e1f0614005911bb5ff0302|What activation function do they use in their model?|relu, selu, tanh|Activation function is hyperparameter. Possible values: relu, selu, tanh.| 
1910.06592|71fe5822d9fccb1cb391c11283b223dc8aa1640c|What baselines do they compare to?|LR + Bag-of-words, Tweet2vec, LR + All Features (tweet-level), LR + All Features (chunk-level), FacTweet (tweet-level), Top-$k$ replies, likes, or re-tweets|Top-$k$ replies, likes, or re-tweets, FacTweet (tweet-level), LR + All Features (chunk-level), LR + All Features (tweet-level), Tweet2vec, LR + Bag-of-words| 
1910.06592|97d0f9a1540a48e0b4d30d7084a8c524dd09a4c3|How are chunks defined?|Chunks is group of tweets from single account that  is consecutive in time - idea is that this group can show secret intention of malicious accounts.|sequence of $s$ tweets| 
1910.06592|1062a0506c3691a93bb914171c2701d2ae9621cb|What features are extracted?|Sentiment, Morality, Style, Words embeddings|15 emotion types, sentiment classes, positive and negative, care, harm, fairness, cheating, loyalty, betrayal, authority, subversion, sanctity, and degradation, count of question marks, exclamation marks, consecutive characters and letters, links, hashtags, users' mentions, uppercase ratio, tweet length, words embeddings| 
1910.06592|483a699563efcb8804e1861b18809279f21c7610|Was the approach used in this work to detect fake news fully supervised?|Yes| | 
1910.06592|d3ff2986ca8cb85a9a5cec039c266df756947b43|Based on this paper, what is the more predictive set of features to detect fake news?|words embeddings, style, and morality features|words embeddings, style, and morality features| 
1910.06592|2317ca8d475b01f6632537b95895608dc40c4415|"How is a ""chunk of posts"" defined in this work?"|chunk consists of a sorted sequence of tweets labeled by the label of its corresponding account|sequence of $s$ tweets| 
1910.06592|3e88fb3d28593309a307eb97e875575644a01463|What baselines were used in this work?|LR + Bag-of-words, Tweet2vec, LR + All Features (tweet-level), LR + All Features (chunk-level), FacTweet (tweet-level), Top-$k$ replies, likes, or re-tweets|LR + Bag-of-words, Tweet2vec, LR + All Features (tweet-level), LR + All Features (chunk-level), FacTweet (tweet-level), Top-$k$ replies, likes, or re-tweets| 
1706.01678|e8f969ffd637b82d04d3be28c51f0f3ca6b3883e|Which evaluation methods are used?|Quantitative evaluation methods using ROUGE, Recall, Precision and F1.|standard ROGUE metric, Recall, Precision and INLINEFORM0 scores for ROGUE-1,  INLINEFORM2 scores for ROGUE-2 and ROGUE-L| 
1706.01678|46227b4265f1d300a5ed71bf40822829de662bc2|What dataset is used in this paper?|AMR Bank, CNN-Dailymail|AMR Bank BIBREF10, CNN-Dailymail ( BIBREF11 BIBREF12 )| 
1706.01678|a6a48de63c1928238b37c2a01c924b852fe752f8|Which other methods do they compare with?|Lead-3, Lead-1-AMR|Lead-3 model,  Lead-1-AMR, BIBREF0 | 
1706.01678|b65a83a24fc66728451bb063cf6ec50134c8bfb0|How are sentences selected from the summary graph?| finding the important sentences from the story, extracting the key information from those sentences using their AMR graphs| Two methods: first is to simply pick initial few sentences,  second is to capture the relation between the two most important entities  (select the first sentence which contains both these entities).| 
1902.09666|8c852fc29bda014d28c3ee5b5a7e449ab9152d35|What models are used in the experiment?|linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)|linear SVM, bidirectional Long Short-Term-Memory (BiLSTM), Convolutional Neural Network (CNN)|linear SVM trained on word unigrams,  bidirectional Long Short-Term-Memory (BiLSTM),  Convolutional Neural Network (CNN) 
1902.09666|682e26262abba473412f68cbeb5f69aa3b9968d7|What are the differences between this dataset and pre-existing ones?|no prior work has explored the target of the offensive language| | 
1902.09666|5daeb8d4d6f3b8543ec6309a7a35523e160437eb|In what language are the tweets?|English|English |English
1902.09666|d015faf0f8dcf2e15c1690bbbe2bf1e7e0ce3751|What kinds of offensive content are explored?|non-targeted profanity and swearing, targeted insults such as cyberbullying, offensive content related to ethnicity, gender or sexual orientation, political affiliation, religious belief, and anything belonging to hate speech|Targeted Insult (TIN): Posts which contain an insult/threat to an individual, group, or others , Untargeted (UNT): Posts containing non-targeted profanity and swearing.|offensive (OFF) and non-offensive (NOT), targeted (TIN) and untargeted (INT) insults, targets of insults and threats as individual (IND), group (GRP), and other (OTH)
1902.09666|55bd59076a49b19d3283af41c5e3ccb875f3eb0c|What is the best performing model?|CNN | | 
1902.09666|521280a87c43fcdf9f577da235e7072a23f0673e|How many annotators participated?|five annotators| | 
1902.09666|5a8cc8f80509ea77d8213ed28c5ead501c68c725|What is the definition of offensive language?| Most prior work focuses on a different aspect of offensive language such as abusive language BIBREF0 , BIBREF1 , (cyber-)aggression BIBREF2 , (cyber-)bullying BIBREF3 , BIBREF4 , toxic comments INLINEFORM0 , hate speech BIBREF5 , BIBREF6 , BIBREF7 , BIBREF8 , BIBREF9 , BIBREF10 , and offensive language BIBREF11 . Prior work has focused on these aspects of offensive language in Twitter BIBREF3 , BIBREF7 , BIBREF8 , BIBREF11 , Wikipedia comments, and Facebook posts BIBREF2 .| | 
1902.09666|290ee79b5e3872e0496a6a0fc9b103ab7d8f6c30|What are the three layers of the annotation scheme?|"Level A: Offensive language Detection
, Level B: Categorization of Offensive Language
, Level C: Offensive Language Target Identification
"| | 
1604.00400|c49ee6ac4dc812ff84d255886fd5aff794f53c39|Do the authors report results only on English data?|Yes| | 
1604.00400|3f856097be2246bde8244add838e83a2c793bd17|In the proposed metric, how is content relevance measured?|The content relevance between the candidate summary and the human summary is evaluated using information retrieval - using the summaries as search queries and compare the overlaps of the retrieved results. |On high level, we indirectly evaluate the content relevance between the candidate summary and the human summary using information retrieval.| 
1604.00400|74e866137b3452ec50fb6feaf5753c8637459e62|What manual Pyramid scores are used?| higher tiers of the pyramid|following the pyramid framework, we design an annotation scheme| 
1604.00400|184b0082e10ce191940c1d24785b631828a9f714|What is the common belief that this paper refutes? (c.f. 'contrary to the common belief, ROUGE is not much [sic] reliable'|correlations between Rouge and the Pyramid scores are weak, which challenges its effectiveness for scientific summarization| | 
1905.12801|c59078efa7249acfb9043717237c96ae762c0a8c|which existing strategies are compared?|CDA, REG| | 
1905.12801|73bddaaf601a4f944a3182ca0f4de85a19cdc1d2|what dataset was used?|Daily Mail news articles released by BIBREF9 |Daily Mail news articles| 
1905.12801|d4e5e3f37679ff68914b55334e822ea18e60a6cf|what kinds of male and female words are looked at?|gendered word pairs like he and she| | 
1905.12801|5f60defb546f35d25a094ff34781cddd4119e400|how is mitigation of gender bias evaluated?|Using INLINEFORM0 and INLINEFORM1| | 
1905.12801|90d946ccc3abf494890e147dd85bd489b8f3f0e8|what bias evaluation metrics are used?|gender bias, normalized version of INLINEFORM0, ratio of occurrence of male and female words in the model generated text, Causal occupation bias conditioned on occupation, causal occupation bias conditioned on gender, INLINEFORM1| | 
1810.12196|b962cc817a4baf6c56150f0d97097f18ad6cd9ed|What kind of questions are present in the dataset?|These 8 tasks require different competencies and a different level of understanding of the document to be well answered| | 
1810.12196|fb5fb11e7d01b9f9efe3db3417b8faf4f8d6931f|What baselines are presented?|Logistic regression, LSTM, End-to-end memory networks, Deep projective reader|Logistic regression, LSTM, End-to-end memory networks, Deep projective reader| 
1810.12196|2236386729105f5cf42f73cc055ce3acdea2d452|What language are the reviews in?|English| | 
1810.12196|18942ab8c365955da3fd8fc901dfb1a3b65c1be1|Where are the hotel reviews from?|TripAdvisor|TripAdvisor| 
1707.05236|7b4992e2d26577246a16ac0d1efc995ab4695d24|What was the baseline used?|error detection system by Rei2016|error detection system by Rei2016| 
1707.05236|9a9d225f9ac35ed35ea02f554f6056af3b42471d|What textual patterns are extracted?|(VVD shop_VV0 II, VVD shopping_VVG II)|patterns for generating all types of errors| 
1707.05236|ea56148a8356a1918bedcf0a99ae667c27792cfe|Which annotated corpus did they use?| FCE test data (41K tokens) and the two alternative annotations of the CoNLL 2014 Shared Task dataset (30K tokens) |FCE ,  two alternative annotations of the CoNLL 2014 Shared Task dataset| 
1707.05236|cd32a38e0f33b137ab590e1677e8fb073724df7f|Which languages are explored in this paper?|English |English | 
1810.04428|2c6b50877133a499502feb79a682f4023ddab63e|what language does this paper focus on?|English|Simple English| 
1810.04428|f651cd144b7749e82aa1374779700812f64c8799|what evaluation metrics did they use?|BLEU , FKGL , SARI |BLEU, FKGL, SARI, Simplicity| 
1810.04428|4625cfba3083346a96e573af5464bc26c34ec943|by how much did their model improve?|"For the WikiLarge dataset, the improvement over baseline NMT is 2.11 BLEU, 1.7 FKGL and 1.07 SARI.
For the WikiSmall dataset, the improvement over baseline NMT is  8.37 BLEU."|6.37 BLEU| 
1810.04428|326588b1de9ba0fd049ab37c907e6e5413e14acd|what state of the art methods did they compare with?|OpenNMT, PBMT-R, Hybrid, SBMT-SARI, Dress| | 
1810.04428|ebf0d9f9260ed61cbfd79b962df3899d05f9ebfb|what are the sizes of both datasets?|training set has 89,042 sentence pairs, and the test set has 100 pairs, training set contains 296,402, 2,000 for development and 359 for testing|WikiSmall  89 142 sentence pair and  WikiLarge 298 761 sentence pairs. | 
2004.02192|55507f066073b29c1736b684c09c045064053ba9|What are the distinctive characteristics of how Arabic speakers use offensive language?|Frequent use of direct animal name calling, using simile and metaphors, through indirect speech like sarcasm, wishing evil to others, name alteration, societal stratification, immoral behavior and sexually related uses.|Direct name calling, Simile and metaphor, Indirect speech, Wishing Evil, Name alteration, Societal stratification, Immoral behavior, Sexually related| 
2004.02192|e838275bb0673fba0d67ac00e4307944a2c17be3|How did they analyze which topics, dialects and gender are most associated with tweets?|ascertain the distribution of: types of offensive language, genres where it is used, the dialects used, and the gender of users using such language| | 
2004.02192|8dda1ef371933811e2a25a286529c31623cca0c6|How many annotators tagged each tweet?|One|One experienced annotator tagged all tweets| 
2004.02192|b3de9357c569fb1454be8f2ac5fcecaea295b967|How many tweets are in the dataset?|10,000 Arabic tweet dataset |10,000| 
2004.02192|59e58c6fc63cf5b54b632462465bfbd85b1bf3dd|In what way is the offensive dataset not biased by topic, dialect or target?|It does not use a seed list to gather tweets so the dataset does not skew to specific topics, dialect, targets.|our methodology does not use a seed list of offensive words| 
1909.06200|5c3e98e3cebaecd5d3e75ec2c9fc3dd267ac3c83|What experiments are conducted?|Irony Classifier, Sentiment Classifier for Irony, Sentiment Classifier for Non-irony, transformation from ironic sentences to non-ironic sentences| | 
1909.06200|3f0ae9b772eeddfbfd239b7e3196dc6dfa21365f|What is the combination of rewards for reinforcement learning?|irony accuracy, sentiment preservation| irony accuracy and sentiment preservation| 
1909.06200|14b8ae5656e7d4ee02237288372d9e682b24fdb8|What are the difficulties in modelling the ironic pattern?|obscure and hard to understand,  lack of previous work and baselines on irony generation|ironies are often obscure and hard to understand| 
1909.06200|e3a2d8886f03e78ed5e138df870f48635875727e|How did the authors find ironic data on twitter?|They developed a classifier to find ironic sentences in twitter data|by crawling| 
1909.06200|62f27fe08ddb67f16857fab2a8a721926ecbb6fb|Who judged the irony accuracy, sentiment preservation and content preservation?|Irony accuracy is judged only by human ; senriment preservation and content preservation are judged  both by human and using automatic metrics (ACC and BLEU).|four annotators who are proficient in English| 
1706.06894|9ca447c8959a693a3f7bdd0a2c516f4b86f95718|How were the tweets annotated?|tweets are annotated with only Favor or Against for two targets - Galatasaray and Fenerbahçe| | 
1706.06894|05887a8466e0a2f0df4d6a5ffc5815acd7d9066a|Which SVM approach resulted in the best performance?|Target-1| | 
1706.06894|c87fcc98625e82fdb494ff0f5309319620d69040|What are hashtag features?|hashtag features contain whether there is any hashtag in the tweet| | 
1706.06894|500a8ec1c56502529d6e59ba6424331f797f31f0|How many tweets did they collect?|700 |700| 
1706.06894|ff6c9af28f0e2bb4fb6a69f124665f8ceb966fbc|Which sports clubs are the targets?|Galatasaray, Fenerbahçe|Galatasaray , Fenerbahçe | 
1908.11047|9132d56e26844dc13b3355448d0f14b95bd2178a|Which syntactic features are obtained automatically on downstream task data?|token-level chunk label embeddings,  chunk boundary information is passed into the task model via BIOUL encoding of the labels| | 
1908.09246|0602a974a879e6eae223cdf048410b5a0111665e|What baseline approaches does this approach out-perform?|K-means, LEM BIBREF13, DPEMM BIBREF14|K-means, LEM, DPEMM| 
1908.09246|56b034c303983b2e276ed6518d6b080f7b8abe6a|What datasets are used?|FSD BIBREF12 , Twitter, and Google datasets|FSD dataset, Twitter dataset, Google dataset| 
1908.09246|15e481e668114e4afe0c78eefb716ffe1646b494|What alternative to Gibbs sampling is used?|generator network to capture the event-related patterns| | 
1908.09246|3d7a982c718ea6bc7e770d8c5da564fbb9d11951|How does this model overcome the assumption that all words in a document are generated from a single event?|flexibility of neural networks, the generator is capable of learning complicated nonlinear distributions, supervision signal provided by the discriminator will help generator to capture the event-related patterns|by learning a projection function between the document-event distribution and four event related word distributions | 
1612.08205|692c9c5d9ff9cd3e0ce8b5e4fa68dda9bd23dec1|How many users do they look at?|22,880 users|20,000| 
1612.08205|935d6a6187e6a0c9c0da8e53a42697f853f5c248|What do they mean by a person's industry?|the aggregate of enterprises in a particular field|the aggregate of enterprises in a particular field| 
1612.08205|3b77b4defc8a139992bd0b07b5cf718382cb1a5f|What model did they use for their system?|AllWords model by counting the frequencies of all the remaining words and training a multinomial Naive Bayes classifier| | 
1612.08205|01a41c0a4a7365cd37d28690735114f2ff5229f2|What social media platform did they look at?| http://www.blogger.com|http://www.blogger.com| 
1907.09369|de3b1145cb4111ea2d4e113f816b537d052d9814|What baseline is used?| Wang et al. BIBREF21, paper by BIBREF33 in which they used maximum entropy classifier with bag of words model to classify various emotional datasets|Wang et al. , maximum entropy classifier with bag of words model| 
1907.09369|132f752169adf6dc5ade3e4ca773c11044985da4|What data is used in experiments?|Wang et al., CrowdFlower dataset | tweet dataset created by Wang et al. , CrowdFlower dataset| 
1907.09369|1d9aeeaa6efa1367c22be0718f5a5635a73844bd|What meaningful information does the GRU model capture, which traditional ML models do not?| the context and sequential nature of the text|information about the context and sequential nature of the text| 
1911.07555|012b8a89aea27485797373adbcda32f16f9d7b54|What is the approach of previous work?|'shallow' naive Bayes, SVM, hierarchical stacked classifiers, bidirectional recurrent neural networks|BIBREF11 that uses a character level n-gram language model, 'shallow' naive Bayes classifiers BIBREF12, BIBREF8, BIBREF13, BIBREF14, SVMs BIBREF15, BIBREF16 used an SVM with character n-gram, parts of speech tag features and some other engineered features, The winning approach for DSL 2015 used an ensemble naive Bayes classifier, The fasttext classifier BIBREF17, hierarchical stacked classifiers (including lexicons), bidirectional recurrent neural networks BIBREF23 or ensembles of recurrent neural networks BIBREF24| 
1911.07555|c598028815066089cc1e131b96d6966d2610467a|Is the lexicon the same for all languages?|Yes|Yes| 
1911.07555|ca4daafdc23f4e23d933ebabe682e1fe0d4b95ed|How do they obtain the lexicon?|built over all the data and therefore includes the vocabulary from both the training and testing sets| | 
1911.07555|0ab3df10f0b7203e859e9b62ffa7d6d79ffbbe50|What evaluation metric is used?|average classification accuracy|average classification accuracy, execution performance| 
1911.07555|92dfacbbfa732ecea006e251be415a6f89fb4ec6|Which languages are similar to each other?|Nguni languages (zul, xho, nbl, ssw), Sotho languages (nso, sot, tsn)|The Nguni languages are similar to each other, The same is true of the Sotho languages| 
1911.07555|c8541ff10c4e0c8e9eb37d9d7ea408d1914019a9|Which datasets are employed for South African languages LID?|DSL 2015, DSL 2017, JW300 parallel corpus , NCHLT text corpora| | 
1503.00841|50be4a737dc0951b35d139f51075011095d77f2a|What background knowledge do they leverage?|labeled features|labelled features, which are words whose presence strongly indicates a specific class or topic| 
1503.00841|6becff2967fe7c5256fe0b00231765be5b9db9f1|What are the three regularization terms?|a regularization term associated with neutral features, the maximum entropy of class distribution regularization term, the KL divergence between reference and predicted class distribution|a regularization term associated with neutral features,  the maximum entropy of class distribution, KL divergence between reference and predicted class distribution| 
1503.00841|76121e359dfe3f16c2a352bd35f28005f2a40da3|What NLP tasks do they consider?|text classification for themes including sentiment, web-page, science, medical and healthcare| | 
1503.00841|02428a8fec9788f6dc3a86b5d5f3aa679935678d|How do they define robustness of a model?|ability to accurately classify texts even when the amount of prior knowledge for different classes is unbalanced, and when the class distribution of the dataset is unbalanced|Low sensitivity to bias in prior knowledge| 
1804.11346|7793805982354947ea9fc742411bec314a6998f6|Are the annotations automatic or manually created?|Automatic|We performed the annotation with freely available tools for the Portuguese language.| 
1611.08661|b49598b05358117ab1471b8ebd0b042d2f04b2a4|What neural models are used to encode the text?|NBOW, LSTM, attentive LSTM|neural bag-of-words (NBOW) model, bidirectional long short-term memory network (LSTM), attention-based encoder| 
1611.08661|932b39fd6c47c6a880621a62e6a978491d881d60|What baselines are used for comparison?|TransE|TransE| 
1611.08661|b36f867fcda5ad62c46d23513369337352aa01d2|What datasets are used to evaluate this paper?|WordNet BIBREF0, Freebase BIBREF1, WN18 (a subset of WordNet) BIBREF24 , FB15K (a subset of Freebase) BIBREF2| | 
1910.07601|c6a0b9b5dabcefda0233320dd1548518a0ae758e|Which approach out of two proposed in the paper performed better in experiments?|CJFA encoder |CJFA encoder| 
1910.07601|1e185a3b8cac1da939427b55bf1ba7e768c5dae4|What classification baselines are used for comparison?|VAE|VAE based phone classification| 
1910.07601|26e2d4d0e482e6963a76760323b8e1c26b6eee91|What TIMIT datasets are used for testing?|Once split into 8 subsets (A-H), the test set used are blocks D+H and blocks F+H| this paper makes use of the official training and test sets, covering in total 630 speakers with 8 utterances each| 
1909.00175|f56d07f73b31a9c72ea737b40103d7004ef6a079|What datasets are used in evaluation?|The homographic dataset contains 2,250 contexts, 1,607 of which contain a pun. The heterographic dataset consists of 1,780 contexts with 1,271 containing a pun.|A homographic and heterographic benchmark datasets by BIBREF9.| 
1909.00175|38e4aaeabf06a63a067b272f8950116733a7895c|What is the tagging scheme employed?|A new tagging scheme that tags the words before and after the pun as well as the pun words.|a new tagging scheme consisting of three tags, namely { INLINEFORM0 }| 
1910.06036|1d197cbcac7b3f4015416f0152a6692e881ada6c|"How they extract ""structured answer-relevant relation""?"|Using the OpenIE toolbox and applying heuristic rules to select the most relevant relation.|off-the-shelf toolbox of OpenIE| 
1910.06036|477d9d3376af4d938bb01280fe48d9ae7c9cf7f7|What metrics do they use?|BLEU-1 (B1), BLEU-2 (B2), BLEU-3 (B3), BLEU-4 (B4) BIBREF17, METEOR (MET) BIBREF18 and ROUGE-L (R-L) BIBREF19|BLEU-1 (B1), BLEU-2 (B2), BLEU-3 (B3), BLEU-4 (B4), METEOR (MET), ROUGE-L (R-L)| 
1910.06036|f225a9f923e4cdd836dd8fe097848da06ec3e0cc|On what datasets are experiments performed?|SQuAD|SQuAD| 
2002.01984|ff338921e34c15baf1eae0074938bf79ee65fdd2|What was the baseline model?|by answering always YES (in batch 2 and 3) | | 
2002.01984|e807d347742b2799bc347c0eff19b4c270449fee|What dataset did they use?|BioASQ  dataset|A dataset provided by BioASQ consisting of questions, gold standard documents, snippets, concepts  and ideal and ideal answers.| 
2002.01984|31b92c03d5b9be96abcc1d588d10651703aff716|What was their highest recall score?|0.7033|0.7033| 
1909.00326|384bf1f55c34b36cb03f916f50bbefade6c86a75|Does their model suffer exhibit performance drops when incorporating word importance?|No| | 
1909.00326|aef607d2ac46024be17b1ddd0ed3f13378c563a6|How do they measure which words are under-translated by NMT models?|"They measured the under-translated words with low word importance score as calculated by Attribution.
method"|we ask ten human annotators to manually label the under-translated input words, and at least two annotators label each input-hypothesis pair| 
1909.00326|93beae291b455e5d3ecea6ac73b83632a3ae7ec7|How do their models decide how much improtance to give to the output words?|Given the contribution matrix, we can obtain the word importance of each input word to the entire output sentence. |They compute the gradient of the output at each time step with respect to the input words to decide the importance.| 
1909.00326|6c91d44d5334a4ac80100eead4e105d34e99a284|Which model architectures do they test their word importance approach on?| Transformer BIBREF1 model and the conventional RNN-Search model BIBREF0|Transformer, RNN-Search model| 
1910.14599|a69a59b6c0ab27bcee1a780d6867df21e30aec08|Do they compare human-level performance to model performance for their dataset?|No|No| 
1910.14599|b3d01ac226ee979e188a4141877a6d2a5482de98|What are the weaknesses found by non-expert annotators of current state-of-the-art NLI models?|state-of-the-art models learn to exploit spurious statistical patterns in datasets, human annotators—be they seasoned NLP researchers or non-experts—might easily be able to construct examples that expose model brittleness| | 
1910.14599|af5730d82535464cedfa707a03415ac2e7a21295|What data sources do they use for creating their dataset?|Wikipedia (of 250-600 characters) from the manually curated HotpotQA training set, Manually Annotated Sub-Corpus (MASC) of the Open American National Corpus), RTE5|Wikipedia (of 250-600 characters) from the manually curated HotpotQA training set, formal spoken text (excerpted from court and presidential debate transcripts in the Manually Annotated Sub-Corpus (MASC) of the Open American National Corpus), causal or procedural text, which describes sequences of events or actions, extracted from WikiHow, annotations using the longer contexts present in the GLUE RTE training data, which came from the RTE5 dataset| 
1910.14599|ee2c9bc24d70daa0c87e38e0558e09ab97feb4f2|Do they use active learning to create their dataset?|Yes|No| 
1906.00790|b249b60a8c94d0e40d65f1ffdfcac527dab57516|Do the hashtag and SemEval datasets contain only English data?|Yes|Yes| 
1906.00790|0f567251a6566f65170a1329eeeb5105932036b2|What current state of the art method was used for comparison?|current state-of-the-art approach BIBREF14 , BIBREF15| BIBREF14, BIBREF15 | 
1906.00790|4aa9b60c0ccd379c6fb089c84a6c7b872ee9ec4f|What set of approaches to hashtag segmentation are proposed?|"Adaptive Multi-task Learning
, Margin Ranking (MR) Loss
, Pairwise Neural Ranking Model
"| | 
1906.00790|60ce4868af45753c9e124e64e518c32376f12694|How is the dataset of hashtags sourced?|1,268 randomly selected tweets in the Stanford Sentiment Analysis Dataset BIBREF36, all 12,594 unique English hashtags and their associated tweets from the same Stanford dataset|Stanford Sentiment Analysis Dataset BIBREF36| 
1903.03530|1b1a30e9e68a9ae76af467e60cefb180d135e285|How big is their created dataset?|353 conversations from 40 speakers (11 nurses, 16 patients, and 13 caregivers), we build templates and expression pools using linguistic analysis| | 
1903.03530|2c85865a65acd429508f50b5e4db9674813d67f2|Which data do they use as a starting point for the dialogue dataset?|A sample from nurse-initiated telephone conversations for congestive heart failure patients undergoing telepmonitoring, post-discharge from the Health Management Unit at Changi General Hospital|recordings of nurse-initiated telephone conversations for congestive heart failure patients| 
1903.03530|73a7acf33b26f5e9475ee975ba00d14fd06f170f|What labels do they create on their dataset?|(1) the time the patient has been experiencing the symptom, (2) activities that trigger the symptom (to occur or worsen), (3) the extent of seriousness, (4) the frequency occurrence of the symptom, and (5) the location of symptom, No Answer|the time the patient has been experiencing the symptom, activities that trigger the symptom, the extent of seriousness, the frequency occurrence of the symptom, the location of symptom, 9 symptoms| 
1903.03530|dd53baf26dad3d74872f2d8956c9119a27269bd5|How do they select instances to their hold-out test set?|1264 instances from simulated data, 1280 instances by adding two out-of-distribution symptoms and 944 instances manually delineated from the symptom checking portions of real-word dialogues|held out from the simulated data| 
1712.05608|218bc82796eb8d91611996979a4a42500131a936|Which models/frameworks do they compare to?|MLP|Eusboost, MWMOTE| 
1712.05608|b21bc09193699dc9cfad523f3d5542b0b2ff1b8e|Which classification algorithm do they use for s2sL?|MLP|MLP| 
1712.05608|352bc6de5c5068c6c19062bad1b8f644919b1145|Up to how many samples do they experiment with?|535|we considered 4 different proportions i.e., INLINEFORM0 , INLINEFORM1 , INLINEFORM2 and INLINEFORM3 of the training data to train the classifier| 
1712.05608|d667731ea20605580c398a1224a0094d1155ebbb|Do they use pretrained models?|No| | 
1809.03449|8bb0011ad1d63996d5650770f3be18abdd9f7fc6|Do they report results only on English datasets?|Yes| | 
1809.03449|b0dbe75047310fec4d4ce787be5c32935fc4e37b|How do the authors examine whether a model is robust to noise or not?|By evaluating their model on adversarial sets containing misleading sentences|we also use two of its adversarial sets, namely AddSent and AddOneSent BIBREF6 , to evaluate the robustness to noise| 
1809.03449|d64383e39357bd4177b49c02eb48e12ba7ffd4fb|What type of model is KAR?|Lexicon Embedding Layer, Context Embedding Layer, Coarse Memory Layer, Refined Memory Layer, Answer Span Prediction Layer| | 
1806.05513|dea9e7fe8e47da5e7f31d9b1a46ebe34e731a596|What type of system does the baseline classification use?|support vector machine BIBREF18 , random forest, extra tree and naive bayes classifier BIBREF19|Classification system use n-grams, bag-of-words, common words and hashtags as features and SVM, random forest, extra tree and NB classifiers.| 
1806.05513|955cbea7e5ead36fb89cd6229a97ccb3febcf8bc|What experiments were carried out on the corpus?|task of humor identification in social media texts is analyzed as a classification problem| | 
1806.05513|04d1b3b41fb62a7b896afe55e0e8bc5ffb8c6e39|How many annotators tagged each text?|three |three annotators| 
1806.05513|15cdd9ea4bae8891c1652da2ed34c87bbbd0edb8|Where did the texts in the corpus come from?|tweets from the past two years from domains like `sports', `politics', `entertainment'|twitter| 
1903.09722|6ca938324dc7e1742a840d0a54dc13cc207394a1|What dataset do they use?|German newscrawl distributed by WMT'18 , English newscrawl data, WMT'18 English-German (en-de) news translation task , WMT'18 English-Turkish (en-tr) news task|German newscrawl, English newscrawl, WMT'18 English-German (en-de) news, WMT'18 English-Turkish (en-tr) news task, WMT'18 English-German (en-de) news translation task and we validate our findings on the WMT'18 English-Turkish (en-tr) news task| 
1903.09722|4fa6fbb9df1a4c32583d4ef70d2b29ece4b3d802|What other models do they compare to?|BIBREF11 , BIBREF26 | | 
1903.09722|4d47bef19afd70c10bbceafd1846516546641a2f|What language model architectures are used?|uni-directional model to augment the decoder|bi-directional language model to augment the sequence to sequence encoder ,  uni-directional model to augment the decoder| 
1806.11432|506d21501d54a12d0c9fd3dbbf19067802439a04|What are the user-defined keywords?|Words that a user wants them to appear in the generated output.|terms common to hosts' descriptions of popular Airbnb properties, like 'subway', 'manhattan', or 'parking'| 
1806.11432|ee7e9a948ee6888aa5830b1a3d0d148ff656d864|What is the size of the Airbnb?|roughly 40,000 Manhattan listings| | 
1910.14537|709feae853ec0362d4e883db8af41620da0677fe|How does Gaussian-masked directional multi-head attention works?|pays attentions to adjacent characters and casts a localness relationship between the characters as a fixed Gaussian weight assuming the weight relies on the distance between characters|Gaussian-masked directional attention can be described as a function to map queries and key-value pairs to the representation of input, Gaussian-masked directional attention expects to pay attention to the adjacent characters of each positions and cast the localness relationship between characters as a fix Gaussian weight for attention, Gaussian weight only relys on the distance between characters| 
1910.14537|186b7978ee33b563a37139adff1da7d51a60f581|What is meant by closed test setting?|closed test limits all the data for learning should not be beyond the given training set, while open test does not take this limitation|closed test limits all the data for learning should not be beyond the given training set| 
1808.10245|da9c0637623885afaf023a319beee87898948fe9|Does the dataset feature only English language data?|Yes| | 
1808.10245|8a1c0ef69b6022a0642ca131a8eacb5c97016640|What additional features and context are proposed?|using tweets that one has replied or quoted to as contextual information|text sequences of context tweets| 
1808.10245|48088a842f7a433d3290eb45eb0d4c6ab1d8f13c|What learning models are used on the dataset?|Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT),  Convolutional Neural Networks (CNN), Recurrent Neural Networks (RNN)|Naïve Bayes (NB), Logistic Regression (LR), Support Vector Machine (SVM), Random Forests (RF), Gradient Boosted Trees (GBT), CNN, RNN| 
1808.10245|4907096cf16d506937e592c50ae63b642da49052|What examples of the difficulties presented by the context-dependent nature of online aggression do they authors give?|detecting abusive language extremely laborious, it is difficult to build a large and reliable dataset| | 
1910.12574|8748e8f64af57560d124c7b518b853bf2711c13e|Do they report results only on English data?|Yes| | 
1910.12574|893ec40b678a72760b6802f6abf73b8f487ae639|What evidence do the authors present that the model can capture some biases in data annotation and collection?|The authors showed few tweets where neither and implicit hatred content exist but the model was able to discriminate| | 
1910.12574|c81f215d457bdb913a5bade2b4283f19c4ee826c|Which publicly available datasets are used?|Waseem-dataset, Davidson-dataset,|Waseem and Hovey BIBREF5, Davidson et al. BIBREF9| 
1910.12574|e101e38efaa4b931f7dd75757caacdc945bb32b4|What baseline is used?|Waseem and Hovy BIBREF5, Davidson et al. BIBREF9, Waseem et al. BIBREF10|Waseem and Hovy BIBREF5, Davidson et al. BIBREF9, and Waseem et al. BIBREF10| 
1910.12574|afb77b11da41cd0edcaa496d3f634d18e48d7168|What new fine-tuning methods are presented?|BERT based fine-tuning, Insert nonlinear layers, Insert Bi-LSTM layer, Insert CNN layer|BERT based fine-tuning, Insert nonlinear layers, Insert Bi-LSTM layer, Insert CNN layer| 
1910.12574|41b2355766a4260f41b477419d44c3fd37f3547d|What are the existing biases?|systematic and substantial racial biases, biases from data collection, rules of annotation|sampling tweets from specific keywords create systematic and substancial racial biases in datasets| 
1910.12574|96a4091f681872e6d98d0efee777d9e820cb8dae|What biases does their model capture?|Data annotation biases where tweet containing disrespectful words are annotated as hate or offensive without any presumption about the social context of tweeters| | 
1910.12574|81a35b9572c9d574a30cc2164f47750716157fc8|What existing approaches do they compare to?|Waseem and Hovy BIBREF5, Davidson et al. BIBREF9, and Waseem et al. BIBREF10|Waseem and Hovy BIBREF5, Davidson et al. BIBREF9, Waseem et al. BIBREF10| 
1702.03342|f4496316ddd35ee2f0ccc6475d73a66abf87b611|What is the benchmark dataset?|a benchmark dataset created by ceccarelli2013learning from the CoNLL 2003 data|dataset created by ceccarelli2013learning from the CoNLL 2003 data| 
1702.03342|e8a32460fba149003566969f92ab5dd94a8754a4|What are the two neural embedding models?|Our first model is the Concept Raw Context model (CRC) which utilizes concept mentions in a large scale KB to jointly learn embeddings of both words and concepts. Our second model is the Concept-Concept Context model (3C) which learns the embeddings of concepts from their conceptual contexts (i.e., contexts containing surrounding concepts only)|Concept Raw Context model, Concept-Concept Context model| 
1805.03710|b7381927764536bd97b099b6a172708125364954|How do they evaluate their resulting word embeddings?|We also evaluate all five models on downstream tasks from the VecEval suite BIBREF13 , using only the tasks for which training and evaluation data is freely available: chunking, sentiment and question classification, and natural language identification (NLI). The default settings from the suite are used, but we run only the fixed settings, where the embeddings themselves are not tunable parameters of the models, forcing the system to use only the information already in the embeddings.| | 
1805.03710|df95b3cb6aa0187655fd4856ae2b1f503d533583|What types of subwords do they incorporate in their model?|n-gram subwords, unsupervised morphemes identified using Morfessor BIBREF11 to learn whether more linguistically motivated subwords |simple n-grams (like fastText) and unsupervised morphemes| 
1805.03710|f7ed3b9ed469ed34f46acde86b8a066c52ecf430|Which matrix factorization methods do they use?|weighted factorization of a word-context co-occurrence matrix |The LexVec BIBREF7| 
1807.07279|c7eb71683f53ab7acffd691a36cad6edc7f5522e|Do they report results only on English data?|Yes| | 
1807.07279|17a1eff7993c47c54eddc7344e7454fbe64191cd|What experiments do they use to quantify the extent of interpretability?|Human evaluation for interpretability using the word intrusion test and automated evaluation for interpretability using a semantic category-based approach based on the method and category dataset (SEMCAT).|semantic category-based approach| 
1807.07279|a5e5cda1f6195ab1336855f1e39a609d61326d62|Along which dimension do the semantically related words take larger values?|dimension corresponding to the concept that the particular word belongs to| | 
1807.07279|32d99dcd8d46e2cda04a9a9fa0e6693d2349a7a9|What is the additive modification to the objective function?|The cost function for any one of the words of concept word-groups is modified by the introduction of an additive term to the cost function. . Each embedding vector dimension is first associated with a concept. For a word belonging to any one of the word-groups representing these concepts, the modified cost term favors an increase for the value of this word's embedding vector dimension corresponding to the concept that the particular word belongs to,|An additive term added to the cost function for any one of the words of concept word-groups| 
1706.09673|eda4869c67fe8bbf83db632275f053e7e0241e8c|Which dataset do they use?| Paraphrase Database (PPDB) ,  book corpus| | 
1706.09673|2c7494d47b2a69f182e83455fe4c75ae3b2893e9|Do they evaluate their learned representations on downstream tasks?|No|No| 
1706.09673|ecc63972b2783ee39b3e522653cfb6dc5917d522|How do they encourage understanding of literature as part of their objective function?|They group the existing works in terms of the objective function they optimize - within-tweet relationships, inter-tweet relationships, autoencoder, and weak supervision.| | 
1906.07662|8d074aabf4f51c8455618c5bf7689d3f62c4da1d|What are the limitations of existing Vietnamese word segmentation systems?| ambiguous words, unknown words|lacks of complete review approaches, datasets and toolkits | 
1906.07662|fe2666ace293b4bfac3182db6d0c6f03ea799277|Why challenges does word segmentation in Vietnamese pose?|Acquire very large Vietnamese corpus and build a classifier with it, design a develop a big data warehouse and analytic framework, build a system to incrementally learn new corpora and interactively process feedback.|to acquire very large Vietnamese corpus and to use them in building a classifier,  design and development of big data warehouse and analytic framework for Vietnamese documents, to building a system, which is able to incrementally learn new corpora and interactively process feedback| 
1906.07662|70a1b0f9f26f1b82c14783f1b76dfb5400444aa4|How successful are the approaches used to solve word segmentation in Vietnamese?|Their accuracy in word segmentation is about 94%-97%.| | 
1906.07662|d3ca5f1814860a88ff30761fec3d860d35e39167|Which approaches have been applied to solve word segmentation in Vietnamese?|Maximum Entropy, Weighted Finite State Transducer (WFST),  support vector machines (SVM), conditional random fields (CRF)|Maximum matching, Hidden Markov model , Maximum Entropy, Conditional Random Fields , Support Vector Machines| 
2002.12612|dd20d93166c14f1e57644cd7fa7b5e5738025cd0|Which two news domains are country-independent?|mainstream news and disinformation|mainstream and disinformation news| 
2002.12612|dc2a2c177cd5df6da5d03e6e74262bf424850ec9|How is the political bias of different sources included in the model?|By assigning a political bias label to each news article and training only on left-biased or right-biased outlets of both disinformation and mainstream domains|we also account for political biases inherent to different news sources, referring to the procedure proposed in BIBREF2 to label different outlets. Overall we show that we are able to classify credible vs non-credible diffusion networks (and consequently news articles) with high accuracy (AUROC up to 94%), even when accounting for the political bias of sources (and training only on left-biased or right-biased articles). We observe that the layer of mentions alone conveys useful information for the classification, denoting a different usage of this functionality when sharing news belonging to the two news domains. We also show that most discriminative features, which are relative to the breadth and depth of largest cascades in different layers, are the same across the two countries.| 
2002.12612|ae90c5567746fe25af2fcea0cc5f355751e05c71|What are the two large-scale datasets used?|US dataset, Italian dataset|US dataset, Italian dataset| 
2002.12612|d7644c674887ca9708eb12107acd964ae53b216d|What are the global network features which quantify different aspects of the sharing process?|Number of Strongly Connected Components (SCC), Size of the Largest Strongly Connected Component (LSCC), Number of Weakly Connected Components (WCC), Size of the Largest Weakly Connected Component (LWCC), Diameter of the Largest Weakly Connected Component (DWCC), Average Clustering Coefficient (CC), Main K-core Number (KC), Density (d)| | 
1908.05441|a3bb9a936f61bafb509fa12ac0a61f91abcc5106|Which datasets are used for evaluation?|ARC , TREC, GARD , MLBioMedLAT |ARC, TREC, GARD, MLBioMedLAT| 
1908.05441|df6d327e176740da9edcc111a06374c54c8e809c|What previous methods is their model compared to?|bag-of-words model, CNN| | 
1908.05441|49764eee7fb523a6a28375cc699f5e0220b81766|Did they use a crowdsourcing platform?|No|No| 
1908.05441|3321d8d0e190d25958e5bfe0f3438b5c2ba80fd1|How was the dataset collected?|from 3rd to 9th grade science questions collected from 12 US states|Used from  science exam questions of the Aristo Reasoning Challenge (ARC) corpus.| 
1811.08603|bb3267c3f0a12d8014d51105de5d81686afe5f1b|Which datasets do they use?|CoNLL-YAGO, TAC2010, ACE2004, AQUAINT, WW|CoNLL-YAGO, TAC2010, ACE2004, AQUAINT, WW| 
1811.08603|114934e1a1e818630ff33ac5c4cd4be6c6f75bb2|How effective is their NCEL approach overall?|NCEL consistently outperforms various baselines with a favorable generalization ability| | 
1811.08603|2439b6b92d73f660fe6af8d24b7bbecf2b3a3d72|How do they verify generalization ability?|By calculating Macro F1 metric at the document level.|by evaluating their model on five different benchmarks| 
1811.08603|b8d0e4e0e820753ffc107c1847fe1dfd48883989|Do they only use adjacent entity mentions or use more than that in some cases (next to adjacent)?|NCEL considers only adjacent mentions.|More than that in some cases (next to adjacent) | 
1909.03135|5aa12b4063d6182a71870c98e4e1815ff3dc8a72|Do the authors mention any downside of lemmatizing input before training ELMo?|Yes|Yes| 
1909.03135|22815878083ebd2f9e08bc33a5e733063dac7a0f|What other examples of morphologically-rich languages do the authors give?|Russian|Russian| 
1909.03135|220d11a03897d85af91ec88a9b502815c7d2b6f3|Why is lemmatization not necessary in English?|Advanced neural architectures and contextualized embedding models learn how to handle spelling and morphology variations.| | 
1804.07789|c2e475adeddcdc4d637ef0d4f5065b6a9b299827|What metrics are used for evaluation?|BLEU-4, NIST-4, ROUGE-4|BLEU-4, NIST-4, ROUGE-4| 
1804.07789|cb6a8c642575d3577d1840ca2f4cd2cc2c3397c5|Do they use pretrained embeddings?|Yes|Yes| 
1804.07789|1088255980541382a2aa2c0319427702172bbf84|What is a bifocal attention mechanism?|At the macro level, it is important to decide which is the appropriate field to attend to next, micro level (i.e., within a field) it is important to know which values to attend to next, fuse the attention weights at the two levels| | 
1905.11268|0d9fcc715dee0ec85132b3f4a730d7687b6a06f4|"What does the ""sensitivity"" quantity denote?"|the number of distinct word recognition outputs that an attacker can induce|The expected number of unique outputs a word recognition system assigns to a set of adversarial perturbations |the number of distinct word recognition outputs that an attacker can induce, not just the number of words on which the model is “fooled”
1905.11268|8910ee2236a497c92324bbbc77c596dba39efe46|What end tasks do they evaluate on?|Sentiment analysis and paraphrase detection under adversarial attacks| | 
1905.11268|2c59528b6bc5b5dc28a7b69b33594b274908cca6|What is a semicharacter architecture?|A semi-character based RNN (ScRNN) treats the first and last characters individually, and is agnostic to the ordering of the internal characters|processes a sentence of words with misspelled characters, predicting the correct words at each step| 
1905.11268|6b367775a081f4d2423dc756c9b65b6eef350345|Do they experiment with offering multiple candidate corrections and voting on the model output, since this seems highly likely to outperform a one-best correction?|No| | 
1905.11268|bc01853512eb3c11528e33003ceb233d7c1d7038|Why is the adversarial setting appropriate for misspelling recognition?|Adversarial misspellings are a real-world problem| | 
1905.11268|ba539cab80d25c3e20f39644415ed48b9e4e4185|How do the backoff strategies work?|In pass-through, the recognizer passes on the possibly misspelled word, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.|Pass-through passes the possibly misspelled word as is, backoff to neutral word backs off to a word with similar distribution across classes and backoff to background model backs off to a more generic word recognition model trained with larger and less specialized corpus.|"Backoff to ""a"" when an UNK-predicted word is encountered, backoff to a more generic word recognition model when the model predicts UNK"
1603.01514|6bf5620f295b5243230bc97b340fae6e92304595|What baseline model is used?|same baseline as used by lang2011unsupervised|We use the same baseline as used by lang2011unsupervised which has been shown to be difficult to outperform. This baseline assigns a semantic role to a constituent based on its syntactic function, i.e. the dependency relation to its head.| 
1603.01514|4986f420884f917d1f60d3cea04dc8e64d3b5bf1|Which additional latent variables are used in the model?|CLV as a parent of the two corresponding role variables|crosslingual latent variables| 
1603.01514|747b847d687f703cc20a87877c5b138f26ff137d|Which parallel corpora are used?|English (EN) and German (DE) sections of the CoNLL 2009 corpus BIBREF13, EN-DE section of the Europarl corpus BIBREF14|the English (EN) and German (DE) sections of the CoNLL 2009 corpus BIBREF13 , and EN-DE section of the Europarl corpus BIBREF14 | 
1603.01514|111afb77cfbf4c98e0458606378fa63a0e965e36|Overall, does having parallel data improve semantic role induction across multiple languages?|No|No| 
1603.01514|6568a31241167f618ef5ede939053feaa2fb0d7e|Do they add one latent variable for each language pair in their Bayesian model?|Yes| | 
1603.01514|50cc6c5f2dcf5fb87b56007f6a825fa7c90b64ed|What does an individual model consist of?|Bayesian model of garg2012unsupervised as our base monolingual model| | 
1603.01514|0fc2b5bc2ead08a6fe0280fb3a47477c6df1587c|Do they improve on state-of-the-art semantic role induction?|Yes| | 
1908.04042|4dc268e3d482e504ca80d2ab514e68fd9b1c3af1|how many tags do they look at?|48,705| | 
1908.04042|ab54cd2dc83141bad3cb3628b3f0feee9169a556|which algorithm was the highest performer?|A hybrid model consisting of best performing popularity-based approach with the best similarity-based approach| | 
1908.04042|249c805ee6f2ebe4dbc972126b3d82fb09fa3556|how is diversity measured?|average dissimilarity of all pairs of tags in the list of recommended tags| the average dissimilarity of all pairs of tags in the list of recommended tags| 
1908.04042|b4f881331b975e6e4cab1868267211ed729d782d|how large is the vocabulary?|33,663|33,663 distinct review keywords | 
1908.04042|79413ff5d98957c31866f22179283902650b5bb6|what dataset was used?|48,705 e-books from 13 publishers, search query logs of 21,243 e-books for 12 months| E-book annotation data: editor tags, Amazon search terms, and  Amazon review keywords.| 
1908.04042|29c014baf99fb9f40b5171aab3e2c7f12a748f79|what algorithms did they use?|popularity-based, similarity-based, hybrid| | 
1610.00956|09c86ef78e567033b725fc56b85c5d2602c1a7c3|How does their ensemble method work?|simply averaging the predictions from the constituent single models| | 
1610.00956|d67c01d9b689c052045f3de1b0918bab18c3f174|How large are the improvements of the Attention-Sum Reader model when using the BookTest dataset?|INLINEFORM2 |Answer with content missing: (Table 2) Accuracy of best AS reader results including ensembles are 78.4 and 83.7 when trained on BookTest compared to 71.0 and 68.9 when trained on CBT for Named endity and Common noun respectively.| 
1610.00956|e5bc73974c79d96eee2b688e578a9de1d0eb38fd|How do they show there is space for further improvement?| by testing humans on a random subset of 50 named entity and 50 common noun validation questions that the psr ensemble could not answer correctly|majority of questions that our system could not answer so far are in fact answerable| 
1601.02403|2cd37743bcc7ea3bd405ce6d91e79e5339d7642e|Do they report results only on English data?|Yes|Yes| 
1601.02403|eac9dae3492e17bc49c842fb566f464ff18c049b|What argument components do the ML methods aim to identify?|claim, premise, backing, rebuttal, and refutation|claim, premise, backing, rebuttal, refutation| 
1601.02403|7697baf8d8d582c1f664a614f6332121061f87db|Which machine learning methods are used in experiments?|Structural Support Vector Machine|SVMhmm | 
1601.02403|1cb100182508cf55b3509283c0e2bbcd527d625e|How is the data in the new corpus come sourced?|user comments to newswire articles or to blog posts, forum posts, blog posts, newswire articles|refer to each article, blog post, comment, or forum posts as a document| 
1601.02403|d6401cece55a14d2a35ba797a0878dfe2deabedc|What challenges do different registers and domains pose to this task?|linguistic variability| | 
1912.03627|fde700d5134a9ae8f7579bea1f1b75f34d7c1c4c|how was the speech collected?|The speech was collected from respondents using an android application.|Android application| 
1912.03627|f9edd8f9c13b54d8b1253ed30e7decc1999602da|what evaluation protocols are provided?|three experimental setups with different numbers of speakers in the evaluation set, three experimental setups with different number of speaker in the evaluation set are defined,  first one, respondents with at least 17 recording sessions are included to the evaluation set, respondents with 16 sessions to the development and the rest of respondents to the background set, second setup, respondents with at least 8 sessions are included to the evaluation set, respondents with 6 or 7 sessions to the development and the rest of respondents to the background set| | 
1912.03627|30af1926559079f59b0df055da76a3a34df8336f|what is the source of the data?|Android application| | 
1810.12085|c571deefe93f0a41b60f9886db119947648e967c|what datasets were used?|MIMIC-III|MIMIC-III| 
1610.07809|e54257585cc75564341eb02bdc63ff8111992f82|what keyphrase extraction models were reassessed?|"Answer with content missing: (LVL1, LVL2, LVL3) 
- Stanford CoreNLP
- Optical Character Recognition (OCR) system, ParsCIT 
- further abridge the input text from level 2 preprocessed documents to the following: title, headers, abstract, introduction, related work, background and conclusion."| | 
1610.07809|2a3e36c220e7b47c1b652511a4fdd7238a74a68f|how many articles are in the dataset?|244|244 | 
2003.03044|9658b5ffb5c56e5a48a3fea0342ad8fc99741908|Is this dataset publicly available for commercial use?|No|Yes| 
1610.08815|3a6e843c6c81244c14730295cfb8b865cd7ede46|What are the state of the art models?|BIBREF9 , BIBREF8 |BIBREF9 , BIBREF8| 
1610.08815|fabf6fdcfb4c4c7affaa1e4336658c1e6635b1bf|Which benchmark datasets are used?|Semeval 2014 BIBREF34 Twitter Sentiment Analysis Dataset ,  dataset was created by BIBREF8,  English dataset from BIBREF8,  dataset from The Sarcasm Detector|This dataset was created by BIBREF8, another English dataset from BIBREF8 ,  dataset from The Sarcasm Detector| 
1610.08815|1beb4a590fa6127a138f4ed1dd13d5d51cc96809|What are the network's baseline features?| The features extracted from CNN.| | 
1909.00015|5c5aeee83ea3b34f5936404f5855ccb9869356c1|What tasks are used for evaluation?|four machine translation tasks: German -> English, Japanese -> English, Romanian -> English, English -> German| four machine translation tasks, IWSLT 2017 German $\rightarrow $ English BIBREF27, KFTT Japanese $\rightarrow $ English BIBREF28, WMT 2016 Romanian $\rightarrow $ English BIBREF29, WMT 2014 English $\rightarrow $ German BIBREF30| 
1909.00015|5913930ce597513299e4b630df5e5153f3618038|How does their model improve interpretability compared to softmax transformers?|the attention heads in the proposed adaptively sparse Transformer can specialize more and with higher confidence|We introduce sparse attention into the Transformer architecture| 
2001.01269|81d193672090295e687bc4f4ac1b7a9c76ea35df|What baseline method is used?|using word2vec to create features that are used as input to the SVM|use the word2vec algorithm, create several unsupervised hand-crafted features, generate document vectors and feed them as input into the support vector machines (SVM) approach| 
2001.01269|cf171fad0bea5ab985c53d11e48e7883c23cdc44|What details are given about the Twitter dataset?|Those tweets are mostly much noisier and shorter compared to the reviews in the movie corpus. In total, there are 1,716 tweets. 973 of them are negative and 743 of them are positive.|one of the Twitter datasets is about Turkish mobile network operators, there are positive, neutral and negative labels and provide the total amount plus the distribution of labels| 
2001.01269|2a564b092916f2fabbfe893cf13de169945ef2e1|What details are given about the movie domain dataset?|there are 20,244 reviews divided into positive and negative with an average 39 words per review, each one having a star-rating score|The number of reviews in this movie corpus is 20,244 and the average number of words in reviews is 39. Each of these reviews has a star-rating score which is indicative of sentiment.| 
2001.01269|0d34c0812f1e69ea33f76ca8c24c23b0415ebc8d|Which hand-crafted features are combined with word2vec?|three hand-crafted polarity scores, which are minimum, mean, and maximum polarity scores|polarity scores, which are minimum, mean, and maximum polarity scores, from each review| 
2001.01269|73e83c54251f6a07744413ac8b8bed6480b2294f|What word-based and dictionary-based feature are used?|generate word embeddings specific to a domain, TDK (Türk Dil Kurumu - “Turkish Language Institution”) dictionary to obtain word polarities| | 
2001.01269|3355918bbdccac644afe441f085d0ffbbad565d7|How are the supervised scores of the words calculated?|"(+1 or -1), words of opposite polarities (e.g. “happy"" and “unhappy"") get far away from each other"| | 
1708.06185|e48e750743aef36529fbea4328b8253dbe928b4d|what dataset was used?|WASSA-2017 Shared Task on Emotion Intensity| | 
1708.06185|c08aab979dcdc8f4fe8ec1337c3c8290ab13414e|how many total combined features were there?|Fourteen | | 
1708.06185|8756b7b9ff5e87e4efdf6c2f73a0512f05b5ae3f|what pretrained word embeddings were used?|Pretrained word embeddings  were not used|GloVe, Edinburgh embeddings BIBREF14, Emoji embeddings BIBREF16| 
1705.01214|3e432d71512ffbd790a482c716e7079ee78ce732|What datasets are used?|Custom dataset with user questions; set of documents, twitter posts and news articles, all related to finance.|a self-collected financial intents dataset in Portuguese| 
1705.01214|dd76130ec5fac477123fe8880472d03fbafddef6|What is the state of the art described in the paper?|ELIZA,  PARRY, A.L.I.C.E., Cleverbot| | 
1908.07195|43eecc576348411b0634611c81589f618cd4fddf|What GAN models were used as baselines to compare against?|MLE, SeqGAN, LeakGAN, MaliGAN, IRL, RAML, DialogGAN, DPGAN|SeqGAN, LeakGAN, MaliGAN, DialogGAN, DPGAN| 
2001.07786|1038542243efe5ab3e65c89385e53c4831cd9981|What is the corpus used for the task?|DTA18, DTA19|Diachronic Usage Relatedness (DURel) gold standard data set| 
2001.07786|e2b0cd30cf56a4b13f96426489367024310c3a05|How is evaluation performed?|As the metric to assess how well the model's output fits the gold ranking Spearman's $\rho $ was used|Spearman's rank-order correlation| 
1912.00903|e831041d50f3922265330fcbee5a980d0e2586dd|What is a normal reading paradigm?|read the sentences normally without any special instructions|participants were instructed to read the sentences naturally, without any specific task other than comprehension| 
1912.00903|7438b6b146e41c08cf8f4c5e1d130c3b4cfc6d93|Did they experiment with this new dataset?|No| | 
1912.00903|ac7f6497be4bcca64e75f28934b207c9e8097576|What kind of sentences were read?|sentences that were selected from the Wikipedia corpus provided by culotta2006integrating|seven of the originally defined relation types: political_affiliation, education, founder, wife/husband, job_title, nationality, and employer| 
1903.11437|87bb3105e03ed6ac5abfde0a7ca9b8de8985663c|why are their techniques cheaper to implement?|They use a slightly modified copy of the target to create the pseudo-text instead of full BT to make their technique cheaper|They do not require the availability of a backward translation engine.| 
1903.11437|d9980676a83295dda37c20cfd5d58e574d0a4859|what data simulation techniques were introduced?|copy, copy-marked, copy-dummies|copy, copy-marked, copy-dummies| 
1903.11437|9225b651e0fed28d4b6261a9f6b443b52597e401|what is their explanation for the effectiveness of back-translation?|when using BT, cases where the source is shorter than the target are rarer; cases when they have the same length are more frequent, automatic word alignments between artificial sources tend to be more monotonic than when using natural sources| | 
1903.11437|565189b672efee01d22f4fc6b73cd5287b2ee72c|what dataset is used?|Europarl corpus , WMT newstest 2014, News-Commentary-11, Wikipedia from WMT 2014, Multi-UN, EU-Bookshop, Rapid, Common-Crawl (WMT 2017)|Europarl tests from 2006, 2007, 2008; WMT newstest 2014.| 
1903.11437|b6f7fadaa1bb828530c2d6780289f12740229d84|what language pairs are explored?|English-German, English-French.|English-German, English-French| 
1903.11437|7b9ca0e67e394f1674f0bcf1c53dfc2d474f8613|what language is the data in?|English , German, French| | 
1703.04001|4e1a67f8dc68b55a5ce18e6cd385ae9ab90d891f|Does the experiments focus on a specific domain?|No|No| 
1703.04001|9af3142630b350c93875441e1e1767312df76d17|Do the answered questions measure for the usefulness of the answer?|No| | 
1909.10012|e374169ee10f835f660ab8403a5701114586f167|What profile metadata is used for this analysis?|username, display name, profile image, location, description|username, display name, profile image, location and description| 
1909.10012|82595ca5d11e541ed0c3353b41e8698af40a479b|What are the organic and inorganic ways to show political affiliation through profile changes?|"Organic: mention of political parties names in the profile attributes, specific mentions of political handles in the profile attributes.
Inorganic:  adding Chowkidar to the profile attributes, the effect of changing the profile attribute in accordance with Prime Minister's campaign, the addition of election campaign related keywords to the profile."|Mentioning of political parties names and political twitter handles is the organic way to show political affiliation; adding Chowkidar or its variants to the profile is the inorganic way.| 
1909.10012|d4db7df65aa4ece63e1de813e5ce98ce1b4dbe7f|How do profile changes vary for influential leads and their followers over the social movement?|Influential leaders are more likely to change their profile attributes than their followers; the leaders do not change their usernames, while their followers change their usernames a lot; the leaders  tend to make new changes related to previous attribute values, while the followers make comparatively less related changes to previous attribute values.| | 
1907.06292|53dfcd5d7d2a81855ec1728f0d8e6e24c5638f1e|What evaluation metrics do they use?|BLEU-1, Meteor , Rouge-L |BLEU-1, Meteor ,  Rouge-L | 
1907.06292|869feb7f47606105005efdb6bea1c549824baea0|What is the size of this dataset?|13,757|10,898 articles, 17,794 tweets, and 13,757 crowdsourced question-answer pairs| 
1907.06292|c497e8701060583d91bb64b9f9202d40047effc4|How do they determine if tweets have been used by journalists?| we look into the archived snapshots of two major news websites (CNN, NBC), and then extract the tweet blocks that are embedded in the news articles| | 
1703.07090|8060a773f6a136944f7b59758d08cc6f2a59693b|how small of a dataset did they train on?|1000 hours data|23085 hours of data| 
1810.04635|9de2f73a3db0c695e5e0f5a3d791fdc370b1df6e|Do they use datasets with transcribed text or do they determine text from the audio?|They use text transcription.|both| 
1810.04635|e0122fc7b0143d5cbcda2120be87a012fb987627|By how much does their model outperform the state of the art results?|the attention model, MDREA, also outperforms the best existing research results (WAP 0.690 to 0.688)| | 
1810.04635|5da26954fbcd3cf6a7dba9f8b3c9a4b0391f67d4|How do they combine audio and text sequences in their RNN?|combines the information from these sources using a feed-forward neural model|encodes the information from audio and text sequences using dual RNNs and then combines the information from these sources using a feed-forward neural model| 
1707.03569|876700622bd6811d903e65314ac75971bbe23dcc|What dataset did they use?| high-quality datasets  from SemEval-2016 “Sentiment Analysis in Twitter” task| SemEval-2016 “Sentiment Analysis in Twitter”| 
1912.10011|d915b401bb96c9f104a0353bef9254672e6f5a47|What future possible improvements are listed?|rther constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions|to further constrain the model on the data structure in order to prevent inaccurate of even contradictory descriptions| 
1912.10011|79a44a68bb57b375d8a57a0a7f522d33476d9f33|Which qualitative metric are used for evaluation?| Relation Generation (RG) , Content Selection (CS),  Content Ordering (CO)|Relation Generation (RG), Content Selection (CS), Content Ordering (CO)| 
1909.09534|f20a389ace2267aa61eddcc235535452ccdae0e6|Do they report results only on English data?|Yes|Yes| 
1909.09534|6411622cc8b2fbedbfa468859d453596d3bd2f03|What objective function is used in the GAN?|language modeling objective| | 
1909.09534|fc77d70c305fa80447b191248aba93da63ac3704|Which datasets are used?|A corpus of 740 classical and contemporary English poems,  a corpus of 14950 metaphor sentences retrieved from a metaphor database website , a corpus of 1500 song lyrics ranging across genres, Gutenberg dataset |(1) A corpus of 740 classical and contemporary English poems, (2) a corpus of 14950 metaphor sentences retrieved from a metaphor database website, (3) a corpus of 1500 song lyrics ranging across genres, Gutenberg dataset BIBREF24| 
1909.00105|9bfebf8e5bc0bacf0af96a9a951eb7b96b359faa|What were their results on the new dataset?|average recipe-level coherence scores of 1.78-1.82, human evaluators preferred personalized model outputs to baseline 63% of the time| | 
1909.00105|34dc0838632d643f33c8dbfe7bd4b656586582a2|What are the baseline models?|name-based Nearest-Neighbor model (NN), Encoder-Decoder baseline with ingredient attention (Enc-Dec)| | 
1909.00105|c77359fb9d3ef96965a9af0396b101f82a0a9de6|How did they obtain the interactions?|from Food.com| | 
1909.00105|1bdc990c7e948724ab04e70867675a334fdd3051|Where do they get the recipes from?|from Food.com| | 
2001.02885|78536da059b884d6ad04680baeb894895458055c|What were the baselines?|varied from Maximum Entropy Classifiers (BIBREF4) to Support Vector Machines (BIBREF5,BIBREF6,BIBREF7,BIBREF8), Recursive Neural Networks (BIBREF9,BIBREF10), Convolutional Neural Networks (BIBREF11) and most recently transfer learning-based architectures like Bidirectional Encoder Representation from Transformers (BERT) (BIBREF12)| | 
2001.02885|96b07373756d7854bccc3c12e8d41454ab8741f5|Does RoBERTa outperform BERT?|No| | 
2001.02885|511517efc96edcd3e91e7783821c9d6d5a6562af|Which multiple datasets did they train on during joint training?|BF, BA, SFU and Sherlock|BioScope Abstracts, SFU, and BioScope Full Papers| 
2001.02885|9122de265577e8f6b5160cd7d28be9e22da752b2|What were the previously reported results?|Figures FIGREF1 and FIGREF1 contain a summary of the papers addressing speculation detection and scope resolution| | 
1801.10293|22b740cc3c8598247ee102279f96575bdb10d53f|Do they study numerical properties of their obtained vectors (such as orthogonality)?|No|No| 
1801.10293|74b4779de437c697fe702e51f23e2b0538b0f631|How do they score phrasal compositionality?|Spearman's INLINEFORM0 between phrasal similarities derived from our compositional functions and the human annotators| | 
1801.10293|435570723b37ee1f5898c1a34ef86a0b2e8701bb|Which translation systems do they compare against?|hierarchical phrase-based system BIBREF29, appropriate additional baseline would be to mark translation rules with these indicator functions but without the scores, akin to identifying rules with phrases in them (Baseline + SegOn)| English-Spanish MT system | 
1809.06537|aa2948209cc33b071dbf294822e72bb136678345|what are their results on the constructed dataset?|AutoJudge consistently and significantly outperforms all the baselines, RC models achieve better performance than most text classification models (excluding GRU+Attention), Comparing with conventional RC models, AutoJudge achieves significant improvement| | 
1809.06537|d9412dda3279729e95fcb35cbed09e61577a896e|what evaluation metrics are reported?|precision, recall, F1 and accuracy|precision, recall, F1 , accuracy | 
1809.06537|41b70699514703820435b00efbc3aac4dd67560a|what civil field is the dataset about?|divorce |divorce| 
1809.06537|06cc8fcafc0880cf69a2514bb7341642b9833041|what is the size of the real-world civil case dataset?|100 000 documents| INLINEFORM1 cases| 
1809.06537|d650101712e36594bd77b45930a990402a455222|what datasets are used in the experiment?|build a new one, collect INLINEFORM0 cases from China Judgments Online| | 
2003.03014|cb384dc5366b693f28680374d31ff45356af0461|Do they model semantics |Yes|Yes| 
2003.03014|d41e20ec716b5904a272938e5a8f5f3f15a7779e|How do they identify discussions of LGBTQ people in the New York Times?|act paragraphs containing any word from a predetermined list of LGTBQ terms | | 
1908.08345|97d1ac71eed13d4f51f29aac0e1a554007907df8|What is novel about their document-level encoder?|Bert model have a maximum length of 512; we overcome this limitation by adding more position embeddings, we insert external [cls] tokens at the start of each sentence, and each [cls] symbol collects features for the sentence preceding it, document representations are learned hierarchically| | 
1908.08345|53014cfb506f6fffb22577bf580ae6f4d5317ce5|What are the datasets used for evaluation?|CNN/DailyMail news highlights, New York Times Annotated Corpus, XSum|the CNN/DailyMail news highlights dataset BIBREF24, the New York Times Annotated Corpus (NYT; BIBREF25), XSum BIBREF22| 
1611.02988|fa30a938b58fc05131c3854f12efe376cbad887f|What was their performance on emotion detection?|Answer with content missing: (Table 3) Best author's model B-M average micro f-score is 0.409, 0.459, 0.411 on Affective, Fairy Tales and ISEAR datasets respectively. | | 
1611.02988|f875337f2ecd686cd7789e111174d0f14972638d|Which existing benchmarks did they compare to?|Affective Text, Fairy Tales, ISEAR| Affective Text dataset, Fairy Tales dataset, ISEAR dataset| 
1611.02988|de53af4eddbc30c808d90b8a11a29217d377569e|Which Facebook pages did they look at?|FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney|FoxNews, CNN, ESPN, New York Times, Time magazine, Huffington Post Weird News, The Guardian, Cartoon Network, Cooking Light, Home Cooking Adventure, Justin Bieber, Nickelodeon, Spongebob, Disney.| 
1604.08504|dac087e1328e65ca08f66d8b5307d6624bf3943f|LDA is an unsupervised method; is this paper introducing an unsupervised approach to spam detection?|No|No| 
1604.08504|a1645d0ba50e4c29f0feb806521093e7b1459081|What is the benchmark dataset and is its quality high?|Social Honeypot dataset (public) and Weibo dataset (self-collected); yes|Social Honeypot, which is not of high quality| 
1604.08504|3cd185b7adc835e1c4449eff81222f5fc15c8500|How do they detect spammers?|Extract features from the LDA model and use them in a binary classification task| | 
1802.08636|f03112b868b658c954db62fc64430bebbaa7d9e0|Do they use other evaluation metrics besides ROUGE?|Yes|No| 
1802.08636|a6d3e57de796172c236e33a6ceb4cca793dc2315|What are the baselines?|"Answer with content missing: (Experimental Setup missing subsections)
To be selected: We compared REFRESH against a baseline which simply selects the first m leading sentences from each document (LEAD) and two neural models similar to ours (see left block in Figure 1), both trained with cross-entropy loss.
Answer: LEAD"| | 
2001.07820|395b61d368e8766014aa960fde0192e4196bcb85|What datasets do they use?|three datasets based on IMDB reviews and Yelp reviews|1 IMDB dataset and 2 Yelp datasets| 
2001.07820|92bb41cf7bd1f7886784796a8220ed5aa07bc49b|What other factors affect the performance?|architecture of the classifier, sentence length,  input domain| | 
2001.07820|4ef11518b40cc55d86c485f14e24732123b0d907|What are the benchmark attacking methods?|FGM, FGVM, DeepFool BIBREF5, HotFlip BIBREF3) and TYC BIBREF4|FGM, FGVM, DeepFool, HotFlip, TYC| 
2002.01320|6a219d7c58451842aa5d6819a7cdf51c55e9fc0f|What domains are covered in the corpus?|No specific domain is covered in the corpus.| | 
2002.01320|cee8cfaf26e49d98e7d34fa1b414f8f31d6502ad|What is the architecture of their model?|follow the architecture in berard2018end, but have 3 decoder layers like that in pino2019harnessing| | 
2002.01320|f8f4e4a50d2b3fbd193327e79ea32d8d057e1414|How was the dataset collected?|Contributors record voice clips by reading from a bank of donated sentences.|crowdsourcing| 
2002.01320|bc84c5a58c57038910f7720d7a784560054d3e1a|Which languages are part of the corpus?|French (Fr), German (De), Dutch (Nl), Russian (Ru), Spanish (Es), Italian (It), Turkish (Tr), Persian (Fa), Swedish (Sv), Mongolian (Mn) and Chinese (Zh)|French, German, Dutch, Russian, Spanish, Italian, Turkish, Persian, Swedish, Mongolian and Chinese| 
2002.01320|29923a824c98b3ba85ced964a0e6a2af35758abe|How is the quality of the data empirically evaluated? |Validated transcripts were sent to professional translators., various sanity checks to the translations,  sanity check the overlaps of train, development and test sets|computed sentence-level BLEU, We manually inspected examples where the source transcript was identical to the translation, measured the perplexity of the translations, computed the ratio of English characters in the translations, calculate similarity scores between transcripts and translations| 
2002.01320|559c68802ee2bb8b11e2188127418ca3a6155ba7|Is the data in CoVoST annotated for dialect?|No| | 
2002.01320|8dc707a0daf7bff61a97d9d854283e65c0c85064|Is Arabic one of the 11 languages in CoVost?|No|No| 
1605.07333|30eacb4595014c9c0e5ee9669103d003cfdfe1e5|Which dataset do they train their models on?|relation classification dataset of the SemEval 2010 task 8|SemEval 2010 task 8 BIBREF8| 
1605.07333|0f7867f888109b9e000ef68965df4dde2511a55f|How does their simple voting scheme work?|we apply several CNN and RNN models presented in Tables TABREF12 and TABREF14 and predict the class with the most votes, In case of a tie, we pick one of the most frequent classes randomly|Among all the classes predicted by several models, for each test sentence, class with most votes are picked. In case of a tie, one of the most frequent classes are picked randomly.| 
1605.07333|e2e977d7222654ee8d983fd8ba63b930e9a5a691|Which variant of the recurrent neural network do they use?|uni-directional RNN| | 
1605.07333|0cfe0e33fbb100751fc0916001a5a19498ae8cb5|How do they obtain the new context represetation?|They use two independent convolutional and max-pooling layers on (1) a combination of the left context, the left entity and the middle context; and (2) a combination of the middle context, the right entity and the right context. They concatenated the two results after pooling to get the new context representation.| | 
2003.08385|612c3675b6c55b60ae6d24265ed8e20f62cb117e|Did they pefrorm any cross-lingual vs single language evaluation?|Yes| | 
2003.08385|787c4d4628eac00dbceb1c96020bff0090edca46|What annotations are present in dataset?|answer each question with either `yes', `rather yes', `rather no', or `no'., can supplement each answer with a comment of at most 500 characters| | 
1901.10133|559920ebe19e99e43418c2f0455a0ffdc8edaaa2|What is an unordered text document, do these arise in real-world corpora?|A unordered text document is one where sentences in the document are disordered or jumbled. It doesn't appear that unordered text documents appear in corpora, but rather are introduced as part of processing pipeline.| | 
1901.10133|43f56301c5d2f50b6449b582652f2351cbe90e70|What kind of model do they use?|Our methodology is described in the Figure 1 | | 
1901.10133|68cd8433a75eee7d100dbbfedebbf53873a21720|Do they release a data set?|No|No| 
1901.10133|0a01341ddca3fe4c8727be7cf5841091c6ca3d0b|Do they release code?|No|No| 
1911.00841|3c3807f226ba72fc41f59f0338f12a49a0c35605|Are the experts comparable to real-world users?|No| | 
1911.00841|c70bafc35e27be9d1efae60596bc0dd390c124c0|Are the answers double (and not triple) annotated?|Yes| | 
1911.00841|81d607fc206198162faa54a796717c2805282d9b|Who were the experts used for annotation?|Individuals with legal training|Yes| 
1911.00841|51fe4d44887c5cc5fc98b65ca4cb5876f0a56dad|What type of neural model was used?|Bert + Unanswerable|CNN, BERT| 
1911.00841|f0848e7a339da0828278f6803ed7990366c975f0|Were other baselines tested to compare with the neural baseline?|SVM, No-Answer Baseline (NA) , Word Count Baseline, Human Performance|No-Answer Baseline (NA), Word Count Baseline, Human Performance| 
1605.03481|b85fc420eb2f77f6f14f375cc1fcc5155eb5c0a8|Does the paper clearly establish that the challenges listed here exist in this dataset and task?|Yes| | 
1605.03481|792f6d76d2befba2af07198584aac1b189583ae4|Is this hashtag prediction task an established task, or something new?|established task|Hashtag prediction for social media has been addressed earlier| 
1605.03481|127d5ddfabec5c58832e5865cbd8ed0978c25a13|What is the word-level baseline?|a simple word-level encoder, The encoder is essentially the same as tweet2vec, with the input as words instead of characters.|The encoder is essentially the same as tweet2vec, with the input as words instead of characters.| 
1605.03481|b91671715ad4fad56c67c28ce6f29e180fe08595|What other tasks do they test their method on?|None| | 
1605.03481|a6d37b5975050da0b1959232ae756fc09e5f87e8|what is the word level baseline they compare to?|a simple word-level encoder, with the input as words instead of characters|The encoder is essentially the same as tweet2vec, with the input as words instead of characters| 
1908.07245|7ab9c0b4ceca1c142ff068f85015a249b14282d0|Do they incoprorate WordNet into the model?|construct context-gloss pairs from all possible senses of the target word in WordNet, thus treating WSD task as a sentence-pair classification problem|construct context-gloss pairs from glosses of all possible senses (in WordNet) of the target word| 
1908.07245|00050f7365e317dc0487e282a4c33804b58b1fb3|Is SemCor3.0 reflective of English language data in general?|Yes| | 
1908.07245|c5b0ed5db65051eebd858beaf303809aa815e8e5|Do they use large or small BERT?|small BERT|small BERT| 
1908.07245|10fb7dc031075946153baf0a0599e126de29e3a4|How does the neural network architecture accomodate an unknown amount of senses per word?|converts WSD to a sequence learning task,  leverage gloss knowledge, by extending gloss knowledge| | 
1901.01010|12f7fac818f0006cf33269c9eafd41bbb8979a48|What kind of model do they use?|visual model is based on fine-tuning an Inception V3 model BIBREF1 over visual renderings of documents, while our textual model is based on a hierarchical biLSTM. We further combine the two into a joint model. , neural network models|Inception V3, biLSTM| 
1901.01010|d5a8fd8bb48dd1f75927e874bdea582b4732a0cd|Did they release their data set of academic papers?|No| | 
1901.01010|23e2971c962bb6486bc0a66ff04242170dd22a1d|Which is more useful, visual or textual features?|It depends on the dataset. Experimental results over two datasets reveal that textual and visual features are complementary. | | 
1901.01010|c9bc6f53b941863e801280343afa14248521ce43|Which languages do they use?|English|English| 
1901.01010|07b70b2b799b9efa630e8737df8b1dd1284f032c|How large is their data set?|a sample of  29,794 wikipedia articles and 2,794 arXiv papers | | 
1901.01010|71a0c4f19be4ce1b1bae58a6e8f2a586e125d074|Where do they get their ground truth quality judgments?|Wikipedia articles are labelled with one of six quality classes, in descending order of quality: Featured Article (“FA”), Good Article (“GA”), B-class Article (“B”), C-class Article (“C”), Start Article (“Start”), and Stub Article (“Stub”)., The quality class of a Wikipedia article is assigned by Wikipedia reviewers or any registered user, who can discuss through the article's talk page to reach consensus., The arXiv dataset BIBREF2 consists of three subsets of academic articles under the arXiv repository of Computer Science (cs), from the three subject areas of: Artificial Intelligence (cs.ai), Computation and Language (cs.cl), and Machine Learning (cs.lg). In line with the original dataset formulation BIBREF2 , a paper is considered to have been accepted (i.e. is positively labeled) if it matches a paper in the DBLP database or is otherwise accepted by any of the following conferences: ACL, EMNLP, NAACL, EACL, TACL, NIPS, ICML, ICLR, or AAAI. Failing this, it is considered to be rejected (noting that some of the papers may not have been submitted to one of these conferences). |quality class labels assigned by the Wikipedia community, a paper is considered to have been accepted (i.e. is positively labeled) if it matches a paper in the DBLP database or is otherwise accepted by any of the following conferences: ACL, EMNLP, NAACL, EACL, TACL, NIPS, ICML, ICLR, or AAAI| 
1809.02279|c2eb743c9d0baf1781c3c0df9533fab588250af3|Which models did they experiment with?|Stacked LSTMs, Cell-aware Stacked LSTMs, Sentence Encoders, Top-layer Classifiers| | 
1809.02279|f7d0fa52017a642a9f70091a252857fccca31f12|What were the baselines?|(i) models that use plain stacked LSTMs, (ii) models with different INLINEFORM0, (iii) models without INLINEFORM1, (iv) models that integrate lower contexts via peephole connections| | 
1809.02279|01209a3bead7c87bcdc628be2a5a26b41abde9d1|Which datasets were used?|SNLI BIBREF22 and MultiNLI BIBREF23, Quora Question Pairs dataset BIBREF24,  Stanford Sentiment Treebank (SST) BIBREF25|SNLI BIBREF22 and MultiNLI BIBREF23 datasets, Quora Question Pairs dataset BIBREF24, Stanford Sentiment Treebank (SST) BIBREF25| 
2002.01207|2740e3d7d33173664c1c5ab292c7ec75ff6e0802|what datasets were used?|diacritized corpus that was used to train the RDI BIBREF7 diacritizer and the Farasa diacritizer BIBREF31, WikiNews test set BIBREF31,  large collection of fully diacritized classical texts (2.7M tokens) from a book publisher|the diacritized corpus that was used to train the RDI BIBREF7 diacritizer , WikiNews , a large collection of fully diacritized classical texts| 
2002.01207|db72a78a7102b5f0e75a4d9e1a06a3c2e7aabb21|what are the previous state of the art?|Farasa, RDI|Farasa BIBREF31, MADAMIRA BIBREF29, RDI (Rashwan et al., 2015), MIT (Belinkow and Glass, 2015), Microsoft ATKS BIBREF28| 
2002.01207|48bd71477d5f89333fa7ce5c4556e4d950fb16ed|what surface-level features are used?|affixes, leading and trailing characters in words and stems, and the presence of words in large gazetteers of named entities| | 
1803.05223|ad1be65c4f0655ac5c902d17f05454c0d4c4a15d|what dataset statistics are provided?|More than 2,100 texts were paired with 15 questions each, resulting in a total number of approx. 32,000 annotated questions. 13% of the questions are not answerable.  Out of the answerable questions, 10,160 could be answered from the text directly (text-based) and 3,914 questions required the use of commonsense knowledge (script-based).  The final dataset comprises 13,939 questions, 3,827 of which require commonsense knowledge (i.e. 27.4%).|Distribution of category labels, number of answerable-not answerable questions, number of text-based and script-based questions, average text, question, and answer length, number of questions per text| 
1803.05223|2eb9280d72cde9de3aabbed993009a98a5fe0990|what is the size of their dataset?|13,939| | 
1803.05223|154a721ccc1d425688942e22e75af711b423e086|what crowdsourcing platform was used?|Amazon Mechanical Turk|Amazon Mechanical Turk| 
1803.05223|84bad9a821917cb96584cf5383c6d2a035358d7c|how was the data collected?|The data was collected using 3 components: describe a series of pilot studies that were conducted to collect commonsense inference questions, then discuss the resulting data collection of questions, texts and answers via crowdsourcing on Amazon Mechanical Turk and gives information about some necessary postprocessing steps and the dataset validation.| | 
1905.10851|49eb52b3ec0647e165a5e41488088c80a20cc78f|What aspects of discussion are relevant to instructor intervention, according to the attention mechanism?|context inference| | 
1905.10851|9bb7ae50bff91571a945c1af025ed2e67714a788|What was the previous state of the art for this task?|hLSTM|hLSTM| 
1905.10851|81dbe9a9ddaa5d02b02e01a306d898015a56ffb6|What type of latent context is used to predict instructor intervention?|the series of posts that trigger an intervention| | 
1906.07701|348886b4762db063711ef8b7a10952375fbdcb57|Do they report results only on English dataset?|No|No| 
1906.07701|1ed49a8c07ef0ac15cfa6b7decbde6604decbd5b|What dataset does this approach achieve state of the art results on?|the English-German dataset| | 
2002.07306|f9aa055bf73185ba939dfb03454384810eb17ad1|How much training data from the non-English language is used by the system?|No data. Pretrained model is used.| | 
2002.07306|d571e0b0f402a3d36fb30d70cdcd2911df883bc7|Is the system tested on low-resource languages?|Yes|Yes| 
2002.07306|ce2b921e4442a21555d65d8ce4ef7e3bde931dfc|What languages are the model transferred to?|French (fr), Russian (ru), Arabic (ar), Chinese (zh), Hindi (hi), and Vietnamese (vi)|French (fr), Russian (ru), Arabic (ar), Chinese (zh), Hindi (hi), and Vietnamese (vi)| 
2002.07306|2275b0e195cd9cb25f50c5c570da97a4cce5dca8|How is the model transferred to other languages?|Build a bilingual language model,   learn the target language specific parameters starting from a pretrained English LM , fine-tune both English and target model to obtain the bilingual LM.| | 
2002.07306|37f8c034a14c7b4d0ab2e0ed1b827cc0eaa71ac6|What metrics are used for evaluation?|translation probabilities, Labeled Attachment Scores (LAS)|accuracy, Labeled Attachment Scores (LAS)| 
2002.07306|d01c51155e4719bf587d114bcd403b273c77246f|What datasets are used for evaluation?|United Nations Parallel Corpus, IIT Bombay corpus, OpenSubtitles 2018| | 
1810.12091|9b4dc790e4ff49562992aae4fad3a38621fadd8b|what are the existing approaches?|BOW-Tags, BOW-KL(Tags), BOW-All, GloVe| | 
1810.12091|a1dac888f63c9efaf159d9bdfde7c938636f07b1|what dataset is used in this paper?| the same datasets as BIBREF7|same datasets as BIBREF7| 
1810.05241|1e4dbfc556cf237accb8b370de2f164fa723687b|How is keyphrase diversity measured?|average unique predictions, illustrate the difference of predictions between our proposed models, we show an example chosen from the KP20k validation set| | 
1810.05241|fff5c24dca92bc7d5435a2600e6764f039551787|How was the StackExchange dataset collected?|they obtained computer science related topics by looking at titles and user-assigned tags| | 
1810.05241|19b7312cfdddb02c3d4eaa40301a67143a72a35a|What two metrics are proposed?|average unique predictions, randomly sample 2000 decoder hidden states at INLINEFORM4 steps following a delimiter ( INLINEFORM5 ) and apply an unsupervised clustering method (t-SNE BIBREF35 )| | 
1805.01445|22744c3bc68f120669fc69490f8e539b09e34b94|Can the findings of this paper be generalized to a general-purpose task?|Yes|Yes| 
1805.01445|dcea88698949da4a1bd00277c06df06c33f6a5ff|Why does the proposed task a good proxy for the general-purpose sequence to sequence tasks?|The task is set up to mimic (albeit, in an oversimplified manner) the input-output symbol alignments and local syntactic properties that models must learn in many natural language tasks, such as translation, tagging and summarization.| | 
1706.01875|d7b60abb0091246e29d1a9c28467de598e090c20|What was the baseline?|stochastic gradient descent, naive bayes, decision tree| | 
1706.01875|bdf93053b1b9b0a21f77ed370cf4d5a10df70e3e|What was their system's performance?|accuracy and F1-score of 89.6% and 89.2%, respectively|accuracy and F1-score of 89.6% and 89.2%, respectively| 
1706.01875|5a6926de13a8cc25ce687c22741ba97a6e63d4ee|What other political events are included in the database?|US presidential primaries, Democratic and Republican National Conventions| | 
1706.01875|dcc1115aeaf87118736e86f3e3eb85bf5541281c|What classifier did they use?|Random Forest| | 
1909.01362|c74185bced810449c5f438f11ed6a578d1e359b4|What labels for antisocial events are available in datasets?|"The Conversations Gone Awry dataset is labelled as either containing a personal attack from withint (i.e. hostile behavior by one user in the conversation directed towards another) or remaining civil throughout. The Reddit Change My View dataset is labelled with whether or not a coversation eventually had a comment removed by a moderator for violation of Rule 2: ""Don't be rude or hostile to others users."""| | 
1909.01362|88e5d37617e14d6976cc602a168332fc23644f19|What are two datasets model is applied to?| `Conversations Gone Awry' dataset, subreddit ChangeMyView|An expanded version of the existing 'Conversations Gone Awry' dataset and the ChangeMyView dataset, a subreddit whose only annotation is whether the conversation required action by the Reddit moderators. | 
2004.01862|45f7c03a686b68179cadb1413c5f3c1d373328bd|What is the CORD-19 dataset?|which contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses|contains over 45,000 scholarly articles, including over 33,000 with full text, about COVID-19, SARS-CoV-2, and related coronaviruses| 
2004.01862|a2015f02dfb376bf9b218d1c897018f4e70424d7|How large is the collection of COVID-19 literature?|45,000 scholarly articles, including over 33,000 with full text| | 
1908.01294|f697d00a82750b14376fe20a5a2b249e98bebe9b|Which deep learning architecture do they use for sentence segmentation?|Bi-LSTM-CRF|Bi-LSTM-CRF| 
1906.01040|70148c8d0f345ea36200d5ba19d021924d98e759|What is the McGurk effect?|a perceptual illusion, where listening to a speech sound while watching a mouth pronounce a different sound changes how the audio is heard|When the perception of what we hear is influenced by what we see.| 
1909.01383|126ff22bfcc14a2f7e1a06a91ba7b646003e9cf0|what was the baseline?| MT system on the data released by BIBREF11|Transformer base, two-pass CADec model| 
1909.01383|7e4ef0a4debc048b244b61b4f7dc2518b5b466c0|what phenomena do they mention is hard to capture?|Four discourse phenomena - deixis, lexical cohesion, VP ellipsis, and ellipsis which affects NP inflection.| | 
1705.05437|cf874cd9023d901e10aa8664b813d32501e7e4d2|What is NER?|Named Entity Recognition|Named Entity Recognition, including entities such as proteins, genes, diseases, treatments, drugs, etc. in the biomedical domain| 
1705.05437|42084c41343e5a6ae58a22e5bfc5ce987b5173de|Does the paper explore extraction from electronic health records?|Yes| | 
2003.02249|b637d6393ef3af7462917b81861531022b291933|Does jiant involve datasets for the 50 NLU tasks?|Yes| | 
2003.02249|8b9c12df9f89040f1485b3847a29f11b5c9262e0|Is jiant compatible with models in any programming language?|Yes| | 
1910.03634|72e4e26d0dd79c590c28b10938952a9f9497ff1e|What models are used for painting embedding and what for language style transfer?|generating a poem from images we use an existing actor-critic architecture, various types of sequence to sequence models| | 
1910.03634|58ee0cbf1d8e3711c617b1cd3d7aca8620e26187|What limitations do the authors demnostrate of their model?|Since we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer|"we do not have an end-to-end dataset, the generated English poem may not work well with Shakespeare style transfer as shown in Figure FIGREF12 for ""Starry Night"" with a low average content score"| 
1910.03634|f71b52e00e0be80c926f153b9fe0a06dd93af11e|How does final model rate on Likert scale?|average content score across the paintings is 3.7, average creativity score is 3.9, average style score is 3.9 |average content score across the paintings is 3.7, average creativity score is 3.9, average style score is 3.9| 
1910.03634|54e945ea4b014e11ed4e1e61abc2aa9e68fea310|What is best BLEU score of language style transfer authors got?|seq2seq model with global attention gives the best results with an average target BLEU score of 29.65|average target BLEU score of 29.65| 
2001.08868|568fb7989a133564d84911e7cb58e4d8748243ef|How is trajectory with how rewards extracted?|explores the state space through keeping track of previously visited states by maintaining an archive| | 
2001.08868|2c947447d81252397839d58c75ebcc71b34379b5|On what Text-Based Games are experiments performed?|CoinCollector , CookingWorld |CoinCollector, CookingWorld| 
2001.08868|c01784b995f6594fdb23d7b62f20a35ae73eaa77|How do the authors show that their learned policy generalize better than existing solutions to unseen games?|promising results by solving almost half of the unseen games, most of the lost games are in the hardest set, where a very long sequence of actions is required for winning the game| | 
1910.12795|223dc2b9ea34addc0f502003c2e1c1141f6b36a7|What off-the-shelf reward learning algorithm from RL for joint data manipulation learning and model training is adapted?|BIBREF7| reward learning algorithm BIBREF7| 
1805.10824|e1ab11885f72b4658263a60751d956ba661c1d61|What subtasks did they participate in?|"Answer with content missing: (Subscript 1: ""We did not participate in subtask 5 (E-c)"") Authors participated in EI-Reg, EI-Oc, V-Reg and V-Oc subtasks."| | 
1805.10824|c85b6f9bafc4c64fc538108ab40a0590a2f5768e|What were the scores of their system?|column Ens Test in Table TABREF19| | 
1805.10824|8e52637026bee9061f9558178eaec08279bf7ac6|How was the training data translated?|using the machine translation platform Apertium |machine translation platform Apertium BIBREF5| 
1805.10824|0f6216b9e4e59252b0c1adfd1a848635437dfcdc|What dataset did they use?| Selection of tweets with for each tweet a label describing the intensity of the emotion or sentiment provided by organizers and  tweets translated form English to Spanish.|Spanish tweets were scraped between November 8, 2017 and January 12, 2018, Affect in Tweets Distant Supervision Corpus (DISC)| 
1805.10824|22ccee453e37536ddb0c1c1d17b0dbac04c6c607|What other languages did they translate the data from?|English |English| 
1805.10824|d00bbeda2a45495e6261548710afa6b21ea32870|What semi-supervised learning is applied?|first a model is trained on the training set and then this model is used to predict the labels of the silver data, This silver data is then simply added to our training set, after which the model is retrained| | 
2003.04866|71b1af123fe292fd9950b8439db834212f0b0e32|How were the datasets annotated?|1. Each annotator must assign an integer score between 0 and 6 (inclusive) indicating how semantically similar the two words in a given pair are. A score of 6 indicates very high similarity (i.e., perfect synonymy), while zero indicates no similarity., 2. Each annotator must score the entire set of 1,888 pairs in the dataset.,  able to use external sources (e.g. dictionaries, thesauri, WordNet) if required, not able to communicate with each other during the annotation process| | 
1704.04452|1522ccedbb1f668958f24cca070f640274bc2549|What type of evaluation is proposed for this task?|Answer with content missing: (Evaluation Metrics section) Precision, Recall, F1-scores, Strict match, METEOR, ROUGE-2| | 
1704.04452|97466a37525536086ed5d6e5ed143df085682318|What baseline system is proposed?|Answer with content missing: (Baseline Method section) We implemented a simple approach inspired by previous work on concept map generation and keyphrase extraction.| | 
1704.04452|e9cfbfdf30e48cffdeca58d4ac6fdd66a8b27d7a|How were crowd workers instructed to identify important elements in large document collections?|provide only a description of the document cluster's topic along with the propositions|They break down the task of importance annotation to the level of single propositions and obtain a score for each proposition indicating its importance in a document cluster, such that a ranking according to the score would reveal what is most important and should be included in a summary.| 
1704.04452|d6191c4643201262a770947fc95a613f57bedb6b|Which collections of web documents are included in the corpus?|DIP corpus BIBREF37| | 
1704.04452|ffeb67a61ecd09542b1c53c3e4c3abd4da0496a8|How do the authors define a concept map?|concept map BIBREF5 , a labeled graph showing concepts as nodes and relationships between them as edges| | 
2003.11562|fc4ae12576ea3a85ea6d150b46938890d63a7d18|Is the LSTM baseline a sub-word model?|Yes|Yes| 
2003.11562|19cf7884c0c509c189b1e74fe92c149ff59e444b|How is pseudo-perplexity defined?|Answer with content missing: (formulas in selection): Pseudo-perplexity is perplexity where conditional joint probability is approximated.| | 
1608.08188|ecd5770cf8cb12cb34285e26ab834301c17c53e1|What is the model architecture used?|LSTM to encode the question, VGG16 to extract visual features. The outputs of LSTM and VGG16 are multiplied element-wise and sent to a softmax layer.|random forest, The question is encoded with a 1024-dimensional LSTM model that takes in a one-hot descriptor of each word in the question. The image is described with the 4096-dimensional output from the last fully connected layer of the Convolutional Neural Network (CNN), VGG16 BIBREF25 . The system performs an element-wise multiplication of the image and question features, after linearly transforming the image descriptor to 1024 dimensions. The final layer of the architecture is a softmax layer.| 
1608.08188|4a4ce942a7a6efd1fa1d6c91dedf7a89af64b729|How is the data used for training annotated?|The number of redundant answers to collect from the crowd is predicted to efficiently capture the diversity of all answers from all visual questions.| | 
1906.06849|5529f26f72ce47440c2a64248063a6d5892b9fde|what quantitative analysis is done?|Answer with content missing: (Evaluation section) Given that in CLIR the primary goal is to get a better ranked list of documents against a translated query, we only report Mean Average Precision (MAP).| | 
1906.06849|f85ca6135b101736f5c16c5b5d40895280016023|what are the baselines?|the baseline transformer BIBREF8|baseline transformer BIBREF8| 
1901.08079|d98847340e46ffe381992f1a594e75d3fb8d385e|What machine learning and deep learning methods are used for RQE?|Logistic Regression, neural networks| | 
1805.06966|7006c66a15477b917656f435d66f63760d33a304|by how much did nus outperform abus?|Average success rate is higher by 2.6 percent points.| | 
1805.06966|a15bc19674d48cd9919ad1cf152bf49c88f4417d|what corpus is used to learn behavior?|DSTC2|The manual transcriptions of the DSTC2 training set | 
1806.03125|440faf8d0af8291d324977ad0f68c8d661fe365e|Which dataset has been used in this work?|Reuters-8 dataset without stop words|The Reuters-8 dataset (with stop words removed)| 
1806.03125|0ec56e15005a627d0b478a67fd627a9d85c3920e|What can word subspace represent?|Word vectors, usually in the context of others within the same class| | 
1910.14076|3116453e35352a3a90ee5b12246dc7f2e60cfc59|To what baseline models is proposed model compared?|support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self|support vector machine classifier (SVM), logistic regression classifier (LR), Naive Bayes classifier (NB), random forest (RF), CNN, LSTM , LSTM-soft, LSTM-self| 
1910.14076|dfca00be3284cc555a6a4eac4831471fb1f5875b|How big is dataset for testing?|30 terms, each term-sanse pair has around 15 samples for testing| | 
1910.14076|a9a532399237b514c1227f2d6be8601474e669be|What existing dataset is re-examined and corrected for training?| UM Inventory | | 
1911.06118|26126068d72408555bcb52977cd669faf660bdf7|What are the qualitative experiments performed on benchmark datasets?|"Spearman correlation values of GM_KL model evaluated on the benchmark word similarity datasets.
Evaluation results of GM_KL model on the entailment datasets such as entailment pairs dataset created from WordNet, crowdsourced dataset of 79 semantic relations labelled as entailed or not and annotated distributionally similar nouns dataset."|Given a query word and component id, the set of nearest neighbours along with their respective component ids are listed| 
1911.06118|660284b0a21fe3801e64dc9e0e51da5400223fe3|How does this approach compare to other WSD approaches employing word embeddings?|GM$\_$KL achieves better correlation than existing approaches for various metrics on SCWS dataset.| | 
1908.08717|c22394a3fb0dbf2fc7d3a70ad6435803f5a16ebd|What tasks did they use to evaluate performance for male and female speakers?|ASR| | 
1908.08717|f85f2a532e7e700d9f8f9c09cd08d4e47b87bdd3|What is the goal of investigating NLP gender bias specifically in the news broadcast domain and Anchor role?|create fair systems| broadcast recordings are also a valuable source of data for the speech processing community, recent works uncovering gender bias in several natural language processing (NLP) tools| 
1908.08717|a253749e3b4c4f340778235f640ce694642a4555|Which corpora does this paper analyse?|ESTER1, ESTER2, ETAPE, REPERE|ESTER1, ESTER2, ETAPE, REPERE| 
1908.08717|1142784dc4e0e4c0b4eca1feaf1c10dc46dd5891|How many categories do authors define for speaker role?| two salient roles called Anchors and Punctual speakers| | 
1908.08717|777bb3dcdbc32e925df0f7ec3adb96f15dd3dc47|How big is imbalance in analyzed corpora?|Women represent 33.16% of the speakers| | 
1908.08717|2da4c3679111dd92a1d0869dae353ebe5989dfd2|What are four major corpora of French broadcast?|ESTER1, ESTER2, ETAPE, REPERE| | 
1907.03187|a5505e25ee9ae84090e1442034ddbb3cedabcf04|What were their results on the classification and regression tasks|F1 of 0.8099|F1 score result of 0.8099| 
1608.01884|1dc2da5078a7e5ea82ccd1c90d81999a922bc9bf|Do the authors conduct experiments on the tasks mentioned?|Yes|No| 
1608.01884|7fa3c2c0cf7f559d43e84076a9113a390c5ba03a|Did they collect their own datasets?|No| | 
1608.01884|9a7ba5ed1779c664d2cac92494a43517d3e87c96|What data do they look at?|WSC collection| | 
1608.01884|662870a90890c620a964720b2ca122a1139410ea|What language do they explore?|English, French, German |French, English, Spanish, Italian, Portuguese, Hebrew, Arabic| 
1705.01265|92d1a6df3041667dc662376938bc65527a5a1c3c|Do they report results only on English datasets?|Yes|Yes| 
1705.01265|a4a1fcef760b133e9aa876ac28145ad98a609927|Which other hyperparameters, other than number of clusters are typically evaluated in this type of research?|selection of word vectors| | 
1705.01265|63bb2040fa107c5296351c2b5f0312336dad2863|How were the cluster extracted? |Word clusters are extracted using k-means on word embeddings| | 
1906.10225|218615a005f7f00606223005fef22c07057d9d77|what english datasets were used?|Answer with content missing: (Data section) Penn Treebank (PTB)| | 
1906.10225|867290103f762e1ddfa6f2ea30dd0a327f595182|which chinese datasets were used?|Answer with content missing: (Data section) Chinese with version 5.1 of the Chinese Penn Treebank (CTB)| | 
1712.05999|56a8826cbee49560592b2d4b47b18ada236a12b9|How did they determine fake news tweets?|an expert annotator determined if the tweet fell under a specific category|Exposure, Characterization, Polarization| 
1712.05999|968b7c3553a668ba88da105eff067d57f393c63f|What is their definition of tweets going viral?|Viral tweets are the ones that are retweeted more than 1000 times|those that contain a high number of retweets| 
1712.05999|f03df5d99b753dc4833ef27b32bb95ba53d790ee|What are the characteristics of the accounts that spread fake news?|Accounts that spread fake news are mostly unverified, recently created and have on average high friends/followers ratio|have a larger proportion of friends/followers (i.e. they have, on average, the same number of friends but a smaller number of followers) than those spreading viral content only| 
1712.05999|a8f51b4e334a917702422782329d97304a2fe139|What is the threshold for determining that a tweet has gone viral?|1000| | 
1712.05999|dca86fbe1d57b44986055b282a03c15ef7882e51|How is the ground truth for fake news established?|Ground truth is not established in the paper| | 
2001.05865|d2a0142150cc3788475572f82458ef88087bd7ac|Which three discriminative models did they use?|LF-RCNN, MN-RCNN, MN-RCNN-Wt|LF-RCNN, MN-RCNN, MN-RCNN-Wt| 
1808.03738|27dbbd63c86d6ca82f251d4f2f030ed3e88f58fa|what NMT models did they compare with?|RNN-based NMT model, Transformer-NMT| | 
1808.03738|b9d07757e2d2c4be41823dd1ea3b9c7f115b5f72|Where does the ancient Chinese dataset come from?|ancient Chinese history records in several dynasties (about 1000BC-200BC) and articles written by celebrities of that era|Ancient Chinese history records in several dynasties and articles written by celebrities during 1000BC-200BC collected from the internet | 
1910.08293|808f0ad46ca4eb4ea5492f9e14ca043fe1e206cc|How many different characters were in dataset?|45,821 characters|45,821 characters| 
1910.08293|36ae003c7cb2a1bbfa90b89c671bc286bd3b3dfd|How does dataset model character's profiles?|attributes are determined by human viewers and their impressions of the characters, and are correlated with human-like characteristics| | 
1910.08293|f0b1d8c0a44dbe8d444a5dbe2d9c3d51e048a6f6|How big is the difference in performance between proposed model and baselines?|"Metric difference between Aloha and best baseline score:
Hits@1/20: +0.061 (0.3642 vs 0.3032)
MRR: +0.0572(0.5114 vs 0.4542)
F1: -0.0484 (0.3901 vs 0.4385)
BLEU: +0.0474 (0.2867 vs 0.2393)"| | 
1910.08293|357eb9f0c07fa45e482d998a8268bd737beb827f|What baseline models are used?|the Poly-encoder from BIBREF7 humeau2019real, Feed Yourself BIBREF13 is an open-domain dialogue agent with a self-feeding model, Kvmemnn BIBREF14 is a key-value memory network with a knowledge base that uses a key-value retrieval mechanism to train over multiple domains simultaneously, We compare against four dialogue system baselines: Kvmemnn, Feed Yourself, Poly-encoder, and a BERT bi-ranker baseline trained on the Persona-Chat dataset using the same training hyperparameters (including learning rate scheduler and length capping settings) described in Section SECREF20., a BERT bi-ranker|Kvmemnn,  Feed Yourself, Poly-encoder, BERT bi-ranker| 
1909.01296|ad08b215dca538930ef1f50b4e49cd25527028ad|Was PolyReponse evaluated against some baseline?|No|No| 
1909.01296|e4a315e9c190cf96493eefe04ce4ba6ae6894550|How does PolyResponse architecture look like?|Henderson:2017, MobileNet model| | 
1909.01296|6263b2cba18207474786b303852d2f0d7068d4b6|In what 8 languages is PolyResponse engine used for restourant search and booking system?|English, German, Spanish, Mandarin, Polish, Russian, Korean and Serbian|English (Edinburgh), German (Berlin), Spanish (Madrid), Mandarin (Taipei), Polish (Warsaw), Russian (Moscow), Korean (Seoul), and Serbian (Belgrade)| 
1902.09243|c1c44fd96c3fa6e16949ae8fa453e511c6435c68|Why masking words in the decoder is helpful?|ecause this process is similar to the cloze task in BERT's pre-train process, therefore by using the ability of the contextual language model the decoder can generate more fluent and natural sequences.| | 
1902.09243|d28d86524292506d4b24ae2d486725a6d57a3db3|What is the ROUGE score of the highest performing model?|33.33 average of ROUGE-1, ROUGE-2 and ROUGE-L |33.33| 
1902.09243|feafcc1c4026d7f55a2c8ce7850d7e12030b5c22|How are the different components of the model trained? Is it trained end-to-end?|"the objective of our model is sum of the two processes, jointly trained using ""teacher-forcing"" algorithm, we feed the ground-truth summary to each decoder and minimize the objective, At test time, each time step we choose the predicted word by $\hat{y} = argmax_{y^{\prime }} P(y^{\prime }|x)$ , use beam search to generate the draft summaries, and use greedy search to generate the refined summaries., the model can be trained end-to-end"|the model can be trained end-to-end| 
1801.02073|a9337636b52de375c852682a2561af2c1db5ec63|Do they employ their indexing-based method to create a sample of a QA Wikipedia dataset?|Yes|No| 
1801.02073|45a5961a4e1d1c22874c4918e5c98bd3c0a670b3|How many question types do they find in the datasets analyzed?|seven |7| 
1801.02073|30e21f5bc1d2f80f422c56d62abca9cd3f2cd4a1|How do they analyze contextual similaries across datasets?|They compare the tasks that the datasets are suitable for, average number of answer candidates per question, number of token types, average answer candidate lengths, average question lengths, question-answer word overlap.| | 
1801.06482|5c6fa86757410aee6f5a0762328637de03a569e9|What were their performance results?|best model achieves 0.94 F1 score for Wikipedia and Twitter datasets and 0.95 F1 on Formspring dataset| | 
1801.06482|7e38e0279a620d3df05ab9b5e2795044f18d4471|What cyberbulling topics did they address?|personal attack, racism, and sexism|racism, sexism, personal attack, not specifically about any single topic| 
1909.05359|8b0abc1907c2bf3e0256f8cf85e0ba66a839bd92|Were any of the pipeline components based on deep learning models?|No|No| 
1909.05359|03ebb29c08375afc42a957c7b2dc1a42bed7b713|How is the effectiveness of this pipeline approach evaluated?|proposed ontology, which, in our evaluation procedure, was populated with 3121 events entries from 51 documents.| | 
1603.08594|9cf070d6671ee4a6353f79a165aa648309e01295|What is the size of the parallel corpus used to train the model constraints?|1500 sentences|1500 sentences| 
1603.08594|87bc6f83f7f90df3c6c37659139b92657c3f7a38|How does enforcing agreement between parse trees work across different languages?|we can expect a dependency edge between i and i' in the English parse tree to correspond to an edge between j and j' in the Hindi parse tree, dual decomposition inference algorithm tries to bring the parse trees in the two languages through its constraints| | 
1703.07476|01e2d10178347d177519f792f86f25575106ddc7|What datasets are used to assess the performance of the system?|Switchboard Telephone Speech Corpus BIBREF21, LORELEI (Low Resource Languages for Emergent Incidents) Program|LORELEI datasets of Uzbek, Mandarin and Turkish| 
1703.07476|021bfb7e180d67112b74f05ecb3fa13acc036c86|How is the vocabulary of word-like or phoneme-like units automatically discovered?|Zero Resource Toolkit (ZRTools) BIBREF7| | 
1909.08970|bd419f4094186a5ce74ba6ac1622b24e29e553f4|How well did the baseline perform?|accuracy of 30.3% on single sentences and 0.3 on complete paragraphs| | 
1909.08970|11e8bd4abf5f8bdabad3e8f0691e6d0ad6c326af|What is the baseline?|NO-MOVE, RANDOM, JUMP|NO-MOVE: the only position considered is the starting point, RANDOM: As in BIBREF4, turn to a randomly selected heading, then execute a number of *WALK actions of an average route, JUMP: at each sentence, extract entities from the map and move between them in the order they appear| 
1805.07133|1269c5d8f61e821ee0029080c5ba2500421d5fa6|what methods were used to reduce data sparsity effects?|Back Translation, Mix-Source Approach|data augmentation| 
1805.07133|e35a7f9513ff1cc0f0520f1d4ad9168a47dc18bb|what was the baseline?|traditional phrase-based statistical machine translation (SMT), NMT system|traditional phrase-based statistical machine translation (SMT), NMT system| 
1805.07133|b9ea841b817ba23281c95c7a769873b840dee8d5|did they collect their own data?|No| | 
1805.07133|219af68afeaecabdfd279f439f10ba7c231736e4|what japanese-vietnamese dataset do they use?|WIT3's corpus| | 
1903.11283|b6f466e0fdcb310ecd212fd90396d9d13e0c0504|Do they introduce errors in the data or does the data already contain them?| all three languages have error-corrected corpora for testing purposes|Data already contain errors| 
1903.11283|62ea141d0fb342dfb97c69b49d1c978665b93b3c|What error types is their model more reliable for?|grammatical, spelling and word order errors|spelling, word order and grammatical errors| 
1709.05036|50cb50657572e315fd452a89f3e0be465094b66f|Do they experiment with their proposed model on any other dataset other than MovieQA?|Yes|Yes| 
1706.00139|981fd79dd69581659cb1d4e2b29178e82681eb4d|What is the difference of the proposed model with a standard RNN encoder-decoder?|"Introduce a ""Refinement Adjustment LSTM-based component"" to the decoder"| | 
1706.00139|03e9ac1a2d90152cd041342a11293a1ebd33bcc3|Does the model evaluated on NLG datasets or dialog datasets?|NLG datasets|NLG datasets| 
1705.04153|ef396a34436072cb3c40b0c9bc9179fee4a168ae|What tasks do they experiment with?|text classification and text semantic matching|text classification and text semantic matching| 
1912.01852|d6e353e0231d09fd5dcba493544d53706f3fe1ab|How is the quality of singing voice measured?|To compare the conversions between USVC and PitchNet, we employed an automatic evaluation score and a human evaluation score.|"Automatic: Normalized cross correlation (NCC)
Manual: Mean Opinion Score (MOS)"| 
1808.09029|7bd6a6ec230e1efb27d691762cc0674237dc7967|what data did they use?| Penn Treebank, WikiText2|Penn Treebank (PTB) , WikiText2 (WT-2)| 
2004.04721|73906462bd3415f23d6378590a5ba28709b17605|What are examples of these artificats?|the degree of lexical overlap between them, presence of negation words|hypothesis-only baseline performs better than chance due to cues on their lexical choice and sentence length, NLI models tend to predict entailment for sentence pairs with a high lexical overlap| 
2004.04721|88bf368491f9613767f696f84b4bb1f5a7d7cb48|Does the professional translation or the machine translation introduce the artifacts?|Yes| | 
2004.04721|0737954caf66f2b4c898b356d2a3c43748b9706b|Do they recommend translating the premise and hypothesis together?|No|No| 
2004.04721|664b3eadc12c8dde309e8bbd59e9af961a433cde|Is the improvement over state-of-the-art statistically significant?|Yes| | 
2004.04721|b3307d5b68c57a074c483636affee41054be06d1|What are examples of these artifacts?|hypothesis-only baseline performs better than chance due to cues on their lexical choice and sentence length, NLI models tend to predict entailment for sentence pairs with a high lexical overlap| | 
2004.04721|bfc1de5fa4da2f0e301fd22aea19cf01e2bb5b31|What languages do they use in their experiments?|English, Spanish, Finnish| | 
1905.07791|12d7055baf5bffb6e9e95e977c000ef2e77a4362|How much higher quality is the resulting annotated data?|improvement when the difficult subset with expert annotations is mixed with the remaining crowd annotation is 3.5 F1 score, much larger than when a random set of expert annotations are added| | 
1905.07791|498c0229f831c82a5eb494cdb3547452112a66a0|How do they match annotators to instances?|Annotations from experts are used if they have already been collected.| | 
1905.07791|8c48c726bb17a17d70ab29db4d65a93030dd5382|How much data is needed to train the task-specific encoder?|57,505 sentences|57,505 sentences| 
1905.07791|06b5272774ec43ee5facfa7111033386f06cf448|Is an instance a sentence or an IE tuple?|sentence| | 
2002.04181|08b57deb237f15061e4029b6718f1393fa26acce|Who are the crowdworkers?|people in the US that use Amazon Mechanical Turk|located in the US, hired on the BIBREF22 platform| 
2002.04181|9b7655d39c7a19a23eb8944568eb5618042b9026|Which toolkits do they use?|BIBREF17, BIBREF18, TensiStrength BIBREF13, TwitterNLP BIBREF6, BIBREF19, CogComp-NLP BIBREF20, Stanford NLP NER BIBREF21|BIBREF23, BIBREF17, BIBREF18, BIBREF19, BIBREF24, BIBREF25, BIBREF26| 
2002.04181|1329280df5ee9e902b2742bde4a97bc3e6573ff3|Is datasets for sentiment analysis balanced?|No| | 
2002.04181|58c6737070ef559e9220a8d08adc481fdcd53a24|What measures are used for evaluation?|correct classification rate (CCR)| | 
1908.06264|78a4ec72d76f0a736a4a01369a42b092922203b6|what datasets were used?|Friends, EmotionPush|EmotionLines BIBREF6| 
1908.06264|81588e0e207303c2867c896f3911a54a1ef7c874|What are the sources of the datasets?|Friends TV sitcom, Facebook messenger chats| | 
1908.06264|dd09db5eb321083dba16c2550676e60682f9a0cd|What labels does the dataset have?|Ekman’s six basic emotions,  neutral| | 
1709.10367|777217e025132ddc173cf33747ee590628a8f62f|What experiments are used to demonstrate the benefits of this approach?|On each dataset, we compare four approaches based on sefe with two efe BIBREF10 baselines. All are fit using sgd BIBREF34 . In particular, we compare the following methods:|Calculate test log-likelihood on the three considered datasets| 
1709.10367|2dbf6fe095cd879a9bf40f110b7b72c8bdde9475|What hierarchical modelling approach is used?|the group-specific embedding representations are tied through a global embedding| | 
1709.10367|de830c534c23f103288c198eb19174c76bfd38a1|Which words are used differently across ArXiv?|intelligence| | 
1909.03582|b0d66760829f111b8fad0bd81ca331ddd943ef41|What is future work planed?|ethical questions about generating sensational headlines, which can be further explored,  improving the sensationalism scorer, investigating the applications of dynamic balancing methods between RL and MLE| | 
1909.03582|ae7c93646aa5f3206cd759904965b4d484d12f83|What is this method improvement over the best performing state-of-the-art?|absolute improvement of 18.2% over the Pointer-Gen baseline| | 
1909.03582|d1ec42b2b5a3c956ff528543636e024bfde5e5ba|Which baselines are used for evaluation?|Pointer-Gen, Pointer-Gen+Pos, Pointer-Gen+Same-FT, Pointer-Gen+Pos-FT, Pointer-Gen+RL-ROUGE, Pointer-Gen+RL-SEN| | 
1909.03582|1dac4bc5af239024566fcb0f43bb9ff1c248ecec|Did they used dataset from another domain for evaluation?|No|No| 
1909.03582|3bf0306e9bd044f723e38170c13455877b2aeec3|How is sensationalism scorer trained?|by classifying sensational and non-sensational headlines using a one-layer CNN with a binary cross entropy loss $L_{\text{sen}}$|classifying sensational and non-sensational headlines using a one-layer CNN with a binary cross entropy loss| 
1908.06267|cb12c19f9d14bef7b2f778892d9071eea2d6c63d|What is the state-of-the-art system?|doc2vec , CNN, DAN, Tree-LSTM, DRNN, LSTMN, C-LSTM, SPGK, WMD, S-WMD, Semantic-CNN, LSTM-GRNN, HN-ATT| | 
1908.06267|9193006f359c53eb937deff1248ee3317978e576|Which datasets are used?|Reuters,  BBCSport, Polarity, Subjectivity, MPQA, IMDB, TREC, SST-1, SST-2, Yelp2013| Reuters, BBCSport BIBREF30, Polarity BIBREF31, Subjectivity BIBREF32, MPQA BIBREF33, IMDB BIBREF34, TREC BIBREF35, SST-1 BIBREF36, SST-2 BIBREF36, Yelp2013 BIBREF26| 
1908.06267|bc67b91dd73acded2d52fd4fee732b7a9722ea8b|What is the message passing framework?|It is a framework used to describe algorithms for neural networks represented as graphs. Main idea is that that representation of each vertex is updated based on messages from its neighbors.| | 
1701.05574|49c32a2a64eb41381e5f12ccea4150cac9f3303d|What other evaluation metrics are looked at?|F-score, Kappa| | 
1907.01468|d6ea7a30b0b61ae126b00b59d2a14fff2ef887bf|What approaches do they use towards text analysis?|Domain experts and fellow researchers can provide feedback on questions and help with dynamically revising them., connect to multiple disciplines, dual use|Modeling considerations:  the variables (both predictors and outcomes)  are rarely simply binary or categorical;  using a particular classification scheme means deciding which variations are visible,; Supervised and unsupervised learning are the most common approaches to learning from data;  the unit of text that we are labeling (or annotating, or coding), either automatic or manual, can sometimes be different than one's final unit of analysis.| 
1907.01468|ba28ce9a2f7e8524243adf288cc3f11055e667bb|Do they demonstrate why interdisciplinary insights are important?|No|No| 
1907.01468|b970f48d30775d3468952795bc72976baab3438e|What kind of issues (that are not on the forefront of computational text analysis) do they tackle?|identifying the questions we wish to explore, Can text analysis provide a new perspective on a “big question” that has been attracting interest for years?, How can we explain what we observe?, hope to connect to multiple disciplines| | 
1909.00694|753990d0b621d390ed58f20c4d9e4f065f0dc672|What is the seed lexicon?|a vocabulary of positive and negative predicates that helps determine the polarity score of an event|seed lexicon consists of positive and negative predicates| 
1909.00694|02e4bf719b1a504e385c35c6186742e720bcb281|How are relations used to propagate polarity?|based on the relation between events, the suggested polarity of one event can determine the possible polarity of the other event |cause relation: both events in the relation should have the same polarity; concession relation: events should have opposite polarity| 
1909.00694|86abeff85f3db79cf87a8c993e5e5aa61226dc98|What are labels available in dataset for supervision?|negative, positive| | 
1909.00694|39f8db10d949c6b477fa4b51e7c184016505884f|How does their model learn using mostly raw data?|by exploiting discourse relations to propagate polarity from seed predicates to final sentiment polarity| | 
1909.00694|d0bc782961567dc1dd7e074b621a6d6be44bb5b4|How big is seed lexicon used for training?|30 words| | 
1909.00694|a592498ba2fac994cd6fad7372836f0adb37e22a|How large is raw corpus used for training?|100 million sentences| | 
2003.07723|8d8300d88283c73424c8f301ad9fdd733845eb47|How is the annotation experiment evaluated?|confusion matrices of labels between annotators| | 
2003.07723|48b12eb53e2d507343f19b8a667696a39b719807|What are the aesthetic emotions formalized?|feelings of suspense experienced in narratives not only respond to the trajectory of the plot's content, but are also directly predictive of aesthetic liking (or disliking), Emotions that exhibit this dual capacity have been defined as “aesthetic emotions”| | 
1705.09665|003f884d3893532f8c302431c9f70be6f64d9be8|Do they report results only on English data?|No| | 
1705.09665|bb97537a0a7c8f12a3f65eba73cefa6abcd2f2b2|How do the various social phenomena examined manifest in different types of communities?|"Dynamic communities have substantially higher rates of monthly user retention than more stable communities. More distinctive communities exhibit moderately higher monthly retention rates than more generic communities. There is also a strong positive relationship between a community's dynamicity and the average number of months that a user will stay in that community - a short-term trend observed for monthly retention translates into longer-term engagement and suggests that long-term user retention might be strongly driven by the extent to which a community continually provides novel content.
"| | 
1705.09665|eea089baedc0ce80731c8fdcb064b82f584f483a|What patterns do they observe about how user engagement varies with the characteristics of a community?|communities that are characterized by specialized, constantly-updating content have higher user retention rates, but also exhibit larger linguistic gaps that separate newcomers from established members, within distinctive communities, established users have an increased propensity to engage with the community's specialized content, compared to newcomers | | 
1705.09665|edb2d24d6d10af13931b3a47a6543bd469752f0c|How did the select the 300 Reddit communities for comparison?|They selected all the subreddits from January 2013 to December 2014 with at least 500 words in the vocabulary and at least 4 months of the subreddit's history. They also removed communities with the bulk of the contributions are in foreign language.|"They collect subreddits from January 2013 to December 2014,2 for which there are at
least 500 words in the vocabulary used to estimate the measures,
in at least 4 months of the subreddit’s history. They compute our measures over the comments written by users in a community in time windows of months, for each sufficiently active month, and manually remove communities where the bulk of the contributions are in a foreign language."| 
1705.09665|938cf30c4f1d14fa182e82919e16072fdbcf2a82|How do the authors measure how temporally dynamic a community is?|the average volatility of all utterances| | 
1705.09665|93f4ad6568207c9bd10d712a52f8de25b3ebadd4|How do the authors measure how distinctive a community is?| the average specificity of all utterances| | 
1908.06606|71a7153e12879defa186bfb6dbafe79c74265e10|What data is the language model pretrained on?|Chinese general corpus| | 
1908.06606|85d1831c28d3c19c84472589a252e28e9884500f|What baselines is the proposed model compared against?|BERT-Base, QANet|QANet BIBREF39, BERT-Base BIBREF26| 
1908.06606|1959e0ebc21fafdf1dd20c6ea054161ba7446f61|How is the clinical text structuring task defined?|Clinical text structuring (CTS) is a critical task for fetching medical research data from electronic health records (EHRs), where structural patient medical data, such as whether the patient has specific symptoms, diseases, or what the tumor size is, how far from the tumor is cut at during the surgery, or what the specific laboratory test result is, are obtained., Unlike the traditional CTS task, our QA-CTS task aims to discover the most related text from original paragraph text. |CTS is extracting structural data from medical research data (unstructured). Authors define QA-CTS task that aims to discover most related text from original text.| 
1908.06606|77cf4379106463b6ebcb5eb8fa5bb25450fa5fb8|What are the specific tasks being unified?| three types of questions, namely tumor size, proximal resection margin and distal resection margin| | 
1908.06606|06095a4dee77e9a570837b35fc38e77228664f91|Is all text in this dataset a question, or are there unrelated sentences in between questions?|the dataset consists of pathology reports including sentences and questions and answers about tumor size and resection margins so it does include additional sentences | | 
1908.06606|19c9cfbc4f29104200393e848b7b9be41913a7ac|How many questions are in the dataset?|2,714 | | 
1908.06606|08a5f8d36298b57f6a4fcb4b6ae5796dc5d944a4|How they introduce domain-specific features into pre-trained language model?|integrate clinical named entity information into pre-trained language model| | 
1908.06606|975a4ac9773a4af551142c324b64a0858670d06e|How big is QA-CTS task dataset?|17,833 sentences, 826,987 characters and 2,714 question-answer pairs| | 
1908.06606|326e08a0f5753b90622902bd4a9c94849a24b773|How big is dataset of pathology reports collected from Ruijing Hospital?|17,833 sentences, 826,987 characters and 2,714 question-answer pairs| | 
1908.06606|bd78483a746fda4805a7678286f82d9621bc45cf|What are strong baseline models in specific tasks?|state-of-the-art question answering models (i.e. QANet BIBREF39) and BERT-Base BIBREF26| | 
1811.00942|dd155f01f6f4a14f9d25afc97504aefdc6d29c13|What aspects have been compared between various language models?|Quality measures using perplexity and recall, and performance measured using latency and energy usage. | | 
1811.00942|a9d530d68fb45b52d9bad9da2cd139db5a4b2f7c|what classic language models are mentioned in the paper?|Kneser–Ney smoothing| | 
1811.00942|e07df8f613dbd567a35318cd6f6f4cb959f5c82d|What is a commonly used evaluation metric for language models?|perplexity|perplexity| 
1805.02400|1a43df221a567869964ad3b275de30af2ac35598|Which dataset do they use a starting point in generating fake reviews?|the Yelp Challenge dataset|Yelp Challenge dataset BIBREF2| 
1805.02400|98b11f70239ef0e22511a3ecf6e413ecb726f954|Do they use a pretrained NMT model to help generating reviews?|No|No| 
1805.02400|12f1919a3e8ca460b931c6cacc268a926399dff4|What kind of model do they use for detection?|AdaBoost-based classifier| | 
1805.02400|cd1034c183edf630018f47ff70b48d74d2bb1649|Does their detection tool work better than human detection?|Yes| | 
1805.02400|bd9930a613dd36646e2fc016b6eb21ab34c77621|How many reviews in total (both generated and true) do they evaluate on Amazon Mechanical Turk?|1,006 fake reviews and 994 real reviews| | 
1907.05664|6e2ad9ad88cceabb6977222f5e090ece36aa84ea|Which baselines did they compare?|The baseline model is a deep sequence-to-sequence encoder/decoder model with attention. The encoder is a bidirectional Long-Short Term Memory(LSTM) cell BIBREF14 and the decoder a single LSTM cell with attention mechanism. The attention mechanism is computed as in BIBREF9 and we use a greedy search for decoding. We train end-to-end including the words embeddings. The embedding size used is of 128 and the hidden state size of the LSTM cells is of 254.|The baseline model is a deep sequence-to-sequence encoder/decoder model with attention. The encoder is a bidirectional Long-Short Term Memory(LSTM) cell BIBREF14 and the decoder a single LSTM cell with attention mechanism. The attention mechanism is computed as in BIBREF9 and we use a greedy search for decoding. We train end-to-end including the words embeddings. The embedding size used is of 128 and the hidden state size of the LSTM cells is of 254.| 
1907.05664|aacb0b97aed6fc6a8b471b8c2e5c4ddb60988bf5|How many attention layers are there in their model?|one| | 
1907.05664|710c1f8d4c137c8dad9972f5ceacdbf8004db208|Is the explanation from saliency map correct?|No| | 
1910.14497|47726be8641e1b864f17f85db9644ce676861576|How is embedding quality assessed?|"We compare this method of bias mitigation with the no bias mitigation (""Orig""), geometric bias mitigation (""Geo""), the two pieces of our method alone (""Prob"" and ""KNN"") and the composite method (""KNN+Prob""). We note that the composite method performs reasonably well according the the RIPA metric, and much better than traditional geometric bias mitigation according to the neighborhood metric, without significant performance loss according to the accepted benchmarks. To our knowledge this is the first bias mitigation method to perform reasonably both on both metrics."| | 
1912.02481|347e86893e8002024c2d10f618ca98e14689675f|What turn out to be more important high volume or high quality data?|only high-quality data helps|high-quality| 
1912.02481|cbf1137912a47262314c94d36ced3232d5fa1926|What two architectures are used?|fastText, CWE-LP| | 
1810.04528|99a10823623f78dbff9ccecb210f187105a196e9|What were the word embeddings trained on?|large Portuguese corpus| | 
1810.04528|09f0dce416a1e40cc6a24a8b42a802747d2c9363|Which word embeddings are analysed?|Continuous Bag-of-Words (CBOW)| | 
2002.02224|ac706631f2b3fa39bf173cd62480072601e44f66|Did they experiment on this dataset?|No|Yes| 
2002.02224|8b71ede8170162883f785040e8628a97fc6b5bcb|How is quality of the citation measured?|it is necessary to evaluate the performance of the above mentioned part of the pipeline before proceeding further. The evaluation of the performance is summarised in Table TABREF11. It shows that organising the two models into the pipeline boosted the performance of the reference recognition model, leading to a higher F1 measure in the initial recognition of the text spans and their classification.| | 
2002.02224|fa2a384a23f5d0fe114ef6a39dced139bddac20e|How big is the dataset?|903019 references| | 
2003.07433|0bffc3d82d02910d4816c16b390125e5df55fd01|Do the authors mention any possible confounds in this study?|No| | 
2003.07433|bdd8368debcb1bdad14c454aaf96695ac5186b09|How is the intensity of the PTSD established?|Given we have four intensity, No PTSD, Low Risk PTSD, Moderate Risk PTSD and High Risk PTSD with a score of 0, 1, 2 and 3 respectively, the estimated intensity  is established as mean  squared error.|defined into four categories from high risk, moderate risk, to low risk| 
2003.07433|3334f50fe1796ce0df9dd58540e9c08be5856c23|How is LIWC incorporated into this system?| For each user, we calculate the proportion of tweets scored positively by each LIWC category.|to calculate the possible scores of each survey question using PTSD Linguistic Dictionary | 
2003.07433|7081b6909cb87b58a7b85017a2278275be58bf60|How many twitter users are surveyed using the clinically validated survey?|210| | 
2003.07433|1870f871a5bcea418c44f81f352897a2f53d0971|Which clinically validated survey tools are used?|DOSPERT, BSSS and VIAS| | 
2003.12218|ce6201435cc1196ad72b742db92abd709e0f9e8d|Did they experiment with the dataset?|Yes| | 
2003.12218|928828544e38fe26c53d81d1b9c70a9fb1cc3feb|What is the size of this dataset?|29,500 documents|29,500 documents in the CORD-19 corpus (2020-03-13)| 
1904.09678|8fc14714eb83817341ada708b9a0b6b4c6ab5023|what sentiment sources do they compare with?|manually created lexicon in Czech BIBREF11 , German BIBREF12 , French BIBREF13 , Macedonian BIBREF14 , and Spanish BIBREF15| | 
2003.06651|d94ac550dfdb9e4bbe04392156065c072b9d75e1|Is the method described in this work a clustering-based method?|Yes|Yes| 
2003.06651|eeb6e0caa4cf5fdd887e1930e22c816b99306473|How are the different senses annotated/labeled? |The contexts are manually labelled with WordNet senses of the target words| | 
2003.06651|3c0eaa2e24c1442d988814318de5f25729696ef5|Was any extrinsic evaluation carried out?|Yes| | 
1910.04269|922f1b740f8b13fdc8371e2a275269a44c86195e|Is the performance compared against a baseline model?|Yes|No| 
1910.04269|b39f2249a1489a2cef74155496511cc5d1b2a73d|What is the accuracy reported by state-of-the-art methods?|"Answer with content missing: (Table 1)
Previous state-of-the art on same dataset: ResNet50 89% (6 languages), SVM-HMM 70% (4 languages)"| | 
1906.00378|591231d75ff492160958f8aa1e6bfcbbcd85a776|Which vision-based approaches does this approach outperform?|CNN-mean, CNN-avgmax| | 
1906.00378|9e805020132d950b54531b1a2620f61552f06114|What baseline is used for the experimental setup?|CNN-mean, CNN-avgmax| | 
1906.00378|95abda842c4df95b4c5e84ac7d04942f1250b571|Which languages are used in the multi-lingual caption model?|German-English, French-English, and Japanese-English|multiple language pairs including German-English, French-English, and Japanese-English.| 
1912.13072|2419b38624201d678c530eba877c0c016cccd49f|Did they experiment on all the tasks?|Yes| | 
1912.13072|b99d100d17e2a121c3c8ff789971ce66d1d40a4d|What models did they compare to?| we do not explicitly compare to previous research since most existing works either exploit smaller data (and so it will not be a fair comparison), use methods pre-dating BERT (and so will likely be outperformed by our models)| | 
1912.13072|578d0b23cb983b445b1a256a34f969b34d332075|What datasets are used in training?|Arap-Tweet BIBREF19 , an in-house Twitter dataset for gender, the MADAR shared task 2 BIBREF20, the LAMA-DINA dataset from BIBREF22, LAMA-DIST, Arabic tweets released by IDAT@FIRE2019 shared-task BIBREF24, BIBREF25, BIBREF26, BIBREF27, BIBREF1, BIBREF28, BIBREF29, BIBREF30, BIBREF31, BIBREF32, BIBREF33, BIBREF34| Arap-Tweet , UBC Twitter Gender Dataset, MADAR , LAMA-DINA , IDAT@FIRE2019, 15 datasets related to sentiment analysis of Arabic, including MSA and dialects| 
1712.09127|6548db45fc28e8a8b51f114635bad14a13eaec5b|Which GAN do they use?|We construct a GAN model which combines different sets of word embeddings INLINEFORM4 , INLINEFORM5 , into a single set of word embeddings INLINEFORM6 . |weGAN, deGAN| 
1712.09127|4c4f76837d1329835df88b0921f4fe8bda26606f|Do they evaluate grammaticality of generated text?|No| | 
1712.09127|819d2e97f54afcc7cdb3d894a072bcadfba9b747|Which corpora do they use?|CNN, TIME, 20 Newsgroups, and Reuters-21578| | 
2001.00137|637aa32a34b20b4b0f1b5dfa08ef4e0e5ed33d52|Do they report results only on English datasets?|Yes| | 
2001.00137|4b8257cdd9a60087fa901da1f4250e7d910896df|How do the authors define or exemplify 'incorrect words'?|typos in spellings or ungrammatical words| | 
2001.00137|abc5836c54fc2ac8465aee5a83b9c0f86c6fd6f5|Do they test their approach on a dataset without incomplete data?|No|No| 
2001.00137|4debd7926941f1a02266b1a7be2df8ba6e79311a|Should their approach be applied only when dealing with incomplete data?|No|No| 
1910.03042|44c7c1fbac80eaea736622913d65fe6453d72828|What is the sample size of people used to measure user satisfaction?|34,432 user conversations|34,432 | 
1910.03042|3e0c9469821cb01a75e1818f2acb668d071fcf40|What are all the metrics to measure user engagement?|overall rating, mean number of turns|overall rating, mean number of turns| 
1910.03042|a725246bac4625e6fe99ea236a96ccb21b5f30c6|What the system designs introduced?|Amazon Conversational Bot Toolkit, natural language understanding (NLU) (nlu) module, dialog manager, knowledge bases, natural language generation (NLG) (nlg) module, text to speech (TTS) (tts)| | 
1910.03042|516626825e51ca1e8a3e0ac896c538c9d8a747c8|Do they specify the model they use for Gunrock?|No| | 
1910.03042|77af93200138f46bb178c02f710944a01ed86481|Do they gather explicit user satisfaction data on Gunrock?|Yes| | 
1910.03042|71538776757a32eee930d297f6667cd0ec2e9231|How do they correlate user backstory queries to user satisfaction?|modeled the relationship between word count and the two metrics of user engagement (overall rating, mean number of turns) in separate linear regressions| | 
2002.06644|bd5379047c2cf090bea838c67b6ed44773bcd56f|Which experiments are perfomed?|They used BERT-based models to detect subjective language in the WNC corpus| | 
1809.08731|7aa8375cdf4690fc3b9b1799b0f5a9ec1c1736ed|Is ROUGE their only baseline?|No|No, other baseline metrics they use besides ROUGE-L are n-gram overlap, negative cross-entropy, perplexity, and BLEU.| 
1809.08731|3ac30bd7476d759ea5d9a5abf696d4dfc480175b|what language models do they use?|LSTM LMs| | 
1707.00995|f0317e48dafe117829e88e54ed2edab24b86edb1|What misbehavior is identified?|"if the attention loose track of the objects in the picture and ""gets lost"", the model still takes it into account and somehow overrides the information brought by the text-based annotations"|"if the attention loose track of the objects in the picture and ""gets lost"", the model still takes it into account and somehow overrides the information brought by the text-based annotations"| 
1707.00995|f129c97a81d81d32633c94111018880a7ffe16d1|Which attention mechanisms do they compare?|Soft attention, Hard Stochastic attention, Local Attention| | 
1809.04960|100cf8b72d46da39fedfe77ec939fb44f25de77f|Which paired corpora did they use in the other experiment?|dataset that contains article-comment parallel contents INLINEFORM0 , and an unpaired dataset that contains the documents (articles or comments) INLINEFORM1|Chinese dataset BIBREF0| 
1809.04960|5fa431b14732b3c47ab6eec373f51f2bca04f614|Which lexicon-based models did they compare with?|TF-IDF, NVDM| | 
1809.04960|33ccbc401b224a48fba4b167e86019ffad1787fb|How many comments were used?|from 50K to 4.8M| | 
1809.04960|cca74448ab0c518edd5fc53454affd67ac1a201c|How many articles did they have?|198,112| | 
1809.04960|b69ffec1c607bfe5aa4d39254e0770a3433a191b|What news comment dataset was used?|Chinese dataset BIBREF0| | 
1909.08402|f5cf8738e8d211095bb89350ed05ee7f9997eb19|By how much do they outperform standard BERT?|up to four percentage points in accuracy| | 
1909.08402|bed527bcb0dd5424e69563fba4ae7e6ea1fca26a|What dataset do they use?|2019 GermEval shared task on hierarchical text classification|GermEval 2019 shared task| 
1909.08402|aeab5797b541850e692f11e79167928db80de1ea|How do they combine text representations with the knowledge graph embeddings?|all three representations are concatenated and passed into a MLP| | 
1909.11189|bfa3776c30cb30e0088e185a5908e5172df79236|What is the algorithm used for the classification tasks?|Random Forest Ensemble classifiers| | 
1909.11189|a2a66726a5dca53af58aafd8494c4de833a06f14|Is the outcome of the LDA analysis evaluated in any way?|Yes| | 
1909.11189|ee87608419e4807b9b566681631a8cd72197a71a|What is the corpus used in the study?|TextGrid Repository|The Digital Library in the TextGrid Repository| 
1810.05320|cda4612b4bda3538d19f4b43dde7bc30c1eda4e5|What are the traditional methods to identifying important attributes?|automated attribute-value extraction, score the attributes using the Bayes model, evaluate their importance with several different frequency metrics, aggregate the weights from different sources into one consistent typicality score using a Ranking SVM model, OntoRank algorithm|TextRank, Word2vec BIBREF19, GloVe BIBREF20| 
1810.05320|e12674f0466f8c0da109b6076d9939b30952c7da|What do you use to calculate word/sub-word embeddings|FastText| | 
2003.08529|b5c3787ab3784214fc35f230ac4926fe184d86ba|Did they propose other metrics?|Yes| | 
2003.08529|9174aded45bc36915f2e2adb6f352f3c7d9ada8b|Which real-world datasets did they use?|SST-2 (Stanford Sentiment Treebank, version 2), Snips|SST-2, Snips| 
1708.05873|a2103e7fe613549a9db5e65008f33cf2ee0403bd|What are the country-specific drivers of international development rhetoric?|wealth , democracy , population, levels of ODA, conflict | | 
1708.05873|13b36644357870008d70e5601f394ec3c6c07048|Is the dataset multilingual?|No|No| 
1708.05873|e4a19b91b57c006a9086ae07f2d6d6471a8cf0ce|How are the main international development topics that states raise identified?| They focus on exclusivity and semantic coherence measures: Highly frequent words in a given topic that do not appear very often in other topics are viewed as making that topic exclusive. They select select the 16-topic model, which has the largest positive residual in the regression fit, and provides higher exclusivity at the same level of semantic coherence.| | 
2003.08553|fd0ef5a7b6f62d07776bf672579a99c67e61a568|What experiments do the authors present to validate their system?| we measure our system's performance for datasets across various domains, evaluations are done by managed judges who understands the knowledge base and then judge user queries relevance to the QA pairs| | 
2003.08553|f399d5a8dbeec777a858f81dc4dd33a83ba341a2|What components is the QnAMaker composed of?|QnAMaker Portal, QnaMaker Management APIs, Azure Search Index, QnaMaker WebApp, Bot|QnAMaker Portal, QnaMaker Management APIs, Azure Search Index, QnaMaker WebApp, Bot| 
1909.09491|d28260b5565d9246831e8dbe594d4f6211b60237|How they measure robustness in experiments?|We empirically provide a formula to measure the richness in the scenario of machine translation.|boost the training BLEU very greatly, the over-fitting problem of the Plackett-Luce models PL($k$) is alleviated with moderately large $k$| 
1909.09491|923b12c0a50b0ee22237929559fad0903a098b7b|What experiments with large-scale features are performed?|Plackett-Luce Model for SMT Reranking| | 
2001.05284|67131c15aceeb51ae1d3b2b8241c8750a19cca8e|Which ASR system(s) is used in this work?|Oracle | | 
2001.05284|579a0603ec56fc2b4aa8566810041dbb0cd7b5e7|What are the series of simple models?|perform experiments to utilize ASR $n$-best hypotheses during evaluation| | 
2001.05284|c9c85eee41556c6993f40e428fa607af4abe80a9|Over which datasets/corpora is this work evaluated?|$\sim $ 8.7M annotated anonymised user utterances|on $\sim $ 8.7M annotated anonymised user utterances| 
1909.12140|f8281eb49be3e8ea0af735ad3bec955a5dedf5b3|Is the semantic hierarchy representation used for any task?|Yes, Open IE|Yes| 
1909.12140|a5ee9b40a90a6deb154803bef0c71c2628acb571|What are the corpora used for the task?|For the English version, we performed both a thorough manual analysis and automatic evaluation across three commonly used TS datasets from two different domains, The evaluation of the German version is in progress.| | 
1909.12140|e286860c41a4f704a3a08e45183cb8b14fa2ad2f|Is the model evaluated?|the English version is evaluated. The German version evaluation is in progress | | 
1709.00947|982979cb3c71770d8d7d2d1be8f92b66223dec85|What new metrics are suggested to track progress?| For example, one metric could consist in checking whether for any given word, all words that are known to belong to the same class are closer than any words belonging to different classes, independently of the actual cosine| | 
1709.00947|5ba6f7f235d0f5d1d01fd97dd5e4d5b0544fd212|What intrinsic evaluation metrics are used?|Class Membership Tests, Class Distinction Test, Word Equivalence Test|coverage metric, being distinct (cosine INLINEFORM0 0.7 or 0.8), belonging to the same class (cosine INLINEFORM1 0.7 or 0.8), being equivalent (cosine INLINEFORM2 0.85 or 0.95)| 
1709.00947|7ce7edd06925a943e32b59f3e7b5159ccb7acaf6|What experimental results suggest that using less than 50% of the available training examples might result in overfitting?|consistent increase in the validation loss after about 15 epochs| | 
1909.08859|a883bb41449794e0a63b716d9766faea034eb359|What multimodality is available in the dataset?|context is a procedural text, the question and the multiple choice answers are composed of images|images and text| 
1909.08859|5d83b073635f5fd8cd1bdb1895d3f13406583fbd|What are previously reported models?|Hasty Student, Impatient Reader, BiDAF, BiDAF w/ static memory| | 
1908.08419|3c3cb51093b5fd163e87a773a857496a4ae71f03|How does the scoring model work?|First, mapping the segmented sentence to a sequence of candidate word embeddings. Then, the scoring model takes the word embedding sequence as input, scoring over each individual candidate word| the scoring model takes the word embedding sequence as input, scoring over each individual candidate word from two perspectives: (1) the possibility that the candidate word itself can be regarded as a legal word; (2) the rationality of the link that the candidate word directly follows previous segmentation history| 
1908.08419|53a0763eff99a8148585ac642705637874be69d4|How does the active learning model work?|Active learning methods has a learning engine (mainly used for training of classification problems) and the selection engine (which chooses samples that need to be relabeled by annotators from unlabeled data). Then, relabeled samples are added to training set for classifier to re-train, thus continuously improving the accuracy of the classifier. In this paper, CRF-based segmenter and a scoring model are employed as learning engine and selection engine, respectively.| | 
1908.08419|0bfed6f9cfe93617c5195c848583e3945f2002ff|Which neural network architectures are employed?|gated neural network | | 
1703.05260|a37ef83ab6bcc6faff3c70a481f26174ccd40489|How many subjects have been used to create the annotations?| four different annotators| | 
1905.00563|185841e979373808d99dccdade5272af02b98774|How is this approach used to detect incorrect facts?|if there is an error in the graph, the triple is likely to be inconsistent with its neighborhood, and thus the model should put least trust on this triple. In other words, the error triple should have the least influence on the model's prediction of the training data. | | 
1905.00563|d427e3d41c4c9391192e249493be23926fc5d2e9|Can this adversarial approach be used to directly improve model accuracy?|Yes| | 
1808.05902|330f2cdeab689670b68583fc4125f5c0b26615a8|what are the advantages of the proposed model?|he proposed model outperforms all the baselines, being the svi version the one that performs best., the svi version converges much faster to higher values of the log marginal likelihood when compared to the batch version, which reflects the efficiency of the svi algorithm.| | 
1808.05902|c87b2dd5c439d5e68841a705dd81323ec0d64c97|what are the state of the art approaches?|Bosch 2006 (mv), LDA + LogReg (mv), LDA + Raykar, LDA + Rodrigues, Blei 2003 (mv), sLDA (mv)| | 
1808.05902|f7789313a804e41fcbca906a4e5cf69039eeef9f|what datasets were used?|Reuters-21578 BIBREF30,  LabelMe BIBREF31, 20-Newsgroups benchmark corpus BIBREF29 | 20-Newsgroups benchmark corpus , Reuters-21578, LabelMe| 
2002.11893|2376c170c343e2305dac08ba5f5bda47c370357f|How was the dataset collected?|Database Construction: we crawled travel information in Beijing from the Web, including Hotel, Attraction, and Restaurant domains (hereafter we name the three domains as HAR domains). Then, we used the metro information of entities in HAR domains to build the metro database. , Goal Generation: a multi-domain goal generator was designed based on the database. The relation across domains is captured in two ways. One is to constrain two targets that locate near each other. The other is to use a taxi or metro to commute between two targets in HAR domains mentioned in the context., Dialogue Collection: before the formal data collection starts, we required the workers to make a small number of dialogues and gave them feedback about the dialogue quality. Then, well-trained workers were paired to converse according to the given goals. The workers were also asked to annotate both user states and system states., Dialogue Annotation: we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories. |They crawled travel information from the Web to build a database, created a multi-domain goal generator from the database, collected dialogue between workers an automatically annotated dialogue acts. | 
2002.11893|0137ecebd84a03b224eb5ca51d189283abb5f6d9|What are the benchmark models?|BERTNLU from ConvLab-2, a rule-based model (RuleDST) , TRADE (Transferable Dialogue State Generator) , a vanilla policy trained in a supervised fashion from ConvLab-2 (SL policy)| | 
2002.11893|5f6fbd57cce47f20a0fda27d954543c00c4344c2|How was the corpus annotated?|The workers were also asked to annotate both user states and system states, we used some rules to automatically annotate dialogue acts according to user states, system states, and dialogue histories| | 
1910.07181|d6e2b276390bdc957dfa7e878de80cee1f41fbca|What models other than standalone BERT is new model compared to?|Only Bert base and Bert large are compared to proposed approach.| | 
1910.07181|32537fdf0d4f76f641086944b413b2f756097e5e|How much is representaton improved for rare/medum frequency words compared to standalone BERT and previous work?|improving the score for WNLaMPro-medium by 50% compared to BERT$_\text{base}$ and 31% compared to Attentive Mimicking| | 
1910.07181|ef081d78be17ef2af792e7e919d15a235b8d7275|What are three downstream task datasets?|MNLI BIBREF21, AG's News BIBREF22, DBPedia BIBREF23|MNLI, AG's News, DBPedia| 
1910.07181|537b2d7799124d633892a1ef1a485b3b071b303d|What is dataset for word probing task?|WNLaMPro dataset| | 
1902.00330|dad8cc543a87534751f9f9e308787e1af06f0627|What datasets used for evaluation?|AIDA-B, ACE2004, MSNBC, AQUAINT, WNED-CWEB, WNED-WIKI|AIDA-CoNLL, ACE2004, MSNBC, AQUAINT, WNED-CWEB, WNED-WIKI, OURSELF-WIKI| 
1909.00542|cfffc94518d64cb3c8789395707e4336676e0345|What approaches without reinforcement learning have been tried?|classification, regression, neural methods| Support Vector Regression (SVR) and Support Vector Classification (SVC), deep learning regression models of BIBREF2 to convert them to classification models| 
1909.00542|f60629c01f99de3f68365833ee115b95a3388699|What classification approaches were experimented for this task?|NNC SU4 F1, NNC top 5, Support Vector Classification (SVC)| | 
1909.00542|a7cb4f8e29fd2f3d1787df64cd981a6318b65896|Did classification models perform better than previous regression one?|Yes| | 
1810.06743|642c4704a71fd01b922a0ef003f234dcc7b223cd|What are the main sources of recall errors in the mapping?|irremediable annotation discrepancies, differences in choice of attributes to annotate, The resources themselves would need updating to encode the relevant morphosyntactic information. Some languages had a very low number of overlapping forms, and no tag matches or near-matches between them, the two annotations encode distinct information, incorrectly applied UniMorph annotation, cross-lingual inconsistency in both resources| | 
1810.06743|e477e494fe15a978ff9c0a5f1c88712cdaec0c5c|Do they look for inconsistencies between different languages' annotations in UniMorph?|Yes| | 
1810.06743|04495845251b387335bf2e77e2c423130f43c7d9|Do they look for inconsistencies between different UD treebanks?|Yes| | 
1909.02764|f3d0e6452b8d24b7f9db1fd898d1fbe6cd23f166|Does the paper evaluate any adjustment to improve the predicion accuracy of face and audio features?|No| | 
1909.02764|9b1d789398f1f1a603e4741a5eee63ccaf0d4a4f|How is face and audio data analysis evaluated?|confusion matrices, $\text{F}_1$ score| | 
1909.02764|00bcdffff7e055f99aaf1b05cf41c98e2748e948|What is the baseline method for the task?|"For the emotion recognition from text they use described neural network as baseline.
For audio and face there is no baseline."| | 
1909.02764|f92ee3c5fce819db540bded3cfcc191e21799cb1|What are the emotion detection tools used for audio and face input?|We apply an off-the-shelf tool for emotion recognition (the manufacturer cannot be disclosed due to licensing restrictions)|cannot be disclosed due to licensing restrictions| 
1905.11901|07d7652ad4a0ec92e6b44847a17c378b0d9f57f5|what were their experimental results in the low-resource dataset?|10.37 BLEU| | 
1905.11901|9f3444c9fb2e144465d63abf58520cddd4165a01|what are the methods they compare with in the korean-english dataset?|gu-EtAl:2018:EMNLP1| | 
1905.11901|2348d68e065443f701d8052018c18daa4ecc120e|what pitfalls are mentioned in the paper?|highly data-inefficient, underperform phrase-based statistical machine translation| | 
1912.01252|5679fabeadf680e35a4f7b092d39e8638dca6b4d|Does the paper report the results of previous models applied to the same tasks?|Yes|No| 
1912.01252|312417675b3dc431eb7e7b16a917b7fed98d4376|What are the causal mapping methods employed?|Axelrod's causal mapping method| | 
1912.13109|792d7b579cbf7bfad8fe125b0d66c2059a174cf9|What is the previous work's model?|Ternary Trans-CNN| | 
1912.13109|44a2a8e187f8adbd7d63a51cd2f9d2d324d0c98d|What dataset is used?|HEOT , A labelled dataset for a corresponding english tweets|HEOT| 
1912.13109|cca3301f20db16f82b5d65a102436bebc88a2026|How is the dataset collected?|A labelled dataset for a corresponding english tweets were also obtained from a study conducted by Davidson et al, HEOT obtained from one of the past studies done by Mathur et al| | 
1912.13109|cfd67b9eeb10e5ad028097d192475d21d0b6845b|Was each text augmentation technique experimented individually?|No| | 
1912.13109|e1c681280b5667671c7f78b1579d0069cba72b0e|What models do previous work use?|Ternary Trans-CNN , Hybrid multi-channel CNN and LSTM| | 
1912.13109|58d50567df71fa6c3792a0964160af390556757d|Does the dataset contain content from various social media platforms?|No| | 
1912.13109|07c79edd4c29635dbc1c2c32b8df68193b7701c6|What dataset is used?|HEOT , A labelled dataset for a corresponding english tweets | | 
1911.03310|222b2469eede9a0448e0226c6c742e8c91522af3|Are language-specific and language-neutral components disjunctive?|No| | 
1911.03310|6f8386ad64dce3a20bc75165c5c7591df8f419cf|How they show that mBERT representations can be split into a language-specific component and a language-neutral component?|We thus try to remove the language-specific information from the representations by centering the representations of sentences in each language so that their average lies at the origin of the vector space.| | 
1911.03310|81dc39ee6cdacf90d5f0f62134bf390a29146c65|What challenges this work presents that must be solved to build better language-neutral representations?|contextual embeddings do not represent similar semantic phenomena similarly and therefore they are not directly usable for zero-shot cross-lingual tasks| | 
1907.12108|1f1a9f2dd8c4c10b671cb8affe56e181948e229e|What pretrained LM is used?|Generative Pre-trained Transformer (GPT)|Generative Pre-trained Transformer (GPT)| 
2004.03685|eeaceee98ef1f6c971dac7b0b8930ee8060d71c2|What approaches they propose?|Across models and tasks: The degree (as grayscale) of faithfulness at the level of specific models and tasks., Across input space: The degree of faithfulness at the level of subspaces of the input space, such as neighborhoods of similar inputs, or singular inputs themselves.| | 
2004.03685|3371d586a3a81de1552d90459709c57c0b1a2594|What faithfulness criteria does they propose?|Across models and tasks: The degree (as grayscale) of faithfulness at the level of specific models and tasks., Across input space: The degree of faithfulness at the level of subspaces of the input space, such as neighborhoods of similar inputs, or singular inputs themselves.| | 
2004.03685|d4b9cdb4b2dfda1e0d96ab6c3b5e2157fd52685e|Which are three assumptions in current approaches for defining faithfulness?|Two models will make the same predictions if and only if they use the same reasoning process., On similar inputs, the model makes similar decisions if and only if its reasoning is similar., Certain parts of the input are more important to the model reasoning than others. Moreover, the contributions of different parts of the input are independent from each other.|Two models will make the same predictions if and only if they use the same reasoning process., On similar inputs, the model makes similar decisions if and only if its reasoning is similar., Certain parts of the input are more important to the model reasoning than others. Moreover, the contributions of different parts of the input are independent from each other.| 
2004.03685|2a859e80d8647923181cb2d8f9a2c67b1c3f4608|Which are key points in guidelines for faithfulness evaluation?|Be explicit in what you evaluate., Faithfulness evaluation should not involve human-judgement on the quality of interpretation., Faithfulness evaluation should not involve human-provided gold labels., Do not trust “inherent interpretability” claims., Faithfulness evaluation of IUI systems should not rely on user performance.| | 
1808.03894|aceac4ad16ffe1af0f01b465919b1d4422941a6b|Did they use the state-of-the-art model to analyze the attention?|we provide an extensive analysis of the state-of-the-art model| | 
1808.03894|2efdcebebeb970021233553104553205ce5d6567|How many layers are there in their model?|two LSTM layers| | 
1703.04617|a891039441e008f1fd0a227dbed003f76c140737|What MC abbreviate for?|machine comprehension| | 
1703.04617|73738e42d488b32c9db89ac8adefc75403fa2653|how much of improvement the adaptation model can get?| 69.10%/78.38%| | 
1703.04617|fa218b297d9cdcae238cef71096752ce27ca8f4a|What is the exact performance on SQUAD?|Our model achieves a 68.73% EM score and 77.39% F1 score| | 
1909.00578|ae8354e67978b7c333094c36bf9d561ca0c2d286|What dataset do they use?|datasets from the NIST DUC-05, DUC-06 and DUC-07 shared tasks| | 
1909.00578|02348ab62957cb82067c589769c14d798b1ceec7|What simpler models do they look at?|BiGRU s with attention, ROUGE, Language model (LM), Next sentence prediction|BiGRUs with attention, ROUGE, Language model, and next sentence prediction | 
1911.09419|6852217163ea678f2009d4726cb6bd03cf6a8f78|What benchmark datasets are used for the link prediction task?|WN18RR, FB15k-237, YAGO3-10|WN18RR BIBREF26, FB15k-237 BIBREF18, YAGO3-10 BIBREF27| 
1911.09419|cd1ad7e18d8eef8f67224ce47f3feec02718ea1a|What are state-of-the art models for this task?|TransE, DistMult, ComplEx, ConvE, RotatE| | 
1911.09419|9c9e90ceaba33242342a5ae7568e89fe660270d5|How better does HAKE model peform than state-of-the-art methods?|0.021 higher MRR, a 2.4% higher H@1, and a 2.4% higher H@3 against RotatE, respectively, doesn't outperform the previous state-of-the-art as much as that of WN18RR and YAGO3-10, HAKE gains a 0.050 higher MRR, 6.0% higher H@1 and 4.6% higher H@3 than RotatE, respectively| | 
1911.09419|2a058f8f6bd6f8e80e8452e1dba9f8db5e3c7de8|How are entities mapped onto polar coordinate system?|radial coordinate and the angular coordinates correspond to the modulus part and the phase part, respectively| | 
1910.11471|db9021ddd4593f6fadf172710468e2fdcea99674|What additional techniques are incorporated?|incorporating coding syntax tree model| | 
1910.11471|8ea4bd4c1d8a466da386d16e4844ea932c44a412|What dataset do they use?|A parallel corpus where the source is an English expression of code and the target is Python code.| text-code parallel corpus| 
1910.11471|92240eeab107a4f636705b88f00cefc4f0782846|Do they compare to other models?|No| | 
1910.11471|4196d329061f5a9d147e1e77aeed6a6bd9b35d18|What is the architecture of the system?|seq2seq translation| | 
1910.11471|321429282557e79061fe2fe02a9467f3d0118cdd|What additional techniques could be incorporated to further improve accuracy?|phrase-based word embedding, Abstract Syntax Tree(AST)| | 
1910.11471|891cab2e41d6ba962778bda297592c916b432226|What programming language is target language?|Python| | 
1910.11471|1eeabfde99594b8d9c6a007f50b97f7f527b0a17|What dataset is used to measure accuracy?|validation data| | 
1910.09399|e96adf8466e67bd19f345578d5a6dc68fd0279a1|Is text-to-image synthesis trained is suppervized or unsuppervized manner?|unsupervised |Even though natural language and image synthesis were part of several contributions on the supervised side of deep learning, unsupervised learning saw recently a tremendous rise in input from the research community specially on two subproblems: text-based natural language and image synthesis| 
1910.09399|c1477a6c86bd1670dd17407590948000c9a6b7c6|What challenges remain unresolved?|give more independence to the several learning methods (e.g. less human intervention) involved in the studies, increasing the size of the output images| | 
1910.09399|e020677261d739c35c6f075cde6937d0098ace7f|What is the conclusion of comparison of proposed solution?|HDGAN produced relatively better visual results on the CUB and Oxford datasets while AttnGAN produced far more impressive results than the rest on the more complex COCO dataset, In terms of inception score (IS), which is the metric that was applied to majority models except DC-GAN, the results in Table TABREF48 show that StackGAN++ only showed slight improvement over its predecessor, text to image synthesis is continuously improving the results for better visual perception and interception| | 
1904.05584|7fe48939ce341212c1d801095517dc552b98e7b3|Where do they employ feature-wise sigmoid gating?|gating mechanism acts upon each dimension of the word and character-level vectors| | 
1904.05584|65ad17f614b7345f0077424c04c94971c831585b|Which model architecture do they use to obtain representations?|BiLSTM with max pooling| | 
1904.05584|9f89bff89cea722debc991363f0826de945bc582|Which similarity datasets do they use?|MEN, MTurk287, MTurk771, RG, RW, SimLex999, SimVerb3500, WS353, WS353R, WS353S|WS353S, SimLex999, SimVerb3500| 
1911.09886|735f58e28d84ee92024a36bc348cfac2ee114409|Are there datasets with relation tuples annotated, how big are datasets available?|Yes| | 
1911.09886|710fa8b3e74ee63d2acc20af19f95f7702b7ce5e|Which one of two proposed approaches performed better in experiments?|WordDecoding (WDec) model| | 
1911.09886|56123dd42cf5c77fc9a88fc311ed2e1eb672126e|What is previous work authors reffer to?|SPTree, Tagging, CopyR, HRL, GraphR, N-gram Attention| | 
1911.09886|1898f999626f9a6da637bd8b4857e5eddf2fc729|How higher are F1 scores compared to previous work?|WordDecoding (WDec) model achieves F1 scores that are $3.9\%$ and $4.1\%$ higher than HRL on the NYT29 and NYT24 datasets respectively, PtrNetDecoding (PNDec) model achieves F1 scores that are $3.0\%$ and $1.3\%$ higher than HRL on the NYT29 and NYT24 datasets respectively|Our WordDecoding (WDec) model achieves F1 scores that are $3.9\%$ and $4.1\%$ higher than HRL on the NYT29 and NYT24 datasets respectively, In the ensemble scenario, compared to HRL, WDec achieves $4.2\%$ and $3.5\%$ higher F1 scores| 
1611.01400|d32b6ac003cfe6277f8c2eebc7540605a60a3904|what were the baselines?|Rank by the number of times a citation is mentioned in the document,  Rank by the number of times the citation is cited in the literature (citation impact). , Rank using Google Scholar Related Articles., Rank by the TF*IDF weighted cosine similarity. , ank using a learning-to-rank model trained on text similarity rankings|(1) Rank by the number of times a citation is mentioned in the document., (2) Rank by the number of times the citation is cited in the literature (citation impact)., (3) Rank using Google Scholar Related Articles., (4) Rank by the TF*IDF weighted cosine similarity., (5) Rank using a learning-to-rank model trained on text similarity rankings.| 
1611.01400|c10f38ee97ed80484c1a70b8ebba9b1fb149bc91|what is the supervised model they developed?|SVMRank| | 
1611.01400|340501f23ddc0abe344a239193abbaaab938cc3a|what is the size of this built corpus?|90 annotated documents with 5 citations each ranked 1 to 5, where 1 is least relevant and 5 is most relevant for a total of 450 annotated citations| | 
1611.01400|fbb85cbd41de6d2818e77e8f8d4b91e431931faa|what crowdsourcing platform is used?|asked the authors to rank by closeness five citations we selected from their paper| | 
1808.05077|1951cde612751410355610074c3c69cec94824c2|Which deep learning model performed better?|autoencoders|CNN| 
1808.05077|61272b1d0338ed7708cf9ed9c63060a6a53e97a2|What was their performance on the dataset?|accuracy of 82.6%| | 
1807.03367|0cd0755ac458c3bafbc70e4268c1e37b87b9721b|Did the authors use crowdsourcing platforms?|Yes|Yes| 
1807.03367|c1ce652085ef9a7f02cb5c363ce2b8757adbe213|How was the dataset collected?|crowd-sourced the collection of the dataset on Amazon Mechanical Turk (MTurk)| | 
1807.03367|96be67b1729c3a91ddf0ec7d6a80f2aa75e30a30|What language do the agents talk in?|English| | 
1807.03367|b85ab5f862221fac819cf2fef239bcb08b9cafc6|What evaluation metrics did the authors look at?|localization accuracy| | 
1807.03367|7e34501255b89d64b9598b409d73f96489aafe45|What data did they use?| dataset on Mechanical Turk involving human perception, action and communication| | 
1907.02030|bd6cec2ab620e67b3e0e7946fc045230e6906020|How is the accuracy of the system measured?|F1 score of 0.71 for this task without any specific training, simply by choosing a threshold below which all sentence pairs are considered duplicates, distances between duplicate and non-duplicate questions using different embedding systems| | 
1907.02030|4b0ba460ae3ba7a813f204abd16cf631b871baca|How is an incoming claim used to retrieve similar factchecked claims?|text clustering on the embeddings of texts| | 
1907.02030|63b0c93f0452d0e1e6355de1d0f3ff0fd67939fb|What existing corpus is used for comparison in these experiments?|Quora duplicate question dataset BIBREF22| | 
1910.04601|b11ee27f3de7dd4a76a1f158dc13c2331af37d9f|What is the baseline?| path ranking-based KGC (PRKGC)| | 
1910.04601|7aba5e4483293f5847caad144ee0791c77164917|What dataset was used in the experiment?|WikiHop| | 
1910.04601|565d668947ffa6d52dad019af79289420505889b|Did they use any crowdsourcing platform?|Yes|Yes| 
1910.04601|d83304c70fe66ae72e78aa1d183e9f18b7484cd6|How was the dataset annotated?|True, Likely (i.e. Answerable), or Unsure (i.e. Unanswerable), why they are unsure from two choices (“Not stated in the article” or “Other”), The “summary” text boxes| | 
1912.05066|5b029ad0d20b516ec11967baaf7d2006e8d7199f|How many label options are there in the multi-label task?| two labels | | 
1912.05066|d3a1a53521f252f869fdae944db986931d9ffe48|Who are the experts?|political pundits of the Washington Post|the experts in the field| 
1912.05066|38e11663b03ac585863742044fd15a0e875ae9ab|Who is the crowd in these experiments?| peoples' sentiments expressed over social media| | 
1912.05066|14421b7ae4459b647033b3ccba635d4ba7bb114b|How do you establish the ground truth of who won a debate?|experts in Washington Post| | 
1910.03891|00e6324ecd454f5d4b2a4b27fcf4104855ff8ee2|What further analysis is done?|we use t-SNE tool BIBREF27 to visualize the learned embedding| | 
1910.03891|aa0d67c2a1bc222d1f2d9e5d51824352da5bb6dc|What seven state-of-the-art methods are used for comparison?|TransE, TransR and TransH, PTransE, and ALL-PATHS, R-GCN BIBREF24 and KR-EAR BIBREF26| | 
1910.03891|cf0085c1d7bd9bc9932424e4aba4e6812d27f727|What three datasets are used to measure performance?|FB24K, DBP24K, Game30K|Freebase BIBREF0, DBpedia BIBREF1 and a self-construction game knowledge graph| 
1910.03891|586b7470be91efe246c3507b05e30651ea6b9832|How does KANE capture both high-order structural and attribute information of KGs in an efficient, explicit and unified manner?|To capture both high-order structural information of KGs, we used an attention-based embedding propagation method.| | 
1910.03891|31b20a4bab09450267dfa42884227103743e3426|What are recent works on knowedge graph embeddings authors mention?|entity types or concepts BIBREF13, relations paths BIBREF17,  textual descriptions BIBREF11, BIBREF12, logical rules BIBREF23, deep neural network models BIBREF24| | 
1610.00879|0c08af6e4feaf801185f2ec97c4da04c8b767ad6|Do the authors mention any confounds to their study?|No| | 
1610.00879|368317b4fd049511e00b441c2e9550ded6607c37|Is the data acquired under distant supervision verified by humans at any stage?|Yes| | 
1610.00879|387970ebc7ef99f302f318d047f708274c0e8f21|Do the authors equate drunk tweeting with drunk texting? |Yes| | 
1704.05572|2fffff59e57b8dbcaefb437a6b3434fc137f813b|What corpus was the source of the OpenIE extractions?|domain-targeted $~$ 80K sentences and 280 GB of plain text extracted from web pages used by BIBREF6 aristo2016:combining| | 
1704.05572|cd1792929b9fa5dd5b1df0ae06fc6aece4c97424|Is an entity linking process used?|No| | 
1704.05572|65d34041ffa4564385361979a08706b10b92ebc7|Are the OpenIE extractions all triples?|No| | 
1704.05572|e215fa142102f7f9eeda9c9eb8d2aeff7f2a33ed|What method was used to generate the OpenIE extractions?|for each multiple-choice question $(q,A) \in Q_\mathit {tr}$ and each choice $a \in A$ , we use all non-stopword tokens in $q$ and $a$ as an ElasticSearch query against S, take the top 200 hits, run Open IE v4, and aggregate the resulting tuples over all $a \in A$ and over all questions in $Q_\mathit {tr}$| | 
1704.05572|a8545f145d5ea2202cb321c8f93e75ad26fcf4aa|Can the method answer multi-hop questions?|Yes| | 
1704.05572|417dabd43d6266044d38ed88dbcb5fdd7a426b22|What was the textual source to which OpenIE was applied?|domain-targeted $~$ 80K sentences and 280 GB of plain text extracted from web pages used by BIBREF6 aristo2016:combining| | 
1704.05572|fed230cef7c130f6040fb04304a33bbc17ca3a36|What OpenIE method was used to generate the extractions?|for each multiple-choice question $(q,A) \in Q_\mathit {tr}$ and each choice $a \in A$ , we use all non-stopword tokens in $q$ and $a$ as an ElasticSearch query against S, take the top 200 hits, run Open IE v4, and aggregate the resulting tuples over all $a \in A$ and over all questions in $Q_\mathit {tr}$| | 
1704.05572|7917d44e952b58ea066dc0b485d605c9a1fe3dda|Is their method capable of multi-hop reasoning?|Yes| | 
1804.10686|7d5ba230522df1890619dedcfb310160958223c1|Do the authors offer any hypothesis about why the dense mode outperformed the sparse one?|Yes| | 
1804.10686|a48cc6d3d322a7b159ff40ec162a541bf74321eb|What evaluation is conducted?|Word Sense Induction & Disambiguation| | 
1804.10686|2bc0bb7d3688fdd2267c582ca593e2ce72718a91|Which corpus of synsets are used?|Wiktionary| | 
1804.10686|8c073b7ea8cb5cc54d7fecb8f4bf88c1fb621b19|What measure of semantic similarity is used?|cosine similarity| | 
1707.03904|dcb18516369c3cf9838e83168357aed6643ae1b8|Which retrieval system was used for baselines?|The dataset comes with a ranked set of relevant documents. Hence the baselines do not use a retrieval system.| | 
1911.07228|f46a907360d75ad566620e7f6bf7746497b6e4a9|What word embeddings were used?|Kyubyong Park, Edouard Grave et al BIBREF11| | 
1911.07228|79d999bdf8a343ce5b2739db3833661a1deab742|What type of errors were produced by the BLSTM-CNN-CRF system?|No extraction, No annotation, Wrong range, Wrong tag, Wrong range and tag| | 
1603.07044|efc65e5032588da4a134d121fe50d49fe8fe5e8c|What supplemental tasks are used for multitask learning?|Multitask learning is used for the task of predicting relevance of a comment on a different question to a given question, where the supplemental tasks are predicting relevance between the questions, and between the comment and the corresponding question| | 
1603.07044|a30958c7123d1ad4723dcfd19d8346ccedb136d5|Is the improvement actually coming from using an RNN?|No| | 
1603.07044|942eb1f7b243cdcfd47f176bcc71de2ef48a17c4|Did they experimnet in other languages?|Yes| | 
1902.09314|9bffc9a9c527e938b2a95ba60c483a916dbd1f6b|Do they use multi-attention heads?|Yes| | 
1902.09314|b67420da975689e47d3ea1c12b601851018c4071|How is their model different from BERT?|overall architecture of the proposed Attentional Encoder Network (AEN), which mainly consists of an embedding layer, an attentional encoder layer, a target-specific attention layer, and an output layer.| | 
1904.03339|01d91d356568fca79e47873bd0541bd22ba66ec0|What datasets were used?|datasets given on the shared task, without using any additional external data| | 
1904.03339|37e45a3439b048a80c762418099a183b05772e6a|How did they do compared to other teams?|second on Subtask A with an F1 score of 77.78% among 33 other team submissions, performs well on Subtask B with an F1 score of 79.59%| | 
1910.11769|cb78e280e3340b786e81636431834b75824568c3|How many emotions do they look at?|9| | 
1910.11769|2941874356e98eb2832ba22eae9cb08ec8ce0308|What are the baseline benchmarks?|TF-IDF + SVM, Depeche + SVM, NRC + SVM, TF-NRC + SVM, Doc2Vec + SVM,  Hierarchical RNN, BiRNN + Self-Attention, ELMo + BiRNN,  Fine-tuned BERT| | 
1910.11769|4e50e9965059899d15d3c3a0c0a2d73e0c5802a0|What is the size of this dataset?|9710 passages, with an average of 6.24 sentences per passage, 16.16 words per sentence, and an average length of 86 words| | 
1910.11769|67d8e50ddcc870db71c94ad0ad7f8a59a6c67ca6|How many annotators were there?|3 | | 
1702.06378|aecb485ea7d501094e50ad022ade4f0c93088d80|Can SCRF be used to pretrain the model?|No| | 
1903.03467|2fea3c955ff78220b2c31a8ad1322bc77f6706f8|What conclusions are drawn from the syntactic analysis?| our method enables to control the morphological realization of first and second-person pronouns, together with verbs and adjectives related to them| | 
1903.03467|faa4f28a2f2968cecb770d9379ab2cfcaaf5cfab|What type of syntactic analysis is performed?|Speaker's Gender Effects, Interlocutors' Gender and Number Effects| | 
1903.03467|da068b20988883bc324e55c073fb9c1a5c39be33|How is it demonstrated that the correct gender and number information is injected using this system?| correct information substantially improves it - we see an increase of up to 2.3 BLEU over the baseline, Finally, the “She said” prefixes substantially increase the number of feminine-marked verbs, bringing the proportion much closer to that of the reference| | 
1903.03467|0d6d5b6c00551dd0d2519f117ea81d1e9e8785ec|Which neural machine translation system is used?|Google's machine translation system (GMT)| | 
1903.03467|edcde2b675cf8a362a63940b2bbdf02c150fe01f|What are the components of the black-box context injection system?|supply an NMT system with knowledge regarding the speaker and interlocutor of first-person sentences| | 
1807.00868|d20d6c8ecd7cb0126479305d27deb0c8b642b09f|What normalization techniques are mentioned?|FBanks with cepstral mean normalization (CMN), variance with mean normalization (CMVN)| | 
1807.00868|11e6b79f1f48ddc6c580c4d0a3cb9bcb42decb17|What features do they experiment with?|40 mel-scaled log filterbank enegries (FBanks) computed every 10 ms with 25 ms window, deltas and delta-deltas (120 features in vector), spectrogram| | 
1807.00868|2677b88c2def3ed94e25a776599555a788d197f2|Which architecture is their best model?|6-layer bLSTM with 1024 hidden units| | 
1909.13375|9ab43f941c11a4b09a0e4aea61b4a5b4612e7933|What approach did previous models use for multi-span questions?|Only MTMSM specifically tried to tackle the multi-span questions. Their approach consisted of two parts: first train a dedicated categorical variable to predict the number of spans to extract and the second was to generalize the single-span head method of extracting a span| | 
1909.13375|5a02a3dd26485a4e4a77411b50b902d2bda3731b|How they use sequence tagging to answer multi-span questions?|To model an answer which is a collection of spans, the multi-span head uses the $\mathtt {BIO}$ tagging format BIBREF8: $\mathtt {B}$ is used to mark the beginning of a span, $\mathtt {I}$ is used to mark the inside of a span and $\mathtt {O}$ is used to mark tokens not included in a span| | 
1909.13375|a9def7958eac7b9a780403d4f136927f756bab83|What is the previous model that attempted to tackle multi-span questions as a part of its design?|MTMSN BIBREF4| | 
1909.00430|e574f0f733fb98ecef3c64044004aa7a320439be|How is the expectation regularization loss defined?|DISPLAYFORM0| | 
1910.11493|b65b1c366c8bcf544f1be5710ae1efc6d2b1e2f1|What were the non-neural baselines used for the task?|The Lemming model in BIBREF17| | 
1910.00912|bd3ccb63fd8ce5575338d7332e96def7a3fabad6|Which publicly available NLU dataset is used?|ROMULUS dataset, NLU-Benchmark dataset| | 
1908.10449|1ef5fc4473105f1c72b4d35cf93d312736833d3d|Do they provide decision sequences as supervision while training models?|No| | 
1908.10449|5f9bd99a598a4bbeb9d2ac46082bd3302e961a0f|What are the models evaluated on?|They evaluate F1 score and agent's test performance on their own built interactive datasets (iSQuAD and iNewsQA)| | 
1908.10449|b2fab9ffbcf1d6ec6d18a05aeb6e3ab9a4dbf2ae|How do they train models in this setup?|Thus, our task requires models to `feed themselves' rather than spoon-feeding them with information. This casts MRC as a sequential decision-making problem amenable to reinforcement learning (RL).| | 
1908.10449|e9cf1b91f06baec79eb6ddfd91fc5d434889f652|What commands does their setup provide to models seeking information?|previous, next, Ctrl+F $<$query$>$, stop| | 
1910.03814|6976296126e4a5c518e6b57de70f8dc8d8fde292|What models do they propose?|Feature Concatenation Model (FCM), Spatial Concatenation Model (SCM), Textual Kernels Model (TKM)| | 
1910.03814|b2b0321b0aaf58c3aa9050906ade6ef35874c5c1|How large is the dataset?| $150,000$ tweets| | 
1910.03814|2e632eb5ad611bbd16174824de0ae5efe4892daf|What is author's opinion on why current multimodal models cannot outperform models analyzing only text?|Noisy data, Complexity and diversity of multimodal relations, Small set of multimodal examples| | 
1910.03814|d1ff6cba8c37e25ac6b261a25ea804d8e58e09c0|What metrics are used to benchmark the results?|F-score, Area Under the ROC Curve (AUC), mean accuracy (ACC), Precision vs Recall plot, ROC curve (which plots the True Positive Rate vs the False Positive Rate)| | 
1910.03814|24c0f3d6170623385283dfda7f2b6ca2c7169238|How is data collected, manual collection or Twitter api?|Twitter API| | 
1910.03814|21a9f1cddd7cb65d5d48ec4f33fe2221b2a8f62e|How many tweats does MMHS150k contains, 150000?|$150,000$ tweets| | 
1910.03814|a0ef0633d8b4040bf7cdc5e254d8adf82c8eed5e|What unimodal detection models were used?| single layer LSTM with a 150-dimensional hidden state for hate / not hate classification| | 
1910.03814|b0799e26152197aeb3aa3b11687a6cc9f6c31011|What different models for multimodal detection were proposed?|Feature Concatenation Model (FCM), Spatial Concatenation Model (SCM), Textual Kernels Model (TKM)| | 
1910.03814|4ce4db7f277a06595014db181342f8cb5cb94626|What annotations are available in the dataset - tweat used hate speach or not?|No attacks to any community,  racist, sexist, homophobic, religion based attacks, attacks to other communities| | 
1701.00185|62a6382157d5f9c1dce6e6c24ac5994442053002|What were the evaluation metrics used?|accuracy, normalized mutual information| | 
1701.00185|a5dd569e6d641efa86d2c2b2e970ce5871e0963f|Which popular clustering methods did they experiment with?|K-means, Skip-thought Vectors, Recursive Neural Network and Paragraph Vector based clustering methods| | 
1701.00185|785c054f6ea04701f4ab260d064af7d124260ccc|What datasets did they use?|SearchSnippets, StackOverflow, Biomedical| | 
1912.00871|3f6610d1d68c62eddc2150c460bf1b48a064e5e6|Does pre-training on general text corpus improve performance?|No| | 
1912.00871|4c854d33a832f3f729ce73b206ff90677e131e48|What neural configurations are explored?|tried many configurations of our network models, but report results with only three configurations, Transformer Type 1, Transformer Type 2, Transformer Type 3| | 
1912.00871|163c15da1aa0ba370a00c5a09294cd2ccdb4b96d|Are the Transformers masked?|Yes| | 
1912.00871|90dd5c0f5084a045fd6346469bc853c33622908f|How is this problem evaluated?|BLEU-2, average accuracies over 3 test trials on different randomly sampled test sets| | 
1912.00871|095888f6e10080a958d9cd3f779a339498f3a109|What datasets do they use?|AI2 BIBREF2, CC BIBREF19, IL BIBREF4, MAWPS BIBREF20| | 
1912.03234|57e783f00f594e08e43a31939aedb235c9d5a102|What evaluation metrics were used?|AUC-ROC| | 
1912.03234|9646fa1abbe3102a0364f84e0a55d107d45c97f0|Where did the real production data come from?| jokes of different categories (sci-fi, sports, etc) and types (puns, limerick, etc)| | 
1912.03234|29983f4bc8a5513a198755e474361deee93d4ab6|What feedback labels are used?|five-minute reuse and one-day return| | 
1911.11750|6c0f97807cd83a94a4d26040286c6f89c4a0f8e0|What representations for textual documents do they use?|finite sequence of terms| | 
1911.11750|13ca4bf76565564c8ec3238c0cbfacb0b41e14d2|Which dataset(s) do they use?|14 TDs, BIBREF15| | 
1911.11750|70797f66d96aa163a3bee2be30a328ba61c40a18|How do they evaluate knowledge extraction performance?|SRCC| | 
1911.03894|71f2b368228a748fd348f1abf540236568a61b07|What is CamemBERT trained on?|unshuffled version of the French OSCAR corpus| | 
1911.03894|d3d4eef047aa01391e3e5d613a0f1f786ae7cfc7|Which tasks does CamemBERT not improve on?|its performance still lags behind models trained on the original English training set in the TRANSLATE-TEST setting, 81.2 vs. 82.91 for RoBERTa| | 
1911.03894|63723c6b398100bba5dc21754451f503cb91c9b8|What is the state of the art?|"POS and DP task: CONLL 2018
NER task: (no extensive work) Strong baselines CRF and BiLSTM-CRF
NLI task: mBERT or XLM (not clear from text)"| | 
1911.03894|5471766ca7c995dd7f0f449407902b32ac9db269|How much better was results of CamemBERT than previous results on these tasks?|2.36 point increase in the F1 score with respect to the best SEM architecture, on the TRANSLATE-TRAIN setting (81.2 vs. 80.2 for XLM), lags behind models trained on the original English training set in the TRANSLATE-TEST setting, 81.2 vs. 82.91 for RoBERTa, For POS tagging, we observe error reductions of respectively 0.71% for GSD, 0.81% for Sequoia, 0.7% for Spoken and 0.28% for ParTUT, For parsing, we observe error reductions in LAS of 2.96% for GSD, 3.33% for Sequoia, 1.70% for Spoken and 1.65% for ParTUT| | 
1911.03894|dc49746fc98647445599da9d17bc004bafdc4579|Was CamemBERT compared against multilingual BERT on these tasks?|Yes| | 
1911.03894|b573b36936ffdf1d70e66f9b5567511c989b46b2|What data is used for training CamemBERT?|unshuffled version of the French OSCAR corpus| | 
2001.09899|bf25a202ac713a34e09bf599b3601058d9cace46|What are the state of the art measures?|Randomwalk, Walktrap, Louvain clustering| | 
2001.09899|abebf9c8c9cf70ae222ecb1d3cabf8115b9fc8ac|What controversial topics are experimented with?|political events such as elections, corruption cases or justice decisions| | 
2001.09899|2df910c9806f0c379d7bb1bc2be2610438e487dc|What datasets did they use?|BIBREF32, BIBREF23, BIBREF33, discussions in four different languages: English, Portuguese, Spanish and French, occurring in five regions over the world: South and North America, Western Europe, Central and Southern Asia. | | 
2001.09899|a2a3af59f3f18a28eb2ca7055e1613948f395052|What social media platform is observed?|Twitter| | 
2001.09899|d92f1c15537b33b32bfc436e6d017ae7d9d6c29a|How many languages do they experiment with?|four different languages: English, Portuguese, Spanish and French| | 
1710.01492|fa3663567c48c27703e09c42930e51bacfa54905|What is the current SOTA for sentiment analysis on Twitter at the time of writing?|deep convolutional networks BIBREF53 , BIBREF54| | 
1710.01492|7997b9971f864a504014110a708f215c84815941|What difficulties does sentiment analysis on Twitter have, compared to sentiment analysis in other domains?|Tweets noisy nature, use of creative spelling and punctuation, misspellings, slang, new words, URLs, and genre-specific terminology and abbreviations, short (length limited) text| | 
1912.01673|a3d83c2a1b98060d609e7ff63e00112d36ce2607|How many sentence transformations on average are available per unique sentence in dataset?|27.41 transformation on average of single seed sentence is available in dataset.| | 
1912.01673|aeda22ae760de7f5c0212dad048e4984cd613162|What annotations are available in the dataset?|For each source sentence, transformation sentences that are transformed according to some criteria (paraphrase, minimal change etc.)| | 
1912.01673|d5fa26a2b7506733f3fa0973e2fe3fc1bbd1a12d|How are possible sentence transformations represented in dataset, as new sentences?|Yes, as new sentences.| | 
1912.01673|18482658e0756d69e39a77f8fcb5912545a72b9b|Is this dataset publicly available?|Yes| | 
1912.01673|9d336c4c725e390b6eba8bb8fe148997135ee981|Are some baseline models trained on this dataset?|Yes| | 
1912.01673|016b59daa84269a93ce821070f4f5c1a71752a8a|Do they do any analysis of of how the modifications changed the starting set of sentences?|Yes| | 
1912.01673|771b373d09e6eb50a74fffbf72d059ad44e73ab0|How do they introduce language variation?| we were looking for original and uncommon sentence change suggestions| | 
1912.01673|efb52bda7366d2b96545cf927f38de27de3b5b77|Do they use external resources to make modifications to sentences?|No| | 
1909.12231|48cc41c372d44b69a477998be449f8b81384786b|How better are state-of-the-art results than this model? |we achieve better results than GCN+PADG but without any use of domain-specific hand-crafted features,  RegSum achieves a similar ROUGE-2 score| | 
1706.08032|0619fc797730a3e59ac146a5a4575c81517cc618|What was the baseline?|We compare our model performance with the approaches of BIBREF0 BIBREF5 on STS Corpus. BIBREF0 reported the results of Maximum Entropy (MaxEnt), NB, SVM on STS Corpus having good performance in previous time. The model of BIBREF5 is a state-of-the-art so far by using a CharSCNN., we compare results with the model of BIBREF14 that used a ensemble of multiple base classifiers (ENS) such as NB, Random Forest (RF), SVM and Logistic Regression (LR). The ENS model is combined with bag-of-words (BoW), feature hashing (FH) and lexicons. The model of BIBREF14 is a state-of-the-art on Sanders and HCR datasets. | | 
1706.08032|846a1992d66d955fa1747bca9a139141c19908e8|Which datasets did they use?|Stanford - Twitter Sentiment Corpus (STS Corpus), Sanders - Twitter Sentiment Corpus, Health Care Reform (HCR)| | 
1706.08032|1ef8d1cb1199e1504b6b0daea52f2e4bd2ef7023|Are results reported only on English datasets?|Yes| | 
1706.08032|12d77ac09c659d2e04b5e3955a283101c3ad1058|Which three Twitter sentiment classification datasets are used for experiments?|Stanford - Twitter Sentiment Corpus (STS Corpus), Sanders - Twitter Sentiment Corpus, Health Care Reform (HCR)| | 
1811.01399|69a7a6675c59a4c5fb70006523b9fe0f01ca415c|Which knowledge graph completion tasks do they experiment with?|link prediction , triplet classification| | 
1811.01399|60cb756d382b3594d9e1f4a5e2366db407e378ae|Apart from using desired properties, do they evaluate their LAN approach in some other way?|No| | 
1811.01399|352a1bf734b2d7f0618e9e2b0dbed4a3f1787160|Do they evaluate existing methods in terms of desired properties?|Yes| | 
1909.00124|c20b012ad31da46642c553ce462bc0aad56912db|What is the dataset used to train the model?| movie sentence polarity dataset from BIBREF19, laptop and restaurant datasets collected from SemEval-201, we collected 2,000 reviews for each domain from the same review source| | 
1909.00124|89b9a2389166b992c42ca19939d750d88c5fa79b|Is the model evaluated against a CNN baseline?|Yes| | 
1909.00088|98ba7a7aae388b1a77dd6cab890977251d906359|Has STES been previously used in the literature to evaluate similar tasks?|No| | 
1909.00088|3da9a861dfa25ed486cff0ef657d398fdebf8a93|What are the baseline models mentioned in the paper?|Noun WordNet Semantic Text Exchange Model (NWN-STEM), General WordNet Semantic Text Exchange Model (GWN-STEM), Word2Vec Semantic Text Exchange Model (W2V-STEM)| | 
1911.01799|529dabe7b4a8a01b20ee099701834b60fb0c43b0|What kind of settings do the utterances come from?|entertainment, interview, singing, play, movie, vlog, live broadcast, speech, drama, recitation and advertisement| | 
1911.01799|5699996a7a2bb62c68c1e62e730cabf1e3186eef|Do they experiment with cross-genre setups?|No| | 
1812.06705|de2d33760dc05f9d28e9dabc13bab2b3264cadb7|Does the new objective perform better than the original objective bert is trained on?|Yes| | 
1812.06705|6333845facb22f862ffc684293eccc03002a4830|Do the authors report performance of conditional bert on tasks without data augmentation?|Yes| | 
1905.08949|a12a08099e8193ff2833f79ecf70acf132eda646|Do they cover data augmentation papers?|No| | 
1905.08949|ca4b66ffa4581f9491442dcec78ca556253c8146|Do they survey visual question generation work?|Yes| | 
1905.08949|b3ff166bd480048e099d09ba4a96e2e32b42422b|Do they survey multilingual aspects?|No| | 
1905.08949|3703433d434f1913307ceb6a8cfb9a07842667dd|What learning paradigms do they cover in this survey?|"Considering ""What"" and ""How"" separately versus jointly optimizing for both."| | 
1905.08949|f7c34b128f8919e658ba4d5f1f3fc604fb7ff793|What are all the input modalities considered in prior work in question generation?|Textual inputs, knowledge bases, and images.| | 
1905.08949|d42031893fd4ba5721c7d37e1acb1c8d229ffc21|Do they survey non-neural methods for question generation?|No| | 
1909.00170|a999761aa976458bbc7b4f330764796446d030ff|What is their model?|cross-lingual NE recognition| | 
1909.00170|f229069bcb05c2e811e4786c89b0208af90d9a25|Do they evaluate on NER data sets?|Yes| | 
1701.03051|6b55b558ed581759425ede5d3a6fcdf44b8082ac|What previously proposed methods is this method compared against?|Naive Bayes, SVM, Maximum Entropy classifiers| | 
1701.03051|3e3f5254b729beb657310a5561950085fa690e83|How is effective word score calculated?|"We define the Effective Word Score of score x as

EFWS(x) = N(+x) - N(-x),

where N(x) is the number of words in the tweet with polarity score x."| | 
1603.01417|129c03acb0963ede3915415953317556a55f34ee|Why is supporting fact supervision necessary for DMN?|First, the GRU only allows sentences to have context from sentences before them, but not after them. This prevents information propagation from future sentences. Second, the supporting sentences may be too far away from each other on a word level to allow for these distant sentences to interact through the word level GRU.| | 
1603.01417|58b3b630a31fcb9bffb510390e1ec30efe87bfbf|What does supporting fact supervision mean?| the facts that are relevant for answering a particular question) are labeled during training.| | 
1603.01417|141dab98d19a070f1ce7e7dc384001d49125d545|What changes they did on input module?|For the DMN+, we propose replacing this single GRU with two different components. The first component is a sentence reader, The second component is the input fusion layer| | 
1603.01417|afdad4c9bdebf88630262f1a9a86ac494f06c4c1|What improvements they did for DMN?|the new DMN+ model does not require that supporting facts (i.e. the facts that are relevant for answering a particular question) are labeled during training., In addition, we introduce a new input module to represent images.| | 
1603.01417|bfd4fc82ffdc5b2b32c37f4222e878106421ce2a|How does the model circumvent the lack of supporting facts during training?|the input fusion layer to allow interactions between input facts and a novel attention based GRU that allows for logical reasoning over ordered inputs. | | 
1603.01417|1ce26783f0ff38925bfc07bbbb65d206e52c2d21|Does the DMN+ model establish state-of-the-art ?|Yes| | 
1911.03385|9213159f874b3bdd9b4de956a88c703aac988411|Is this style generator compared to some baseline?|Yes| | 
1911.03385|5f4e6ce4a811c4b3ab07335d89db2fd2a8d8d8b2|How they perform manual evaluation, what is criteria?|accuracy| | 
1911.03385|a234bcbf2e41429422adda37d9e926b49ef66150|What metrics are used for automatic evaluation?|classification accuracy, BLEU scores, model perplexities of the reconstruction| | 
1911.03385|c383fa9170ae00a4a24a8e39358c38395c5f034b|How they know what are content words?| words found in the control word lists are then removed, The remaining words, which represent the content| | 
1911.03385|83251fd4a641cea8b180b49027e74920bca2699a|How they model style as a suite of low-level linguistic controls, such as frequency of pronouns, prepositions, and subordinate clause constructions?|style of a sentence is represented as a vector of counts of closed word classes (like personal pronouns) as well as counts of syntactic features like the number of SBAR non-terminals in its constituency parse, since clause structure has been shown to be indicative of style| | 
1902.06843|97dac7092cf8082a6238aaa35f4b185343b914af|What insights into the relationship between demographics and mental health are provided?|either likely depressed-user population is younger, or depressed youngsters are more likely to disclose their age, more women than men were given a diagnosis of depression| | 
1902.06843|195611926760d1ceec00bd043dfdc8eba2df5ad1|What model is used to achieve 5% improvement on F1 for identifying depressed individuals on Twitter?|Random Forest classifier| | 
1902.06843|445e792ce7e699e960e2cb4fe217aeacdd88d392|How do this framework facilitate demographic inference from social media?|Demographic information is predicted using weighted lexicon of terms.| | 
1902.06843|a3b1520e3da29d64af2b6e22ff15d330026d0b36|What types of features are used from each data type?|facial presence, Facial Expression, General Image Features,  textual content, analytical thinking, clout, authenticity, emotional tone, Sixltr,  informal language markers, 1st person singular pronouns| | 
1902.06843|2cf8825639164a842c3172af039ff079a8448592|How is the data annotated?|The data are self-reported by Twitter users and then verified by two human experts.| | 
1902.06843|36b25021464a9574bf449e52ae50810c4ac7b642|Where does the information on individual-level demographics come from?|From Twitter profile descriptions of the users.| | 
1902.06843|98515bd97e4fae6bfce2d164659cd75e87a9fc89|What is the source of the user interaction data? |Sociability from ego-network on Twitter| | 
1902.06843|53bf6238baa29a10f4ff91656c470609c16320e1|What is the source of the textual data? |Users' tweets| | 
1902.06843|b27f7993b1fe7804c5660d1a33655e424cea8d10|What is the source of the visual data? |Profile pictures from the Twitter users' profiles.| | 
1905.06512|e21a8581cc858483a31c6133e53dd0cfda76ae4c|Is there an online demo of their system?|No| | 
1905.06512|9f6e877e3bde771595e8aee10c2656a0e7b9aeb2|Do they perform manual evaluation?|Yes| | 
1905.06512|a3783e42c2bf616c8a07bd3b3d503886660e4344|Do they compare against Noraset et al. 2017?|Yes| | 
1905.06512|0d0959dba3f7c15ee4f5cdee51682656c4abbd8f|What is a sememe?|Sememes are minimum semantic units of word meanings, and the meaning of each word sense is typically composed of several sememes| | 
2001.06286|589be705a5cc73a23f30decba23ce58ec39d313b|What data did they use?|the Dutch section of the OSCAR corpus| | 
2001.06286|594a6bf37eab64a16c6a05c365acc100e38fcff1|What language tasks did they experiment on?|sentiment analysis, the disambiguation of demonstrative pronouns,| | 
1910.02789|d79d897f94e666d5a6fcda3b0c7e807c8fad109e|What result from experiments suggest that natural language based agents are more robust?|Average reward across 5 seeds show that NLP representations are robust to changes in the environment as well task-nuisances| | 
1910.02789|8e857e44e4233193c7b2d538e520d37be3ae1552|What experiments authors perform?|a basic scenario, a health gathering scenario, a scenario in which the agent must take cover from fireballs, a scenario in which the agent must defend itself from charging enemies, and a super scenario, where a mixture of the above scenarios| | 
1910.02789|084fb7c80a24b341093d4bf968120e3aff56f693|How is state to learn and complete tasks represented via natural language?| represent the state using natural language| | 
2001.07209|31ee92e521be110b6a5a8d08cc9e6f90a3a97aae|Does the paper discuss previous models which have been applied to the same task?|Yes| | 
2001.07209|737397f66751624bcf4ef891a10b29cfc46b0520|Which datasets are used in the paper?|"Google N-grams
COHA
Moral Foundations Dictionary (MFD)
"| | 
2001.07209|87cb19e453cf7e248f24b5f7d1ff9f02d87fc261|How does the parameter-free model work?|A Centroid model summarizes each set of seed words by its expected vector in embedding space, and classifies concepts into the class of closest expected embedding in Euclidean distance following a softmax rule;, A Naïve Bayes model considers both mean and variance, under the assumption of independence among embedding dimensions, by fitting a normal distribution with mean vector and diagonal covariance matrix to the set of seed words of each class;| | 
2001.07209|5fb6a21d10adf4e81482bb5c1ec1787dc9de260d|How do they quantify moral relevance?|By complementing morally relevant seed words with a set of morally irrelevant seed words based on the notion of valence| | 
2001.07209|542a87f856cb2c934072bacaa495f3c2645f93be|Which fine-grained moral dimension examples do they showcase?|Care / Harm, Fairness / Cheating, Loyalty / Betrayal, Authority / Subversion, and Sanctity / Degradation| | 
2001.10161|76d62e414a345fe955dc2d99562ef5772130bc7e|How is the information extracted?|neural question-answering technique to extract relations from a story text, OpenIE5, a commonly used rule-based information extraction technique| | 
1909.00279|6b9310b577c6232e3614a1612cbbbb17067b3886|What are some guidelines in writing input vernacular so model can generate | if a vernacular paragraph contains more poetic images used in classical literature, its generated poem usually achieves higher score, poems generated from descriptive paragraphs achieve higher scores than from logical or philosophical paragraphs| | 
1909.00279|5787ac3e80840fe4cf7bfae7e8983fa6644d6220|What dataset is used for training?|We collected a corpus of poems and a corpus of vernacular literature from online resources| | 
1909.06762|ee31c8a94e07b3207ca28caef3fbaf9a38d94964|What were the evaluation metrics?|BLEU, Micro Entity F1, quality of the responses according to correctness, fluency, and humanlikeness on a scale from 1 to 5| | 
1909.06762|66d743b735ba75589486e6af073e955b6bb9d2a4|What were the baseline systems?|Attn seq2seq, Ptr-UNK, KV Net, Mem2Seq, DSR| | 
1909.06762|b9f852256113ef468d60e95912800fab604966f6|Which dialog datasets did they experiment with?|Camrest, InCar Assistant| | 
1812.07023|bd74452f8ea0d1d82bbd6911fbacea1bf6e08cab|Do they use pretrained word vectors for dialogue context embedding?|Yes| | 
1812.07023|6472f9d0a385be81e0970be91795b1b97aa5a9cf|Do they train a different training method except from scheduled sampling?|"Answer with content missing: (list missing) 
Scheduled sampling: In our experiments, we found that models trained with scheduled sampling performed better (about 0.004 BLEU-4 on validation set) than the ones trained using teacher-forcing for the AVSD dataset. Hence, we use scheduled sampling for all the results we report in this paper.

Yes."| | 
1610.04377|17de58c17580350c9da9c2f3612784b432154d11|What classifier is used for emergency categorization?|multi-class Naive Bayes| | 
1610.04377|ff27d6e6eb77e55b4d39d343870118d1a6debd5e|What classifier is used for emergency detection?|SVM| | 
1610.04377|29772ba04886bee2d26b7320e1c6d9b156078891|Do the tweets come from any individual?|Yes| | 
1610.04377|d27e3a099954e917b6491e81b2e907478d7f1233|Are the tweets specific to a region?|No| | 
1906.06448|c0a11ba0f6bbb4c69b5a0d4ae9d18e86a4a8f354|Do they release MED?|Yes| | 
1906.06448|dfc393ba10ec4af5a17e5957fcbafdffdb1a6443|What NLI models do they analyze?|BiMPM, ESIM, Decomposable Attention Model, KIM, BERT| | 
1906.06448|311a7fa62721e82265f4e0689b4adc05f6b74215|How do they define upward and downward reasoning?|Upward reasoning is defined as going from one specific concept to a more general one. Downward reasoning is defined as the opposite, going from a general concept to one that is more specific.| | 
1906.06448|82bcacad668351c0f81bd841becb2dbf115f000e|What is monotonicity reasoning?|a type of reasoning based on word replacement, requires the ability to capture the interaction between lexical and syntactic structures| | 
1912.00819|dcd6f18922ac5c00c22cef33c53ff5ae08b42298|How does the ensemble annotator extract the final label?|First preference is given to the labels that are perfectly matching in all the neural annotators., In case two out of three context models are correct, then it is being checked if that label is also produced by at least one of the non-context models., When we see that none of the context models is producing the same results, then we rank the labels with their respective confidence values produced as a probability distribution using the $softmax$ function. The labels are sorted in descending order according to confidence values. Then we check if the first three (case when one context model and both non-context models produce the same label) or at least two labels are matching, then we allow to pick that one. , Finally, when none the above conditions are fulfilled, we leave out the label with an unknown category.| | 
1912.00819|2965c86467d12b79abc16e1457d848cb6ca88973|How were dialogue act labels defined?|Dialogue Act Markup in Several Layers (DAMSL) tag set| | 
1912.00819|b99948ac4810a7fe3477f6591b8cf211d6398e67|How many models were used?|five| | 
1907.00758|73d657d6faed0c11c65b1ab60e553db57f4971ca|Do they compare their neural network against any other model?|No| | 
1907.00758|9ef182b61461d0d8b6feb1d6174796ccde290a15|Do they annotate their own dataset or use an existing one?|Use an existing one| | 
1907.00758|f6f8054f327a2c084a73faca16cf24a180c094ae|Does their neural network predict a single offset in a recording?|Yes| | 
1710.06536|3bf429633ecbbfec3d7ffbcfa61fa90440cc918b|How are aspects identified in aspect extraction?|apply an ensemble of deep learning and linguistics t| | 
1904.05862|ad67ca844c63bf8ac9fdd0fa5f58c5a438f16211|Which unlabeled data do they pretrain with?|1000 hours of WSJ audio data| | 
1904.05862|12eaaf3b6ebc51846448c6e1ad210dbef7d25a96|How many convolutional layers does their model have?|wav2vec has 12 convolutional layers| | 
1904.05862|828615a874512844ede9d7f7d92bdc48bb48b18d|Do they explore how much traning data is needed for which magnitude of improvement for WER? |Yes| | 
1708.09157|a43c400ae37a8705ff2effb4828f4b0b177a74c4|How are character representations from various languages joint?|shared character embeddings for taggers in both languages together through optimization of a joint loss function| | 
1708.09157|4056ee2fd7a0a0f444275e627bb881134a1c2a10|On which dataset is the experiment conducted?|We use the morphological tagging datasets provided by the Universal Dependencies (UD) treebanks (the concatenation of the $4^\text{th}$ and $6^\text{th}$ columns of the file format) BIBREF13 . | | 
1911.00069|f6496b8d09911cdf3a9b72aec0b0be6232a6dba1|Do they train their own RE model?|Yes| | 
1911.00069|3c3b4797e2b21e2c31cf117ad9e52f327721790f|What languages do they experiment on?|English, German, Spanish, Italian, Japanese and Portuguese,  English, Arabic and Chinese| | 
1911.00069|a7d72f308444616a0befc8db7ad388b3216e2143|What datasets are used?|in-house dataset, ACE05 dataset | | 
1910.04887|a130aa735de3b65c71f27018f20d3c068bafb826|How big is data provided by this research?|16k images and 740k corresponding region descriptions| | 
1910.04887|0c1663a7f7750b399f40ef7b4bf19d5c598890ff|How they complete a user query prefix conditioned upon an image?|we replace user embeddings with a low-dimensional image representation| | 
1810.00663|aa800b424db77e634e82680f804894bfa37f2a34|Did the collection process use a WoZ method?|No| | 
1810.00663|fbd47705262bfa0a2ba1440a2589152def64cbbd|By how much did their model outperform the baseline?|increasing accuracy by 35% and 25% in comparison to the Baseline and Ablation models, respectively, over INLINEFORM0 increase in EM and GM between our model and the next best two models| | 
1810.00663|51aaec4c511d96ef5f5c8bae3d5d856d8bc288d3|What baselines did they compare their model with?|the baseline where path generation uses a standard sequence-to-sequence model augmented with attention mechanism and path verification uses depth-first search| | 
1810.00663|f42d470384ca63a8e106c7caf1cb59c7b92dbc27|What evaluation metrics are used?|exact match, f1 score, edit distance and goal match| | 
1810.00663|29bdd1fb20d013b23b3962a065de3a564b14f0fb|Did the authors use a crowdsourcing platform?|Yes| | 
1810.00663|25b2ae2d86b74ea69b09c140a41593c00c47a82b|How were the navigation instructions collected?|using Amazon Mechanical Turk using simulated environments with topological maps| | 
1810.00663|fd7f13b63f6ba674f5d5447b6114a201fe3137cb|What language is the experiment done in?|english language| | 
1809.05752|c82e945b43b2e61c8ea567727e239662309e9508|What additional features are proposed for future work?|distinguishing between clinically positive and negative phenomena within each risk factor domain and accounting for structured data collected on the target cohort| | 
1809.05752|39cf0b3974e8a19f3745ad0bcd1e916bf20eeab8|What datasets did the authors use?| a corpus of discharge summaries, admission notes, individual encounter notes, and other clinical notes from 220 patients in the OnTrackTM program at McLean Hospital, an additional data set for training our vector space model, comprised of EHR texts queried from the Research Patient Data Registry (RPDR)| | 
2001.01589|57388bf2693d71eb966d42fa58ab66d7f595e55f|How is morphology knowledge implemented in the method?|A BPE model is applied to the stem after morpheme segmentation.| | 
2001.01589|47796c7f0a7de76ccb97ccbd43dc851bb8a613d5|How does the word segmentation method work?|morpheme segmentation BIBREF4 and Byte Pair Encoding (BPE) BIBREF5, Zemberek, BIBREF12| | 
2001.01589|9d5153a7553b7113716420a6ddceb59f877eb617|Is the word segmentation method independently evaluated?|No| | 
1910.10324|fa5357c56ea80a21a7ca88a80f21711c5431042c|How many layers do they use in their best performing network?|36| | 
1910.10324|35915166ab2fd3d39c0297c427d4ac00e8083066|Do they just sum up all the loses the calculate to end up with one single loss?|No| | 
1910.05456|fc29bb14f251f18862c100e0d3cd1396e8f2c3a1|Are agglutinative languages used in the prediction of both prefixing and suffixing languages?|Yes| | 
1910.05456|f3e96c5487d87557a661a65395b0162033dc05b3|What is an example of a prefixing language?|Zulu| | 
1910.05456|74db8301d42c7e7936eb09b2171cd857744c52eb|How is the performance on the task evaluated?|Comparison of test accuracies of neural network models on an inflection task and qualitative analysis of the errors| | 
1910.05456|587885bc86543b8f8b134c20e2c62f6251195571|What are the tree target languages studied in the paper?|English, Spanish and Zulu| | 
1910.05154|b72264a73eea36c828e7de778a8b4599a5d02b39|Is the model evaluated against any baseline?|No| | 
1910.05154|24cc1586e5411a7f8574796d3c576b7c677d6e21|Does the paper report the accuracy of the model?|No| | 
1910.05154|db291d734524fa51fb314628b64ebe1bac7f7e1e|How is the performance of the model evaluated?|The experiment settings from this paper and evaluation protocol for the Mboshi corpus (Boundary F-scores using the ZRC speech reference) are the same from BIBREF8., For the multilingual selection experiments, we experimented combining the languages from top to bottom as they appear Table (ranked by performance; e.g. 1-3 means the combination of FR(1), EN(2) and PT(3)). , Lastly, following the methodology from BIBREF8, we extract the most confident alignments (in terms of ANE) discovered by the bilingual models.| | 
1910.05154|50f09a044f0c0795cc636c3e25a4f7c3231fb08d|How does the well-resourced language impact the quality of the output?|Results show that similar languages rank better in terms of segmentation performance, and that by combining the information learned by different models, segmentation is further improved.| | 
1806.00722|26b5c090f72f6d51e5d9af2e470d06b2d7fc4a98|what are the baselines?| 4-layer encoder, 4-layer decoder, residual-connected model, with embedding and hidden size set as 256| | 
1806.00722|8c0621016e96d86a7063cb0c9ec20c76a2dba678|did they outperform previous methods?|Yes| | 
1806.00722|f1214a05cc0e6d870c789aed24a8d4c768e1db2f|what language pairs are explored?|German-English, Turkish-English, English-German| | 
1806.00722|41d3ab045ef8e52e4bbe5418096551a22c5e9c43|what datasets were used?|IWSLT14 German-English, IWSLT14 Turkish-English, WMT14 English-German| | 
2003.03612|62736ad71c76a20aee8e003c462869bab4ab4b1e|How is order of binomials tracked across time?|draw our data from news publications, wine reviews, and Reddit, develop new metrics for the agreement of binomial orderings across communities and the movement of binomial orderings over time,  develop a null model to determine how much variation in binomial orderings we might expect across communities and across time| | 
2003.03612|aaf50a6a9f449389ef212d25d0fae59c10b0df92|What types of various community texts have been investigated for exploring global structure of binomials?|news publications, wine reviews, and Reddit| | 
2003.03612|a1917232441890a89b9a268ad8f987184fa50f7a|Are there any new finding in analasys of trinomials that was not present binomials?|Trinomials are likely to appear in exactly one order| | 
2003.03612|574f17134e4dd041c357ebb75a7ef590da294d22|What new model is proposed for binomial lists?|null model | | 
2003.03612|41fd359b8c1402b31b6f5efd660143d1414783a0|How was performance of previously proposed rules at very large scale?| close to random,| | 
2003.03612|d216d715ec27ee2d4949f9e908895a18fb3238e2|What previously proposed rules for predicting binoial ordering are used?|word length, number of phonemes, number of syllables, alphabetical order, and frequency| | 
2003.03612|ba973b13f26cd5eb1da54663c0a72842681d5bf5|What online text resources are used to test binomial lists?|news publications, wine reviews, and Reddit| | 
1904.08386|508580af51483b5fb0df2630e8ea726ff08d537b|How do they model a city description using embeddings?|We experiment with three different pretrained representations: ELMo BIBREF5 , BERT BIBREF6 , and GloVe BIBREF18 . To produce a single city embedding, we compute the TF-IDF weighted element-wise mean of the token-level representations. For all pretrained methods, we additionally reduce the dimensionality of the city embeddings to 40 using PCA for increased compatibility with our clustering algorithm.| | 
1904.08386|89d1687270654979c53d0d0e6a845cdc89414c67|How do they obtain human judgements?|Using crowdsourcing | | 
1904.08386|fc6cfac99636adda28654e1e19931c7394d76c7c|Which clustering method do they use to cluster city description embeddings?| We devise a simple clustering algorithm to approximate this process. First, we initialize with random cluster assignments and define “cluster strength” to be the relative difference between “intra-group” Euclidean distance and “inter-group” Euclidean distance. Then, we iteratively propose random exchanges of memberships, only accepting these proposals when the cluster strength increases, until convergence. | | 
1909.00754|72ceeb58e783e3981055c70a3483ea706511fac3|What are the performance metrics used?|joint goal accuracy| | 
1909.00754|9bfa46ad55136f2a365e090ce585fc012495393c|Which datasets are used to evaluate performance?|the single domain dataset, WoZ2.0 , the multi-domain dataset, MultiWoZ| | 
1906.00180|f258ada8577bb71873581820a94695f4a2c223b3|How many samples did they generate for the artificial language?|70,000| | 
1806.02847|05bb75a1e1202850efa9191d6901de0a34744af0|Which of their training domains improves performance the most?|documents from the CommonCrawl dataset that has the most overlapping n-grams with the question| | 
1806.02847|770aeff30846cd3d0d5963f527691f3685e8af02|Do they fine-tune their model on the end task?|Yes| | 
1906.04571|f7817b949605fb04b1e4fec9dd9ca8804fb92ae9|Why does not the approach from English work on other languages?|Because, unlike other languages, English does not mark grammatical genders| | 
1906.04571|8255f74cae1352e5acb2144fb857758dda69be02|How do they measure grammaticality?|by calculating log ratio of grammatical phrase over ungrammatical phrase| | 
1906.04571|db62d5d83ec187063b57425affe73fef8733dd28|Which model do they use to convert between masculine-inflected and feminine-inflected sentences?|Markov random field with an optional neural parameterization| | 
1909.04625|a1e07c7563ad038ee2a7de5093ea08efdd6077d4|What is the size of the datasets employed?|(about 4 million sentences, 138 million word tokens), one trained on the Billion Word benchmark| | 
1909.04625|a1c4f9e8661d4d488b8684f055e0ee0e2275f767|What are the baseline models?|Recurrent Neural Network (RNN), ActionLSTM, Generative Recurrent Neural Network Grammars (RNNG)| | 
1809.07629|c5171daf82107fce0f285fa18f19e91fbd1215c5|What evaluation metrics are used?|the evaluation metrics include BLEU and ROUGE (1, 2, L) scores| | 
1809.07629|baeb6785077931e842079e9d0c9c9040947ffa4e|What datasets did they use?|The E2E NLG challenge dataset BIBREF21| | 
1807.05154|bb570d4a1b814f508a07e74baac735bf6ca0f040|Why does their model do better than prior models?|better sentence pair representations| | 
2002.11402|a1885f807753cff7a59f69b5cf6d0fdef8484057|How large is the dataset they used?|English wikipedia dataset has more than 18 million, a dump of 15 million English news articles | | 
1804.09301|c2553166463b7b5ae4d9786f0446eb06a90af458|Which coreference resolution systems are tested?|the BIBREF5 sieve system from the rule-based paradigm (referred to as RULE), BIBREF6 from the statistical paradigm (STAT), and the BIBREF11 deep reinforcement system from the neural paradigm (NEURAL).| | 
2002.00652|69e678666d11731c9bfa99953e2cd5a5d11a4d4f|What two large datasets are used for evaluation?|SParC BIBREF2 and CoSQL BIBREF6| | 
2002.00652|f858031ebe57b6139af46ee0f25c10870bb00c3c|What are two datasets models are tested on?|SParC BIBREF2 and CoSQL BIBREF6| | 
1909.00324|1763a029daca7cab10f18634aba02a6bd1b6faa7|How big is the improvement over the state-of-the-art results?|AGDT improves the performance by 2.4% and 1.6% in the “DS” part of the two dataset, Our AGDT surpasses GCAE by a very large margin (+11.4% and +4.9% respectively) on both datasets, In the “HDS” part, the AGDT model obtains +3.6% higher accuracy than GCAE on the restaurant domain and +4.2% higher accuracy on the laptop domain| | 
1909.00324|f9de9ddea0c70630b360167354004ab8cbfff041|Is the model evaluated against other Aspect-Based models?|Yes| | 
2003.04032|58e65741184c81c9e7fe0ca15832df2d496beb6f|Do they build a model to recognize discourse relations on their dataset?|No| | 
2003.04032|269b05b74d5215b09c16e95a91ae50caedd9e2aa|Which inter-annotator metric do they use?|agreement rates, Kappa value| | 
2003.04032|0d7f514f04150468b2d1de9174c12c28e52c5511|How high is the inter-annotator agreement?|agreement of 0.85 and Kappa value of 0.83| | 
2003.04032|4d223225dbf84a80e2235448a4d7ba67bfb12490|How are resources adapted to properties of Chinese text?|removing AltLexC and adding Progression into our sense hierarchy| | 
2004.03034|ca26cfcc755f9d0641db0e4d88b4109b903dbb26|How better are results compared to baseline models?|F1 score of best authors' model is 55.98 compared to BiLSTM and FastText that have F1 score slighlty higher than 46.61.| | 
2004.03034|6cdd61ebf84aa742155f4554456cc3233b6ae2bf|What models that rely only on claim-specific linguistic features are used as baselines?|SVM with RBF kernel| | 
2004.03034|8e8097cada29d89ca07166641c725e0f8fed6676|How is pargmative and discourse context added to the dataset?|While evaluating the impact of a claim, users have access to the full argument context and therefore, they can assess how impactful a claim is in the given context of an argument.| | 
2004.03034|951098f0b7169447695b47c142384f278f451a1e|What annotations are available in the dataset?|5 possible impact labels for a particular claim: no impact, low impact, medium impact, high impact and very high impact| | 
1910.12618|07c59824f5e7c5399d15491da3543905cfa5f751|How big is dataset used for training/testing?|4,261  days for France and 4,748 for the UK| | 
1910.12618|77f04cd553df691e8f4ecbe19da89bc32c7ac734|Is there any example where geometric property is visible for context similarity between words?|Yes| | 
1910.12618|728a55c0f628f2133306b6bd88af00eb54017b12|What geometric properties do embeddings display?|Winter and summer words formed two separate clusters. Week day and week-end day words also formed separate clusters.| | 
1910.12618|d5498d16e8350c9785782b57b1e5a82212dbdaad|How accurate is model trained on text exclusively?|Relative error is less than 5%| | 
1911.12569|3e839783d8a4f2fe50ece4a9b476546f0842b193|What was their result on Stance Sentiment Emotion Corpus?|F1 score of 66.66%| | 
1911.12569|2869d19e54fb554fcf1d6888e526135803bb7d75|What performance did they obtain on the SemEval dataset?|F1 score of 82.10%| | 
1911.12569|894c086a2cbfe64aa094c1edabbb1932a3d7c38a|What are the state-of-the-art systems?|For sentiment analysis UWB, INF-UFRGS-OPINION-MINING, LitisMind, pkudblab and SVM + n-grams + sentiment and for emotion analysis MaxEnt, SVM, LSTM, BiLSTM and CNN| | 
1911.12569|722e9b6f55971b4c48a60f7a9fe37372f5bf3742|How is multi-tasking performed?|The proposed system consists of a Bi-directional Long Short-Term Memory (BiLSTM) BIBREF16, a two-level attention mechanism BIBREF29, BIBREF30 and a shared representation for emotion and sentiment analysis tasks., Each of the shared representations is then fed to the primary attention mechanism| | 
1911.12569|9c2f306044b3d1b3b7fdd05d1c046e887796dd7a|What are the datasets used for training?|SemEval 2016 Task 6 BIBREF7, Stance Sentiment Emotion Corpus (SSEC) BIBREF15| | 
1911.12569|9219eef636ddb020b9d394868959325562410f83|What is the previous state-of-the-art model?|BIBREF7, BIBREF39, BIBREF37, LitisMind, Maximum entropy, SVM, LSTM, Bi-LSTM, and CNN| | 
1910.01363|0ee20a3a343e1e251b74a804e9aa1393d17b46d6|How can the classifier facilitate the annotation task for human annotators?|quality of the classifier predictions is too low to be integrated into the network analysis right away, the classifier drastically facilitates the annotation process for human annotators compared to annotating unfiltered tweets| | 
1910.01363|f0e8f045e2e33a2129e67fb32f356242db1dc280|What recommendations are made to improve the performance in future?|applying reasoning BIBREF36 or irony detection methods BIBREF37| | 
1910.01363|b6c235d5986914b380c084d9535a7b01310c0278|What type of errors do the classifiers use?|correct class can be directly inferred from the text content easily, even without background knowledge, correct class can be inferred from the text content, given that event-specific knowledge is provided, orrect class can be inferred from the text content if the text is interpreted correctly| | 
1910.01363|e9b1e8e575809f7b80b1125305cfa76ae4f5bdfb|What neural classifiers are used?| convolutional neural network (CNN) BIBREF29| | 
1910.01363|02ce4c288df14a90a210cb39973c6ac0fb4cec59|What languages are included in the dataset?|English| | 
1910.01363|60726d9792d301d5ff8e37fbb31d5104a520dea3|What dataset is used for this study?|MH17 Twitter dataset| | 
1910.01363|e39d90b8d959697d9780eddce3a343e60543be65|What proxies for data annotation were used in previous datasets?|widely used method for classifying misleading content is to use distant annotations, for example to classify a tweet based on the domain of a URL that is shared by the tweet, or a hashtag that is contained in the tweet, Natural Language Processing (NLP) models can be used to automatically label text content| | 
1901.04899|c6e63e3b807474e29bfe32542321d015009e7148|What are the supported natural commands?|Set/Change Destination, Set/Change Route, Go Faster, Go Slower, Stop, Park, Pull Over, Drop Off, Open Door, Other | | 
1901.04899|4ef2fd79d598accc54c084f0cca8ad7c1b3f892a|What is the size of their collected dataset?|3347 unique utterances | | 
1901.04899|8383e52b2adbbfb533fbe8179bc8dae11b3ed6da|What intents does the paper explore?|Set/Change Destination, Set/Change Route, Go Faster, Go Slower, Stop, Park, Pull Over, Drop Off, Open Door, Other | | 
1606.05320|7cdce4222cea6955b656c1a3df1129bb8119e2d0|Which methods do the authors use to reach the conclusion that LSTMs and HMMs learn complementary information?|decision trees to predict individual hidden state dimensions, apply k-means clustering to the LSTM state vectors, and color-code the training data with the clusters| | 
1809.10644|50690b72dc61748e0159739a9a0243814d37f360|Do they report results only on English data?|Yes| | 
1809.10644|8266642303fbc6a1138b4e23ee1d859a6f584fbb|Which publicly available datasets are used?|BIBREF3, BIBREF4, BIBREF9| | 
1809.10644|3685bf2409b23c47bfd681989fb4a763bcab6be2|What embedding algorithm and dimension size are used?|300 Dimensional Glove| | 
1809.10644|19225e460fff2ac3aebc7fe31fcb4648eda813fb|What data are the embeddings trained on?|Common Crawl | | 
1809.10644|f37026f518ab56c859f6b80b646d7f19a7b684fa|how much was the parameter difference between their model and previous methods?|our model requires 100k parameters , while BIBREF8 requires 250k parameters| | 
1809.10644|1231934db6adda87c1b15e571468b8e9d225d6fe|how many parameters did their model use?|Excluding the embedding weights, our model requires 100k parameters| | 
1809.10644|81303f605da57ddd836b7c121490b0ebb47c60e7|which datasets were used?|Sexist/Racist (SR) data set, HATE dataset, HAR| | 
1809.10644|118ff1d7000ea0d12289d46430154cc15601fd8e|what was the baseline?|logistic regression| | 
1606.02006|102a0439739428aac80ac11795e73ce751b93ea1|What datasets were used?|KFTT BIBREF12 and BTEC BIBREF13| | 
1606.02006|d9c26c1bfb3830c9f3dbcccf4c8ecbcd3cb54404|What language pairs did they experiment with?|English-Japanese| | 
1911.03243|04f72eddb1fc73dd11135a80ca1cf31e9db75578|How much more coverage is in the new dataset?|278 more annotations| | 
1911.03243|f74eaee72cbd727a6dffa1600cdf1208672d713e|How was coverage measured?|QA pairs per predicate| | 
1911.03243|068dbcc117c93fa84c002d3424bafb071575f431|How was quality measured?|Inter-annotator agreement, comparison against expert annotation, agreement with PropBank Data annotations.| | 
1911.03243|96526a14820b7debfd6f7c5beeade0a854b93d1a|How was the corpus obtained?| trained annotators BIBREF4, crowdsourcing BIBREF5 | | 
1911.03243|32ba4d2d15194e889cbc9aa1d21ff1aa6fa27679|How are workers trained?|extensive personal feedback| | 
1911.03243|78c010db6413202b4063dc3fb6e3cc59ec16e7e3|What is different in the improved annotation protocol?|a trained worker consolidates existing annotations | | 
1911.03243|a69af5937cab861977989efd72ad1677484b5c8c|How was the previous dataset annotated?|the annotation machinery of BIBREF5| | 
1911.03243|8847f2c676193189a0f9c0fe3b86b05b5657b76a|How big is the dataset?|1593 annotations| | 
1809.04686|545ff2f76913866304bfacdb4cc10d31dbbd2f37|What data were they used to train the multilingual encoder?|WMT 2014 En-Fr parallel corpus| | 
1703.09684|cf93a209c8001ffb4ef505d306b6ced5936c6b63|From when are many VQA datasets collected?|late 2014| | 
1911.11744|fb5ce11bfd74e9d7c322444b006a27f2ff32a0cf|What is task success rate achieved? |96-97.6% using the objects color or shape and 79% using shape alone| | 
1911.11744|1e2ffa065b640e912d6ed299ff713a12195e12c4|What simulations are performed by the authors to validate their approach?|a simulated binning task in which the robot is tasked to place a cube into a bowl as outlined by the verbal command| | 
1911.11744|28b2a20779a78a34fb228333dc4b93fd572fda15|Does proposed end-to-end approach learn in reinforcement or supervised learning manner?|supervised learning| | 
1910.03467|b367b823c5db4543ac421d0057b02f62ea16bf9f|Are synonymous relation taken into account in the Japanese-Vietnamese task?|Yes| | 
1908.09156|f80d89fb905b3e7e17af1fe179b6f441405ad79b|Does the paper consider the use of perplexity in order to identify text anomalies?|No| | 
1908.09156|5f6fac08c97c85d5f4f4d56d8b0691292696f8e6|Does the paper report a baseline for the task?|No| | 
1911.00523|6adec34d86095643e6b89cda5c7cd94f64381acc|What non-contextual properties do they refer to?|These features are derived directly from the word and capture the general tendency of a word being echoed in explanations.| | 
1911.00523|62ba1fefc1eb826fe0cbac092d37a3e2098967e9|What is the baseline?|random method , LSTM | | 
1911.00523|93ac147765ee2573923f68aa47741d4bcbf88fa8|What are their proposed features?|Non-contextual properties of a word, Word usage in an OP or PC (two groups), How a word connects an OP and PC., General OP/PC properties| | 
1911.00523|14c0328e8ec6360a913b8ecb3e50cb27650ff768|What are overall baseline results on new this new task?|all of our models outperform the random baseline by a wide margin, he best F1 score in content words more than doubles that of the random baseline (0.286 vs. 0.116)| | 
1911.00523|6073fa9050da76eeecd8aa3ccc7ecb16a238d83f|What metrics are used in evaluation of this task?|F1 score| | 
1911.00523|eacd7e540cc34cb45770fcba463f4bf968681d59|Do authors provide any explanation for intriguing patterns of word being echoed?|No| | 
1911.00523|1124804c3702499b78cf0678bab5867e81284b6c|What features are proposed?|Non-contextual properties of a word, Word usage in an OP or PC (two groups), How a word connects an OP and PC, General OP/PC properties| | 
1803.03664|2b78052314cb730824836ea69bc968df7964b4e4|Which datasets are used to train this model?|SQUAD| | 
1910.11949|11d2f0d913d6e5f5695f8febe2b03c6c125b667c|How is performance of this system measured?|using the BLEU score as a quantitative metric and human evaluation for quality| | 
1910.11949|1c85a25ec9d0c4f6622539f48346e23ff666cd5f|How many questions per image on average are available in dataset?|5 questions per image| | 
1910.11949|37d829cd42db9ae3d56ab30953a7cf9eda050841|Is machine learning system underneath similar to image caption ML systems?|Yes| | 
1910.11949|4b41f399b193d259fd6e24f3c6e95dc5cae926dd|How big dataset is used for training this system?|For the question generation model 15,000 images with 75,000 questions. For the chatbot model, around 460k utterances over 230k dialogues.| | 
1902.09087|76377e5bb7d0a374b0aefc54697ac9cd89d2eba8|How do they obtain word lattices from words?|By considering words as vertices and generating directed edges between neighboring words within a sentence| | 
1902.09087|85aa125b3a15bbb6f99f91656ca2763e8fbdb0ff|Which metrics do they use to evaluate matching?|Precision@1, Mean Average Precision, Mean Reciprocal Rank| | 
1902.09087|4b128f9e94d242a8e926bdcb240ece279d725729|Which dataset(s) do they evaluate on?|DBQA, KBRE| | 
1907.01413|abad9beb7295d809d7e5e1407cbf673c9ffffd19|Do they propose any further additions that could be made to improve generalisation to unseen speakers?|Yes| | 
1907.01413|265c9b733e4dfffb76acfbade4c0c9b14d3ccde1|What are the characteristics of the dataset?|synchronized acoustic and ultrasound data from 58 typically developing children, aged 5-12 years old (31 female, 27 male), data was aligned at the phone-level, 121fps with a 135 field of view, single ultrasound frame consists of 412 echo returns from each of the 63 scan lines (63x412 raw frames)| | 
1907.01413|0f928732f226185c76ad5960402e9342c0619310|What type of models are used for classification?|feedforward neural networks (DNNs), convolutional neural networks (CNNs)| | 
1907.01413|11c5b12e675cfd8d1113724f019d8476275bd700|Do they compare to previous work?|No| | 
1907.01413|d24acc567ebaec1efee52826b7eaadddc0a89e8b|How many instances does their dataset have?|10700| | 
1907.01413|2d62a75af409835e4c123a615b06235a352a67fe|What model do they use to classify phonetic segments? |feedforward neural networks, convolutional neural networks| | 
1907.01413|fffbd6cafef96eeeee2f9fa5d8ab2b325ec528e6|How many speakers do they have in the dataset?|58| | 
1908.07816|9cbea686732b5b85f77868ca47d2f93cf34516ed|How does the multi-turn dialog system learns?|we extract the emotion information from the utterances in $\mathbf {X}$ by leveraging an external text analysis program, and use an RNN to encode it into an emotion context vector $\mathbf {e}$, which is combined with $\mathbf {c}_t$ to produce the distribution| | 
1908.07816|6aee16c4f319a190c2a451c1c099b66162299a28|How is human evaluation performed?|(1) grammatical correctness, (2) contextual coherence, (3) emotional appropriateness| | 
1908.07816|4d4b9ff2da51b9e0255e5fab0b41dfe49a0d9012|Is some other metrics other then perplexity measured?|No| | 
1908.07816|180047e1ccfc7c98f093b8d1e1d0479a4cca99cc|What two baseline models are used?| sequence-to-sequence model (denoted as S2S), HRAN| | 
1703.03097|fb3687ea05d38b5e65fdbbbd1572eacd82f56c0b|Do they evaluate on relation extraction?|No| | 
1808.09409|b5d6357d3a9e3d5fdf9b344ae96cddd11a407875|What is the baseline model for the agreement-based mode?|PCFGLA-based parser, viz. Berkeley parser BIBREF5, minimal span-based neural parser BIBREF6| | 
1808.09409|f33a21c6a9c75f0479ffdbb006c40e0739134716|Do the authors suggest why syntactic parsing is so important for semantic role labelling for interlanguages?|syntax-based system may generate correct syntactic analyses for partial grammatical fragments| | 
1808.09409|8a1d4ed00d31c1f1cb05bc9d5e4f05fe87b0e5a4|Who manually annotated the semantic roles for the set of learner texts?|Authors| | 
1808.00265|17f5f4a5d943c91d46552fb75940b67a72144697|By how much do they outperform existing state-of-the-art VQA models?|the rank-correlation for MFH model increases by 36.4% when is evaluated in VQA-HAT dataset and 7.7% when is evaluated in VQA-X| | 
1808.00265|83f22814aaed9b5f882168e22a3eac8f5fda3882|How do they measure the correlation between manual groundings and model generated ones?|rank-correlation BIBREF25| | 
1808.00265|ed11b4ff7ca72dd80a792a6028e16ba20fccff66|How do they obtain region descriptions and object annotations?|they are available in the Visual Genome dataset| | 
1810.09774|b69897deb5fb80bf2adb44f9cbf6280d747271b3|Which model generalized the best?|BERT| | 
1810.09774|ad1f230f10235413d1fe501e414358245b415476|Which models were compared?|BiLSTM-max, HBMP, ESIM, KIM, ESIM + ELMo, and BERT| | 
1810.09774|0a521541b9e2b5c6d64fb08eb318778eba8ac9f7|Which datasets were used?|SNLI, MultiNLI and SICK| | 
1910.05608|284ea817fd79bc10b7a82c88d353e8f8a9d7e93c|Is the data all in Vietnamese?|Yes| | 
1910.05608|c0122190119027dc3eb51f0d4b4483d2dbedc696|What classifier do they use?|Stacking method, LSTMCNN, SARNN, simple LSTM bidirectional model, TextCNN| | 
1910.05608|1ed6acb88954f31b78d2821bb230b722374792ed|What is private dashboard?|Private dashboard is leaderboard where competitors can see results after competition is finished - on hidden part of test set (private test set).| | 
1910.05608|5a33ec23b4341584a8079db459d89a4e23420494|What is public dashboard?|Public dashboard where competitors can see their results during competition, on part of the test set (public test set).| | 
1910.05608|1b9119813ea637974d21862a8ace83bc1acbab8e|What dataset do they use?|They used Wiki Vietnamese language and Vietnamese newspapers to pretrain embeddings and dataset provided in HSD task to train model (details not mentioned in paper).| | 
1906.07668|f52ec4d68de91dba66668f0affc198706949ff90|What other interesting correlations are observed?|Women-Yoga| | 
1605.04655|225a567eeb2698a9d3f1024a8b270313a6d15f82|what were the baselines?|RNN model, CNN model , RNN-CNN model, attn1511 model, Deep Averaging Network model, avg mean of word embeddings in the sentence with projection matrix| | 
1605.04655|35b10e0dc2cb4a1a31d5692032dc3fbda933bf7d|what is the state of the art for ranking mc test answers?|ensemble of hand-crafted syntactic and frame-semantic features BIBREF16| | 
1605.04655|62613aca3d7c7d534c9f6d8cb91ff55626bb8695|what datasets did they use?|Argus Dataset, AI2-8grade/CK12 Dataset, MCTest Dataset| | 
1911.09483|6e4505609a280acc45b0a821755afb1b3b518ffd|What evaluation metric is used?|The BLEU metric | | 
1911.09483|9bd938859a8b063903314a79f09409af8801c973|What datasets are used?|WMT14 En-Fr and En-De datasets, IWSLT De-En and En-Vi datasets| | 
1911.09483|68ba5bf18f351e8c83fae7b444cc50bef7437f13|What are three main machine translation tasks?|De-En, En-Fr and En-Vi translation tasks| | 
1911.09483|f6a1125c5621a2f32c9bcdd188dff14efa096083|How big is improvement in performance over Transformers?|2.2 BLEU gains| | 
1805.00760|282aa4e160abfa7569de7d99b8d45cabee486ba4|How do they determine the opinion summary?|the weighted sum of the new opinion representations, according to their associations with the current aspect representation| | 
1805.00760|ecfb2e75eb9a8eba8f640a039484874fa0d2fceb|Do they explore how useful is the detection history and opinion summary?|Yes| | 
1805.00760|a6950c22c7919f86b16384facc97f2cf66e5941d|Which dataset(s) do they use to train the model?|INLINEFORM0 (SemEval 2014) contains reviews of the laptop domain and those of INLINEFORM1 (SemEval 2014), INLINEFORM2 (SemEval 2015) and INLINEFORM3 (SemEval 2016) are for the restaurant domain.| | 
1805.00760|54be3541cfff6574dba067f1e581444537a417db|By how much do they outperform state-of-the-art methods?|Compared with the winning systems of SemEval ABSA, our framework achieves 5.0%, 1.6%, 1.4%, 1.3% absolute gains on INLINEFORM0 , INLINEFORM1 , INLINEFORM2 and INLINEFORM3 respectively.| | 
1909.05358|221e9189a9d2431902d8ea833f486a38a76cbd8e|What is the average number of turns per dialog?|The average number of utterances per dialog is about 23 | | 
1909.05358|a276d5931b989e0a33f2a0bc581456cca25658d9|What baseline models are offered?|3-gram and 4-gram conditional language model, Convolution, LSTM models BIBREF27 with and without attention BIBREF28, Transformer, GPT-2| | 
1909.05358|c21d26130b521c9596a1edd7b9ef3fe80a499f1e|Which six domains are covered in the dataset?|ordering pizza, creating auto repair appointments, setting up ride service, ordering movie tickets, ordering coffee drinks and making restaurant reservations| | 
2003.06279|ec8043290356fcb871c2f5d752a9fe93a94c2f71|What other natural processing tasks authors think could be studied by using word embeddings?|general classification tasks, use of the methodology in other networked systems, a network could be enriched with embeddings obtained from graph embeddings techniques| | 
2003.06279|728c2fb445173fe117154a2a5482079caa42fe24|What is the reason that traditional co-occurrence networks fail in establishing links between similar words whenever they appear distant in the text?|long-range syntactical links, though less frequent than adjacent syntactical relationships, might be disregarded from a simple word adjacency approach| | 
2003.06279|23d32666dfc29ed124f3aa4109e2527efa225fbc|Do the use word embeddings alone or they replace some previous features of the model with word embeddings?|They use it as addition to previous model - they add new edge between words if word embeddings are similar.| | 
2003.06279|076928bebde4dffcb404be216846d9d680310622|On what model architectures are previous co-occurence networks based?|in a co-occurrence network each different word becomes a node and edges are established via co-occurrence in a desired window, connects only adjacent words in the so called word adjacency networks| | 
2004.03744|f33236ebd6f5a9ccb9b9dbf05ac17c3724f93f91|Is model explanation output evaluated, what metric was used?|balanced accuracy, i.e., the average of the three accuracies on each class| | 
2004.03744|66bf0d61ffc321f15e7347aaed191223f4ce4b4a|How many annotators are used to write natural language explanations to SNLI-VE-2.0?|2,060 workers| | 
2004.03744|0c557b408183630d1c6c325b5fb9ff1573661290|How much is performance difference of existing model between original and corrected corpus?|73.02% on the uncorrected SNLI-VE test set, achieves 73.18% balanced accuracy when tested on the corrected test set| | 
2004.03744|a08b5018943d4428f067c08077bfff1af3de9703|What is the class with highest error rate in SNLI-VE?|neutral class| | 
2001.09332|9447ec36e397853c04dcb8f67492ca9f944dbd4b|What is the dataset used as input to the Word2Vec algorithm?|Italian Wikipedia and Google News extraction producing final vocabulary of 618224 words| | 
2001.09332|12c6ca435f4fcd4ad5ea5c0d76d6ebb9d0be9177|Are the word embeddings tested on a NLP task?|Yes| | 
2001.09332|32c149574edf07b1a96d7f6bc49b95081de1abd2|Are the word embeddings evaluated?|Yes| | 
2001.09332|3de27c81af3030eb2d9de1df5ec9bfacdef281a9|How big is dataset used to train Word2Vec for the Italian Language?|$421\,829\,960$ words divided into $17\,305\,401$ sentences| | 
2001.09332|cc680cb8f45aeece10823a3f8778cf215ccc8af0|How does different parameter settings impact the performance and semantic capacity of resulting model?|number of epochs is an important parameter and its increase leads to results that rank our two worst models almost equal, or even better than others| | 
2001.09332|9190c56006ba84bf41246a32a3981d38adaf422c|What dataset is used for training Word2Vec in Italian language?|extracted from a dump of the Italian Wikipedia (dated 2019.04.01), from the main categories of Italian Google News (WORLD, NATION, BUSINESS, TECHNOLOGY, ENTERTAINMENT, SPORTS, SCIENCE, HEALTH) and from some anonymized chats between users and a customer care chatbot (Laila)| | 
1804.06506|7aab78e90ba1336950a2b0534cc0cb214b96b4fd|How are the auxiliary signals from the morphology table incorporated in the decoder?|an additional morphology table including target-side affixes., We inject the decoder with morphological properties of the target language.| | 
1804.06506|b7fe91e71da8f4dc11e799b3bd408d253230e8c6|"What type of morphological information is contained in the ""morphology table""?"|target-side affixes| | 
1904.07342|16fa6896cf4597154363a6c9a98deb49fffef15f|Do they report results only on English data?|Yes| | 
1904.07342|0f60864503ecfd5b048258e21d548ab5e5e81772|Do the authors mention any confounds to their study?|No| | 
1904.07342|fe578842021ccfc295209a28cf2275ca18f8d155|Which machine learning models are used?|RNNs, CNNs, Naive Bayes with Laplace Smoothing, k-clustering, SVM with linear kernel| | 
1904.07342|00ef9cc1d1d60f875969094bb246be529373cb1d|What methodology is used to compensate for limited labelled data?|Influential tweeters ( who they define as individuals certain to have a classifiable sentiment regarding the topic at hand) is used to label tweets in bulk in the absence of manually-labeled tweets.| | 
1904.07342|279b633b90fa2fd69e84726090fadb42ebdf4c02|Which five natural disasters were examined?|the East Coast Bomb Cyclone,  the Mendocino, California wildfires, Hurricane Florence, Hurricane Michael, the California Camp Fires| | 
2001.06888|0106bd9d54e2f343cc5f30bb09a5dbdd171e964b|Which social media platform is explored?|twitter | | 
2001.06888|e015d033d4ee1c83fe6f192d3310fb820354a553|What datasets did they use?|BIBREF8 a refined collection of tweets gathered from twitter| | 
1911.00547|acd05f31e25856b9986daa1651843b8dc92c2d99|What is the size of the dataset?| 9,892 stories of sexual harassment incidents| | 
1911.00547|8c78b21ec966a5e8405e8b9d3d6e7099e95ea5fb|What model did they use?|joint learning NLP models that use convolutional neural network (CNN) BIBREF8 and bi-directional long short-term memory (BiLSTM)| | 
1911.00547|af60462881b2d723adeb4acb5fbc07ea27b6bde2|What patterns were discovered from the stories?|we demonstrate that harassment occurred more frequently during the night time than the day time, it shows that besides unspecified strangers (not shown in the figure), conductors and drivers are top the list of identified types of harassers, followed by friends and relatives, we uncovered that there exist strong correlations between the age of perpetrators and the location of harassment, between the single/multiple harasser(s) and location, and between age and single/multiple harasser(s) , We also found that the majority of young perpetrators engaged in harassment behaviors on the streets, we found that adult perpetrators of sexual harassment are more likely to act alone, we also found that the correlations between the forms of harassment with the age, single/multiple harasser, type of harasser, and location , commenting happened more frequently when harassers were in groups. Last but not least, public transportation is where people got indecently touched most frequently both by fellow passengers and by conductors and drivers.| | 
1604.00117|3c378074111a6cc7319c0db0aced5752c30bfffb|Does the performance increase using their method?|The multi-task model outperforms the single-task model at all data sizes, but none have an overall benefit from the open vocabulary system| | 
1604.00117|b464bc48f176a5945e54051e3ffaea9a6ad886d7|What tasks are they experimenting with in this paper?|Slot filling, we consider the actions that a user might perform via apps on their phone, The corresponding actions are booking a flight, renting a home, buying bus tickets, and making a reservation at a restaurant| | 
1908.06725|3c16d4cf5dc23223980d9c0f924cb9e4e6943f13|How do they select answer candidates for their QA task?|AMS method.| | 
1604.05781|4c822bbb06141433d04bbc472f08c48bc8378865|How do they extract causality from text?|They identify documents that contain the unigrams 'caused', 'causing', or 'causes'| | 
1604.05781|1baf87437b70cc0375b8b7dc2cfc2830279bc8b5|"What is the source of the ""control"" corpus?"|Randomly selected from a Twitter dump, temporally matched to causal documents| | 
1604.05781|0b31eb5bb111770a3aaf8a3931d8613e578e07a8|"What are the selection criteria for ""causal statements""?"|Presence of only the exact unigrams 'caused', 'causing', or 'causes'| | 
1604.05781|7348e781b2c3755b33df33f4f0cab4b94fcbeb9b|Do they use expert annotations, crowdsourcing, or only automatic methods to analyze the corpora?|Only automatic methods| | 
1604.05781|f68bd65b5251f86e1ed89f0c858a8bb2a02b233a|how do they collect the comparable corpus?|Randomly from a Twitter dump| | 
1604.05781|e111925a82bad50f8e83da274988b9bea8b90005|How do they collect the control corpus?|Randomly from Twitter| | 
1607.06275|ba48c095c496d01c7717eaa271470c3406bf2d7c|What languages do they experiment with?|Chinese| | 
1607.06275|42a61773aa494f7b12838f71a949034c12084de1|What are the baselines?|MemN2N BIBREF12, Attentive and Impatient Readers BIBREF6| | 
1607.06275|48c3e61b2ed7b3f97706e2a522172bf9b51ec467|What was the inter-annotator agreement?|correctness of all the question answer pairs are verified by at least two annotators| | 
1603.04553|80de3baf97a55ea33e0fe0cafa6f6221ba347d0a|Are resolution mode variables hand crafted?|No| | 
1603.04553|f5707610dc8ae2a3dc23aec63d4afa4b40b7ec1e|What are resolution model variables?|Variables in the set {str, prec, attr} indicating in which mode the mention should be resolved.| | 
1603.04553|e76139c63da0f861c097466983fbe0c94d1d9810|Is the model presented in the paper state of the art?|No, supervised models perform better for this task.| | 
1709.10217|b8b588ca1e876b3094ae561a875dd949c8965b2e|What problems are found with the evaluation scheme?|no gold standard for automatically evaluating two (or more) dialogue systems when considering the satisfaction of the human and the fluency of the generated dialogue| | 
1709.10217|0460019eb2186aef835f7852fc445b037bd43bb7|How many intents were classified?|two| | 
1709.10217|a9cc4b17063711c8606b8fc1c5eaf057b317a0c9|What metrics are used in the evaluation?|For task 1, we use F1-score, Task completion ratio, User satisfaction degree, Response fluency, Number of dialogue turns, Guidance ability for out of scope input| | 
1901.02262|6ead576ee5813164684a8cdda36e6a8c180455d9|How do they measure the quality of summaries?|Rouge-L, Bleu-1| | 
1901.02262|0117aa1266a37b0d2ef429f1b0653b9dde3677fe|Does their model also take the expected answer style as input?|Yes| | 
1901.02262|5455b3cdcf426f4d5fc40bc11644a432fa7a5c8f|What do they mean by answer styles?|well-formed sentences vs concise answers| | 
1901.02262|6c80bc3ed6df228c8ca6e02c0a8a1c2889498688|"Is there exactly one ""answer style"" per dataset?"|Yes| | 
1901.02262|cb8a6f5c29715619a137e21b54b29e9dd48dad7d|"What is an ""answer style""?"|well-formed sentences vs concise answers| | 
1908.04917|8a7bd9579d2783bfa81e055a7a6ebc3935da9d20|What was the previous state of the art model for this task?|WAS, LipCH-Net-seq, CSSMCM-w/o video| | 
1908.04917|27b01883ed947b457d3bab0c66de26c0736e4f90|What syntactic structure is used to model tones?|syllables| | 
1908.04917|9714cb7203c18a0c53805f6c889f2e20b4cab5dd|What visual information characterizes tones?|video sequence is first fed into the VGG model BIBREF9 to extract visual feature| | 
1906.03338|fb2593de1f5cc632724e39d92e4dd82477f06ea1|How do they demonstrate the robustness of their results?|performances of a purely content-based model naturally stays stable| | 
1906.03338|476d0b5579deb9199423bb843e584e606d606bc7|What baseline and classification systems are used in experiments?|BIBREF13, majority baseline| | 
1906.03338|eddabb24bc6de6451bcdaa7940f708e925010912|How are the EAU text spans annotated?|Answer with content missing: (Data and pre-processing section) The data is suited for our experiments because the annotators were explicitly asked to provide annotations on a clausal level.| | 
1602.08741|e51d0c2c336f255e342b5f6c3cf2a13231789fed|Which Twitter corpus was used to train the word vectors?|They collected tweets in Russian language using a heuristic query specific to Russian| | 
1911.12579|5b6aec1b88c9832075cd343f59158078a91f3597|How does proposed word embeddings compare to Sindhi fastText word representations?|"Proposed SG model vs SINDHI FASTTEXT:
Average cosine similarity score: 0.650 vs 0.388
Average semantic relatedness similarity score between countries and their capitals: 0.663 vs 0.391"| | 
1911.12579|a6717e334c53ebbb87e5ef878a77ef46866e3aed|Are trained word embeddings used for any other NLP task?|No| | 
1911.12579|8cb9006bcbd2f390aadc6b70d54ee98c674e45cc|How is the data collected, which web resources were used?|daily Kawish and Awami Awaz Sindhi newspapers, Wikipedia dumps, short stories and sports news from Wichaar social blog, news from Focus Word press blog, historical writings, novels, stories, books from Sindh Salamat literary website, novels, history and religious books from Sindhi Adabi Board,  tweets regarding news and sports are collected from twitter| | 
1908.10275|75043c17a2cddfce6578c3c0e18d4b7cf2f18933|What trends are found in musical preferences?|audiences wanted products more and more contemporary, intense and a little bit novel or sophisticated, but less and less mellow and (surprisingly) unpretentious| | 
1908.10275|95bb3ea4ebc3f2174846e8d422abc076e1407d6a|Which decades did they look at?|between 1900s and 2010s| | 
1908.10275|3ebdc15480250f130cf8f5ab82b0595e4d870e2f|How many genres did they collect from?|77 genres| | 
2004.02929|bbc58b193c08ccb2a1e8235a36273785a3b375fb|Does the paper mention other works proposing methods to detect anglicisms in Spanish?|Yes| | 
2004.02929|3c34187a248d179856b766e9534075da1aa5d1cf|What is the performance of the CRF model on the task described?|the results obtained on development and test set (F1 = 89.60, F1 = 87.82) and the results on the supplemental test set (F1 = 71.49)| | 
2004.02929|8bfbf78ea7fae0c0b8a510c9a8a49225bbdb5383|Does the paper motivate the use of CRF as the baseline model?|the task of detecting anglicisms can be approached as a sequence labeling problem where only certain spans of texts will be labeled as anglicism (in a similar way to an NER task). The chosen model was conditional random field model (CRF), which was also the most popular model in both Shared Tasks on Language Identification for Code-Switched Data| | 
2004.02929|97757a69d9fc28b260e68284fd300726fbe358d0|What are the handcrafted features used?|Bias feature, Token feature, Uppercase feature (y/n), Titlecase feature (y/n), Character trigram feature, Quotation feature (y/n), Word suffix feature (last three characters), POS tag (provided by spaCy utilities), Word shape (provided by spaCy utilities), Word embedding (see Table TABREF26)| | 
1908.06809|45b28a6ce2b0f1a8b703a3529fd1501f465f3fdf|What are three new proposed architectures?|special dedicated discriminator is added to the model to control that the latent representation does not contain stylistic information, shifted autoencoder or SAE, combination of both approaches| | 
1908.06809|d6a27c41c81f12028529e97e255789ec2ba39eaa|How much does the standard metrics for style accuracy vary on different re-runs?|accuracy can change up to 5 percentage points, whereas BLEU can vary up to 8 points| | 
1707.00110|2d3bf170c1647c5a95abae50ee3ef3b404230ce4|Which baseline methods are used?|standard parametrized attention and a non-attention baseline| | 
1707.00110|ab9453fa2b927c97b60b06aeda944ac5c1bfef1e|Which datasets are used in experiments?|Sequence Copy Task and WMT'17| | 
1909.01013|3a8d65eb8e1dbb995981a0e02d86ebf3feab107a|What regularizers were used to encourage consistency in back translation cycles?|an adversarial loss ($\ell _{adv}$) for each model as in the baseline, a cycle consistency loss ($\ell _{cycle}$) on each side| | 
1909.01013|ebeedbb8eecdf118d543fdb5224ae610eef212c8|What are current state-of-the-art methods that consider the two tasks independently?|Procrustes, GPA, GeoMM, GeoMM$_{semi}$, Adv-C-Procrustes, Unsup-SL, Sinkhorn-BT| | 
1901.02534|559c1307610a15427caeb8aff4d2c01ae5c9de20|What baseline do they compare to?|For the entailment classifier we compare Decomposable Attention BIBREF2 , BIBREF3 as implemented in the official baseline, ESIM BIBREF4 , and a transformer network with pre-trained weights BIBREF5 .| | 
1901.02534|4ecb6674bcb4162bf71aea8d8b82759255875df3|Which pre-trained transformer do they use?|BIBREF5| | 
1901.02534|eacc1eb65daad055df934e0e878f417b73b2ecc1|What is the FEVER task?|tests a combination of retrieval and textual entailment capabilities. To verify a claim in the dataset as supported, refuted, or undecided, a system must retrieve relevant articles and sentences from Wikipedia. Then it must decide whether each of those sentences, or some combination of them, entails or refutes the claim, which is an entailment problem| | 
2004.04435|d353a6bbdc66be9298494d0c853e0d8d752dec4b|How is correctness of automatic derivation proved?|empirically compare automatic differentiation (AD, our implementation based on Clad) and numerical differentiation (ND, based on finite difference method)| | 
1910.10408|22c36082b00f677e054f0f0395ed685808965a02|Do they conduct any human evaluation?|Yes| | 
1910.10408|85a7dbf6c2e21bfb7a3a938381890ac0ec2a19e0|What dataset do they use for experiments?|English$\rightarrow $Italian/German portions of the MuST-C corpus, As additional data, we use a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De)| | 
1910.10408|90bc60320584ebba11af980ed92a309f0c1b5507|How do they enrich the positional embedding with length information|They introduce new trigonometric encoding which besides information about position uses additional length information (abs or relative).| | 
1910.10408|f52b2ca49d98a37a6949288ec5f281a3217e5ae8|How do they condition the output to a given target-source class?|They use three groups short/normal/long translation classes to learn length token, which is in inference used to bias network to generate desired length group.| | 
1910.10408|228425783a4830e576fb98696f76f4c7c0a1b906|Which languages do they focus on?|two translation directions (En-It and En-De)| | 
1910.10408|9d1135303212356f3420ed010dcbe58203cc7db4|What dataset do they use?|English$\rightarrow $Italian/German portions of the MuST-C corpus, As additional data, we use a mix of public and proprietary data for about 16 million sentence pairs for English-Italian (En-It) and $4.4$ million WMT14 sentence pairs for the English-German (En-De)| | 
1910.10408|d8bf4a29c7af213a9a176eb1503ec97d01cc8f51|Do they experiment with combining both methods?|Yes| | 
1606.05286|73abb173a3cc973ab229511cf53b426865a2738b|What state-of-the-art models are compared against?|a deep neural network (DNN) architecture proposed in BIBREF24 , maximum entropy (MaxEnt) proposed in BIBREF23 type of discriminative model| | 
2002.00876|1d9b953a324fe0cfbe8e59dcff7a44a2f93c568d|Does API provide ability to connect to models written in some other deep learning framework?|Yes| | 
2002.00876|093039f974805952636c19c12af3549aa422ec43|Is this library implemented into Torch or is framework agnostic?|It uses deep learning framework (pytorch)| | 
2002.00876|8df89988adff57279db10992846728ec4f500eaa|What baselines are used in experiments?|Typical implementations of dynamic programming algorithms are serial in the length of the sequence, Computational complexity is even more of an issue for parsing algorithms, which cannot be as easily parallelized, Unfortunately for other semirings, such as log and max, these operations are either slow or very memory inefficient| | 
2002.00876|94edac71eea1e78add678fb5ed2d08526b51016b|What general-purpose optimizations are included?|Parallel Scan Inference, Vectorized Parsing, Semiring Matrix Operations| | 
1906.10519|9c4ed8ca59ba6d240f031393b01f634a9dc3615d|what baseline do they compare to?|VecMap, Muse, Barista| | 
1905.13413|ca7e71131219252d1fab69865804b8f89a2c0a8f|How does this compare to traditional calibration methods like Platt Scaling?|No reliability diagrams are provided and no explicit comparison is made between confidence scores or methods.| | 
1905.13413|d77c9ede2727c28e0b5a240b2521fd49a19442e0|What's the input representation of OpenIE tuples into the model?|word embeddings| | 
1909.07863|a9610cbcca813f4376fbfbf21cc14689c7fbd677|What statistics on the VIST dataset are reported?|In the overall available data there are 40,071 training, 4,988 validation, and 5,050 usable testing stories.| | 
1806.04535|bcd6befa65cab3ffa6334c8ecedd065a4161028b|What are puns?|a form of wordplay jokes in which one sign (e.g. a word or a phrase) suggests two or more meanings by exploiting polysemy, homonymy, or phonological similarity to another sign, for an intended humorous or rhetorical effect| | 
1806.04535|479fc9e6d6d80e69f425d9e82e618e6b7cd12764|What are the categories of code-mixed puns?|intra-sequential and intra-word| | 
2003.05995|bc26eee4ef1c8eff2ab8114a319901695d044edb|How is dialogue guided to avoid interactions that breach procedures and processes only known to experts?|pairing crowdworkers and having half of them acting as Wizards by limiting their dialogue options only to relevant and plausible ones, at any one point in the interaction| | 
2003.05995|9c94ff8c99d3e51c256f2db78c34b2361f26b9c2|What is meant by semiguided dialogue, what part of dialogue is guided?|The Wizard can select one of several predefined messages to send, or type their own message if needed. Free text messages do not change the dialogue state in the FSM, so it is important to minimise their use by providing enough dialogue options to the Wizard.| | 
2003.05995|8e9de181fa7d96df9686d0eb2a5c43841e6400fa|Is CRWIZ already used for data collection, what are the results?|Yes, CRWIZ has been used for data collection and its initial use resulted in 145 dialogues. The average time taken for the task was close to the estimate of 10 minutes, 14 dialogues (9.66%) resolved the emergency in the scenario, and these dialogues rated consistently higher in subjective and objective ratings than those which did not resolve the emergency. Qualitative results showed that participants believed that they were interacting with an automated assistant.| | 
2003.05995|ff1595a388769c6429423a75b6e1734ef88d3e46|How does framework made sure that dialogue will not breach procedures?|The Wizard can select one of several predefined messages to send, or type their own message if needed. Free text messages do not change the dialogue state in the FSM, so it is important to minimise their use by providing enough dialogue options to the Wizard. Predefined messages can also trigger other associated events such as pop-ups or follow-up non-verbal actions.| | 
1710.07395|dd2046f5481f11b7639a230e8ca92904da75feed|How do they combine the models?|maximum of two scores assigned by the two separate models, average score| | 
1710.07395|47e6c3e6fcc9be8ca2437f41a4fef58ef4c02579|What is their baseline?|Logistic regression model with character-level n-gram features| | 
1710.07395|569ad21441e99ae782d325d5f5e1ac19e08d5e76|What context do they use?|title of the news article, screen name of the user| | 
1710.07395|90741b227b25c42e0b81a08c279b94598a25119d|What is their definition of hate speech?|language which explicitly or implicitly threatens or demeans a person or a group based upon a facet of their identity such as gender, ethnicity, or sexual orientation| | 
1710.07395|1d739bb8e5d887fdfd1f4b6e39c57695c042fa25|What architecture has the neural network?|three parallel LSTM BIBREF21 layers| | 
1904.02357|5c70fdd3d6b67031768d3e28336942e49bf9a500|How is human interaction consumed by the model?|displays three different versions of a story written by three distinct models for a human to compare, human can select the model to interact with (potentially after having chosen it via cross-model), and can collaborate at all stages| | 
1904.02357|f27502c3ece9ade265389d5ace90ca9ca42b46f3|How do they evaluate generated stories?|separate set of Turkers to rate the stories for overall quality and the three improvement areas| | 
1904.02357|aa4b38f601cc87bf93849245d5f65124da3dc112|What are the baselines?|Title-to-Story system| | 
1907.02636|08b87a90139968095433f27fc88f571d939cd433|What is used a baseline?|As the baseline, we simply judge the input token as IOCs on the basis of the spelling features described in BIBREF12| | 
1907.02636|ef872807cb0c9974d18bbb886a7836e793727c3d|What contextual features are used?|The words that can indicate the characteristics of the neighbor words as contextual keywords and generate it from the automatically extracted contextual keywords.| | 
1907.02636|4db3c2ca6ddc87209c31b20763b7a3c1c33387bc|Where are the cybersecurity articles used in the model sourced from?| from a collection of advanced persistent threats (APT) reports which are published from 2008 to 2018| | 
1605.08675|7b44bee49b7cb39cb7d5eec79af5773178c27d4d|How is the data in RAFAEL labelled?|Using a set of annotation tools such as Morfeusz, PANTERA, Spejd, NERF and Liner| | 
1605.08675|6d54bad91b6ccd1108d1ddbff1d217c6806e0842|How do they handle polysemous words in their entity library?|only the first word sense (usually the most common) is taken into account| | 
1709.08858|238ec3c1e1093ce2f5122ee60209b969f7669fae|How is the fluctuation in the sense of the word and its neighbors measured?|"Our method performs a statistical test to determine whether a given word is used polysemously in the text, according to the following steps:
1) Setting N, the size of the neighbor.
2) Choosing N neighboring words ai in the order whose angle with the vector of the given word w is the smallest.
3) Computing the surrounding uniformity for ai(0 < i ≤ N) and w.
4) Computing the mean m and the sample variance σ for the uniformities of ai .
5) Checking whether the uniformity of w is less than m − 3σ. If the value is less than m − 3σ, we may regard w as a polysemic word."| | 
1908.11425|da544015511e535503dee2eaf4912a5e36c806cd|What is the architecture of the model?|BIBREF5 to train neural sequence-to-sequence, NMF topic model with scikit-learn BIBREF14| | 
1908.11425|7bc993b32484d6ae3c86d0b351a68e59fd2757a5|What language do they look at?|Spanish| | 
1711.04457|da495e2f99ee2d5db9cc17eca5517ddaa5ea8e42|Where does the vocabulary come from?|LDC corpus| | 
1711.04457|310e61b9dd4d75bc1bebbcb1dae578f55807cd04|What dataset did they use?|LDC corpus, NIST 2003(MT03), NIST 2004(MT04), NIST 2005(MT05), NIST 2006(MT06), NIST 2008(MT08)| | 
1907.08501|bdc6664cec2b94b0b3769bc70a60914795f39574|How do they measure performance?|average INLINEFORM0 , INLINEFORM1 , and INLINEFORM2 values| | 
1907.08501|9653c89a93ac5c717a0a26cf80e9aa98a5ccf910|Which four QA systems do they use?|WDAqua BIBREF0 , QAKiS BIBREF7 , gAnswer BIBREF6 and Platypus BIBREF8| | 
1907.08501|412aff0b2113b7d61c914edf90b90f2994390088|Do they test performance of their approaches using human judgements?|Yes| | 
2001.02943|010e3793eb1342225857d3f95e147d8f8467192a|What are the sizes of both datasets?|The Dutch section consists of 2,333,816 sentences and 53,487,257 words., The SONAR500 corpus consists of more than 500 million words obtained from different domains.| | 
1910.00825|d5f8707ddc21741d52b3c2a9ab1af2871dc6c90b|What automatic and human evaluation metrics are used to compare SPNet to its counterparts?|ROUGE and CIC, relevance, conciseness and readability on a 1 to 5 scale, and rank the summary pair| | 
1910.00825|73633afbefa191b36cca594977204c6511f9dad4|Is it expected to have speaker role, semantic slot and dialog domain annotations in real world datasets?|Not at the moment, but summaries can be additionaly extended with this annotations.| | 
1910.00825|db39a71080e323ba2ddf958f93778e2b875dcd24|How does SPNet utilize additional speaker role, semantic slot and dialog domain annotations?|Our encoder-decoder framework employs separate encoding for different speakers in the dialog., We integrate semantic slot scaffold by performing delexicalization on original dialogs., We integrate dialog domain scaffold through a multi-task framework.| | 
1910.00825|6da2cb3187d3f28b75ac0a61f6562a8adf716109|What are previous state-of-the-art document summarization methods used?|Pointer-Generator, Transformer| | 
1910.00825|c47e87efab11f661993a14cf2d7506be641375e4|How does new evaluation metric considers critical informative entities?|Answer with content missing: (formula for CIC) it accounts for the most important information within each dialog domain. CIC can be applied to any summarization task with predefined essential entities| | 
1910.00825|14684ad200915ff1e3fc2a89cb614e472a1a2854|Is new evaluation metric extension of ROGUE?|No| | 
1911.03705|8d1f9d3aa2cc2e2e58d3da0f5edfc3047978f3ee|What measures were used for human evaluation?|To have an estimation about human performance in each metric, we iteratively treat every reference sentence in dev/test data as the prediction to be compared with all references (including itself).| | 
1911.03705|5065ff56d3c295b8165cb20d8bcfcf3babe9b1b8|What automatic metrics are used for this task?|BLEU-3/4, ROUGE-2/L, CIDEr, SPICE, BERTScore| | 
1911.03705|c34a15f1d113083da431e4157aceb11266e9a1b2|Are the models required to also generate rationales?|No| | 
1911.03705|061682beb3dbd7c76cfa26f7ae650e548503d977|Are the rationales generated after the sentences were written?|Yes| | 
1911.03705|3518d8eb84f6228407cfabaf509fd63d60351203|Are the sentences in the dataset written by humans who were shown the concept-sets?|Yes| | 
1911.03705|617c77a600be5529b3391ab0c21504cd288cc7c7|Where do the concept sets come from?|These concept-sets are sampled from several large corpora of image/video captions| | 
1910.00458|53d6cbee3606dd106494e2e98aa93fdd95920375|How big are improvements of MMM over state of the art?|test accuracy of 88.9%, which exceeds the previous best by 16.9%| | 
1910.00458|9dc844f82f520daf986e83466de0c84d93953754|What out of domain datasets authors used for coarse-tuning stage?|MultiNLI BIBREF15 and SNLI BIBREF16 | | 
1910.00458|36d892460eb863220cd0881d5823d73bbfda172c|What four representative datasets are used for bechmark?|DREAM, MCTest, TOEFL, and SemEval-2018 Task 11| | 
2001.11268|4cbc56d0d53c4c03e459ac43e3c374b75fd48efe|What baselines did they consider?|LSTM, SCIBERT| | 
1706.07179|082c88e132b4f1bf68abdc3a21ac4af180de1113|How is knowledge retrieved in the memory?|the memory slots correspond to entities and the edges correspond to relationships between entities, each represented as a vector.| | 
1706.07179|74091e10f596428135b0ab06008608e09c051565|How is knowledge stored in the memory?|entity memory and relational memory.| | 
1706.07179|43b4f7eade7a9bcfaf9cc0edba921a41d6036e9c|What are the relative improvements observed over existing methods?|The RelNet model achieves a mean error of 0.285% across tasks which is better than the results of the EntNet model BIBREF17 . The RelNet model is able to achieve 0% test error on 11 of the tasks, whereas the EntNet model achieves 0% error on 7 of the tasks.| | 
1706.07179|a75861e6dd72d69fdf77ebd81c78d26c6f7d0864|What is the architecture of the neural network?|extends memory-augmented neural networks with a relational memory to reason about relationships between multiple entities present within the text. , The model is sequential in nature, consisting of the following steps: read text, process it into a dynamic relational memory and then attention conditioned on the question generates the answer. We model the dynamic memory in a fashion similar to Recurrent Entity Networks BIBREF17 and then equip it with an additional relational memory.| | 
1706.07179|60fd7ef7986a5752b31d3bd12bbc7da6843547a4|What methods is RelNet compared to?|We compare the performance with the Recurrent Entity Networks model (EntNet) BIBREF17| | 
1909.08824|7d59374d9301a0c09ea5d023a22ceb6ce07fb490|How do they measure the diversity of inferences?|by number of distinct n-grams| | 
1909.08824|fb76e994e2e3fa129f1e94f1b043b274af8fb84c|How does the context-aware variational autoencoder learn event background information?| CWVAE is trained on an auxiliary dataset to learn the event background information by using the context-aware latent variable.  Then, in finetute stage, CWVAE is trained on the task-specific dataset to adapt the event background information to each specific aspect of If-Then inferential target.| | 
1708.08615|95d8368b1055d97250df38d1e8c4a2b283d2b57e|what standard speech transcription pipeline was used?|pipeline that is used at Microsoft for production data| | 
1701.03214|46ee1cbbfbf0067747b28bdf4c8c2f7dc8955650|What kinds of neural networks did they use in this paper?|LSTMs| | 
1701.03214|4f12b41bd3bb2610abf7d7835291496aa69fb78c|How did they use the domain tags?|"Appending the domain tag “<2domain>"" to the source sentences of the respective corpora"| | 
1709.05411|65e6a1cc2590b139729e7e44dce6d9af5dd2c3b5|Why mixed initiative multi-turn dialogs are the greatest challenge in building open-domain conversational agents?|do not follow a particular plan or pursue a particular fixed information need,  integrating content found via search with content from structured data, at each system turn, there are a large number of conversational moves that are possible, most other domains do not have such high quality structured data available, live search may not be able to achieve the required speed and efficiency| | 
1805.12032|b54fc86dc2cc6994e10c1819b6405de08c496c7b|How is speed measured?|time elapsed between the moment the link or content was posted/tweeted and the moment that the reaction comment or tweet occurred| | 
1805.12032|b43a8a0f4b8496b23c89730f0070172cd5dca06a|What is the architecture of their model?|we combine a text sequence sub-network with a vector representation sub-network as shown in Figure FIGREF5 . The text sequence sub-network consists of an embedding layer initialized with 200-dimensional GloVe embeddings BIBREF15 followed by two 1-dimensional convolution layers, then a max-pooling layer followed by a dense layer. The vector representation sub-network consists of two dense layers. We incorporate information from both sub-networks through concatenated padded text sequences and vector representations of normalized Linguistic Inquiry and Word Count (LIWC) features BIBREF16 for the text of each post and its parent.| | 
1805.12032|b161febf86cdd58bd247a934120410068b24b7d1|What are the nine types?|agreement, answer, appreciation, disagreement, elaboration, humor, negative reaction, question, other| | 
1611.02550|d40662236eed26f17dd2a3a9052a4cee1482d7d6|How do they represent input features of their model to train embeddings?|a vector of frame-level acoustic features| | 
1611.02550|1d791713d1aa77358f11501f05c108045f53c8aa|Which dimensionality do they use for their embeddings?|1061| | 
1611.02550|6b6360fab2edc836901195c0aba973eae4891975|Which dataset do they use?|Switchboard conversational English corpus| | 
2003.05522|86a93a2d1c19cd0cd21ad1608f2a336240725700|How does Frege's holistic and functional approach to meaning relates to general distributional hypothesis?|interpretation of Frege's work are examples of holistic approaches to meaning| | 
2003.05522|6090d3187c41829613abe785f0f3665d9ecd90d9|What does Frege's holistic and functional approach to meaning states?|Only in the context of a sentence does a word have a meaning.| | 
1601.06068|117aa7811ed60e84d40cd8f9cb3ca78781935a98|Do they evaluate the quality of the paraphrasing model?|No| | 
1601.06068|c359ab8ebef6f60c5a38f5244e8c18d85e92761d|How many paraphrases are generated per question?|10*n paraphrases, where n depends on the number of paraphrases that contain the entity mention spans| | 
1601.06068|ad362365656b0b218ba324ae60701eb25fe664c1|What latent variables are modeled in the PCFG?|syntactic information, semantic and topical information| | 
1601.06068|423bb905e404e88a168e7e807950e24ca166306c|What are the baselines?|GraphParser without paraphrases, monolingual machine translation based model for paraphrase generation| | 
1709.07916|e5ae8ac51946db7475bb20b96e0a22083b366a6d|Do they evaluate only on English data?|Yes| | 
1709.07916|b5e883b15e63029eb07d6ff42df703a64613a18a|How were topics of interest about DDEO identified?|using topic modeling model Latent Dirichlet Allocation (LDA)| | 
1909.00154|c45a160d31ca8eddbfea79907ec8e59f543aab86|What datasets are used for evaluation?|Swissmetro dataset| | 
1909.00154|7358a1ce2eae380af423d4feeaa67d2bd23ae9dd|How do their train their embeddings?|The embeddings are learned several times using the training set, then the average is taken.| | 
1909.00154|1165fb0b400ec1c521c1aef7a4e590f76fee1279|How do they model travel behavior?|The data from collected travel surveys is used to model travel behavior.| | 
1909.00154|f2c5da398e601e53f9f545947f61de5f40ede1ee|How do their interpret the coefficients?|The coefficients are projected back to the dummy variable space.| | 
1908.05434|43761478c26ad65bec4f0fd511ec3181a100681c|Do they use pretrained word embeddings?|Yes| | 
1908.05434|01866fe392d9196dda1d0b472290edbd48a99f66|How is the lexicon of trafficking flags expanded?|re-train the skip-gram model and update the emoji map periodically on new escort ads, when traffickers switch to new emojis, the map can link the new emojis to the old ones| | 
1612.05310|394cf73c0aac8ccb45ce1b133f4e765e8e175403|Do they experiment with the dataset?|Yes| | 
1612.05310|2c4003f25e8d95a3768204f52a7a5f5e17cb2102|Do they use a crowdsourcing platform for annotation?|No| | 
1612.05310|65e32f73357bb26a29a58596e1ac314f7e9c6c91|What is an example of a difficult-to-classify case?|The lack of background, Non-cursing aggressions and insults, the presence of controversial topic words ,  shallow meaning representation, directly ask the suspected troll if he/she is trolling or not, a blurry line between “Frustrate” and “Neutralize”, distinction between the classes “Troll” and “Engage”| | 
1612.05310|46f175e1322d648ab2c0258a9609fe6f43d3b44e|What potential solutions are suggested?| inclusion of longer parts of the conversation| | 
1612.05310|7cc22fd8c9d0e1ce5e86d0cbe90bf3a177f22a68|What is the size of the dataset?|1000 conversations composed of 6833 sentences and 88047 tokens| | 
1912.09713|d2b3f2178a177183b1aeb88784e48ff7e3e5070c|How strong is negative correlation between compound divergence and accuracy in performed experiment?| between 0.81 and 0.88| | 
1912.09713|d5ff8fc4d3996db2c96cb8af5a6d215484991e62|What are results of comparison between novel method to other approaches for creating compositional generalization benchmarks?|The MCD splits achieve a significantly higher compound divergence at a similar atom divergence when compared to the other experiments| | 
1912.09713|d9c6493e1c3d8d429d4ca608f5acf29e4e7c4c9b|How authors justify that question answering dataset presented is realistic?|CFQ contains the most query patterns by an order of magnitude and also contains significantly more queries and questions than the other datasets| | 
1912.09713|0427ca83d6bf4ec113bc6fec484b2578714ae8ec|What three machine architectures are analyzed?|LSTM+attention, Transformer , Universal Transformer| | 
1912.09713|f1c70baee0fd02b8ecb0af4b2daa5a56f3e9ccc3|How big is new question answering dataset?|239,357 English question-answer pairs| | 
1912.09713|8db45a8217f6be30c31f9b9a3146bf267de68389|What are other approaches into creating compositional generalization benchmarks?|random , Output length, Input length, Output pattern, Input pattern| | 
1901.03860|4e379d6d5f87554fabf6f7f7b6ed92d2025e7280|What problem do they apply transfer learning to?|CSKS task| | 
1901.03860|518d0847b02b4f23a8f441faa38b935c9b892e1e|What are the baselines?|Honk, DeepSpeech-finetune| | 
1901.03860|8112d18681e266426cf7432ac4928b87f5ce8311|What languages are considered?|English, Hindi| | 
1909.02480|65e72ad72a9cbfc379f126b10b0ce80cfe44579b|What non autoregressive NMT models are used for comparison?|NAT w/ Fertility, NAT-IR, NAT-REG, LV NAR, CTC Loss, CMLM| | 
1909.02480|cf8edc6e8c4d578e2bd9965579f0ee81f4bf35a9|What are three neural machine translation (NMT) benchmark datasets used for evaluation?|WMT2014, WMT2016 and IWSLT-2014| | 
1910.02754|04aff4add28e6343634d342db92b3ac36aa8c255|What is result of their attention distribution analysis?|visual attention is very sparse,  visual component of the attention hasn't learnt any variation over the source encodings| | 
1910.02754|a8e4522ce2ce7336e731286654d6ad0931927a4e|What is result of their Principal Component Analysis?|existing visual features aren't sufficient enough to expect benefits from the visual modality in NMT| | 
1910.02754|f6202100cfb83286dc51f57c68cffdbf5cf50a3f|What are 3 novel fusion techniques that are proposed?|Step-Wise Decoder Fusion, Multimodal Attention Modulation, Visual-Semantic (VS) Regularizer| | 
2004.02393|bd7039f81a5417474efa36f703ebddcf51835254|What are two models' architectures in proposed solution?|Reasoner model, also implemented with the MatchLSTM architecture, Ranker model| | 
2004.02393|022e5c996a72aeab890401a7fdb925ecd0570529|How do two models cooperate to select the most confident chains?|Reasoner learns to extract the linking entity from chains selected by a well-trained Ranker, and it benefits the Ranker training by providing extra rewards| | 
2004.02393|34af2c512ec38483754e94e1ea814aa76552d60a|What benchmarks are created?|Answer with content missing: (formula) The accuracy is defined as the ratio # of correct chains predicted to # of evaluation samples| | 
2004.01694|c1429f7fed5a4dda11ac7d9643f97af87a83508b|What empricial investigations do they reference?|empirically test to what extent changes in the evaluation design affect the outcome of the human evaluation| | 
2004.01694|a93d4aa89ac3abbd08d725f3765c4f1bed35c889|What languages do they investigate for machine translation?|English , Chinese | | 
2004.01694|bc473c5bd0e1a8be9b2037aa7006fd68217c3f47|What recommendations do they offer?| Choose professional translators as raters,  Evaluate documents, not sentences, Evaluate fluency in addition to adequacy, Do not heavily edit reference translations for fluency, Use original source texts| | 
2004.01694|9299fe72f19c1974564ea60278e03a423eb335dc|What was the weakness in Hassan et al's evaluation design?|"MT developers to which crowd workers were compared are usually not professional translators, evaluation of sentences in isolation prevents raters from detecting translation errors, used not originally written Chinese test set
"| | 
2003.00576|2ed02be0c183fca7031ccb8be3fd7bc109f3694b|By how much they improve over the previous state-of-the-art?|1.08 points in ROUGE-L over our base pointer-generator model , 0.6 points in ROUGE-1| | 
2003.00576|be73a88d5b695200e2ead4c2c24e2a977692970e|Is there any evidence that encoders with latent structures work well on other tasks?|Yes| | 
1909.02635|c515269b37cc186f6f82ab9ada5d9ca176335ded|What evidence do they present that the model attends to shallow context clues?|Using model gradients with respect to input features they presented that the most important model inputs are verbs associated with entities which shows that the model attends to shallow context clues| | 
1909.02635|43f86cd8aafe930ebb35ca919ada33b74b36c7dd|In what way is the input restructured?|In four entity-centric ways - entity-first, entity-last, document-level and sentence-level| | 
1904.00648|aa60b0a6c1601e09209626fd8c8bdc463624b0b3|What are their results on the entity recognition task?|With both test sets performances decrease, varying between 94-97%| | 
1904.00648|3837ae1e91a4feb27f11ac3b14963e9a12f0c05e|What task-specific features are used?|"6)Contributor first names, 7)Contributor last names, 8)Contributor types (""soprano"", ""violinist"", etc.), 9)Classical work types (""symphony"", ""overture"", etc.), 10)Musical instruments, 11)Opus forms (""op"", ""opus""), 12)Work number forms (""no"", ""number""), 13)Work keys (""C"", ""D"", ""E"", ""F"" , ""G"" , ""A"", ""B"", ""flat"", ""sharp""), 14)Work Modes (""major"", ""minor"", ""m"")"| | 
1904.00648|ef4d6c9416e45301ea1a4d550b7c381f377cacd9|What kind of corpus-based features are taken into account?|standard linguistic features, such as Part-Of-Speech (POS) and chunk tag, series of features representing tokens' left and right context| | 
1904.00648|689d1d0c4653a8fa87fd0e01fa7e12f75405cd38|Which machine learning algorithms did the explore?|biLSTM-networks| | 
1904.00648|7920f228de6ef4c685f478bac4c7776443f19f39|What language is the Twitter content in?|English| | 
1709.00387|41844d1d1ee6d6d38f31b3a17a2398f87566ed92|What is the architecture of the siamese neural network?|two parallel convolutional networks, INLINEFORM0 , that share the same set of weights| | 
1709.00387|e87f47a293e0b49ab8b15fc6633d9ca6dc9de071|Which are the four Arabic dialects?|Egyptian (EGY), Levantine (LEV), Gulf (GLF), North African (NOR)| | 
1806.11322|7426a6e800d6c11795941616fc4a243e75716a10|What factors contribute to interpretive biases according to this research?|Which events authors choose to include in their history, which they leave out, and the way the events chosen relate to the march| | 
1806.11322|da4535b75e360604e3ce4bb3631b0ba96f4dadd3|Which interpretative biases are analyzed in this paper?|in an ME game there are typically several interpretive biases at work: each player has her own bias, as does the Jury| | 
2004.00139|4d30c2223939b31216f2e90ef33fe0db97e962ac|How many words are coded in the dictionary?|11'248| | 
2004.00139|7b47aa6ba247874eaa8ab74d7cb6205251c01eb5|Is the model evaluated on the graphemes-to-phonemes task?|Yes| | 
1811.08048|ce14b87dacfd5206d2a5af7c0ed1cfeb7b181922|How does the QuaSP+Zero model work?|does not just consider the question tokens, but also the relationship between those tokens and the properties| | 
1811.08048|709a4993927187514701fe3cc491ac3030da1215|Which off-the-shelf tools do they use on QuaRel?|information retrieval system, word-association method,  CCG-style rule-based semantic parser written specifically for friction questions, state-of-the-art neural semantic parser| | 
1811.08048|a3c6acf900126bc9bd9c50ce99041ea00761da6a|How do they obtain the logical forms of their questions in their dataset?| workers were given a seed qualitative relation, asked to enter two objects, people, or situations to compare, created a question, guided by a large number of examples, LFs were elicited using a novel technique of reverse-engineering them from a set of follow-up questions| | 
1811.08048|31b631a8634f6180b20a72477040046d1e085494|Do all questions in the dataset allow the answers to pick from 2 options?|Yes| | 
1904.10500|ab78f066144936444ecd164dc695bec1cb356762|What is shared in the joint model?|jointly trained with slots| | 
1704.00177|4e4d377b140c149338446ba69737ea191c4328d9|What dataset is used?|ACL Anthology Reference Corpus| | 
1704.00177|828ce5faed7783297cf9ce202364f999b8d4a1f6|What metrics are considered?|F-score, micro-F, macro-F, weighted-F | | 
1711.11221|9d016eb3913b41f7a18c6fa865897c12b5fe0212|Did the authors evaluate their system output for coherence?|Yes| | 
1912.07025|79bb1a1b71a1149e33e8b51ffdb83124c18f3e9c|What accuracy does CNN model achieve?|Combined per-pixel accuracy for character line segments is 74.79| | 
1912.07025|26faad6f42b6d628f341c8d4ce5a08a591eea8c2|How many documents are in the Indiscapes dataset?|508| | 
1709.01256|3bfb8c12f151dada259fbd511358914c4b4e1b0e|What metrics are used to evaluation revision detection?|precision, recall, F-measure| | 
1709.01256|3f85cc5be84479ba668db6d9f614fedbff6d77f1|How large is the Wikipedia revision dump dataset?|eight GB| | 
1709.01256|126e8112e26ebf8c19ca7ff3dd06691732118e90|What are simulated datasets collected?|There are 6 simulated datasets collected which is initialised with a corpus of size 550 and simulated by generating new documents from Wikipedia extracts and replacing existing documents| | 
1709.01256|be08ef81c3cfaaaf35c7414397a1871611f1a7fd|Which are the state-of-the-art models?|WMD, VSM, PV-DTW, PV-TED| | 
1909.03526|18412237f7faafc6befe975d5bcd348e2b499b55|Where did this model place in the final evaluation of the shared task?|$4th$| | 
1909.03526|02945c85d6cc4cdd1757b2f2bfa5e92ee4ed14a0|What in-domain data is used to continue pre-training?|dialectal tweet data| | 
1909.03526|6e51af9088c390829703c6fa966e98c3a53114c1|What dialect is used in the Google BERT model and what is used in the task data?|Modern Standard Arabic (MSA), MSA as well as dialects at various degrees of granularity such as Egyptian, Gulf, and Levantine| | 
1909.03526|07ee4e0277ad1083270131d32a71c3fe062a916d|What are the tasks used in the mulit-task learning setup?|Author profiling and deception detection in Arabic, LAMA+DINA Emotion detection, Sentiment analysis in Arabic tweets| | 
1902.11049|bfce2afe7a4b71f9127d4f9ef479a0bfb16eaf76|What human evaluation metrics were used in the paper?|rating questions on a scale of 1-5 based on fluency of language used and relevance of the question to the context| | 
1905.06906|dfbab3cd991f86d998223726617d61113caa6193|For the purposes of this paper, how is something determined to be domain specific knowledge?|reviews under distinct product categories are considered specific domain knowledge| | 
1905.06906|df510c85c277afc67799abcb503caa248c448ad2|Does the fact that GCNs can perform well on this tell us that the task is simpler than previously thought?|No| | 
1905.06906|d95180d72d329a27ddf2fd5cc6919f469632a895|Are there conceptual benefits to using GCNs over more complex architectures like attention?|Yes| | 
1809.09795|e196e2ce72eb8b2d50732c26e9bf346df6643f69|Do they evaluate only on English?|Yes| | 
1809.09795|982d375378238d0adbc9a4c987d633ed16b7f98f|What are the three different sources of data?|Twitter, Reddit, Online Dialogues| | 
1809.09795|bbdb2942dc6de3d384e3a1b705af996a5341031b|What type of model are the ELMo representations used in?|A bi-LSTM with max-pooling on top of it| | 
1809.09795|4ec538e114356f72ef82f001549accefaf85e99c|Which morphosyntactic features are thought to indicate irony or sarcasm?|all caps, quotation marks, emoticons, emojis, hashtags| | 
1812.03593|40a45d59a2ef7a67c8ab0f2b2d5b43fc85b85498|Is the model evaluated on other datasets?|No| | 
1812.03593|b29b5c39575454da9566b3dd27707fced8c6f4a1|Does the model incorporate coreference and entailment?|As the question has integrated previous utterances, the model needs to directly relate previously mentioned concept with the current question. This is helpful for concept carry-over and coreference resolution.| | 
1812.03593|4040f5c9f365f9bc80b56dce944ada85bb8b4ab4|Is the incorporation of context separately evaluated?|No| | 
2003.01769|7dce1b64c0040500951c864fce93d1ad7a1809bc|Which frozen acoustic model do they use?|a masking speech enhancement model BIBREF11, BIBREF12, BIBREF13| | 
1910.04006|186ccc18c6361904bee0d58196e341a719fb31c2|What features are used?|Sociodemographics: gender, age, marital status, etc., Past medical history: number of previous admissions, history of suicidality, average length of stay (up until that admission), etc., Information from the current admission: length of stay (LOS), suicidal risk, number and length of notes, time of discharge, evaluation scores, etc.| | 
1910.04006|fd5412e2784acefb50afc3bfae1e087580b90ab9|Do they compare to previous models?|Yes| | 
1910.04006|c7f087c78768d5c6f3ff26921858186d627fd4fd|How do they incorporate sentiment analysis?|features per admission were extracted as inputs to the readmission risk classifier| | 
1910.04006|82596190560dc2e2ced2131779730f40a3f3eb8c|What is the dataset used?|EHRs of 183 psychosis patients from McLean Psychiatric Hospital in Belmont, MA| | 
1910.04006|345f65eaff1610deecb02ff785198aa531648e75|How do they extract topics?| automatically obtained through the topic extraction and sentiment analysis pipeline introduced in our prior work BIBREF15| | 
1911.01188|96c20af8bbef435d0d534d10c42ae15ff2f926f8|What translationese effects are seen in the analysis?|potentially indicating a shining through effect, explicitation effect| | 
1911.01188|9544cc0244db480217ce9174aa13f1bf09ba0d94|What languages are seen in the news and TED datasets?|English, German| | 
1911.01188|3758669426e8fb55a4102564cf05f2864275041b|How are the (possibly incorrect) coreference chains in the MT outputs annotated?|allows the annotator to define each markable as a certain mention type (pronoun, NP, VP or clause), The mentions referring to the same discourse item are linked between each other., chain members are annotated for their correctness| | 
1911.01188|1ebd6f703458eb6690421398c79abf3fc114148f|Which three neural machine translation systems are analyzed?|first two systems are transformer models trained on different amounts of data, The third system includes a modification to consider the information of full coreference chains| | 
1911.01188|15a1df59ed20aa415a4daf0acb256747f6766f77|Which coreference phenomena are analyzed?|shining through, explicitation| | 
1909.08191|b124137e62178a2bd3b5570d73b1652dfefa2457|What new interesting tasks can be solved based on the uncanny semantic structures of the embedding space?| analogy query, analogy browsing| | 
1909.08191|c6aa8a02597fea802890945f0b4be8d631e4d5cd|What are the uncanny semantic structures of the embedding space?|Semantic similarity structure, Semantic direction structure| | 
1909.08191|bfad30f51ce3deea8a178944fa4c6e8acdd83a48|What is the general framework for data exploration by semantic queries?|three main components, namely data processing, task processing, and query processing| | 
1909.08191|dd9883f4adf7be072d314d7ed13fe4518c5500e0|What data exploration is supported by the analysis of these semantic structures?|Task processing: converting data exploration tasks to algebraic operations on the embedding space, Query processing: executing semantic query on the embedding space and return results| | 
1910.06701|81669c550d32d756f516dab5d2b76ff5f21c0f36|what are the existing models they compared with?|Syn Dep, OpenIE, SRL, BiDAF, QANet, BERT, NAQANet, NAQANet+| | 
1904.11942|4266aacb575b4be7dbcdb8616766324f8790763c|What conclusions do the authors draw from their detailed analyses?|neural network-based models can outperform feature-based models with wide margins, contextualized representation learning can boost performance of NN models| | 
1904.11942|191107cd112f7ee6d19c1dc43177e6899452a2c7|Do the BERT-based embeddings improve results?|Yes| | 
1904.11942|b0dca7b74934f51ff3da0c074ad659c25d84174d|What were the traditional linguistic feature-based models?|CAEVO| | 
1904.11942|601e58a3d2c03a0b4cd627c81c6228a714e43903|What type of baseline are established for the two datasets?|CAEVO| | 
1705.02394|a0fbf90ceb520626b80ff0f9160b3cd5029585a5|What model achieves state of the art performance on this task?|BIBREF16| | 
1705.02394|e8ca81d5b36952259ef3e0dbeac7b3a622eabe8e|Which multitask annotated corpus is used?|IEMOCAP| | 
1705.02394|e75685ef5f58027be44f42f30cb3988b509b2768|What are the tasks in the multitask learning setup?|set of related tasks are learned (e.g., emotional activation), primary task (e.g., emotional valence)| | 
1806.09103|859e0bed084f47796417656d7a68849eb9cb324f|how are rare words defined?|low-frequency words| | 
1806.09103|04e90c93d046cd89acef5a7c58952f54de689103|which public datasets were used?|CMRC-2017, People's Daily (PD), Children Fairy Tales (CFT) , Children's Book Test (CBT)| | 
1911.13087|eb5ed1dd26fd9adb587d29225c7951a476c6ec28|What are the results of the experiment?|They were able to create a language model from the dataset, but did not test.| | 
1911.13087|0828cfcf0e9e02834cc5f279a98e277d9138ffd9|How was the dataset collected?|extracted text from Sorani Kurdish books of primary school and randomly created sentences| | 
1911.13087|7b2de0109b68f78afa9e6190c82ca9ffaf62f9bd|What is the size of the dataset?|2000 sentences| | 
1911.13087|3f3c09c1fd542c1d9acf197957c66b79ea1baf6e|How many annotators participated?|1| | 
1911.13087|0a82534ec6e294ab952103f11f56fd99137adc1f|How long is the dataset?|2000| | 
1608.04917|938688871913862c9f8a28b42165237b7324e0de|Do the authors mention any possible confounds in their study?|Yes| | 
1608.04917|4170ed011b02663f5b1b1a3c1f0415b7abfaa85d|What is the relationship between the co-voting and retweeting patterns?|we observe a positive correlation between retweeting and co-voting, strongest positive correlations are in the areas Area of freedom, security and justice, External relations of the Union, and Internal markets, Weaker, but still positive, correlations are observed in the areas Economic, social and territorial cohesion, European citizenship, and State and evolution of the Union, significantly negative coefficient, is the area Economic and monetary system| | 
1608.04917|fd08dc218effecbe5137a7e3b73d9e5e37ace9c1|Does the analysis find that coalitions are formed in the same way for different policy areas?|No| | 
1608.04917|a85c2510f25c7152940b5ac4333a80e0f91ade6e|What insights does the analysis give about the cohesion of political groups in the European parliament?|Greens-EFA, S&D, and EPP exhibit the highest cohesion, non-aligned members NI have the lowest cohesion, followed by EFDD and ENL, two methods disagree is the level of cohesion of GUE-NGL| | 
1608.04917|fa572f1f3f3ce6e1f9f4c9530456329ffc2677ca|Do they authors account for differences in usage of Twitter amongst MPs into their model?|No| | 
1608.04917|5e057e115f8976bf9fe70ab5321af72eb4b4c0fc|Did the authors examine if any of the MEPs used the disclaimer that retweeting does not imply endorsement on their twitter profile?|No| | 
1711.02013|d824f837d8bc17f399e9b8ce8b30795944df0d51|How do they show their model discovers underlying syntactic structure?|By visualizing syntactic distance estimated by the parsing network| | 
1711.02013|2ff3898fbb5954aa82dd2f60b37dd303449c81ba|Which dataset do they experiment with?|Penn Treebank, Text8, WSJ10| | 
1909.00183|ee9b95d773e060dced08705db8d79a0a6ef353da|How are content clusters used to improve the prediction of incident severity?|they are used as additional features in a supervised classification task| | 
1909.00183|dbdf13cb4faa1785bdee90734f6c16380459520b|What cluster identification method is used in this paper?|A combination of Minimum spanning trees, K-Nearest Neighbors and Markov Stability BIBREF15, BIBREF16, BIBREF17, BIBREF18| | 
1703.08885|73e715e485942859e1db75bfb5f35f1d5eb79d2e|How can a neural model be used for a retrieval if the input is the entire Wikipedia?|Using the answer labels in the training set, we can find appropriate articles that include the information requested in the question.| | 
1908.06138|12391aab31c899bac0ecd7238c111cb73723a6b7|Which algorithm is used in the UDS-DFKI system?|Our transference model extends the original transformer model to multi-encoder based transformer architecture. The transformer architecture BIBREF12 is built solely upon such attention mechanisms completely replacing recurrence and convolutions. | | 
1908.06138|8b43201e7e648c670c02e16ba189230820879228|Does the use of out-of-domain data improve the performance of the method?|No| | 
1801.09030|5d5a571ff04a5fdd656ca87f6525a60e917d6558|Do they impose any grammatical constraints over the generated output?|No| | 
1801.09030|3c362bfa11c60bad6c7ea83f8753d427cda77de0|Why did they think this was a good idea?|They think it will help human TCM practitioners make prescriptions.| | 
1804.03396|fd8b6723ad5f52770bec9009e45f860f4a8c4321|What QA models were used?|A pointer network decodes the answer from a bidirectional LSTM with attention flow layer and self-matching layer, whose inputs come from word and character embeddings of the query and input text fed through a highway layer.| | 
1804.03396|4ce3a6632e7d86d29a42bd1fcf325114b3c11d46|Can this approach model n-ary relations?|No| | 
1804.03396|e7c0cdc05b48889905cc03215d1993ab94fb6eaa|Was this benchmark automatically created from an existing dataset?|No| | 
2004.02083|99760276cfd699e55b827ceeb653b31b043b9ceb|How does morphological analysis differ from morphological inflection?|Morphological analysis is the task of creating a morphosyntactic description for a given word,  inflectional realization is framed as a mapping from the pairing of a lemma with a set of morphological tags to the corresponding word form| | 
2004.02083|79cfd1b82c72d18e2279792c66a042c0e9dfa6b7|What are the architectures used for the three tasks?|DyNet| | 
2004.02083|9e1bf306658ef2972159643fdaf149c569db524b|Which language family does Chatino belong to?|the Otomanguean language family| | 
2004.02083|25b24ab1248f14a621686a57555189acc1afd49c|What system is used as baseline?|DyNet| | 
2004.02083|8486e06c03f82ebd48c7cfbaffaa76e8b899eea5|How was annotation done?| hand-curated collection of complete inflection tables for 198 lemmata| | 
1707.03764|9bcc1df7ad103c7a21d69761c452ad3cd2951bda|On which task does do model do worst?|Gender prediction task| | 
1707.03764|8427988488b5ecdbe4b57b3813b3f981b07f53a5|On which task does do model do best?|Variety prediction task| | 
2001.10179|3604c4fba0a82d7139efd5ced47612c90bd10601|Is their implementation on CNN-DSA compared to GPU implementation in terms of power consumption, accuracy and speed?|No| | 
2001.10179|931a2a13a1f6a8d9107d26811089bdccc39b0800|How is Super Character method modified to handle tabular data also?|simply split the image into two parts. One for the text input, and the other for the tabular data| | 
1907.11907|8c981f8b992cb583e598f71741c322f522c6d2ad|How are the substitution rules built?|from the Database of Modern Icelandic Inflection (DMII) BIBREF1| | 
1907.11907|16f33de90b76975a99572e0684632d5aedbd957c|Which dataset do they use?|a reference corpus of 21,093 tokens and their correct lemmas| | 
1911.03842|d0b005cb7ed6d4c307745096b2ed8762612480d2|What baseline is used to compare the experimental results against?|Transformer generation model| | 
1911.03842|9d9b11f86a96c6d3dd862453bf240d6e018e75af|How does counterfactual data augmentation aim to tackle bias?|The training dataset is augmented by swapping all gendered words by their other gender counterparts| | 
1911.03842|415f35adb0ef746883fb9c33aa53b79cc4e723c3|In the targeted data collection approach, what type of data is targetted?|Gendered characters in the dataset| | 
1707.02377|52f1a91f546b8a25a5d72325c503ec8f9c72de23|Which language models do they compare against?|RNNLM BIBREF11| | 
1707.02377|bb5697cf352dd608edf119ca9b82a6b7e51c8d21|Is their approach similar to making an averaged weighted sum of word vectors, where weights reflect word frequencies?|Different from their work, we choose to corrupt the original document by randomly removing significant portion of words, and represent the document using only the embeddings of the words remained.| | 
1707.02377|98785bf06e60fcf0a6fe8921edab6190d0c2cec1|How do they determine which words are informative?|Informative are those that will not be suppressed by regularization performed.| | 
1911.06191|eecf62e18a790bcfdd8a56f0c4f498927ff2fb47|How does soft contextual data augmentation work?|softly augments a randomly chosen word in a sentence by its contextual mixture of multiple related words, replacing the one-hot representation of a word by a distribution provided by a language model over the vocabulary| | 
1911.06191|acda028a21a465c984036dcbb124b7f03c490b41|How does muli-agent dual learning work?|MADL BIBREF0 extends the dual learning BIBREF1, BIBREF2 framework by introducing multiple primal and dual models.| | 
1911.06191|42af0472e6895eaf7b9392674b0d956e64e86b03|Which language directions are machine translation systems of WMT evaluated on?|German$\leftrightarrow $English, German$\leftrightarrow $French, Chinese$\leftrightarrow $English, English$\rightarrow $Lithuanian, English$\rightarrow $Finnish, and Russian$\rightarrow $English, Lithuanian$\rightarrow $English, Finnish$\rightarrow $English, and English$\rightarrow $Kazakh| | 
1701.06538|af073d84b8a7c968e5822c79bef34a28655886de|What improvement does the MOE model make over the SOTA on machine translation?|1.34 and 1.12 BLEU score on top of the strong baselines in BIBREF3, perplexity scores are also better, On the Google Production dataset, our model achieved 1.01 higher test BLEU score| | 
1701.06538|0cd90e5b79ea426ada0203177c28812a7fc86be5|How is the correct number of experts to use decided?|varied the number of experts between models| | 
1701.06538|f01a88e15ef518a68d8ca2bec992f27e7a3a6add|What equations are used for the trainable gating network?|DISPLAYFORM0, DISPLAYFORM0 DISPLAYFORM1| | 
1905.10810|6a31bd676054222faf46229fc1d283322478a020|How is PIEWi annotated?|[error, correction] pairs| | 
1905.10810|e4d16050f0b457c93e590261732a20401def9cde|What methods are tested in PIEWi?|Levenshtein distance metric BIBREF8, diacritical swapping, Levenshtein distance is used in a weighted sum to cosine distance between word vectors, ELMo-augmented LSTM| | 
1905.10810|b25e7137f49f77e7e67ee2f40ca585d3a377f8b5|Which specific error correction solutions have been proposed for specialized corpora in the past?|spellchecking mammography reports and tweets BIBREF7 , BIBREF4| | 
2002.12328|d803b782023553bbf9b36551fbc888ad189b1f29|What was the criteria for human evaluation?|to judge each utterance from 1 (bad) to 3 (good) in terms of informativeness and naturalness| | 
2002.12328|fc5f9604c74c9bb804064f315676520937131e17|What automatic metrics are used to measure performance of the system?|BLEU scores and the slot error rate (ERR)| | 
2002.12328|b37fd665dfa5fad43977069d5623f4490a979305|What existing methods is SC-GPT compared to?|$({1})$ SC-LSTM BIBREF3, $({2})$ GPT-2 BIBREF6 , $({3})$ HDSA BIBREF7| | 
1910.07481|749a307c3736c5b06d7b605dc228d80de36cbabe|Which datasets were used in the experiment?|WMT 2019 parallel dataset, a restricted dataset containing the full TED corpus from MUST-C BIBREF10, sampled sentences from WMT 2019 dataset| | 
1910.07481|102de97c123bb1e247efec0f1d958f8a3a86e2f6|What evaluation metrics did they use?|BLEU and TER scores| | 
1610.09516|d491ee69db39ec65f0f6da9ec03450520389699a|What are the differences in the use of emojis between gang member and the rest of the Twitter population?|32.25% of gang members in our dataset have chained together the police and the pistol emoji, compared to just 1.14% of non-gang members, only 1.71% of non-gang members have used the hundred points emoji and pistol emoji together in tweets while 53% of gang members have used them, gang members have a penchant for using just a small set of emoji symbols that convey their anger and violent behavior| | 
1610.09516|d3839c7acee4f9c8db0a4a475214a8dcbd0bc26f|What are the differences in the use of YouTube links between gang member and the rest of the Twitter population?|76.58% of the shared links are related to hip-hop music, gangster rap, and the culture that surrounds this music genre| | 
1610.09516|a6d00f44ff8f83b6c1787e39333e759b0c3daf15|What are the differences in the use of images between gang member and the rest of the Twitter population?|user holds or points weapons, is seen in a group fashion which displays a gangster culture, or is showing off graffiti, hand signs, tattoos and bulk cash| | 
1610.09516|0d4aa05eb00d9dee74000ea5b21b08f693ba1e62|What are the differences in language use between gang member and the rest of the Twitter population?|Although cursing is frequent in tweets, they represent just 1.15% of all words used BIBREF21 . In contrast, we found 5.72% of all words posted by gang member accounts to be classified as a curse word, gang members talk about material things with terms such as got, money, make, real, need whereas ordinary users tend to vocalize their feelings with terms such as new, like, love, know, want, look, make, us| | 
1610.09516|382bef47d316d7c12ea190ae160bf0912a0f4ffe|How is gang membership verified?|Manual verification| | 
1610.09516|32a232310babb92991c4b1b75f7aa6b4670ec447|Do the authors provide evidence that 'most' street gang members use Twitter to intimidate others?|No| | 
2001.05493|5845d1db7f819dbadb72e7df69d49c3f424b5730|What is English mixed with in the TRAC dataset?|Hindi| | 
2001.05493|54fe8f05595f2d1d4a4fd77f4562eac519711fa6|How have the differences in communication styles between Twitter and Facebook increase the complexity of the problem?|Systems do not perform well both in Facebook and Twitter texts| | 
2001.05493|fbe5e513745d723aad711ceb91ce0c3c2ceb669e|What data/studies do the authors provide to support the assertion that the majority of aggressive conversations contain code-mixed languages?|None| | 
1908.09951|1571e16063b53409f2d1bd6ec143fccc5b29ebb9|What is the baseline?|Majority Class baseline (MC) , Random selection baseline (RAN)| | 
1908.09951|d71937fa5da853f7529f767730547ccfb70e5908|What datasets did they use?|News Articles, Twitter| | 
1606.08140|8d258899e36326183899ebc67aeb4188a86f682c|What scoring function does the model use to score triples?|$ f_r(h, t) & = & \Vert \textbf {W}_{r,1}\textbf {h} + \textbf {r} - \textbf {W}_{r,2}\textbf {t}\Vert _{\ell _{1/2}} $| | 
1606.08140|955ca31999309685c1daa5cb03867971ca99ec52|What datasets are used to evaluate the model?|WN18, FB15k| | 
1911.11698|ac3c88ace59bf75788370062db139f60499c2056|How better are results for pmra algorithm  than Doc2Vec in human evaluation? |"The D2V model has been rated 80 times as ""bad relevance"" while the pmra returned only 24 times badly relevant documents."| | 
1911.11698|26012f57cba21ba44b9a9f7ed8b1ed9e8ee7625d|What Doc2Vec architectures other than PV-DBOW have been tried?|PV-DM| | 
1911.11698|bd26a6d5d8b68d62e1b6eaf974796f3c34a839c4|What four evaluation tasks are defined to determine what influences proximity?|String length, Words co-occurrences, Stems co-occurrences, MeSH similarity| | 
1911.11698|7d4fad6367f28c67ad22487094489680c45f5062|What six parameters were optimized with grid search?|window_size, alpha, sample, dm, hs, vector_size| | 
1710.01507|acc8d9918d19c212ec256181e51292f2957b37d7|What are the differences with previous applications of neural networks for this task?|This approach considers related images| | 
2002.02492|61fb982b2c67541725d6db76b9c710dd169b533d|Is infinite-length sequence generation a result of training with maximum likelihood?|There are is a strong conjecture that it might be the reason but it is not proven.| | 
2001.06354|68edb6a483cdec669c9130c928994654f1c19839|What metrics are used in challenge?|NDCG, MRR, recall@k, mean rank| | 
2001.06354|cee29acec4da1b247795daa4e2e82ef8a7b25a64|What model was winner of the Visual Dialog challenge 2018?|DL-61| | 
2001.06354|7e54c7751dbd50d9d14b9f8b13dc94947a46e42f|Which method for integration peforms better ensemble or consensus dropout fusion with shared parameters?|ensemble model| | 
2001.06354|d3bcfcea00dec99fa26283cdd74ba565bc907632|How big is dataset for this challenge?|133,287 images| | 
1904.12535|cdf65116a7c50edddcb115e9afd86b2b6accb8ad|What open relation extraction tasks did they experiment on?|verb/preposition-based relation, nominal attribute, descriptive phrase and hyponymy relation.| | 
1904.12535|c8031c1629d270dedc3b0c16dcb7410524ff1bab|How is Logician different from traditional seq2seq models?|restricted copy mechanism to ensure literally honestness, coverage mechanism to alleviate the under extraction and over extraction problem, and gated dependency attention mechanism to incorporate dependency information| | 
1904.12535|8c0e8a312b85c4ffdffabeef0d29df1ef8ff7fb2|What's the size of the previous largest OpenIE dataset?|3,200 sentences| | 
1910.08210|c9e9c5f443649593632656a5934026ad8ccc1712|How does propose model model that capture three-way interactions?| We first encode text inputs using bidirectional LSTMs, then compute summaries using self-attention and conditional summaries using attention. We concatenate text summaries into text features, which, along with visual features, are processed through consecutive layers. In this case of a textual environment, we consider the grid of word embeddings as the visual features for . The final output is further processed by MLPs to compute a policy distribution over actions and a baseline for advantage estimation.| | 
1708.02267|4d844c9453203069363173243e409698782bac3f|Do transferring hurt the performance is the corpora are not related?|Yes| | 
1708.02267|5633d93ef356aca02592bae3dfc1b3ec8fce27dc|Is accuracy the only metric they used to compare systems?|No| | 
1708.02267|134598831939a3ae20d177cec7033d133625a88e|How do they transfer the model?|In the MULT method, two datasets are simultaneously trained, and the weights are tuned based on the inputs which come from both datasets. The hyper-parameter $\lambda \in (0,1)$ is calculated based on a brute-force search or using general global search. This hyper parameter is used to calculate the final cost function which is computed from the combination of the cost function of the source dataset and the target datasets. , this paper proposes to use the most relevant samples from the source dataset to train on the target dataset. One way to find the most similar samples is finding the pair-wise distance between all samples of the development set of the target dataset and source dataset., we propose using a clustering algorithm on the development set. The clustering algorithm used ihere is a hierarchical clustering algorithm. The cosine similarity is used as a criteria to cluster each question and answer. Therefore, these clusters are representative of the development set of the target dataset and the corresponding center for each cluster is representative of all the samples on that cluster. In the next step, the distance of each center is used to calculate the cosine similarity. Finally, the samples in the source dataset which are far from these centers are ignored. In other words, the outliers do not take part in transfer learning.| | 
1811.02076|4bae74eb707ed71d5f438ddb3d9c2192ac490f66|Will these findings be robust through different datasets and different question answering algorithms?|Yes| | 
1811.02076|c30c3e0f8450b1c914d29f41c17a22764fa078e0|What is the underlying question answering algorithm?|The system extends BiDAF BIBREF4 with self-attention| | 
1811.02076|21656039994cab07f79e89553cbecc31ba9853d4|What datasets have this method been evaluated on?|document-level variants of the SQuAD dataset | | 
2001.05672|a56fbe90d5d349336f94ef034ba0d46450525d19|What DCGs are used?|Author's own DCG rules are defined from scratch.| | 
2001.05672|b1f2db88a6f89d0f048803e38a0a568f5ba38fc5|What else is tried to be solved other than 12 tenses, model verbs and negative form?|cases of singular/plural, subject pronoun/object pronoun, etc.| | 
2001.05672|7883a52f008f3c4aabfc9f71ce05d7c4107e79bb|Is there information about performance of these conversion methods?|No| | 
2001.05672|cd9776d03fe48903e43e916385df12e1e798070a|Are there some experiments performed in the paper?|No| | 
1908.08593|1a252ffeaebdb189317aefd6c606652ba9677112|How much is performance improved by disabling attention in certain heads?|disabling the first layer in the RTE task gives a significant boost, resulting in an absolute performance gain of 3.2%,  this operation vary across tasks| | 
1908.08593|da4d25dd9de09d16168788bb02ad600f5b0b3ba4|In which certain heads was attention disabled in experiments?|single head, disabling a whole layer, that is, all 12 heads in a given layer| | 
1908.08593|2870fbce43a3cf6daf982f720137c008b30c60dc|What handcrafter features-of-interest are used?|nouns, verbs, pronouns, subjects, objects, negation words, special BERT tokens| | 
1908.08593|65b579b2c62982e2ff154c8160288c2950d509f2|What subset of GLUE tasks is used?|MRPC, STS-B, SST-2, QQP, RTE, QNLI, MNLI| | 
1911.02711|2f9d30e10323cf3a6c9804ecdc7d5872d8ae35e4|Which review dataset do they use?|SNAP (Stanford Network Analysis Project)| | 
2001.11381|327e06e2ce09cf4c6cc521101d0aecfc745b1738|What evaluation metrics did they look at?|accuracy with standard deviation| | 
2001.11381|40b9f502f15e955ba8615822e6fa08cb5fd29c81|What datasets are used?|Corpus 5KL, Corpus 8KF| | 
1909.13362|ba56afe426906c4cfc414bca4c66ceb4a0a68121|What are the datasets used for the task?|Datasets used are Celex (English, Dutch), Festival (Italian), OpenLexuque (French), IIT-Guwahati (Manipuri), E-Hitz (Basque)| | 
1909.13362|14634943d96ea036725898ab2e652c2948bd33eb|What is the accuracy of the model for the six languages tested?|Authors report their best models have following accuracy: English CELEX (98.5%), Dutch CELEX (99.47%), Festival (99.990%), OpenLexique (100%), IIT-Guwahat (95.4%), E-Hitz (99.83%)| | 
1909.13362|d71cb7f3aa585e256ca14eebdc358edfc3a9539c|Which models achieve state-of-the-art performances?|"CELEX (Dutch and English) - SVM-HMM
Festival, E-Hitz and OpenLexique - Liang hyphenation
IIT-Guwahat - Entropy CRF"| | 
1909.13362|f6556d2a8b42b133eaa361f562745edbe56c0b51|Is the LSTM bidirectional?|Yes| | 
1910.13890|def3d623578bf84139d920886aa3bd6cdaaa7c41|What are the three languages studied in the paper?|Arabic, Czech and Turkish| | 
1608.06111|d51069595f67a3a53c044c8a37bae23facbfa45d|Do they use pretrained models as part of their parser?|Yes| | 
1608.06111|1a6e2bd41ee43df83fef2a1c1941e6f95a619ae8|Which subtasks do they evaluate on?| entity recognition, semantic role labeling and co-reference resolution| | 
1908.01060|e6c163f80a11bd057bbd0b6e1451ac82edddc78d|Do they test their approach on large-resource tasks?|Yes| | 
1908.01060|6adfa9eee76b96953a76c03356bf41d8a9378851|By how much do they, on average, outperform the baseline multilingual model on 16 low-resource tasks?|1.6% lower phone error rate on average| | 
1908.01060|450a359d117bcfa2de4ffd987f787945f25b3b25|How do they compute corpus-level embeddings?|First, the embedding matrix INLINEFORM4 for all corpora is initialized, during the training phase, INLINEFORM9 can be used to bias the input feature, Next, we apply the language specific softmax to compute logits INLINEFORM4 and optimize them with the CTC objective| | 
1909.01492|70f84c73172211186de1a27b98f5f5ae25a94e55|Which dataset do they use?|Stanford Sentiment Treebank (SST) BIBREF15 and AG News BIBREF16| | 
1907.08937|10ddc5caf36fe9d7438eb5a3936e24580c4ffe6a|Which competitive relational classification models do they test?|For relation prediction they test TransE and for relation extraction they test position aware neural sequence model| | 
1907.08937|1a678d081f97531d54b7122254301c20b3531198|Which knowledge bases do they use?|Wikidata, ReVerb, FB15K, TACRED| | 
1907.08937|b9f2a30f5ef664ff845d860cf4bfc2afb0a46e5a|How do they gather human judgements for similarity between relations?|By assessing similarity of 360 pairs of relations from a subset of Wikidata using an integer similarity score from 0 to 4| | 
1907.08937|3513682d4ee2e64725b956c489cd5b5995a6acf2|Which sampling method do they use to approximate similarity between the conditional probability distributions over entity pairs?|monte-carlo, sequential sampling| | 
1803.02839|30b5e5293001f65d2fb9e4d1fdf4dc230e8cf320|What text classification task is considered?|To classify a text as belonging to one of the ten possible classes.| | 
1803.02839|993b896771c31f3478f28112a7335e7be9d03f21|What novel class of recurrent-like networks is proposed?|A network, whose learned functions satisfy a certain equation. The  network contains RNN cells with either nested internal memories or dependencies that extend temporally beyond the immediately previous hidden state.| | 
1803.02839|dee116df92f9f92d9a67ac4d30e32822c22158a6|Is there a formal proof that the RNNs form a representation of the group?|No| | 
2001.07263|94bee0c58976b58b4fef9e0adf6856fe917232e5|How much bigger is Switchboard-2000 than Switchboard-300 database?|Switchboard-2000 contains 1700 more hours of speech data.| | 
2001.07263|7efbe48e84894971d7cd307faf5f6dae9d38da31|How big is Switchboard-300 database?|300-hour English conversational speech| | 
1912.06670|7f452eb145d486c15ac4d1107fc914e48ebba60f|What crowdsourcing platform is used for data collection and data validation?|the Common Voice website,  iPhone app| | 
1912.06670|bb71a638668a21c2d446b44cbf51676c839658f7|How is validation of the data performed?|A maximum of three contributors will listen to any audio clip. If an $<$audio,transcript$>$ pair first receives two up-votes, then the clip is marked as valid. If instead the clip first receives two down-votes, then it is marked as invalid.| | 
1907.11499|427252648173c3ba78c211b86fa89fc9f4406653|What domains are detected in this paper?|"Answer with content missing: (Experimental setup not properly rendered) In our experiments we used seven target domains: “Business and Commerce” (BUS), “Government and Politics” (GOV), “Physical and Mental Health” (HEA), “Law and Order” (LAW),
“Lifestyle” (LIF), “Military” (MIL), and “General Purpose” (GEN). Exceptionally, GEN does
not have a natural root category."| | 
1905.11037|b9025c39838ccc2a79c545bec4a676f7cc4600eb|Why do they think this task is hard?  What is the baseline performance?|"1. there may be situations where more than one action is reasonable, and also because writers tell a story playing with elements such as surprise or uncertainty.
2. Macro F1 = 14.6 (MLR, length 96 snippet)
Weighted F1 = 31.1 (LSTM, length 128 snippet)"| | 
1905.11037|19608e727b527562b750949e41e763908566b58e|"Do they literally just treat this as ""predict the next spell that appears in the text""?"|Yes| | 
1710.10609|3f7a7e81908a763e5ca720f90570c5f224ac64f6|Do they study frequent user responses to help automate modelling of those?|Yes| | 
1710.10609|49b38189b8336ce41d0f0b4c5c9459722736e15b|Do they use the same distance metric for both the SimCluster and K-means algorithm?|Yes| | 
1710.10609|40c2bab4a6bf3c0628079fcf19e8b52f27f51d98|How do they generate the synthetic dataset?|using generative process| | 
1705.07830|33d2919f3400cd3c6fbb6960d74187ec80b41cd6|how are multiple answers from multiple reformulated questions aggregated?|The selection model selects the best answer from the set $\lbrace a_i\rbrace _{i=1}^N$ observed during the interaction by predicting the difference of the F1 score to the average F1 of all variants.| | 
1906.03538|fb96c0cd777bb2961117feca19c6d41bfd8cfd42|What debate websites did they look at?|idebate.com, debatewise.org, procon.org| | 
1906.03538|534f69c8c90467d5aa4e38d7c25c53dbc94f4b24|What crowdsourcing platform did they use?|Amazon Mechanical Turk (AMT)| | 
1906.03538|090f2b941b9c5b6b7c34ae18c2cc97e9650f1f0b|Which machine baselines are used?|Information Retrieval| | 
1906.03538|5e032de729ce9fc727b547e3064be04d30009324|What challenges are highlighted?|one needs to develop mechanisms to recognize valid argumentative structures, we ignore trustworthiness and credibility issues| | 
1709.05404|f5e571207d9f4701b4d01199ef7d0bfcfa2c0316|What are the linguistic differences between each class?|Each class has different patterns in adjectives, adverbs and verbs for sarcastic and non-sarcastic classes| | 
1709.05404|c5ac07528cf99d353413c9d9ea61a1a699dd783e|What simple features are used?|unigrams, bigrams, and trigrams, including sequences of punctuation, Word2Vec word embeddings| | 
1709.05404|6608f171b3e0dcdcd51b3e0c697d6e5003ab5f02|What lexico-syntactic cues are used to retrieve sarcastic utterances?|adjective and adverb patterns, verb, subject, and object arguments, verbal patterns| | 
2003.05377|52b113e66fd691ae18b9bb8a8d17e1ee7054bb81|what is the source of the song lyrics?|Vagalume website| | 
2003.05377|163a21c0701d5cda15be2d0eb4981a686e54a842|what genre was the most difficult to classify?| bossa-nova and jovem-guarda genres| | 
2003.05377|36b5f0f62ee9be1ab50d1bb6170e98328d45997d|what word embedding techniques did they experiment with?|Word2Vec, Wang2Vec, and FastText| | 
1909.05478|aa7decee4e3006c2c99b1f331a5b32d44a565ef6|Is the filter based feature selection (FSE) a form of regularization?|No| | 
2001.05467|a09633584df1e4b9577876f35e38b37fdd83fa63|How is human evaluation performed, what was the criteria?|Through Amazon MTurk annotators to determine plausibility and content richness of the response| | 
2001.05467|58edc6ed7d6966715022179ab63137c782105eaf|Which one of the four proposed models performed best?|the hybrid model MinAvgOut + RL| | 
1909.09484|b366706e2fff6dd8edc89cc0c6b9d5b0790f43aa|What metrics are used to measure performance of models?|BPRA, APRA, BLEU| | 
1909.09484|e72a672f8008bbc52b93d8037a5fedf8956136af|What are state-of-the-art baselines?|E2ECM, CDM| | 
1909.09484|57586358dd01633aa2ebeef892e96a549b1d1930|What two benchmark datasets are used?|DSTC2, Maluuba| | 
1908.06006|975085e3b6679cc644fdd6ad11b7c2d1261a2dc6|Do they compare to other models appart from HAN?|No| | 
1908.06006|609fbe627309775de415682f48588937d5dd8748|What are the datasets used|large-scale document classification datasets introduced by BIBREF14| | 
1807.07961|7ce213657f7ee792148988c5a3578b24cd2f9c62|What evidence does visualizing the attention give to show that it helps to obtain a more robust understanding of semantics and sentiments?|The different attention distributions suggest that the proposed senti-emoji embedding is capable of recognizing words with strong sentiments that are closely related to the true sentiment even with the presence of words with conflicting sentiments| | 
1807.07961|89ce18ee52c52a78b38c49b14574407b7ea2fb02|Which SOTA models are outperformed?|Attention-based LSTM with emojis| | 
1807.07961|d3092cd32cd581a57fa4844f80fe18d6b920e903|What is the baseline for experiments?|LSTM with text embedding, LSTM with emoji embedding, Attention-based LSTM with emojis| | 
1807.07961|0b39c20db6e60ce07bf5465bd3c08fedc0587780|What is the motivation for training bi-sense embeddings?| previous emoji embedding methods fail to handle the situation when the semantics or sentiments of the learned emoji embeddings contradict the information from the corresponding contexts BIBREF5 , or when the emojis convey multiple senses of semantics and sentiments | | 
1908.10322|fb427239c8d44f524a6c1bf1ce5c3383d5c33e52|How many parameters does the model have?|model has around 836M parameters| | 
1908.10322|7c45c6e5db6cfca2d6de8751e28403b35420ae38|How many characters are accepted as input of the language model?|input byte embedding matrix has dimensionality 256| | 
1909.02776|49ea25af6f75e2e96318bad5ecf784ce84e4f76b|What dataset is used for this task?|the Pasokh dataset BIBREF42 | | 
1909.02776|aecd09a817c38cf7606e2888d0df7f14e5a74b95|What features of the document are integrated into vectors of every sentence?|Ordinal position, Length of sentence, The Ratio of Nouns, The Ratio of Numerical entities, Cue Words, Cosine position, Relative Length, TF-ISF, POS features, Document sentences, Document words, Topical category, Ratio of Verbs, Ratio of Adjectives, and Ratio of Adverbs| | 
1909.02776|7d841b98bcee29aaa9852ef7ceea1213d703deba|Is new approach tested against state of the art?|No| | 
1911.00133|4e8233826f9e04f5763b307988298e73f841af74|Is the dataset balanced across categories?|Yes| | 
1911.00133|adae0c32a69928929101d0ba37d36c0a45298ad6|What supervised methods are used?|Support Vector Machines (SVMs), logistic regression, Naïve Bayes, Perceptron, and decision trees, a two-layer bidirectional Gated Recurrent Neural Network (GRNN) BIBREF20 and Convolutional Neural Network (CNN) (as designed in BIBREF21)| | 
1911.00133|1ccfd288f746c35006f5847297ab52020729f523|What categories does the dataset come from?|abuse, social, anxiety, PTSD, and financial| | 
1909.09018|ed6462da17c553bda112ef35917fefe6942fce3c|What are all machine learning approaches compared in this work?|Feature selection, Random forest, XGBoost, Hierarchical Model| | 
1709.05413|915cf3d481164217290d7b1eb9d48ed3e249196d|Which patterns and rules are derived?|A request for information act should be issued early in a conversation, followed by an answer, informative statement, or apology towards the end of the conversation,  offering extra help at the end of a conversation, or thanking the customer yields more satisfied customers, and more resolved problems , asking yes-no questions early-on in a conversation is highly associated with problem resolution (ratio 3:1), but asking them at the end of a conversation has as similarly strong association with unsatisfied customers, Giving elaborate answers that are not a simple affirmative, negative, or response acknowledgement (i.e. Answer (Other)) towards the middle of a conversation leads to satisfied customers that are not frustrated, requesting information towards the end of a conversation (implying that more information is still necessary at the termination of the dialogue) leads to unsatisfied and unresolved customers| | 
1709.05413|d6e8b32048ff83c052e978ff3b8f1cb097377786|How are customer satisfaction, customer frustration and overall problem resolution data collected?|By annotators on Amazon Mechanical Turk.| | 
1709.05413|e26e7e9bcd7e2cea561af596c59b98e823653a4b|Which Twitter customer service industries are investigated?| four different companies in the telecommunication, electronics, and insurance industries| | 
1709.05413|b24767fe7e6620369063e646fd3048dc645a8348|Which dialogue acts are more suited to the twitter domain?|overlapping dialogue acts| | 
1704.00253|0a7ac8eccbc286e0ab55bc5949f3f8d2ea2d1a60|How many improvements on the French-German translation benchmark?|one| | 
1704.00253|e84e80067b3343d136fd75300691c8b3d3efbdac|How do they align the synthetic data?|By choosing English (En) as the pivot language, we perform pivot alignments for identical English segments on Europarl Fr-En and En-De parallel corpora BIBREF18 , constructing a multi-parallel corpus of Fr-En-De. Then each of the Fr*-De and Fr-De* pseudo parallel corpora is established from the multi-parallel data by applying the pivot language-based translation described in the previous section.| | 
1704.00253|45bd22f2cfb62a5f79ec3c771c8324b963567cc0|Where do they collect the synthetic data?|Yes| | 
1908.00153|936878cff0e6e327b2554ee5d46686797ee92cf2|Do they analyze what type of content Arabic bots spread in comparison to English?|No| | 
1908.00153|58c1b162a4491d4a5ae0ff86cc8bd64e98739620|Do they propose a new model to better detect Arabic bots specifically?|Yes| | 
1909.11833|4dad15fee1fe01c3eadce8f0914781ca0a6e3f23|How do they prevent the model complexity increasing with the increased number of slots?|They exclude slot-specific parameters and incorporate better feature representation of user utterance and dialogue states using syntactic information and convolutional neural networks (CNN).| | 
1909.11833|892c346617a3391c7dafc9da1b65e5ea3890294d|What network architecture do they use for SIM?|convolutional neural networks (CNN)| | 
1909.11833|36feaac9d9dee5ae09aaebc2019b014e57f61fbf|How do they measure model size?|By the number of parameters.| | 
2002.02562|df25dd9004a3b367202d7731ee912a8052a35780|Does model uses pretrained Transformer encoders?|No| | 
2002.02562|5328cc2588b2bf7b91f4e0f342e8cbfc6dc8ac00|What was previous state of the art model?|LSTM-based RNN-T| | 
2002.02562|766e2e35968ef7434b56330aa41957c5d5f8d0ee|How big is LibriSpeech dataset?|970 hours of audio data with corresponding text transcripts (around 10M word tokens) and an additional 800M word token text only dataset| | 
1804.00079|50be9e6203c40ed3db48ed37103f967ef0ea946c|How do they evaluate their sentence representations?|standard benchmarks BIBREF36 , BIBREF37, to use our learned representations as features for a low complexity classifier (typically linear) on a novel supervised task/domain unseen during training without updating the parameters, transfer learning evaluation in an artificially constructed low-resource setting| | 
1804.00079|36a9230fadf997d3b0c5fc8af8d89bd48bf04f12|Which model architecture do they for sentence encoding?|"Answer with content missing: (Skip-thought vectors-Natural Language Inference paragraphs) The encoder for the current sentence and the decoders for the previous (STP) and next sentence (STN) are typically parameterized as separate RNNs
- RNN"| | 
1804.00079|00e9f088291fcf27956f32a791f87e4a1e311e41|Which training objectives do they combine?|multi-lingual NMT, natural language inference, constituency parsing, skip-thought vectors| | 
1911.03154|317a6f211ecf48c58f008c12fbd5d41901db3e36|Has there been previous work on SNMT?|Yes| | 
1911.03154|a726046eec1e2efa5fe3926963863bf755e64682|Which languages do they experiment on?|German, English, Chinese| | 
1911.03154|6d9fbd42b54313cfdc2665809886330f209e9286|What corpora is used?|IWSLT16, WMT15, NIST| | 
1805.09959|bb8f62950acbd4051774f1bfc50e3d424dd33b7c|Do the authors report results only on English datasets?|Yes| | 
1805.09959|d653d994ef914d76c7d4011c0eb7873610ad795f|How were breast cancer related posts compiled from the Twitter streaming API?|"By using  keywords `breast' AND `cancer' in tweet collecting process. 
"| | 
1805.09959|880a76678e92970791f7c1aad301b5adfc41704f|What machine learning and NLP methods were used to sift tweets relevant to breast cancer experiences?|"ML  logistic regression classifier combined with a Convolutional Neural Network (CNN) to identify self-reported diagnostic tweets.
NLP methods:  tweet conversion to numeric word vector,  removing tweets containing hyperlinks, removing ""retweets"", removing all tweets containing horoscope indicators,  lowercasing and  removing punctuation."| | 
1909.05360|cfb5ab893ed77f9df7eeb4940b6bacdef5acccea|Is this the first paper to propose a joint model for event and temporal relation extraction?|Yes| | 
1909.05360|a5abd4dd91e6f2855e9098bd6ae1481c0fdb0d4a|What datasets were used for this work?|TB-Dense,  MATRES| | 
2003.12738|6aed1122050b2d508dc1790c13cdbe38ff126089|What baselines other than standard transformers are used in experiments?|attention-based sequence-to-sequence model , CVAE| | 
2003.12738|8740c3000e740ac5c0bc8f329d908309f7ffeff6|What three conversational datasets are used for evaluation?|MojiTalk , PersonaChat , Empathetic-Dialogues| | 
1909.03544|48fb76ae9921c9d181f65afc63a42af8ba3bc519|What data is used to build the embeddings?|large raw Czech corpora available from the LINDAT/CLARIN repository, Czech Wikipedia| | 
1910.04519|13149342ccbb7a6df9b4b1bed890cfbdc1331c1f|How big is dataset used for fine-tuning model for detection of red flag medical symptoms in individual statements?|a total of 2,560 pseudo-tweets in three different languages: Japanese (ja), English (en) and Chinese (zh)| | 
1910.04519|fbe149bd76863575b98fafb3679f411d3d21b4a3|Is there any explanation why some choice of language pair is better than the other?|translations that were reasonable but not consistent with the labels| | 
1811.01088|a91abc7983fffa6b2e1e46133f559cec3d7d9438|Does the additional training on supervised tasks hurt performance in some tasks?|Yes| | 
1906.01183|c45feda62f23245f53e855706e2d8ea733b7fd03|Which translation system do they use to translate to English?|Attention-based translation model with convolution sequence to sequence model| | 
1906.01183|9785ecf1107090c84c57112d01a8e83418a913c1|Which languages do they work with?|German, Spanish, Chinese| | 
1906.01183|e051d68a7932f700e6c3f48da57d3e2519936c6d|Which pre-trained English NER model do they use?|Bidirectional LSTM based NER model of Flair| | 
1909.06522|0ec4143a4f1a8f597b435f83c0451145be2ab95b|What are the best within-language data augmentation methods?|Frequency masking, Time masking, Additive noise, Speed and volume perturbation| | 
1909.06522|90159e143487505ddc026f879ecd864b7f4f479e|How much of the ASR grapheme set is shared between languages?|Little overlap except common basic Latin alphabet and that Hindi and Marathi languages use same script.| | 
1909.12642|d10e256f2f724ad611fd3ff82ce88f7a78bad7f7|What is the performance of the model for the German sub-task A?|macro F1 score of 0.62| | 
1909.12642|c691b47c0380c9529e34e8ca6c1805f98288affa|Is the model tested for language identification?|No| | 
1909.12642|892e42137b14d9fabd34084b3016cf3f12cac68a|Is the model compared to a baseline model?|No| | 
1909.12642|dc69256bdfe76fa30ce4404b697f1bedfd6125fe|What are the languages used to test the model?|Hindi, English and German (German task won)| | 
1902.10525|b8d5e9fa08247cb4eea835b19377262d86107a9d|What datasets did they use?|IBM-UB-1 dataset BIBREF25, IAM-OnDB dataset BIBREF42, The ICDAR-2013 Competition for Online Handwriting Chinese Character Recognition BIBREF45, ICFHR2018 Competition on Vietnamese Online Handwritten Text Recognition using VNOnDB BIBREF50| | 
1912.05238|8de64483ae96c0a03a8e527950582f127b43dceb|Do they report results only on English data?|Yes| | 
1912.05238|4d062673b714998800e61f66b6ccbf7eef5be2ac|What is the Moral Choice Machine?|Moral Choice Machine computes the cosine similarity in a sentence embedding space of an arbitrary action embedded in question/answer pairs| | 
1912.05238|f4238f558d6ddf3849497a130b3a6ad866ff38b3|How is moral bias measured?|"Answer with content missing: (formula 1) bias(q, a, b) = cos(a, q) − cos(b, q)
Bias is calculated as substraction of cosine similarities of question and some answer for two opposite answers."| | 
1912.05238|96dcabaa8b6bd89b032da609e709900a1569a0f9|How do the authors define deontological ethical reasoning?|These ask which choices are morally required, forbidden, or permitted, norms are understood as universal rules of what to do and what not to do| | 
2003.00639|f416c6818a7a8acb7ec4682ed424ecdbd7dd6df1|How does framework automatically chooses different curricula at the evolving learning process according to the learning status of the neural dialogue generation model?|The multi-curricula learning scheme is scheduled according to the model's performance on the validation set, where the scheduling mechanism acts as the policy $\pi $ interacting with the dialogue model to acquire the learning status $s$. The reward of the multi-curricula learning mechanism $m_t$ indicates how well the current dialogue model performs.| | 
2003.00639|a1d422cb2e428333961370496ca281a1be99fdff|What human judgement metrics are used?|coherence, logical consistency, fluency and diversity| | 
2003.00639|3de9bf4b0b667b3f1181da9f006da1354565bcbd|What automatic evaluation metrics are used?|BLEU, embedding-based metrics (Average, Extrema, Greedy and Coherence), , entropy-based metrics (Ent-{1,2}), distinct metrics (Dist-{1,2,3} and Intra-{1,2,3})| | 
2003.00639|1a1293e24f4924064e6fb9998658f5a329879109|What state of the art models were used in experiments?|SEQ2SEQ, CVAE, Transformer, HRED, DialogWAE| | 
2003.00639|3ccd337f77c5d2f7294eb459ccc1770796c2eaef|What five dialogue attributes were analyzed?|Model Confidence, Continuity, Query-relatedness, Repetitiveness, Specificity| | 
2003.00639|f6937199e4b06bfbaa22edacc7339410de9703db|What three publicly available coropora are used?|PersonaChat BIBREF12, DailyDialog BIBREF13, OpenSubtitles BIBREF7| | 
1906.08286|61c9f97ee1ac5a4b8654aa152f05f22e153e7e6e|Which datasets do they use?| Wikipedia toxic comments| | 
2002.11268|9ae084e76095194135cd602b2cdb5fb53f2935c1|What metrics are used for evaluation?|word error rate| | 
2002.11268|67ee7a53aa57ce0d0bc1a20d41b64cb20303f4b7|How much training data is used?|163,110,000 utterances| | 
2002.11268|7eb3852677e9d1fb25327ba014d2ed292184210c|How is the training data collected?|from YouTube videos, with associated transcripts obtained from semi-supervised caption filtering, from a Voice Search service| | 
1910.11790|c96a6b30d71c6669592504e4ee8001e9d1eb1fba|was bert used?|Yes| | 
1910.11790|42d66726b5bf8de5b0265e09d76f5ab00c0e851a|what datasets did they use?|Single-Turn, Multi-Turn| | 
1910.11790|c418deef9e44bc8448d9296c6517824cb95bd554|which existing metrics do they compare with?|F1-score, BLEU score| | 
1905.13497|d6d29040e7fafceb188e62afba566016b119b23c|Which datasets do they evaluate on?|PDP-60, WSC-273| | 
1905.13497|21663d2744a28e0d3087fbff913c036686abbb9a|How does their model differ from BERT?|Their model does not differ from BERT.| | 
1811.00625|d8cecea477dfc5163dca6e2078a2fe6bc94ce09f|Which metrics are they evaluating with?|accuracy| | 
1909.13668|dd2f21d60cfca3917a9eb8b192c194f4de85e8b2|What different properties of the posterior distribution are explored in the paper?|interdependence between rate and distortion, impact of KL on the sharpness of the approximated posteriors, demonstrate how certain generative behaviours could be imposed on VAEs via a range of maximum channel capacities, some experiments to find if any form of syntactic information is encoded in the latent space| | 
1909.13668|ccf7415b515fe5c59fa92d4a8af5d2437c591615|Why does proposed term help to avoid posterior collapse?|"by setting a non-zero positive constraint ($C\ge 0$) on the KL term ($|D_{KL}\big (q_\phi ({z}|{x}) || p({z})\big )-C|$)"| | 
1909.13668|fee5aef7ae521ccd1562764a91edefecec34624d|How does explicit constraint on the KL divergence term that authors propose looks like?|"Answer with content missing: (Formula 2) Formula 2 is an answer: 
\big \langle\! \log p_\theta({x}|{z}) \big \rangle_{q_\phi({z}|{x})}  -  \beta |D_{KL}\big(q_\phi({z}|{x}) || p({z})\big)-C|"| | 
2003.01472|90f80a94fabaab72833256572db1d449c2779beb|Did they experiment with the tool?|Yes| | 
2003.01472|da55878d048e4dca3ca3cec192015317b0d630b1|Is this software available to the public?|Yes| | 
1607.05408|7d300176afa04947ac847135ac6ea2929908c0b0|What shared task does this system achieve SOTA in?|tweetLID workshop shared task| | 
1607.05408|df9d16a2c4983a0ff46081e3ff4d6e7ef3338264|How are labels propagated using this approach?|We use Junto (mad) BIBREF5 to propagate labels from labelled to unlabelled nodes. | | 
1607.05408|8c35caf3772637e6297009ceab38f7f5be38ea9d|What information is contained in the social graph of tweet authors?| the graph, composed of three types of nodes: tweets (T), users (U) and the “world” (W). Edges are created between nodes and weighted as follows: T-T the unigram cosine similarity between tweets, T-U weighted 100 between a tweet and its author, U-U weighted 1 between two users in a “follows” relationship and U-W weighted 0.001 to ensure a connected graph for the mad algorithm.| | 
1704.06125|3a1bd3ec1a7ce9514da0cb2dfcaa454ba8c0ed14|What were the five English subtasks?| five subtasks which involve standard classification, ordinal classification and distributional estimation. For a more detailed description see BIBREF0| | 
1704.06125|39492338e27cb90bf1763e4337c2f697cf5082ba|How many CNNs and LSTMs were ensembled?|10 CNNs and 10 LSTMs| | 
1802.05322|a7adb63db5066d39fdf2882d8a7ffefbb6b622f0|what was the baseline?|There is no baseline.| | 
1802.05322|980568848cc8e7c43f767da616cf1e176f406b05|how many movie genres do they explore?|27 | | 
1802.05322|f1b738a7f118438663f9d77b4ccd3a2c4fd97c01|what evaluation metrics are discussed?|precision , recall , Hamming loss, micro averaged precision and recall | | 
2004.01878|2f4acd34eb2d09db9b5ad9b1eb82cb4a88c13f5b|What is dataset used for news-driven stock movement prediction?|the public financial news dataset released by BIBREF4| | 
1603.00968|f7d67d6c6fbc62b2953ab74db6871b122b3c92cc|How much faster is training time for MGNC-CNN over the baselines?|It is an order of magnitude more efficient in terms of training time., his model requires pre-training and mutual-learning and requires days of training time, whereas the simple architecture we propose requires on the order of an hour| | 
1603.00968|a8e4a67dd67ae4a9ebf983a90b0d256f4b9ff6c6|What dataset/corpus is this evaluated over?| SST-1, SST-2, Subj , TREC , Irony | | 
2004.01980|41d3750ae666ea5a9cea498ddfb973a8366cccd6|How is attraction score measured?|annotators are asked how attractive the headlines are, Likert scale from 1 to 10 (integer values)| | 
2004.01980|90b2154ec3723f770c74d255ddfcf7972fe136a2|How is presence of three target styles detected?|human evaluation task about the style strength| | 
2004.01980|f3766c6937a4c8c8d5e954b4753701a023e3da74|How is fluency automatically evaluated?|fine-tuned the GPT-2 medium model BIBREF51 on our collected headlines and then used it to measure the perplexity (PPL) on the generated outputs| | 
1609.06791|2898e4aa7a3496c628e7ddf2985b48fb11aa3bba|"What are the measures of ""performance"" used in this paper?"|test-set perplexity, likelihood convergence and clustering measures, visualizing the topic summaries, authors' topic distributions and by performing an automatic labeling task| | 
1809.08510|fa9df782d743ce0ce1a7a5de6a3de226a7e423df|What are the languages they consider in this paper?|The languages considered were English, Chinese, German, Russian, Arabic, Spanish, French| | 
1809.08510|6270d5247f788c4627be57de6cf30112560c863f|Did they experiment with tasks other than word problems in math?|They experimented with sentiment analysis and natural language inference task| | 
1806.02908|d0c636fa9ef99c4f44ab39e837a680217b140269|Do the authors offer any hypothesis as to why the transformations sometimes disimproved performance?|No| | 
1806.02908|c47f593a5b92abc2e3c536fe2baaca226913688b|What preprocessing techniques are used in the experiments?|See Figure FIGREF3| | 
1806.02908|c3a9732599849ba4a9f07170ce1e50867cf7d7bf|What state of the art models are used in the experiments?|2) Naïve Bayes with SVM (NBSVM), 3) Extreme Gradient Boosting (XGBoost), 4) FastText algorithm with Bidirectional LSTM (FastText-BiLSTM)| | 
1804.08139|b556fd3a9e0cff0b33c63fa1aef3aed825f13e28|What dataset did they use?|16 different datasets from several popular review corpora used in BIBREF20, CoNLL 2000 BIBREF22| | 
1804.08139|0db1ba66a7e75e91e93d78c31f877364c3724a65|What tasks did they experiment with?|Sentiment Classification, Transferability of Shared Sentence Representation, Introducing Sequence Labeling as Auxiliary Task| | 
1911.03597|b44ce9aae8b1479820555b99ce234443168dc1fe|What multilingual parallel data is used for training proposed model?|MultiUN BIBREF20, OpenSubtitles BIBREF21| | 
1808.08850|99c50d51a428db09edaca0d07f4dab0503af1b94|What kind of Youtube video transcripts did they use?|youtube video transcripts on news covering different topics like technology, human rights, terrorism and politics| | 
1808.08850|d1747b1b56fddb05bb1225e98fd3c4c043d74592|Which SBD systems did they compare?|Convolutional Neural Network , bidirectional Recurrent Neural Network model with attention mechanism| | 
1808.08850|5a29b1f9181f5809e2b0f97b4d0e00aea8996892|What makes it a more reliable metric?|It takes into account the agreement between different systems| | 
1909.02560|f5db12cd0a8cd706a232c69d94b2258596aa068c|How much in experiments is performance improved for models trained with generated adversarial examples?|"Answer with content missing: (Table 1) The performance of all the target models raises significantly, while that on the original
examples remain comparable (e.g. the overall accuracy of BERT on modified examples raises from 24.1% to 66.0% on Quora)"| | 
1909.02560|2c8d5e3941a6cc5697b242e64222f5d97dba453c|How much dramatically results drop for models on generated adversarial examples?|BERT on Quora drops from 94.6% to 24.1%| | 
1909.02560|78102422a5dc99812739b8dd2541e4fdb5fe3c7a|What is discriminator in this generative adversarial setup?| current model| | 
1909.02560|930c51b9f3936d936ee745716536a4b40f531c7f|What are benhmark datasets for paraphrase identification?|Quora, MRPC| | 
2003.08132|20eb673b01d202b731e7ba4f84efc10a18616dd3|What representations are presented by this paper?|the number of speakers of each gender category, their speech duration| | 
2001.02380|4059c6f395640a6acf20a0ed451d0ad8681bc59b|How is the delta-softmax calculated?|Answer with content missing: (Formula) Formula is the answer.| | 
2001.02380|99d7bef0ef395360b939a3f446eff67239551a9d|Are some models evaluated using this metric, what are the findings?|Yes| | 
2001.02380|a1097ce59270d6f521d92df8d2e3a279abee3e67|Where does proposed metric differ from juman judgement?|model points out plausible signals which were passed over by an annotator, it also picks up on a recurring tendency in how-to guides in which the second person pronoun referring to the reader is often the benefactee of some action| | 
2001.02380|56e58bdf0df76ad1599021801f6d4c7b77953e29|Where does proposed metric overlap with juman judgement?|influence of each word on the score of the correct relation, that impact should and does still correlate with human judgments| | 
2002.00317|e74ba39c35af53d3960be5a6c86eddd62cef859f|Which baseline performs best?|IR methods perform better than the best neural models| | 
2002.00317|458f3963387de57fdc182875c9ca3798b612b633|Which baselines are explored?|GPT2, SciBERT model of BIBREF11| | 
2002.00317|69a88b6be3b34acc95c5e36acbe069c0a0bc67d6|What is the size of the corpus?|8.1 million scientific documents, 154K computer science articles, 622K citing sentences| | 
1912.10167|da1994421934082439e8fe5071a01d3d17b56601|Are any machine translation sysems tried with these embeddings, what is the performance?|No| | 
1912.10167|30c6d34b878630736f819fd898319ac4e71ee50b|Are any experiments performed to try this approach to word embeddings?|Yes| | 
1809.02494|a4ff1b91643e0c8a0d4cc1502d25ca85995cf428|Which two datasets does the resource come from?|two surveys by two groups - school students and meteorologists to draw on a map a polygon representing a given geographical descriptor| | 
1909.07734|544e29937e0c972abcdd27c953dc494b2376dd76|What model was used by the top team?|Two different BERT models were developed| | 
1909.07734|bdc93ac1b8643617c966e91d09c01766f7503872|What is the size of the second dataset?|1 000 labeled dialogues for training and 240 unlabeled dialogues for evaluation| | 
1909.07734|4ca0d52f655bb9b4bc25310f3a76c5d744830043|How large is the first dataset?|1 000 labeled dialogues for training and 240 unlabeled dialogues for evaluation| | 
1705.07368|4c71ed7d30ee44cf85ffbd7756b985e32e8e07da|What supervised learning tasks are attempted with these representations?|document categorization, regression tasks| | 
1705.07368|1949d84653562fa9e83413796ae55980ab7318f2|What is MRR?|mean reciprocal rank| | 
1705.07368|7ee660927e2b202376849e489faa7341518adaf9|Which techniques for word embeddings and topic models are used?| skip-gram, LDA| | 
1705.07368|f6380c60e2eb32cb3a9d3bca17cf4dc5ae584eca|Why is big data not appropriate for this task?|Training embeddings from small-corpora can increase the performance of some tasks| | 
1705.07368|c7d99e66c4ab555fe3d616b15a5048f3fe1f3f0e|What is an example of a computational social science NLP task?|Visualization of State of the union addresses| | 
1808.03986|4698298d506bef02f02c80465867f2cd12d29182|What were the previous state of the art benchmarks?|BIBREF35 for VQA dataset, BIBREF5, BIBREF36| | 
1808.03986|4e2cb1677df949ee3d1d3cd10962b951da907105|How/where are the natural question generated?|Decoder that generates question using an LSTM-based language model| | 
1808.03986|9cc0fd3721881bd8e246d20fff5d15bd32365655|What is the input to the differential network?|image| | 
1808.03986|82c4863293a179fe5c0d9a1ff17d224bde952f54|How do the authors define a differential network?|The proposed Multimodal Differential Network (MDN) consists of a representation module and a joint mixture module.| | 
1808.03986|88d9d32fb7a22943e1f4868263246731a1726e6e|How do the authors define exemplars?|Exemplars aim to provide appropriate context., joint image-caption embedding for the supporting exemplar are closer to that of the target image-caption| | 
1805.11535|af82043e7d046c2fb1ed86ef9b48c35492e6a48c|Is this a task other people have worked on?|No| | 
1805.11535|1bc8904118eb87fa5949ad7ce5b28ad3b3082bd0|Where did they get the data for this project?|Twitter| | 
2001.05970|5dc1aca619323ea0d4717d1f825606b2b7c21f01|Which major geographical regions are studied?|Northeast U.S, South U.S., West U.S. and Midwest U.S.| | 
2001.05970|39c78924df095c92e058ffa5a779de597e8c43f4|How are the topics embedded in the #MeToo tweets extracted?|Using Latent Dirichlet Allocation on TF-IDF transformed from the corpus| | 
2001.05970|a95188a0f35d3cb3ca70ae1527d57ac61710afa3|How many tweets are explored in this paper?|60,000 | | 
2001.05970|a1557ec0f3deb1e4cd1e68f4880dcecda55656dd|Which geographical regions correlate to the trend?|Northeast U.S., West U.S. and South U.S.| | 
2001.05970|096f5c59f43f49cab1ef37126341c78f272c0e26|How many followers did they analyze?|51,104| | 
1706.04815|c348a8c06e20d5dee07443e962b763073f490079|What two components are included in their proposed framework?|evidence extraction and answer synthesis| | 
1706.04815|0300cf768996849cab7463d929afcb0b09c9cf2a|Which framework they propose in this paper?| extraction-then-synthesis framework| | 
1706.04815|dd8f72cb3c0961b5ca1413697a00529ba60571fe|Why MS-MARCO is different from SQuAD?|there are several related passages for each question in the MS-MARCO dataset., MS-MARCO also annotates which passage is correct| | 
1804.08094|fbd094918b493122b3bba99cefe5da80cf88959c|Did they experiment with pre-training schemes?|No| | 
1804.08094|78661bdd4d11148e07bdf17141cf088db4ad60c6|What were their results on the test set?|an official F1-score of 0.2905 on the test set| | 
1804.08094|95d98b2a7fbecd1990ec9a070f9d5624891a4f26|What is the size of the dataset?|a balanced dataset of 2,396 ironic and 2,396 non-ironic tweets is provided| | 
1804.08094|586566de02abdf20b7bfd0d5a43ba93cb02795c3|What was the baseline model?|a non-parameter optimized linear-kernel SVM that uses TF-IDF bag-of-word vectors as inputs| | 
2004.04228|dfd9302615b27abf8cbef1a2f880a73dd5f0c753|What models are evaluated with QAGS?|bert-large-wwm, bert-base, bert-large| | 
2004.04228|e09dcb6fc163bba7d704178e7edba2e630b573c2|Do they use crowdsourcing to collect human judgements?|Yes| | 
1903.07398|c8f11561fc4da90bcdd72f76414421e1527c0287|Which dataset(s) do they evaluate on?|LJSpeech| | 
1903.07398|51de39c8bad62d3cbfbec1deb74bd8a3ac5e69a8|Which modifications do they make to well-established Seq2seq architectures?|Replacing attention mechanism to query-key attention, and adding a loss to make the attention mask as diagonal as possible| | 
1903.07398|d9cbcaf8f0457b4be59178446f1a280d17a923fa|How do they measure the size of models?|Direct comparison of model parameters| | 
1903.07398|fc69f5d9464cdba6db43a525cecde2bf6ddaaa57|Do they reduce the number of parameters in their architecture compared to other direct text-to-speech models?|Yes| | 
1909.00161|e1f5531ed04d0aae1dfcb0559f1512a43134c43a|Do they use pretrained models?|Yes| | 
1909.00161|4a4b7c0d3e7365440b49e9e6b67908ea5cea687d|What are their baseline models?|Majority, ESA, Word2Vec , Binary-BERT| | 
1710.06700|da845a2a930fd6a3267950bec5928205b6c6e8e8|How was speed measured?|how long it takes the system to lemmatize a set number of words| | 
1710.06700|76ce9e02d97e2d77fe28c0fa78526809e7c195c6|What is the state of the art?| MADAMIRA BIBREF6 system| | 
1710.06700|64c7545ce349265e0c97fd6c434a5f8efdc23777|How was the dataset annotated?|Lemmatization is done by an expert Arabic linguist where spelling corrections are marked, and lemmas are provided with full diacritization| | 
1710.06700|47822fec590e840438a3054b7f512fec09dbd1e1|What is the size of the dataset?|Articles contain 18,300 words, and they are evenly distributed among these 7 genres with 10 articles per each| | 
1710.06700|989271972b3176d0a5dabd1cc0e4bdb671269c96|Where did they collect their dataset from?|from Arabic WikiNews site https://ar.wikinews.org/wiki| | 
1602.01595|b0fd686183b056ea3f63a7ab494620df1d598c24|How does the model work if no treebank is available?|train the parser on six other languages in the Google universal dependency treebanks version 2.0 (de, en, es, fr, it, pt, sv, excluding whichever is the target language), and we use gold coarse POS tags| | 
1602.01595|7065e6140dbaffadebe62c9c9d3863ca0f829d52|How many languages have this parser been tried on?|seven| | 
1910.03484|9508e9ec675b6512854e830fa89fa6a747b520c5|Do they use attention?|Yes| | 
1910.03484|a65e5c97ade6e697ec10bcf3c3190dc6604a0cd5|What non-annotated datasets are considered?|E2E NLG challenge Dataset, The Wikipedia Company Dataset| | 
1808.10113|e28a6e3d8f3aa303e1e0daff26b659a842aba97b|Did they compare to Transformer based large language models?|No| | 
1808.10113|0fce128b8aaa327ac0d58ec30cd2ecbea2019baa|Which baselines are they using?|Seq2Seq, HLSTM, HLSTM+Copy, HLSTM+Graph Attention, HLSTM+Contextual Attention| | 
1709.08299|7a7e279170e7a2f3bc953c37ee393de8ea7bd82f|What two types the Chinese reading comprehension dataset consists of?|cloze-style reading comprehension and user query reading comprehension questions| | 
1709.08299|e3981a11d3d6a8ab31e1b0aa2de96f253653cfb2|For which languages most of the existing MRC datasets are created?|English| | 
1601.00901|74b0d3ee0cc9b0a3d9b264aba9901ff97048a897|How did they induce the CFG?|the parser first learns to parse simple sentences, then proceeds to learn more complex ones. The induction method is iterative, semi-automatic and based on frequent patterns| | 
1601.00901|9eb5b336b3dcb7ab63f673ba9ab1818573cce6c3|How big is their dataset?|1.1 million sentences, 119 different relation types (unique predicates)| | 
1708.00111|0a92352839b549d07ac3f4cb997b8dc83f64ba6f|By how much do they outperform basic greedy and cross-entropy beam decoding?|2 accuracy points| | 
1708.00111|242f96142116cf9ff763e97aecd54e22cb1c8b5a|Do they provide a framework for building a sub-differentiable for any final loss metric?|Yes| | 
1708.00111|fcd0bd2db39898ee4f444ae970b80ea4d1d9b054|Do they compare partially complete sequences (created during steps of beam search) to gold/target sequences?|Yes| | 
1708.00111|5cc937c2dcb8fd4683cb2298d047f27a05e16d43|Which loss metrics do they try in their new training procedure evaluated on the output of beam search?| continuous relaxation to top-k-argmax| | 
1909.08167|37016cc987d33be5ab877013ef26ec7239b48bd9|How are different domains weighted in WDIRL?|To achieve this purpose, we introduce a trainable class weight $\mathbf {w}$ to reweigh source domain examples by class when performing DIRL, with $\mathbf {w}_i > 0$| | 
1909.08167|b3dc6d95d1570ad9a58274539ff1def12df8f474|How is DIRL evaluated?|Through the experiments, we empirically studied our analysis on DIRL and the effectiveness of our proposed solution in dealing with the problem it suffered from.| | 
1909.08167|cc5d3903913fa2e841f900372ec74b0efd5e0c71|Which sentiment analysis tasks are addressed?|12 binary-class classification and multi-class classification of reviews based on rating| | 
1911.03562|c95fd189985d996322193be71cf5be8858ac72b5|Which NLP area have the highest average citation for woman author?|sentiment analysis, information extraction, document summarization, spoken dialogue, cross lingual (research), dialogue, systems, language generation| | 
1911.03562|7955dbd79ded8ef4ae9fc28b2edf516320c1cb55|What aspect of NLP research is examined?|size, demographics, areas of research, impact, and correlation of citations with demographic attributes (age and gender)| | 
1911.03562|205163715f345af1b5523da6f808e6dbf5f5dd47|How many papers are used in experiment?|44,896 articles| | 
1912.10435|8d989490c5392492ad66e6a5047b7d74cc719f30|What ensemble methods are used for best model?|choosing the answer from the network that had the highest probability and choosing no answer if any of the networks predicted no answer| | 
1912.10435|a7829abed2186f757a59d3da44893c0172c7012b|What hyperparameters have been tuned?|number of coattention blocks, the batch size, and the number of epochs trained and ensembled our three best networks| | 
1903.00058|d72548fa4d29115252605d5abe1561a3ef2430ca|Where do they retrieve neighbor n-grams from in their approach?|represent every sentence by their reduced n-gram set| | 
1903.00058|24d06808fa3b903140659ee5a471fdfa86279980|To which systems do they compare their results against?|standard Transformer Base model| | 
1903.00058|dba3d05c495e2c8ca476139e78f65059db2eb72d|Does their combination of a non-parametric retrieval and neural network get trained end-to-end?|Yes| | 
1903.00058|0062ad4aed09a57d0ece6aa4b873f4a4bf65d165|Which similarity measure do they use in their n-gram retrieval approach?|we define the similarity between INLINEFORM7 and INLINEFORM8 by, DISPLAYFORM0| | 
1603.04513|67a28fe78f07c1383176b89e78630ee191cf15db|Where is MVCNN pertained?|on the unlabeled data of each task| | 
1603.04513|eddb18109495976123e10f9c6946a256a55074bd|How is MVCNN compared to CNN?|MVCNN, a novel CNN architecture for sentence classification. It combines multichannel initialization – diverse versions of pretrained word embeddings are used – and variable-size filters – features of multigranular phrases are extracted with variable-size convolution filters. | | 
1909.04181|e9cfe3f15735e2b0d5c59a54c9940ed1d00401a2|Does the paper report F1-scores for the age and language variety tasks?|No| | 
1909.04181|52ed2eb6f4d1f74ebdc4dcddcae201786d4c0463|Are the models compared to some baseline models?|Yes| | 
1909.04181|2c576072e494ab5598667cd6b40bc97fdd7d92d7|What are the in-house data employed?|we manually label an in-house dataset of 1,100 users with gender tags, we randomly sample 20,000 tweets for each class from an in-house dataset gold labeled with the same 15 classes as the shared task| | 
1909.04181|8602160e98e4b2c9c702440da395df5261f55b1f|What are the three datasets used in the paper?|Data released for APDA shared task contains 3 datasets.| | 
1709.05563|3aa43a0d543b88d40e4f3500c7471e263515be40|What elements of natural language processing are proposed to analyze qualitative data?|translated the responses in multiple languages into English using machine translation, words without functional meaning (e.g. `I'), rare words that occurred in only one narrative, numbers, and punctuation were all removed, remaining words were stemmed to remove plural forms of nouns or conjugations of verbs| | 
1909.01093|d82ec1003a3db7370994c7522590f7e5151b1f33|How does the method measure the impact of the event on market prices?|We collected the historical 52 week stock prices prior to this event and calculated the daily stock price change. The distribution of the daily price change of the previous 52 weeks is Figure FIGREF13 with a mean INLINEFORM1 and standard deviation INLINEFORM2 . | | 
1909.01093|58f08d38bbcffb2dd9d660faa8026718d390d64b|How is sentiment polarity measured?|For each cluster, its overall sentiment score is quantified by the mean of the sentiment scores among all tweets| | 
1909.00252|89e1e0dc5d15a05f8740f471e1cb3ddd296b8942|Which part of the joke is more important in humor?|the punchline of the joke | | 
1909.00252|de03e8cc1ceaf2108383114460219bf46e00423c|What kind of humor they have evaluated?|a subset of the population where we can quantitatively measure reactions: the popular Reddit r/Jokes thread, These Reddit posts consist of the body of the joke, the punchline, and the number of reactions or upvotes. | | 
1909.00252|8a276dfe748f07e810b3944f4f324eaf27e4a52c|How they evaluate if joke is humorous or not?|The distribution of joke scores varies wildly, ranging from 0 to 136,354 upvotes. We found that there is a major jump between the 0-200 upvote range and the 200 range and onwards, with only 6% of jokes scoring between 200-20,000. We used this natural divide as the cutoff to decide what qualified as a funny joke, giving us 13884 not-funny jokes and 2025 funny jokes.| | 
1901.03438|0716b481b78d80b012bca17c897c62efbe7f3731|Do they report results only on English data?|Yes| | 
1901.03438|fed0785d24375ebbde51fb0503b93f14da1d8583|Do the authors have a hypothesis as to why morphological agreement is hardly learned by any model?|These models are likely to be deficient in encoding morphological features is that they are word level models, and do not have direct access sub-word information like inflectional endings, which indicates that these features are difficult to learn effectively purely from lexical distributions.| | 
1901.03438|675d7c48541b6368df135f71f9fc13a398f0c8c6|Which models are best for learning long-distance movement?|the transformer models| | 
1901.03438|868c69c8f623e30b96df5b5c8336070994469f60|Where does the data in CoLA come from?| CoLA contains example sentences from linguistics publications labeled by experts| | 
1901.03438|f809fd0d3acfaccbe6c8abb4a9d951a83eec9a32|How is the CoLA grammatically annotated?|labeled by experts| | 
1808.09920|c4a6b727769328333bb48d59d3fc4036a084875d|What baseline did they compare Entity-GCN to?|Human, FastQA, BiDAF, Coref-GRU, MHPGM, Weaver / Jenga, MHQA-GRN| | 
1808.09920|93e8ce62361b9f687d5200d2e26015723721a90f|Did they use a relation extraction method to construct the edges in the graph?|No| | 
1808.09920|d05d667822cb49cefd03c24a97721f1fe9dc0f4c|How did they get relations between mentions?|Assign a value to the relation based on whether mentions occur in the same document, if mentions are identical, or if mentions are in the same coreference chain.| | 
1808.09920|2a1e6a69e06da2328fc73016ee057378821e0754|How did they detect entity mentions?|Exact matches to the entity string and predictions from a coreference resolution system| | 
1904.04019|37db7ba2c155c2f89fc7fb51fffd7f193c103a34|What classical machine learning algorithms are used?|Support Vector Machine (SVM), Logistic regression (Log.Reg), Random Forest (RF), gradient boosting (XGB)| | 
1904.04019|c2cbc2637761a2c2cf50f5f8caa248814277430e|What are the different methods used for different corpora?|Support Vector Machine (SVM), Logistic regression (Log.Reg), Random Forest (RF) and gradient boosting (XGB)| | 
1904.04019|774ead7c642f9a6c59cfbf6994c07ce9c6789a35|In which domains is sarcasm conveyed in different ways?|Amazon reviews| | 
1802.00923|d86c7faf5a61d73a19397a4afa2d53206839b8ad|What modalities are being used in different datasets?|Language, Vision, Acoustic| | 
1802.00923|082bc58e1a2a65fc1afec4064a51e4c785674fd7|What is the difference between Long-short Term Hybrid Memory and LSTMs?|Long-short Term Hybrid Memory (LSTHM) is an extension of the Long-short Term Memory (LSTM) | | 
1809.01060|6b7d76c1e1a2490beb69609ba5652476dde8831b|What provisional explanation do the authors give for the impact of document context?|adding context causes speakers to focus on broader semantic and pragmatic issues of discourse coherence| | 
1809.01060|37753fbffc06ce7de6ada80c89f1bf5f190bbd88|What document context was added?|Preceding and following sentence of each metaphor and paraphrase are added as document context| | 
1809.01060|7ee29d657ccb8eb9d5ec64d4afc3ca8b5f3bcc9f|What were the results of the first experiment?|Best performance achieved is 0.72 F1 score| | 
1705.03261|b42323d60827ecf0d9e478c9a31f90940cfae975|How big is the evaluated dataset?|contains thousands of XML files, each of which are constructed by several records| | 
1705.03261|1a69696034f70fb76cd7bb30494b2f5ab97e134d|By how much does their model outperform existing methods?|Answer with content missing: (Table II) Proposed model has F1 score of  0.7220 compared to 0.7148 best state-state-of-the-art result.| | 
1705.03261|9a596bd3a1b504601d49c2bec92d1592d7635042|What is the performance of their model?|Answer with content missing: (Table II) Proposed model has F1 score of  0.7220.| | 
1705.03261|1ba28338d3f993674a19d2ee2ec35447e361505b|What are the existing methods mentioned in the paper?|Chowdhury BIBREF14 and Thomas et al. BIBREF11, FBK-irst BIBREF10, Liu et al. BIBREF9, Sahu et al. BIBREF12| | 
2002.08899|8ec94313ea908b6462e1f5ee809a977a7b6bdf01|Does having constrained neural units imply word meanings are fixed across different context?|No| | 
2002.08899|f052444f3b3bf61a3f226645278b780ebd7774db|Do they perform a quantitative analysis of their model displaying knowledge distortions?|Yes| | 
1809.08652|790ed4458a23aa23e2b5399259e50083c86d9e14|Do all the instances contain code-switching?|No| | 
1809.08652|562a2feeba9580f5435a94396f2a8751f79a4d5c|What embeddings do they use?|Glove, Twitter word2vec| | 
1809.08652|d4e78d3205e98cafdf93ddaa95627ea00b7c4d55|Do they perform some annotation?|No| | 
1809.08652|8b51db1f7a6293b5bd8be49386cce45989b8079f|Do they use dropout?|Yes| | 
1803.07771|0b9021cefca71081e617a362e7e3995c5f1d2a88|What are the other models they compare to?|CNN-C, CNN-W, CNN-Lex-C, CNN-Lex-W, Bi-LSTM-C , Bi-LSTM-W, Lex-rule, BOW| | 
1803.07771|2d307b43746be9cedf897adac06d524419b0720b|How long are the datasets?|Travel dataset contains 4100 raw samples, 11291 clauses, Hotel dataset contains 3825 raw samples, 11264 clauses, and the Mobile dataset contains 3483 raw samples and 8118 clauses| | 
1803.07771|fe90eec1e3cdaa41d2da55864c86f6b6f042a56c|What are the sources of the data?|User reviews written in Chinese collected online for hotel, mobile phone, and travel domains| | 
1803.07771|9d5df9022cc9eb04b9f5c5a9d8308a332ebdf50c|What is the new labeling strategy?|They use a two-stage labeling strategy where in the first stage single annotators label a large number of short texts with relatively pure sentiment orientations and in the second stage multiple annotators label few text samples with mixed sentiment orientations| | 
1911.06171|dbf606cb6fc1d070418cc25e38ae57bbbb7087a0|Which future direction in NLG are discussed?|1) How to introduce unsupervised pre-training into NLG tasks with cross-modal context?, 2) How to design a generic pre-training algorithm to fit a wide range of NLG tasks?, 3) How to reduce the computing resources required for large-scale pre-training?, 4) What aspect of knowledge do the pre-trained models provide for better language generation?| | 
1911.06171|9651fbd887439bf12590244c75e714f15f50f73d|What experimental phenomena are presented?|The advantage of pre-training gradually diminishes with the increase of labeled data, Fixed representations yield better results than fine-tuning in some cases, pre-training the Seq2Seq encoder outperforms pre-training the decoder| | 
1911.06171|1fd969f53bc714d9b5e6604a7780cbd6b12fd616|How strategy-based methods handle obstacles in NLG?|fine-tuning schedules that elaborately design the control of learning rates for optimization, proxy tasks that leverage labeled data to help the pre-trained model better fit the target data distribution, knowledge distillation approaches that ditch the paradigm of initialization with pre-trained parameters by adopting the pre-trained model as a teacher network| | 
1911.06171|cd37ad149d500e1c7d2de9de1f4bae8dcc443a72|How architecture-based method handle obstacles in NLG?|task-specific architecture during pre-training (task-specific methods), aim at building a general pre-training architecture to fit all downstream tasks (task-agnostic methods)| | 
1907.05403|14eb2b89ba39e56c52954058b6b799a49d1b74bf|How are their changes evaluated?|The changes are evaluated based on accuracy of intent and entity recognition on SNIPS dataset| | 
1707.06945|6b8a3100895f2192e08973006474428319dc298e|What clustering algorithm is used on top of the VerbNet-specialized representations?|MNCut spectral clustering algorithm BIBREF58| | 
1707.06945|74261f410882551491657d76db1f0f2798ac680f|What are the six target languages?|Answer with content missing: (3 Experimental Setup) We experiment with six target languages: French (FR), Brazilian Portuguese (PT), Italian (IT), Polish (PL), Croatian (HR), and Finnish (FI).| | 
1803.03786|3d34a02ceebcc93ee79dc073c408651d25e538bc|what classifiers were used in this paper?|Support Vector Machines (SVM) classifier| | 
1803.03786|96992460cfc5f0b8d065ee427067147293746b7a|what are their evaluation metrics?|F1, accuracy| | 
1803.03786|363ddc06db5720786ed440927d7fbb7d0a8078ae|what types of features were used?|stylometric, lexical, grammatical, and semantic| | 
1803.03786|f12a282571f842b818d4bee86442751422b52337|what lexical features did they experiment with?|TF.IDF-based features| | 
1803.03786|5b1cd21936aeec85233c978ba8d7282931522a3a|what is the size of the dataset?|The number of fake news that have a duplicate in the training dataset are 1018 whereas, the number of articles with genuine content that have a duplicate article in the training set is 322.| | 
1803.03786|964705a100e53a9181d1a5ac8150696de12ecaf0|what datasets were used?| training dataset contains 2,815 examples, 761 testing examples| | 
1909.01638|c65b6470b7ed0a035548cc08e0bc541c2c4a95a7|How are seed dictionaries obtained by fully unsupervised methods?|the latest CLWE developments almost exclusively focus on fully unsupervised approaches BIBREF23 , BIBREF24 , BIBREF25 , BIBREF26 , BIBREF27 , BIBREF28 , BIBREF29 , BIBREF30 : they fully abandon any source of (even weak) supervision and extract the initial seed dictionary by exploiting topological similarities between pre-trained monolingual embedding spaces| | 
1909.01638|6e2899c444baaeb0469599f65722780894f90f29|How does BLI measure alignment quality?|we use mean average precision (MAP) as the main evaluation metric| | 
1802.05574|63c0128935446e26eacc7418edbd9f50cba74455|What is the size of the released dataset?|440 sentences, 2247 triples extracted from those sentences, and 11262 judgements on those triples.| | 
1802.05574|18e915b917c81056ceaaad5d6581781c0168dac9|What is the most common error type?|all annotators that a triple extraction was incorrect| | 
1802.05574|9c68d6d5451395199ca08757157fbfea27f00f69|Which OpenIE systems were used?|OpenIE4 and MiniIE| | 
1802.05574|372fbf2d120ca7a101f70d226057f9639bf1f9f2|What is the role of crowd-sourcing?|Crowd workers were asked to mark whether a triple was correct, namely, did the triple reflect the consequence of the sentence.| | 
1907.06458|f14ff780c28addab1d738f676c4ec0b4106356b6|How are meta vertices computed?|Meta vertex candidate identification. Edit distance and word lengths distance are used to determine whether two words should be merged into a meta vertex (only if length distance threshold is met, the more expensive edit distance is computed)., The meta vertex creation. As common identifiers, we use the stemmed version of the original vertices and if there is more than one resulting stem, we select the vertex from the identified candidates that has the highest centrality value in the graph and its stemmed version is introduced as a novel vertex (meta vertex).| | 
1907.06458|b799936d6580c0e95102027175d3fe184f0ee253|How are graphs derived from a given text?|The given corpus is traversed, and for each element INLINEFORM6 , its successor INLINEFORM7 , together with a given element, forms a directed edge INLINEFORM8 . Finally, such edges are weighted according to the number of times they appear in a given corpus. Thus the graph, constructed after traversing a given corpus, consists of all local neighborhoods (order one), merged into a single joint structure. Global contextual information is potentially kept intact (via weights), even though it needs to be detected via network analysis| | 
1705.00108|c000a43aff3cb0ad1cee5379f9388531b5521e9a|how are the bidirectional lms obtained?|They pre-train forward and backward LMs separately, remove top layer softmax, and concatenate to obtain the bidirectional LMs.| | 
1705.00108|12cfbaace49f9363fcc10989cf92a50dfe0a55ea|what results do they achieve?|91.93% F1 score on CoNLL 2003 NER task and 96.37% F1 score on CoNLL 2000 Chunking task| | 
1705.00108|a9c5252173d3df1c06c770c180a77520de68531b|what are the evaluation datasets?|CoNLL 2003, CoNLL 2000| | 
2002.06053|a45edc04277a458911086752af4f17405501230f|Are datasets publicly available?|Yes| | 
2002.06053|8c8a32592184c88f61fac1eef12c7d233dbec9dc|Are this models usually semi/supervised or unsupervised?|Both supervised and unsupervised, depending on the task that needs to be solved.| | 
2002.06053|16646ee77975fed372b76ce639e2664ae2105dcf|Is there any concrete example in the paper that shows that this approach had huge impact on drug discovery?|Yes| | 
1712.03547|9c0cf1630804366f7a79a40934e7495ad9f32346|Do the authors analyze what kinds of cases their new embeddings fail in where the original, less-interpretable embeddings didn't?|No| | 
1712.03547|9ac923be6ada1ba2aa20ad62b0a3e593bb94e085|How do they evaluate interpretability?|For evaluating the interpretability, we use $Coherence@k$ (Equation 6 ) , automated and manual word intrusion tests.| | 
1908.07218|3b995a7358cefb271b986e8fc6efe807f25d60dc|What types of word representations are they evaluating?|GloVE; SGNS| | 
1707.05853|7cf726db952c12b1534cd6c29d8e7dfa78215f9e|What is a word confusion network?|It is a network used to encode speech lattices to maintain a rich hypothesis space.| | 
1912.00423|f9751e0ca03f49663a5fc82b33527bc8be1ed0aa|What type of simulations of real-time data feeds are used for validaton?|simplified set of input data, in a variety of different formats that occur frequently in a healthcare setting| | 
1912.00423|ce18c50dadab7b9f28141fe615fd7de69355d9dd|How are FHIR and RDF combined?|RDF was designed as an abstract information model and FHIR was designed for operational use in a healthcare setting, RDF makes statements of fact, whereas FHIR makes records of events, RDF is intended to have the property of monotonicity, meaning that previous facts cannot be invalidated by new facts| | 
1912.00423|5a230fe4f0204bf2eebc0e944cf8defaf33d165c|What are the differences between FHIR and RDF?|One of the several formats into which FHIR can be serialized is RDF, there is the potential for a slight mismatch between the models| | 
1912.00423|d3bb06d730efbedd30ec226fe8cf828a4773bf5c|What do FHIR and RDF stand for?|Health Level Seven Fast Healthcare Interoperability Resources (HL7 FHIR), Resource Description Framework (RDF)| | 
1909.05017|2255c36c8c7ed6084da577b480eb01d349f52943|What is the motivation behind the work? Why question generation is an important task?|Such a system would benefit educators by saving time to generate quizzes and tests.| | 
1909.05017|9e391c8325b48f6119ca4f3d428b1b2b037f5c13|Why did they choose WER as evaluation metric?|WER can reflect our model's effectiveness in generating questions that are similar to those of SQuAD, WER can be used for initial analyses| | 
1912.01046|5bcc12680cf2eda2dd13ab763c42314a26f2d993|What evaluation metrics were used in the experiment?|For sentence-level prediction they used tolerance accuracy, for segment retrieval accuracy and MRR and for the pipeline approach they used overall accuracy| | 
1912.01046|7a53668cf2da4557735aec0ecf5f29868584ebcf|What kind of instructional videos are in the dataset?|tutorial videos for a photo-editing software| | 
1912.01046|8051927f914d730dfc61b2dc7a8580707b462e56|What baseline algorithms were presented?|a sentence-level prediction algorithm, a segment retrieval algorithm and a pipeline segment retrieval algorithm| | 
1912.01046|09621c9cd762e1409f22d501513858d67dcd3c7c|What is the source of the triples?|a tutorial website about an image editing program | | 
1912.07976|10ddac87daf153cf674589cc1c64a795907d5d9a|How much better is performance of the proposed model compared to the state of the art in these various experiments?|significantly improves the accuracy and F1 score of aspect polarity classification| | 
1912.07976|6cd874c4ae8e70f3c98c7176191c13a7decfbc45|What was state of the art on SemEval-2014 task4 Restaurant and Laptop dataset?|BERT-ADA, BERT-PT, AEN-BERT, SDGCN-BERT| | 
1912.07976|b807dd3d42251615b881632caa5e331e2203d269|What was previous state-of-the-art on four Chinese reviews datasets?|GANN obtained the state-of-the-art APC performance on the Chinese review datasets| | 
1912.07976|d39c911bf2479fdb7af339b59acb32073242fab3|In what four Chinese review datasets does LCF-ATEPC achieves state of the art?|Car, Phone, Notebook, Camera| | 
1703.05916|7aaaf7bff9947c6d3b954ae25be87e6e1c49db6d|did they use a crowdsourcing platform for annotations?|Yes| | 
1703.05916|4a2248e1c71c0b0183ab0d225440cae2da396b8d|where does the data come from?|Evaluation Dataset of Japanese Lexical Simplification kodaira| | 
1909.09268|1244cf6d75e3aa6d605a0f4b141c015923a3f2e7|What is the criteria for a good metric?|The first one is that it should be highly correlated with human judgement of similarity. The second one is that it should be able to distinguish sentences which are in logical contradiction, logically unrelated or in logical agreement. The third one is that a robust evaluator should also be able to identify unintelligible sentences. The last criteria is that a good evaluation metric should not give high scores to semantically distant sentences and low scores to semantically related sentences.| | 
1909.09268|c8b9b962e4d40c50150b2f8873a4004f25398464|What are the three limitations?|High scores to semantically opposite translations/summaries, Low scores to semantically related translations/summaries and High scores to unintelligible translations/summaries.| | 
1911.06192|616c205142c7f37b3f4e81c0d1c52c79f926bcdc|What is current state-of-the-art model?|SUMBT BIBREF17 is the current state-of-the-art model on WOZ 2.0, TRADE BIBREF3 is the current published state-of-the-art model| | 
1910.00194|496e81769a8d9992dae187ed60639ff2eec531f3|Which language(s) are found in the WSD datasets?| WSD is predominantly evaluated on English, we are also interested in evaluating our approach on Chinese| | 
1910.00194|f103789b85b00ec973076652c639bd31c605381e|What datasets are used for testing?|Senseval-2 (SE2), Senseval-3 (SE3), SemEval 2013 task 12 (SE13), and SemEval 2015 task 13 (SE15), OntoNotes Release 5.0| | 
2003.06044|4e841138f307839fd2c212e9f02489e27a5f830c|What is dialogue act recognition?|DA recognition is aimed to assign a label to each utterance in a conversation. It can be formulated as a supervised classification problem. | | 
1909.04453|479d334b79c1eae3f2aa3701d28aa0d8dd46036a|Does the performance necessarily drop when more control is desired?|Yes| | 
1909.04453|a035472a5c6cf238bed62b63d28100c546d40bd5|How is the model trained to do only content selection?|target some heuristically extracted contents, treat INLINEFORM1 as a latent variable and co-train selector and generator by maximizing the marginal data likelihood, reinforcement learning to approximate the marginal likelihood,  Variational Reinforce-Select (VRS) which applies variational inference BIBREF10 for variance reduction| | 
1902.00821|3213529b6405339dfd0c1d2a0f15719cdff0fa93|What is the baseline model used?|The baseline models used are DrQA modified to support answering no answer questions, DrQA+CoQA which is pre-tuned on CoQA dataset, vanilla BERT, BERT+review tuned on domain reviews, BERT+CoQA tuned on the supervised CoQA data| | 
1909.04556|70afd28b0ecc02eb8e404e7ff9f89879bf71a670|Is this auto translation tool based on neural networks?|Yes| | 
1909.04556|42c02c554ab4ceaf30a8ca770be4f271887554c2|What are results of public code repository study?|Non-English code is a large-scale phenomena., Transliteration is common in identifiers for all languages., Languages clusters into three distinct groups based on how speakers use identifiers/comments/transliteration., Non-latin script users write comments in their L1 script but write identifiers in English., Right-to-left (RTL) language scripts, such as Arabic, have no observed prevalence on GitHub identifiers| | 
2002.01359|5f0bb32d70ee8e4c4c59dc5c193bc0735fd751cc|Where is the dataset from?|dialogue simulator| | 
2002.01359|a88a454ac1a1230263166fd824e5daebb91cb05a|What data augmentation techniques are used?|back translation between English and Chinese| | 
2002.01359|a6b99b7f32fb79a7db996fef76e9d83def05c64b|How are the models evaluated?|Active Intent Accuracy, Requested Slot F1, Average Goal Accuracy, Joint Goal Accuracy| | 
1612.05270|ef8099e2bc0ac4abc4f8216740e80f2fa22f41f6|What eight language are reported on?|Spanish, English, Italian, Arabic, German, Portuguese, Russian and Swedish| | 
1612.05270|1e68a1232ab09b6bff506e442acc8ad742972102|What are the components of the multilingual framework?|text-transformations to the messages, vector space model, Support Vector Machine| | 
2002.09637|0bd992a6a218331aa771d922e3c7bb60b653949a|Is the proposed method compared to previous methods?|Yes| | 
1705.03151|7b89515d731d04dd5cbfe9c2ace2eb905c119cbc|Which is the baseline model?|The three baseline models are the i-vector model, a standard RNN LID system and a multi-task RNN LID system. | | 
1705.03151|79a28839fee776d2fed01e4ac39f6fedd6c6a143|What is the main contribution of the paper? |Proposing an improved RNN model, the phonetic temporal neural LID approach, based on phonetic features that results in better performance| | 
1811.01001|da2b43d7d048f3f59adf26a67ce66bd2d8a06326|What training settings did they try?|Training and testing are done in alternating steps: In each epoch, for training, we first present to an LSTM network 1000 samples in a given language, which are generated according to a certain discrete probability distribution supported on a closed finite interval. We then freeze all the weights in our model, exhaustively enumerate all the sequences in the language by their lengths, and determine the first $k$ shortest sequences whose outputs the model produces inaccurately. , experimented with 1, 2, 3, and 36 hidden units for $a^n b^n$ ; 2, 3, 4, and 36 hidden units for $a^n b^n c^n$ ; and 3, 4, 5, and 36 hidden units for $a^n b^n c^n d^n$ . , Following the traditional approach adopted by BIBREF7 , BIBREF12 , BIBREF9 and many other studies, we train our neural network as follows. At each time step, we present one input character to our model and then ask it to predict the set of next possible characters, based on the current character and the prior hidden states. Given a vocabulary $\mathcal {V}^{(i)}$ of size $d$ , we use a one-hot representation to encode the input values; therefore, all the input vectors are $d$ -dimensional binary vectors. The output values are $(d+1)$ -dimensional though, since they may further contain the termination symbol $\dashv $ , in addition to the symbols in $\mathcal {V}^{(i)}$ . The output values are not always one-hot encoded, because there can be multiple possibilities for the next character in the sequence, therefore we instead use a $k$ -hot representation to encode the output values. Our objective is to minimize the mean-squared error (MSE) of the sequence predictions.| | 
1811.01001|b7708cbb50085eb41e306bd2248f1515a5ebada8|How do they get the formal languages?|These are well-known formal languages some of which was used in the literature to evaluate the learning capabilities of RNNs.| | 
1811.01001|17988d65e46ff7d756076e9191890aec177b081e|Are the unobserved samples from the same distribution as the training data?|No| | 
1908.11860|11c77ee117cb4de825016b6ccff59ff021f84a38|By how much does their model outperform the baseline in the cross-domain evaluation?|$2.2\%$ absolute accuracy improvement on the laptops test set, $3.6\%$ accuracy improvement on the restaurants test set| | 
1908.11860|0b92fb692feb35d4b4bf4665f7754d283d6ad5f3|What are the performance results?|results that for the in-domain training case, our models BERT-ADA Lapt and BERT-ADA Rest achieve performance close to state-of-the-art on the laptops dataset, new state-of-the-art on the restaurants dataset with accuracies of $79.19\%$ and $87.14\%$, respectively.| | 
1810.13024|521a7042b6308e721a7c8046be5084bc5e8ca246|What is a confusion network or lattice?|graph-like structures where arcs connect nodes representing multiple hypothesized words, thus allowing multiple incoming arcs unlike 1-best sequences| | 
1910.08987|06776b8dfd1fe27b5376ae44436b367a71ff9912|What dataset is used for training?|Mandarin dataset, Cantonese dataset| | 
1701.09123|f463db61de40ae86cf5ddd445783bb34f5f8ab67|what are the baselines?|Perceptron model using the local features.| | 
2002.02427|3d7ab856a5cade7ab374fc2f2713a4d0a30bbd56|What multilingual word representations are used?| a multilingual word representation which aims to learn a linear mapping from a source to a target embedding space| | 
2002.02427|212977344f4bf2ae8f060bdac0317db2d1801724|Do the authors identify any cultural differences in irony use?|No| | 
2002.02427|0c29d08f766b06ceb2421aa402e71a2d65a5a381|What neural architectures are used?|Convolutional Neural Network (CNN)| | 
2002.02427|c9ee70c481c801892556eb6b9fd8ee38197923be|What text-based features are used?|language-independent (e.g., punctuation marks, positive and negative emoticons, quotations, personal pronouns, tweet's length, named entities),  language-dependent relying on dedicated lexicons (e.g., negation, opinion lexicons, opposition words)| | 
2002.02427|a24a7a460fd5e60d71a7e787401c68caa4702df6|What monolingual word representations are used?|AraVec for Arabic, FastText for French, and Word2vec Google News for English.| | 
1710.08396|e8d1792fc56a32bd4c95f61c2ea4cf29088edd7c|Does the proposed method outperform a baseline?|No| | 
1710.08396|ceda2a4872132b8e0a526c0f2c701d0df060c3af|What type of RNN is used?|RNN, LSTM| | 
1807.09671|5758ebff49807a51d080b0ce10ba3f86dcf71925|What do they constrain using integer linear programming?|low-rank approximation of the co-occurrence matrix| | 
1807.09671|e84ba95c9a188fda4563f45e53fbc8728d8b5dab|Do they build one model per topic or on all topics?|One model per topic.| | 
1807.09671|caf9819be516d2c5a7bfafc80882b07517752dfa|Do they quantitavely or qualitatively evalute the output of their low-rank approximation to verify the grouping of lexical items?|They evaluate quantitatively.| | 
1807.09671|b1e90a546dc92e96b657fff5dad8e89f4ac6ed5e|Do they evaluate their framework on content of low lexical variety?|No| | 
1610.08597|f8d32088d17b32b0c877d59965b35c4f51f0ceea|Do the authors report on English datasets only?|Yes| | 
1610.08597|4f0f446bf4518af7f539f6283145135192d5c00b|Which supervised learning algorithms are used in the experiments?|Logistic Regression (LR), Random Forest (RF), Support Vector Machines (SVM)| | 
1610.08597|663b36f99ad2422f4d3a8c6398ebf55ceab7770d|How in YouTube content translated into a vector format?|words extracted from YouTube video comments and descriptions for all YouTube videos shared in the user's timeline| | 
1610.08597|be595b2017545b0359db6abf4914a155bdd10d23|How is the ground truth of gang membership established in this dataset?| text in their tweets and profile descriptions, their emoji use, their profile images, and music interests embodied by links to YouTube music videos, can help a classifier distinguish between gang and non-gang member profiles| | 
1809.08386|79b174d20ea5dd4f35e25c9425fb97f40e27cd6f|Do they evaluate ablated versions of their CNN+RNN model?|No| | 
1611.00514|21a96b328b43a568f9ba74cbc6d4689dbc4a3d7b|Do they single out a validation set from the fixed SRE training set?|No| | 
1611.00514|442f8da2c988530e62e4d1d52c6ec913e3ec5bf1|Which are the novel languages on which SRE placed emphasis on?|Cebuano and Mandarin, Tagalog and Cantonese| | 
1709.10445|ae60079da9d3d039965368acbb23c6283bc3da94|Does this approach perform better than context-based word embeddings?|Yes| | 
1709.10445|83f567489da49966af3dc5df2d9d20232bb8cb1e|Have the authors tried this approach on other languages?|No| | 
1806.01733|ff0f77392abc905fe76e0b8c28a76dfb0372a0ec|What features did they train on?|direct similarity over ConceptNet Numberbatch embeddings, the relationships inferred over ConceptNet by SME, features that compose ConceptNet with other resources (WordNet and Wikipedia), and a purely corpus-based feature that looks up two-word phrases in the Google Books dataset| | 
2002.09758|4cc5ba404d6a47363f119d9db7266157d3bb246b|What off-the-shelf QA model was used to answer sub-questions?|$\textsc {BERT}_{\textsc {BASE}}$ ensemble from BIBREF3| | 
2002.09758|1d72770d075b22411ec86d8bdee532f8c643740b|How large is the improvement over the baseline?|3.1 F1 gain on the original dev set, 11 F1 gain on the multi-hop dev set, 10 F1 gain on the out-of-domain dev set.| | 
2002.09758|af1439c68b28c27848203f863675946380d28943|What is the strong baseline that this work outperforms?|RoBERTa baseline| | 
1911.11933|046ff04d1018447b22e00acb125125cae5a23fb7|Which dataset do they use?|small_parallel_enja, Asian Scientific Paper Excerpt Corpus (ASPEC) BIBREF5| | 
1911.11933|5a06f11aa75a8affde3d595c40fb03e06769e368|Do they trim the search space of possible output sequences?|No| | 
1911.11933|ffbd6f583692db66b719a846ba2b7f6474df481a|Which model architecture do they use to build a model?|model is composed of an encoder (§SECREF5) and a decoder with the attention mechanism (§SECREF7) that are both implemented using recurrent neural networks (RNNs)| | 
1911.11933|74fe054f5243c8593ddd2c0628f91657246b7dfa|Do they compare simultaneous translation performance to regular machine translation?|No| | 
1911.11933|cc2b98b46497c71e955e844fb36e9ef6e2784640|Which metrics do they use to evaluate simultaneous translation?|BLEU BIBREF8, RIBES BIBREF9, token-level delay| | 
1909.00997|a7f07ae48eed084c3144214228f4ecb72bc0a0e3|What models other than SAN-VOES are trained on new PlotQA dataset?|IMG-only, QUES-only, SAN, SANDY,  VOES-Oracle, VOES| | 
1611.09441|eced6a6dffe43c28e6d06ab87eed98c135f285a3|Do the authors report only on English language data?|Yes| | 
1611.09441|7fdeef2b1c8f6bd5d7c3a44e533d8aae2bbc155f|What dataset of tweets is used?|tweets about `ObamaCare' in USA collected during march 2010| | 
1611.09441|be074c880263f56e0d4a8f42d9a95d2d77ac2280|What external sources of information are used?|landing pages of URLs| | 
1611.09441|2a57fdc7e985311989b6829c1ceb201096e5c809|What linguistic features are used?|Parts of Speech (POS) tags, Prior polarity of the words, Capitalization, Negation, Text Feature| | 
1912.08320|53807f435d33fe5ce65f5e7bda7f77712194f6ab|What are the key issues around whether the gold standard data produced in such an annotation is reliable? | only 1 in 9 qualitative papers in Human-Computer Interaction reported inter-rater reliability metrics, low-effort responses from crowdworkers| | 
1912.08320|2ec9c1590c96f17a66c7d4eb95dc5d3a447cb973|How were the machine learning papers from ArXiv sampled?|sampled all papers published in the Computer Science subcategories of Artificial Intelligence (cs.AI), Machine Learning (cs.LG), Social and Information Networks (cs.SI), Computational Linguistics (cs.CL), Computers and Society (cs.CY), Information Retrieval (cs.IR), and Computer Vision (CS.CV), the Statistics subcategory of Machine Learning (stat.ML), and Social Physics (physics.soc-ph), filtered for papers in which the title or abstract included at least one of the words “machine learning”, “classif*”, or “supervi*” (case insensitive), filtered to papers in which the title or abstract included at least “twitter” or “tweet” (case insensitive)| | 
1912.08320|208e667982160cfbce49ef49ad96f6ab094292ac|What are the core best practices of structured content analysis?|“coding scheme” is defined, coders are trained with the coding scheme, Training sometimes results in changes to the coding scheme, calculation of “inter-annotator agreement” or “inter-rater reliability.”, there is a process of “reconciliation” for disagreements| | 
1912.08320|35eb8464e934a2769debe14148667c61bf1da243|In what sense is data annotation similar to structured content analysis? |structured content analysis (also called “closed coding”) is used to turn qualitative or unstructured data of all kinds into structured and/or quantitative data, Projects usually involve teams of “coders” (also called “annotators”, “labelers”, or “reviewers”), with human labor required to “code”, “annotate”, or “label” a corpus of items.| | 
2004.04315|5774e019101415a43e0b5a780179fd897fc013fd|What additional information is found in the dataset?|the full tweet object including the id of the tweet, username, hashtags, and geolocation of the tweet| | 
2004.04315|1b046ec7f0e1a33e77078bedef7e83c5c07b61de|Over what period of time were the tweets collected?|from January 1, 2020 until April 15, 2020| | 
2004.04315|19cdce39e8265e7806212eeee2fd55f8ef2f3d47|How big is the dataset?|more than 3,934,610 million tweets| | 
1901.00570|524abe0ab77db168d5b2f0b68dba0982ac5c1d8e|Do the authors suggest any future extensions to this work?|Yes| | 
1901.00570|858c51842fc3c1f3e6d2d7d853c94f6de27afade|Which of the classifiers showed the best performance?|Logistic regression| | 
1901.00570|7c9c73508da628d58aaadb258f3a9d4cc2a8a9b3|Were any other word similar metrics, besides Jaccard metric, tested?|Yes| | 
1901.00570|7b2bf0c1a24a2aa01d49f3c7e1bdc7401162c116|How are the keywords associated with events such as protests selected?|By using a Bayesian approach  and by using word-pairs, where they extract all the pairs of co-occurring words within each tweet.  They search for the words that achieve the highest number of spikes matching the days of events.| | 
1808.06834|e09e89b3945b756609278dcffb5f89d8a52a02cd|How many speeches are in the dataset?|5575 speeches| | 
1808.06834|0cf5132ac7904b7b81e17938d5815f70926a5180|What classification models were used?|fastText and SVM BIBREF16| | 
2002.10832|ed7985e733066cd067b399c36a3f5b09e532c844|What is different in BERT-gen from standard BERT?|They use a left-to-right attention mask so that the input tokens can only attend to other input tokens, and the target tokens can only attend to the input tokens and already generated target tokens.| | 
2002.10832|cd8de03eac49fd79b9d4c07b1b41a165197e1adb|How are multimodal representations combined?|The image feature vectors are mapped into BERT embedding dimensions and treated like a text sequence afterwards.| | 
1908.10383|63850ac98a47ae49f0f49c1c1a6e45c6c447272c|What is the problem with existing metrics that they are trying to address?|"Answer with content missing: (whole introduction) However, recent
studies observe the limits of ROUGE and find in
some cases, it fails to reach consensus with human.
judgment (Paulus et al., 2017; Schluter, 2017)."| | 
1908.10383|313087c69caeab2f58e7abd62664d3bd93618e4e|How do they evaluate their proposed metric?|manually labeled and tell exactly if one sentence should be extracted (assuming our annotations are in agreement), to further verify that FAR correlates with human preference,| | 
1709.02271|cfbccb51f0f8f8f125b40168ed66384e2a09762b|How are discourse embeddings analyzed?|They perform t-SNE clustering to analyze discourse embeddings| | 
1709.02271|feb4e92ff1609f3a5e22588da66532ff689f3bcc|What was the previous state-of-the-art?|character bigram CNN classifier| | 
1709.02271|f10325d022e3f95223f79ab00f8b42e3bb7ca040|How are discourse features incorporated into the model?|They derive entity grid with grammatical relations and RST discourse relations and concatenate them with pooling vector for the char-bigrams before feeding to the resulting vector to the softmax layer.| | 
1709.02271|5e65bb0481f3f5826291c7cc3e30436ab4314c61|What discourse features are used?|Entity grid with grammatical relations and RST discourse relations.| | 
1807.08204|848ab388703c24faad79d83d254e4fd88ab27e2a|How are proof scores calculated?|"'= ( , { ll k(h:, g:) if hV, gV

1 otherwise } )

where $_{h:}$ and $_{g:}$ denote the embedding representations of $h$ and $g$ , respectively."| | 
1704.08960|9893c5f36f9d503678749cb0466eeaa0cfc9413f|What submodules does the model consist of?|five-character window context| | 
2002.05058|5d85d7d4d013293b4405beb4b53fa79ac7c03401|How they add human prefference annotation to fine-tuning process?|human preference annotation is available, $Q(x_1, x_2) \in \lbrace >,<,\approx \rbrace $ is the true label for the pair| | 
2002.05058|6dc9960f046ec6bd280a721724458f66d5a9a585|What previous automated evalution approaches authors mention?|Text Overlap Metrics, including BLEU, Perplexity, Parameterized Metrics| | 
2002.05058|7488855f09b97eb6a027212fb7ace1d338f36a2b|Do the authors suggest that proposed metric replace human evaluation on this task?|No| | 
1906.06045|1083ec9a2a33f7fe2b6b51bbcebd2d9aec4b4de2|What is the training objective of their pair-to-sequence model?|is to minimize the negative likelihood of the aligned unanswerable question $\tilde{q}$ given the answerable question $q$ and its corresponding paragraph $p$ that contains the answer | | 
1906.06045|58a00ca123d67b9be55021493384c0acef4c568d|How do they ensure the generated questions are unanswerable?|learn to ask unanswerable questions by editing answerable ones with word exchanges, negations, etc| | 
1906.06045|199bdb3a6b1f7c89d95ea6c6ddbbb5eff484fa1f|Does their approach require a dataset of unanswerable questions mapped to similar answerable questions?|Yes| | 
1904.04055|5ea87432b9166d6a4ab8806599cd2b1f9178622f|What conclusions are drawn from these experiments?|best results were obtained using new word embeddings, best group of word embeddings is EC, The highest type F1-score was obtained for EC1 model, built using binary FastText Skip-gram method utilising subword information, ability of the model to provide vector representation for the unknown words seems to be the most important| | 
1904.04055|3af9156b95a4c2d67cc54b80b92cc7b918fea2a9|What experiments are presented?|identify the boundaries of timexes and assign them to one of the following classes: date, time, duration, set,  Then we evaluated these results using more detailed measures for timexes| | 
1904.04055|7e328cc3cffa521e73f111d6796aaa9661c8eb07|What is specific about the specific embeddings?|predicting the word given its context| | 
1904.04055|80f19be1cbe1f0ec89bbafb9c5f7a8ded37881fb|What embedding algorithm is used to build the embeddings?|CBOW and Skip-gram methods in the FastText tool BIBREF9| | 
1904.04055|b3238158392684a5a6b62a7eabaa2a10fbecf3e6|How was the KGR10 corpus created?|most relevant content of the website, including all subsites| | 
2002.06675|526ae24fa861d52536b66bcc2d2ddfce483511d6|How big are improvements with multilingual ASR training vs single language training?|relative WER improvement of 10%.| | 
2002.06675|3c0d66f9e55a89d13187da7b7128666df9a742ce|What is the difference between speaker-open and speaker-closed setting?|In the speaker-closed condition, two episodes were set aside from each speaker as development and test sets., In the speaker-open condition, all the data except for the test speaker's were used for training| | 
1909.08041|9df4a7bd0abb99ae81f0ebb29c488f1caa0f268f|How do they train the retrieval modules?|We treated the neural semantic retrieval at both the paragraph and sentence level as binary classification problems with models' parameters updated by minimizing binary cross entropy loss.| | 
1909.08041|b7291845ccf08313e09195befd3c8030f28f6a9e|How do they model the neural retrieval modules?|BERT-Base BIBREF2 to provide the state-of-the-art contextualized modeling| | 
1908.09137|b236b9827253037b2fd7884d7bfec74619d96293|How much better performance of proposed model compared to answer-selection models?|significant MAP performance improvement compared to the previous best model, CompClip-LM (0.696 to 0.734 absolute)| | 
1908.09137|b53efdbb9e53a65cd3828a3eb485c70f782a06e5|How are some nodes initially connected based on text structure?|we fully connect nodes that represent sentences from the same passage, we fully connect nodes that represent the first sentence of each passage, we add an edge between the question and every node for each passage| | 
1907.00854|4d5e2a83b517e9c082421f11a68a604269642f29|how many domains did they experiment with?|2| | 
1907.00854|2c3b2c3bab6d18cb0895462e3cfd91cd0dee7f7d|what pretrained models were used?|BiDAF, BERT | | 
1811.01734|ea51aecd64bd95d42d28ab3f1b60eecadf6d3760|What domains are contained in the polarity classification dataset?|Books, DVDs, Electronics, Kitchen appliances| | 
1811.01734|e4cc2e73c90e568791737c97d77acef83588185f|How long is the dataset?|8000| | 
1811.01734|cc28919313f897358ef864948c65318dc61cb03c|What machine learning algorithms are used?|string kernels, SST, KE-Meta, SFA, CORAL, TR-TrAdaBoost, Transductive string kernels, transductive kernel classifier| | 
1811.01734|b3857a590fd667ecc282f66d771e5b2773ce9632|What is a string kernel?|String kernel is a technique that uses character n-grams to measure the similarity of strings| | 
1804.08782|b653f55d1dad5cd262a99502f63bf44c58ccc8cf|Which dataset do they use to learn embeddings?|Fisher Corpus English Part 1| | 
1804.08782|22c802872b556996dd7d09eb1e15989d003f30c0|How do they correlate NED with emotional bond levels?|They compute Pearson’s correlation between NED measure for patient-to-therapist and patient-perceived emotional bond rating and NED measure for therapist-to-patient and patient-perceived emotional bond rating| | 
1909.09270|869aaf397c9b4da7ab52d6dd0961887ae08da9ae|Which languages are evaluated?|Bengali, English, German, Spanish, Dutch, Amharic, Arabic, Hindi, Somali | | 
1910.08502|871c34219eb623bde9ac3937aa0f28fc3ad69445|Which model have the smallest Character Error Rate and which have the smallest Word Error Rate?|character unit the RNN-transducer with additional attention module, For subword units, classic RNN-transducer, RNN-transducer with attention and joint CTC-attention show comparable performance| | 
1910.08502|285858416b1583aa3d8ba0494fd01c0d4332659f|What will be in focus for future work?|1) investigate errors produced by the end-to-end methods and explore several approaches to correct common errors done in French, 2) compare the end-to-end methods in a SLU context and evaluate the semantic value of the partially correct produced words| | 
1910.08502|acc512c57aef4d5a15c15e3593f0a9b3e7e7e8b8|What are the existing end-to-end ASR approaches for the French language?|1) Connectionist Temporal Classification (CTC), 2) Attention-based methods, 3) RNN-tranducer| | 
2003.09586|e75f5bd7cc7107f10412d61e3202a74b082b0934|How much is decoding speed increased by increasing encoder and decreasing decoder depth?|the Transformer with 10 encoder layers and 2 decoder layers is $2.32$ times as fast as the 6-layer Transformer| | 
2004.03329|58819fd80c9fbe8674f147bd84a45a25f674a093|Did they experiment on this dataset?|No| | 
2004.03329|694dd76a37ad9e8083c546e9bd083c5c3b65695c|What language are the conversations in?|The language is Chinese, which is not easy for non-Chinese-speaking researchers to work on.| | 
1806.04524|675f28958c76623b09baa8ee3c040ff0cf277a5a|What is the size of the dataset?|300,000 sentences with 1.5 million single-quiz questions| | 
1806.04524|79443bf3123170da44396b0481364552186abb91|Which two schemes are used?|sequence classification, sequence labeling| | 
1602.00812|48fa2ccc236e217fcf0e5aab0e7a146faf439b02|Does Grail accept Prolog inputs?|No| | 
1602.00812|2b52d481b30185d2c6e7b403d37277f70337d6ca|What formalism does Grail use?|a family of grammar formalisms built on a foundation of logic and type theory. Type-logical grammars originated when BIBREF4 introduced his Syntactic Calculus (called the Lambek calculus, L, by later authors).| | 
1706.01450|0fa81adf00662694e1dc74475ae2b9283c50748c|Which components of QA and QG models are shared during training?|parameter sharing| | 
1706.01450|4ade72bfa28bd1f6b75cc7fa687fa634717782f2|How much improvement does jointly learning QA and QG give, compared to only training QA?|We see that A-gen performance improves significantly with the joint model: both F1 and EM increase by about 10 percentage points. | | 
1704.02686|fb381a59732474dc71a413e25cac37e239547b55|Do they test their word embeddings on downstream tasks?|Yes| | 
1704.02686|54415efa91566d5d7135fa23bce3840d41a6389e|What dimensions of word embeddings do they produce using factorization?|300-dimensional vectors| | 
1704.02686|dcd22abfc9e7211925c0393adc30dbd4711a9f88|On which dataset(s) do they compute their word embeddings?|10 million sentences gathered from Wikipedia| | 
1704.02686|05238d1fad2128403577822aa4822ef8ca9570ac|Do they measure computation time of their factorizations compared to other word embeddings?|Yes| | 
1912.06813|6ee27ab55b1f64783a9e72e3f83b7c9ec5cc8073|What datasets are experimented with?|the CMU ARCTIC database BIBREF33,  the M-AILABS speech dataset BIBREF34 | | 
1912.06813|bb4de896c0fa4bf3c8c43137255a4895f52abeef|What is the baseline model?|a RNN-based seq2seq VC model called ATTS2S based on the Tacotron model| | 
1604.07236|d9eacd965bbdc468da522e5e6fe7491adc34b93b|What model do they train?|Support Vector Machines (SVM), Gaussian Naive Bayes, Multinomial Naive Bayes, Decision Trees, Random Forests and a Maximum Entropy classifier| | 
1604.07236|ebae0cd1fe0e7ba877d4b3055190e8b1dfcaeb53|What are the eight features mentioned?|User location (uloc), User language (ulang), Timezone (tz), Tweet language (tlang), Offset (offset), User name (name), User description (description), Tweet content (content)| | 
1908.02402|f1bd66bb354e3dabf5dc4a71e6f08b17d472ecc9|How do slot binary classifiers improve performance?|by adding extra supervision to generate the slots that will be present in the response| | 
1908.02402|25fd61bb20f71051fe2bd866d221f87367e81027|What baselines have been used in this work?|NDM, LIDM, KVRN, and TSCP/RL| | 
1709.09119|70e596dd4334a94844454fa7b565889556e2358d|How successful are they at matching names of authors in Japanese and English?|180221 of 231162 author names could be matched successfully| | 
1709.09119|9c2de35d07f0d536bfdefe4828d66dd450de2b61|Do they translate metadata from Japanese papers to English?|No| | 
1601.02543|8d793bda51a53a4605c1c33e7fd20ba35581a518|what bottlenecks were identified?|Confusion in recognizing the words that are active at a given node by a speech recognition solution developed for Indian Railway Inquiry System.| | 
1911.03270|1835f65694698a9153857e33cd9b86a96772fff5|Does the paper report the performance on the task of a Neural Machine Translation model?|No| | 
1911.03270|994ac7aa662d16ea64b86510fcf9efa13d17b478|Is the RNN model evaluated against any baseline?|Yes| | 
1911.03270|9282cf80265a914a13053ab23b77d1a8ed71db1b|Which languages are used in the paper?|English, Russian| | 
2004.03762|41bff17f7d7e899c03b051e20ef01f0ebc5c8bb1|What metrics are used for evaluation?|ROUGE BIBREF29 and METEOR BIBREF30| | 
2004.03762|b03e8e9a0cd2a44a215082773c7338f2f3be412a|What baselines are used?|a two layer recurrent neural language model with GRU cells of hidden size 512, a two layer neural sequence to sequence model equipped with bi-linear attention function with GRU cells of hidden size 512, a linear dynamical system, semi-supervised SLDS models with varying amount of labelled sentiment tags| | 
1909.07593|9439430ff97c6e927d919860b1cb86a0dcff0038|How is the robustness of the model evaluated?|10-fold cross validation| | 
1909.07593|00d6228bcd6b839529e52d0d622bf787a9356158|How is the effectiveness of the model evaluated?|precision ($P.$), recall ($R.$) and $F_1$ scores for target recognition and targeted sentiment| | 
1707.04913|c3d50f1e6942c9894f9a344e7cbc411af01e419c|Do they assume sentence-level supervision?|No| | 
1811.09786|602396d1f5a3c172e60a10c7022bcfa08fa6cbc9|By how much do they outperform BiLSTMs in Sentiment Analysis?|Proposed RCRN outperforms ablative baselines BiLSTM by +2.9% and 3L-BiLSTM by +1.1% on average across 16 datasets.| | 
1811.09786|b984612ceac5b4cf5efd841af2afddd244ee497a|Does their model have more parameters than other models?|approximately equal parameterization| | 
1704.05907|a381ba83a08148ce0324b48b8ff35128e66f580a|what models did they compare to?|High-order CNN, Tree-LSTM, DRNN, DCNN, CNN-MC, NBoW and SVM | | 
1704.05907|edb068df4ffbd73b379590762125990fcd317862|which benchmark tasks did they experiment on?| They used Stanford Sentiment Treebank benchmark for sentiment classification task and   AG English news corpus for the text classification task.| | 
1908.05763|8ea664a72e6d6eca73c1b3e1f75a72a677474ab1|Are recurrent neural networks trained on perturbed data?|No| | 
1908.05763|5e41516a27c587aa2f80dba8cf4c3f616174099b|How does their perturbation algorihm work?|same sentences after applying character level perturbations| | 
1904.12087|edc43e1b75c0970b7003deeabfe3ad247cb1ed83|Which language is divided into six dialects in the task mentioned in the paper?|Akkadian.| | 
1904.12087|0c3924214572579ddbc1b4a87c7f7842ef20ff1b|What is one of the first writing systems in the world?|Cuneiform| | 
1606.01433|4519afe91b1042876d7c021487d98e2d72a09861|How do they obtain distant supervision rules for predicting relations?|dominant temporal associations can be learned from training data| | 
1606.01433|0cfaca6f3f33ebdb338c5f991f6a7a33ff33844d|Which structured prediction approach do they adopt for temporal entity extraction?|DeepDive BIBREF1| | 
1811.00854|70c2dc170a73185c9d1a16953f85aca834ead6d3|Which evaluation metric has been measured?|Mean Average Precision| | 
1811.00854|38854255dbdf2f36eebefc0d9826aa76df9637c6|What is the WordNet counterpart for Persian?|FarsNet| | 
1801.07804|2a0f14740ee14224d116d4f51dacde6863bcdc1e|Do they consider relations other than binary relations?|Yes| | 
1801.07804|82c7d9e92c7d7b784de2cae87fb7293034c551f4|Are the grammar clauses manually created?|Yes| | 
1801.07804|6d623c96cd3898c3758338e337e9157565f34185|Do they use an NER system in their pipeline?|No| | 
2001.00582|827b5bd215599623a3125afe331b56b89b42bf09|What large corpus is used for experiments?|The De7 database| | 
2001.08051|2159062595f24ec29826d517429e1b809ba068b3|Are any of the utterances ungrammatical?|Yes| | 
1611.03382|81e8d42dad08a58fe27eea838f060ec8f314465e|What is the state-of-the art?|neural attention model with a convolutional encoder with an RNN decoder and RNN encoder-decoder| | 
1804.04225|482b4cc7676cf13912e27899c718f4dc5d92846d|How do they identify abbreviations?|identify all abbreviations using regular expressions| | 
1804.04225|0c09ffb337be0feb25e2fd14164b35a0969d7b4c|What kind of model do they build to expand abbreviations?|word2vec BIBREF0| | 
1804.04225|385dc96604e077611fbd877c7f39d3c17cd63bf2|Do they use any knowledge base to expand abbreviations?|Yes| | 
1804.04225|551a17fc1d5b5c3d18bdc4923363cbbda7eb2516|In their used dataset, do they study how many abbreviations are ambiguous?|No| | 
1804.04225|62a3dc90ba427c5985789001a02825c9434ce67d|Which dataset do they use to build their model?|1,160 physician logs of Medical ICU admission requests, 42,506 Wikipedia articles, 6 research papers and 2 critical care medicine textbooks| | 
1909.06937|b4f5bf3b7b37e2f22d13b724ca8fe7d0888e04a2|What is the domain of their collected corpus?|speaker systems in the real world| | 
1909.06937|d71772bfbc27ff1682e552484bc7c71818be50cf|What is the source of the CAIS dataset?|the $\mathbf {C}$hinese $\mathbf {A}$rtificial $\mathbf {I}$ntelligence $\mathbf {S}$peakers (CAIS)| | 
1909.06937|b6858c505936d981747962eae755a81489f62858|What were the baselines models?|BiLSTMs + CRF architecture BIBREF36, sententce-state LSTM BIBREF21| | 
1612.07843|defc17986d3c4aed9eccdbaebda5eb202fbcb6cf|Are the document vectors that the authors introduce evaluated in any way other than the new way the authors propose?|Yes| | 
1612.07843|03895bc75e4d01c359cd269a9eb3b6ea57039817|According to the authors, why does the CNN model exhibit a higher level of explainability?|CNN model which additionally learns intermediate hidden layer representations and convolutional filters. Moreover the CNN model can take advantage of the semantic similarity encoded in the distributed word2vec representations| | 
1612.07843|6bcff3ef61aad6bf1280ea26ed79585e1b838e64|Does the LRP method work in settings that contextualize the words with respect to one another?|Yes| | 
1704.00939|6dfad97356b6e82009ee442d7fd2b97b5dcabfe2|How do they incorporate lexicon into the neural network?|concatenation of GloVe pre-trained embeddings and DepecheMood BIBREF19 lexicon representation, cannot directly concatenate,  re-build the latter in token-based form| | 
1704.00939|59a5959a6abfb81b114e7bfaa945301349d20f0f|What is the source of their lexicon?|DepecheMood| | 
1704.00939|5a2f7e27efdedf3c43498ff0c32f808d406c42ec|What embeddings do they use?|GloVe| | 
1905.10044|5871d258f66b00fb716065086f757ef745645bfe|did they use other pretrained language models besides bert?|Yes| | 
1905.10044|c554a453b6b99d8b59e4ef1511b1b506ff6e5aa4|how was the dataset built?|"Questions are gathered from anonymized, aggregated queries to the Google search engine. Queries that are likely to be yes/no questions are heuristically identified: we found selecting queries where the first word is in a manually constructed set of indicator words and are of sufficient length, to be effective. Questions are only kept if a Wikipedia page is returned as one of the first five results, in which case the question and Wikipedia page are given to a human annotator for further processing. Annotators label question/article pairs in a three-step process. First, they decide if the question is good, meaning it is comprehensible, unambiguous, and requesting factual information. This judgment is made before the annotator sees the Wikipedia page. Next, for good questions, annotators find a passage within the document that contains enough information to answer the question. Annotators can mark questions as “not answerable"" if the Wikipedia article does not contain the requested information. Finally, annotators mark whether the question's answer is “yes"" or “no"""| | 
1905.10044|10210d5c31dc937e765051ee066b971b6f04d3af|what is the size of BoolQ dataset?| 16k questions| | 
1708.04557|5d9b088bb066750b60debfb0b9439049b5a5c0ce|what processing was done on the speeches before being parsed?|Remove numbers and interjections| | 
1708.00481|4c1847f0f3e6f9cc6ac3dfbac9e135d34641a854|What programming language is the tool written in?|JavaScript| | 
2003.12932|0ca02893bda50007f7a76e7c8804101718fbb01c|What kind is noise is present in typical industrial data?| non-canonical text such as spelling mistakes, typographic errors, colloquialisms, abbreviations, slang, internet jargon, emojis, embedded metadata (such as hashtags, URLs, mentions), non standard syntactic constructions and spelling variations, grammatically incorrect text, mixture of two or more languages| | 
2003.12932|751aa2b1531a17496536887288699cc8d5c3cec9|What is the reason behind the drop in performance using BERT for some popular task?|Hence WordPiece tokenizer tokenizes noisy words into subwords. However, it ends up breaking them into subwords whose meaning can be very different from the meaning of the original word. Often, this changes the meaning of the sentence completely, therefore leading to substantial dip in the performance.| | 
2002.08307|dc4096b8bab0afcbbd4fbb015da2bea5d38251cd|How they observe that fine-tuning BERT on a specific task does not improve its prunability?|we compressed English BERT using magnitude weight pruning BIBREF8 and observed the results on transfer learning to the General Language Understanding Evaluation (GLUE) benchmark BIBREF9, a diverse set of natural language understanding tasks including sentiment analysis, NLI, and textual similarity evaluation. | | 
1808.02113|0a4e82dc3728be0bd0325bfe944e7e7de0b98b22|How do they gather human reviews?|human representative to review the IVA chat history and resume the failed task| | 
1808.02113|c635dc8013e63505084b9daaa9ddb021a2d24543|Do they explain model predictions solely on attention weights?|Yes| | 
1808.02113|61aac406b648865f007a400dcd69f28e44efc636|Can their method of creating more informative visuals be applied to tasks other than turn taking in conversations?|computationally inexpensive means to understand what happened at the stopping point| | 
1707.08559|e414d819f10c443cbefa8bdb9bd486ffc6d1fc6a|What is the average length of the recordings?|40 minutes| | 
1707.08559|2e73006e5d007aa08c62030a4d5a7e2e7e0eaf6c|How big was the dataset presented?|321 videos| | 
1702.06589|07c9863e1e86c31b740b5b5a77fe8000be00c273|Does a neural scoring function take both the question and the logical form as inputs?|Yes| | 
1702.06589|bf7cb53f4105f2e6a413d1adef5349ff1e673500|What is the source of the paraphrases of the questions?|WikiTableQuestions| | 
1702.06589|a6419207d2299f25e2688517d1580b7ba07c8e4b|Does the dataset they use differ from the one used by Pasupat and Liang, 2015?|No| | 
1710.11154|b34f6e2cf6b1984afdf18dda2a502db6c2c5224b|Did they experiment on this corpus?|No| | 
1912.10806|5c0b8c1b649df1b07d9af3aa9154ac340ec8b81c|Is the model compared against a linear regression baseline?|No| | 
1912.10806|3b391cd58cf6a61fe8c8eff2095e33794e80f0e3|What is the dataset used in the paper?|"historical S&P 500 component stocks
 306242 news articles"| | 
1912.10806|4d8ca3f7aa65dcb42eba72acf3584d37b416b19c|How does the differential privacy mechanism work?|A mechanism ${f}$ is a random function that takes a dataset $\mathcal {N}$ as input, and outputs a random variable ${f}(\mathcal {N})$.| | 
1904.09708|7182f6ed12fa990835317c57ad1ff486282594ee|How does the SCAN dataset evaluate compositional generalization?|"it systematically holds out inputs in the training set containing basic primitive verb, ""jump"", and tests on sequences containing that verb."| | 
1911.03912|cac0119681f311b2efd14b3251a2a5b69ad5d0cd|Is their model fine-tuned also on all available data, what are results?|No| | 
1909.13695|3241f90a03853fa85d287007d2d51e7843ee3d9b|What standard large speaker verification corpora is used for evaluation?|non-native speech from the BULATS test | | 
1607.00410|2af66730a85b29ff28dbfa58342e0ae6265d2963|How many examples are there in the source domain?|78,976| | 
1607.00410|146fe3e97d8080f04222ed20903dd0d5fd2f551c|How many examples are there in the target domain?|the food dataset has 3,806 images for training | | 
1607.00410|0fc17e51a17efce17577e2db89a24abd6607bb2b|Did they only experiment with captioning task?|Yes| | 
1601.01705|5712a0b1e33484ebc6d71c70ae222109c08dede2|What benchmark datasets they use?|VQA and GeoQA| | 
1901.11117|aee1af55d39145f609da95116ab1b154adb5fa7e|what is the proposed Progressive Dynamic Hurdles method?|allows models that are consistently performing well to train for more steps| | 
1901.11117|feedddb7ae4998b6a3eaa2d6323017ba278748cc|What is in the model search space?|Our search space consists of two stackable cells, one for the model encoder and one for the decoder , Each cell contains NASNet-style blocks, which receive two hidden state inputs and produce new hidden states as outputs, Our search space contains five branch-level search fields (input, normalization, layer, output dimension and activation), one block-level search field (combiner function) and one cell-level search field (number of cells).| | 
1901.11117|2b5dc3595dfc3d52a1525783d943b3dd0cc62473|How does Progressive Dynamic Hurdles work?|It begins as ordinary tournament selection evolutionary architecture search with early stopping, with each child model training for a relatively small $s_0$ number of steps before being evaluated for fitness. However, after a predetermined number of child models, $m$ , have been evaluated, a hurdle, $h_0$ , is created by calculating the the mean fitness of the current population. For the next $m$ child models produced, models that achieve a fitness greater than $h_0$ after $s_0$ train steps are granted an additional $s_1$ steps of training and then are evaluated again to determine their final fitness. Once another $m$ models have been considered this way, another hurdle, $h_1$ , is constructed by calculating the mean fitness of all members of the current population that were trained for the maximum number of steps. For the next $m$ child models, training and evaluation continues in the same fashion, except models with fitness greater than $m$0 after $m$1 steps of training are granted an additional $m$2 number of train steps, before being evaluated for their final fitness. This process is repeated until a satisfactory number of maximum training steps is reached.| | 
1910.08772|ee27e5b56e439546d710ce113c9be76e1bfa1a3d|Do they beat current state-of-the-art on SICK?|No| | 
1910.08772|4688534a07a3cbd8afa738eea02cc6981a4fd285|How do they combine MonaLog with BERT?|They use Monalog for data-augmentation to fine-tune BERT on this task| | 
1910.08772|45893f31ef07f0cca5783bd39c4e60630d6b93b3|How do they select monotonicity facts?|They derive it from Wordnet| | 
1707.07922|182c7919329bc5678cf0c79687a66c0f7782577e|How does the model recognize entities and their relation to answers at inference time when answers are not accessible?|gating function, Dynamic Memory| | 
1710.02772|0747cecb3c72594c5d15ba18490566be1ffdbfad|What other solutions do they compare to?| strong competitive methods on the SQuAD leaderboard and TriviaQA leaderboard| | 
1710.02772|0e9c08b635c1ebfd36472550d619095541bb5af1|How does the gatint mechanism combine word and character information?|when the gate has high value, more information flows from the word-level representation; otherwise, char-level will take the dominating place,  for unfamiliar noun entities, the gates tend to bias towards char-level representation in order to care richer morphological structure| | 
1610.05243|b13902af1bcf0e199a3ea42bbc8fcd8e696a381a|Which dataset do they use?|parallel data available for the WMT 2016| | 
1610.05243|b84bce289c6c81d0a7507ae183b94982533576b3|How is the PBMT system trained?|systems were optimized on the tst2014 using Minimum error rate training BIBREF20| | 
1610.05243|9fd137bf7eabaf8bc234a18b6ea34471cf4a3b95|Which NMT architecture do they use?|trained using Nematus, default configuration| | 
1610.05243|249f2a9bd9d59679cbe82b3fa01572fc7a04f81b|Do they train the NMT model on PBMT outputs?|Yes| | 
1909.11467|fb1c2ff0872084241b9725b4f07750bd3e1df793|Is the corpus annotated?|No| | 
1909.11467|9d9f6cc0f026f7168fcea461baff4b8a925a185f|How is the corpus normalized?|by replacing zero-width-non-joiner (ZWNJ) BIBREF2 and manually verifying the orthography| | 
1909.11467|2cc63f42410eff3bcb15cfddc593d8aab9413eea|Is the corpus annotated with a phonetic transcription?|No| | 
1909.11467|0a9ced54324e70973354978cccef1c70dee5a543|Is the corpus annotated with Part-of-Speech tags?|No| | 
1804.08186|626873982852ec83c59193dd2cf73769bf77b3ed|what evaluation methods are discussed?|document-level accuracy, precision, recall, F-score| | 
1804.08186|b3a09d2e3156c51bd5fdc110a2a00a67bb8c0e42|what are the off-the-shelf systems discussed in the paper?|Answer with content missing: (Names of many identifiers missing) TextCat, ChromeCLD, LangDetect, langid.py, whatlang, whatthelang, YALI, LDIG, Polyglot 3000, Lextek Language Identifier and Open Xerox Language Identifier.| | 
1907.11062|4c026715ee365c709381c5da770bdc8297eed19f|"How is ""hirability"" defined?"|candidates who have been liked or shortlisted are considered part of the hirable class| | 
1907.11062|4ef3bfebabda83a6d5ca55d30de0e05893f241e3|Have the candidates given their consent to have their videos used for the research?|Yes| | 
1907.11062|db264e363f3b3aa83526952bef02f826dff70042|Do they analyze if their system has any bias?|No| | 
1907.11062|e8e6986365f899dead0768ecf7b1eca8a2699f2f|Is there any ethical consideration in the research?|No| | 
2003.10816|ffa4d4bfb226382ca4ecde65ecdc44a3d9e0ce81|What classification task was used to evaluate the cross-lingual adaptation method described in this work?|Paraphrase Identification| | 
1812.02802|a779d452d11f368c66f7b51f7190d0fe9402f505|How many parameters does the presented model have?|(infixes 700K, 318K, and 40K) each representing the number of approximate parameters| | 
1812.02802|cdc5a998cb73262594cdae1dda49576044da3d3d|How do they measure the quality of detection?|We evaluate the false-reject (FR) and false-accept (FA) tradeoff across several end-to-end models of distinct sizes and computational complexities.| | 
1812.02802|1383ddd4619cf81227c72f3d9f30c10c47a0cdad|What previous approaches are considered?|Our baseline system (Baseline_1850K) is taken from BIBREF13 . | | 
1909.05438|d7aed39c359fd381495b12996c4dfc1d3da38ed5|How is the back-translation model trained?| applying the rule INLINEFORM4 to a set of natural language questions INLINEFORM5, both models are improved following the back-translation protocol that target sequences should follow the real data distribution| | 
1909.05438|9c423e3b44e3acc2d4b0606688d4ac9d6285ed0f|Are the rules dataset specific?|Yes| | 
1909.05438|b6fb72437e3779b0e523b9710e36b966c23a2a40|How many rules had to be defined?|"WikiSQL - 2 rules (SELECT, WHERE)
SimpleQuestions - 1 rule
SequentialQA - 3 rules (SELECT, WHERE, COPY)"| | 
1909.05438|e6469135e0273481cf11a6c737923630bc7ccfca|What datasets are used in this paper?|WikiSQL, SimpleQuestions, SequentialQA| | 
2003.08370|06202ab8b28dcf3991523cf163b8844b42b9fc99|How much labeled data is available for these two languages?|10k training and 1k test, 1,101 sentences (26k tokens)| | 
2003.08370|288613077787159e512e46b79190c91cd4e5b04d|What classifiers were used in experiments?|Bi-LSTM, BERT| | 
2003.08370|cf74ff49dfcdda2cd67a896b4b982a1c3ee51531|In which countries are Hausa and Yor\`ub\'a spoken?|Nigeria, Benin, Ghana, Cameroon, Togo, Côte d'Ivoire, Chad, Burkina Faso, and Sudan, Republic of Togo, Ghana, Côte d'Ivoire, Sierra Leone, Cuba and Brazil| | 
1909.00338|827c58f6cab6c6fe7a6c43bdc71150b61ba0eed4|What is the agreement score of their annotated dataset?| Relevance and Subject categorizations are annotated at a percent agreement of $0.71$ and $0.70$, their agreement scores are only fair, at $\alpha =0.27$ and $\alpha =0.29$, Stance and Sentiment, which carry more categories than the former two, is $0.54$ for both. Their agreement scores are also fair, at $\alpha =0.35$ and $\alpha =0.34$, This holds for the Relevant category ($0.81$), the Vaccine category ($0.79$) and the Positive category ($0.64$),  The Negative category yields a mutual F-score of $0.42$, which is higher than the more frequently annotated categories Neutral ($0.23$) and Not clear ($0.31$).| | 
1909.00338|58ad7e8f7190e2a4f1588cae9a7842c56b37694d|What is the size of the labelled dataset?|27,534 messages | | 
1909.00338|12eba1598dca14db64dbc8b73484639363a4618e|Which features do they use to model Twitter messages?|word unigrams, bigrams, and trigrams| | 
1909.00338|4e468ce13b7f6ac05371c62c08c3cec1cd760517|Do they allow for messages with vaccination-related key terms to be of neutral stance?|Yes| | 
1909.00361|a9acd1af4a869c17b95ec489cdb1ba7d76715ea4|Is this a span-based (extractive) QA task?|Yes| | 
1908.11546|6f2118a0c64d5d2f49eee004d35b956cb330a10e|What datasets are used for training/testing models? |Microsoft Research dataset containing movie, taxi and restaurant domains.| | 
1908.11546|b8dea4a98b4da4ef1b9c98a211210e31d6630cf3|What is specific to gCAS cell?|It has three sequentially connected units to output continue, act and slots generating multi-acts in a doble recurrent manner.| | 
1905.10238|4146e1d8f79902c0bc034695998b724515b6ac81|What dataset do they evaluate their model on?|CoNLL-2012 shared task BIBREF21 corpus| | 
1905.10238|42394c54a950bae8cebecda9de68ee78de69dc0d|What is the source of external knowledge?|counts of predicate-argument tuples from English Wikipedia| | 
1906.01840|e9d882775a132e172eea68ab6ab4621a924bb6b8|Which of their proposed attention methods works better overall?|attention parsing| | 
1906.01840|6367877c05beebfdbb31e83c1f25dfddf925b6b6|Which dataset of texts do they use?|Cora, Hepth, Zhihu| | 
1906.01840|d151327c93b67928313f8fad8079a4ff9ef89314|Do they measure how well they perform on longer sequences specifically?|Yes| | 
1906.01840|70f9358dc01fd2db01a6b165e0b4e83e4a9141a7|Which other embeddings do they compare against?|MMB, DeepWalk, LINE,  Node2vec, TADW, CENE, CANE, WANE, DMTE| | 
1905.07464|3752bbc5367973ab5b839ded08c57f51336b5c3d|What training data did they use?|Training-22, NLM-180| | 
1811.05085|30db81df46474363d5749d7f6a94b7ef95cd3e01|What domains do they experiment with?|Twitter, Yelp reviews and movie reviews| | 
1908.06556|5c26388a2c0b0452d529d5dd565a5375fdabdb70|What games are used to test author's methods?|Lurking Horror, Afflicted, Anchorhead, 9:05, TextWorld games| | 
1908.06556|184e1f28f96babf468f2bb4e1734f69646590cda|How is the domain knowledge transfer represented as knowledge graph?|the knowledge graph is used to prune this space by ranking actions based on their presence in the current knowledge graph and the relations between the objects in the graph as in BIBREF7| | 
1901.09755|71fca845edd33f6e227eccde10db73b99a7e157b|What was the baseline?|the baseline provided by BIBREF8, the baselines provided by the ABSA organizers| | 
1901.09755|e755fb599690d0d0c12ddb851ac731a0a7965797|Which six languages are experimented with?|Dutch, French, Russian, Spanish , Turkish, English | | 
1901.09755|7e51490a362135267e75b2817de3c38dfe846e21|What shallow local features are extracted?| Local, shallow features based mostly on orthographic, word shape and n-gram features plus their context| | 
1907.00455|e98d331faacd50f8ec588d2466b5a85da1f37e6f|Do they compare results against state-of-the-art language models?|Yes| | 
1907.00455|f319f2c3f9339b0ce47478f5aa0c32da387a156e|Which dataset do they train their models on?|Penn Treebank, Text8| | 
1708.06022|e91692136033bbc3f19743d0ee5784365746a820|It looks like learning to paraphrase questions, a neural scoring model and a answer selection model cannot be trained end-to-end. How are they trained?|using multiple pivot sentences| | 
2004.01894|94e17980435aaa9fc3b5328f16f3368dc8a736bd|What multimodal representations are used in the experiments?|The second method it to learn a common space for the two modalities before concatenation (project), The first method is concatenation of the text and image representation (concat)| | 
2004.01894|4d8b3928f89d73895a7655850a227fbac08cdae9|How much better is inference that has addition of image representation compared to text-only representations? | largest improvement ($22-26\%$ E.R) when text-based unsupervised models are combined with image representations| | 
2004.01894|b6e4b98fad3681691bcce13f57fb173aee30c592|How they compute similarity between the representations?|similarity is computed as the cosine of the produced $h_{L}$ and $h_{R}$ sentence/image representations| | 
2004.01894|af7a9b56596f90c84f962098f7e836309161badf|How big is vSTS training data?|1338 pairs for training| | 
1910.11768|ba61ed892b4f7930430389e80a0c8e3b701c8e5d|Which evaluation metrics do they use for language modelling?| functional dissimilarity score, nearest neighbours experiment| | 
1910.11768|6a566095e25cbb56330456d7a1f3471693817712|Do they do quantitative quality analysis of learned embeddings?|Yes| | 
1910.11768|56c6ff65c64ca85951fdea54d6b096f28393c128|Do they evaluate on downstream tasks?|Yes| | 
1910.11768|356e462f7966e30665a387ed7a9ad2e830479da6|Which corpus do they use?|The dataset was created by using translations provided by Tatoeba and OpenSubtitles BIBREF16.| | 
1910.11204|cb4727cd5643dabc3f5c95e851d5313f5d979bdc|How big is improvement over the old  state-of-the-art performance on CoNLL-2009 dataset?|our Open model achieves more than 3 points of f1-score than the state-of-the-art result| | 
1910.11204|86f24ecc89e743bb1534ac160d08859493afafe9|What different approaches of encoding syntactic information authors present?|dependency head and dependency relation label, denoted as Dep and Rel for short, Tree-based Position Feature (TPF) as Dependency Path (DepPath), Shortest Dependency Path (SDP) as Relation Path (RelPath)| | 
2003.07758|86cd1228374721db67c0653f2052b1ada6009641|What domain does the dataset fall into?|YouTube videos| | 
2003.07758|7011b26ffc54769897e4859e4932aeddfab82c9f|What ASR system do they use?|YouTube ASR system | | 
2003.08808|5cd5864077e4074bed01e3a611b747a2180088a0|How big are datasets used in experiments?|2000 images| | 
2003.08808|d664054c8d1f8e84169d4ab790f2754274353685|What previously annotated databases are available?|the UBC database BIBREF14| | 
1602.07618|691cba5713c76a6870e35bc248ce1d29c0550bc7|Do they argue that all words can be derived from other (elementary) words?|No| | 
1602.07618|4542a4e7eabb8006fb7bcff2ca6347cfb3fbc56b|Do they break down word meanings into elementary particles as in the standard model of quantum theory?|No| | 
1910.05752|af8d3ee6a282aaa885e9126aa4bcb08ac68837e0|How big is the dataset used?|over 41,250 videos and 825,000 captions in both English and Chinese., over 206,000 English-Chinese parallel translation pairs| | 
1911.03584|0e510d918456f3d2b390b501a145d92c4f125835|How they prove that multi-head self-attention is at least as powerful as convolution layer? |constructively by selecting the parameters of the multi-head self-attention layer so that the latter acts like a convolutional layer| | 
1911.03584|2caa8726222237af482e170c51c88099cefef6fc|Is there any nonnumerical experiment that also support author's claim, like analysis of attention layers in publicly available networks? |No| | 
1911.03584|5367f8979488aaa420d8a69fec656851095ecacb|What numerical experiments they perform?|attention probabilities learned tend to respect the conditions of Lemma UNKREF15, corroborating our hypothesis, validate that our model learns a meaningful classifier we compare it to the standard ResNet18| | 
1811.09353|3cca26a9474d3b0d278e4dd57e24b227e7c2cd41|What dataset is used?|Brent corpus, PTB , Beijing University Corpus, Penn Chinese Treebank| | 
1811.09353|8f8f2b0046e1a78bd34c0c3d6b6cb24463a8ed7f|What language do they look at?|English, Chinese| | 
1909.08306|e37c32fce68759b2272adc1e44ea91c1a7c47059|What dierse domains and languages are present in new datasets?|movies , restaurants, English , Korean| | 
1711.10124|280f863cfd63b711980ca6c7f1409c0306473de7|Are their corpus and software public?|Yes| | 
1906.09774|b5a2b03cfc5a64ad4542773d38372fffc6d3eac7|How are EAC evaluated?|Qualitatively through efficiency, effectiveness and satisfaction aspects and quantitatively through metrics such as precision, recall, accuracy, BLEU score and even human judgement.| | 
1906.09774|b093b440ae3cd03555237791550f3224d159d85b|What are the currently available datasets for EAC?|EMPATHETICDIALOGUES dataset, a dataset containing 1.5 million Twitter conversation, gathered by using Twitter API from customer care account of 62 brands across several industries, SEMAINE corpus BIBREF30| | 
1906.09774|ad16c8261c3a0b88c685907387e1a6904eb15066|What are the research questions posed in the paper regarding EAC studies?|how to incorporate affective information into chatbots, what are resources that available and can be used to build EAC, and how to evaluate EAC performance| | 
1911.03058|d3014683dff9976b7c56b72203df99f0e27e9989|What evaluation metrics did they use?|we report P@1, which is equivalent to accuracy, we also provide results with P@5 and P@10 in the Appendix| | 
1911.03058|ed522090941f61e97ec3a39f52d7599b573492dd|What is triangulation?|Answer with content missing: (Chapter 3) The concept can be easily explained with an example, visualized in Figure 1. Consider the Portuguese (Pt) word trabalho which, according to the MUSE Pt–En dictionary, has the words job and work as possible En translations. In turn, these two En words can be translated to 4 and 5 Czech (Cs) words respectively. By utilizing the transitive property (which translation should exhibit) we can identify the set of 7 possible Cs translations for the Pt word trabalho.| | 
1702.02363|4670e1be9d6a260140d055c7685bce365781d82b|Did they experiment with the dataset on some tasks?|Yes| | 
1811.00266|013a8525dbf7a9e1e69acc1cff18bb7b8261cbad|Do they use pretrained word embeddings?|Yes| | 
1606.08495|5efed109940bf74ed0a9d4a5e97a535502b23d27|Do they use skipgram version of word2vec?|Yes| | 
1606.08495|b8137eb0fa0b41f871c899a54154f640f0e9aca1|What domains are considered that have such large vocabularies?|relational entities, general text-based attributes, descriptive text of images, nodes in graph structure of networks, queries| | 
1606.08495|38b527783330468bf6c4829f7d998e6f17c615f0|Do they perform any morphological tokenization?|No| | 
1810.03459|fb56743e942883d7e74a73c70bd11016acddc348|What data do they train the language models on?| BABEL speech corpus | | 
1810.03459|093dd1e403eac146bcd19b51a2ace316b36c6264|Do they report BLEU scores?|No| | 
1810.03459|da82b6dad2edd4911db1dc59e4ccd7f66c5fd79c|What architectures are explored to improve the seq2seq model?|VGG-BLSTM, character-level RNNLM| | 
1811.11365|cf15c4652e23829d8fb4cf2a25e64408c18734c1|Why is this work different from text-only UNMT?|"the image can play the role of a pivot “language"" to bridge the two languages without paralleled corpus"| | 
1910.06061|439af1232a012fc4d94ef2ffe305dd405bee3888|What is baseline used?|Base , Base+Noise, Cleaning , Dynamic-CM ,  Global-CM,  Global-ID-CM, Brown-CM ,  K-Means-CM| | 
1910.06061|b6a6bdca6dee70f8fe6dd1cfe3bb2c5ff03b1605|Did they evaluate against baseline?|Yes| | 
1910.06061|8951fde01b1643fcb4b91e51f84e074ce3b69743|How they evaluate their approach?|They  evaluate newly proposed models in several low-resource settings across different languages with real, distantly supervised data with non-synthetic noise| | 
2002.10361|ff307b10e56f75de6a32e68e25a69899478a13e4|Which document classifiers do they experiment with?|logistic regression (LR), recurrent neural network (RNN) BIBREF35, convolutional neural network (CNN) BIBREF36 and Google BERT BIBREF37| | 
1812.08879|4319a13a6c4a9494ccb465509c9d4265f63dc9b5|How is some information lost in the RNN-based generation models?|the generated sentences often did not include all desired attributes.| | 
2001.02214|5be62428f973a08c303c66018b081ad140c559c8|What is the model accuracy?|Overall, our AMRAN outperforms all baselines, achieving 0.657 HR@10 and 0.410 NDCG@10.| | 
2001.02214|64b65687b82ddb17c3d068381aaee56eb7fc02cd|What dataset is used?|Twitter dataset obtained from the authors of BIBREF12| | 
1810.10254|9257c578ee19a7d93e2fba866be7b0bf1142c393|Did they use other evaluation metrics?|Yes| | 
1810.10254|235c156d9c2adc895c9113f53c60f2dd8df45834|What languages are explored in this paper?|Mandarin, English| | 
1810.10254|fa2ffc6b4b046e17bc41e199855c4941673e2caf|What parallel corpus did they use?|Parallel monolingual corpus in English and Mandarin| | 
2003.08897|ad7b13579823cbc7825421c84d16f23ed863f6ee|What datasets are used for experiments on three other tasks?|VATEX, WMT 2014 English-to-German, and VQA-v2 datasets| | 
1703.06492|84a4a1f4695eba599d447e030c94f51e5f2f03bb|What accuracy do they approach with their proposed method?|our method still can improve the state-of-the-art accuracy BIBREF7 from 60.32% to 60.34%| | 
1703.06492|785eb3c7c5a5c27db14006ac357299ed1216313a|What they formulate the question generation as?|LASSO optimization problem| | 
1703.06492|bf6c14e9c5f476062cbaaf9179b0c9b751222c8f|What two main modules their approach consists of?|the basic question generation module (Module 1) and co-attention visual question answering module (Module 2)| | 
1808.03815|06c340c7c2ad57d7621c3e8baba6a3d0ed9f4696|Are there syntax-agnostic SRL models before?|Yes| | 
1808.03815|4c88441f8a1b5fce0ca55a6fced34f97260206ae|What is the biaffine scorer?|biaffine attention BIBREF14| | 
1701.08118|7486c9d9e6c407c0c3bc012405d689dbee072327|What languages are were included in the dataset of hateful content?|German| | 
1701.08118|0f2403fa77738bf05534d7f9d83c9dbb0a0d6140|How was reliability measured?|level of agreement (Krippendorff's INLINEFORM0 )| | 
1701.08118|21df76462c76d6e2d52fb7dce573ee5336627cb5|How did the authors demonstrate that showing a hate speech definition caused annotators to partially align their own opinion with the definition?|participants in group one very rarely gave different answers to questions one and two (18 of 500 instances or 3.6%)| | 
1701.08118|45be26c01e82835d9949529003c6b64f90db3d1a|What definition was one of the groups was shown?|Twitter definition of hateful conduct| | 
1701.08118|e28019afcb55c01516998554503bc1b56f923995|Was the degree of offensiveness taken as how generally offensive the text was, or how personally offensive it was to the annotator?|Personal thought of the annotator.| | 
1701.08118|551cc0401674f7c363e0018b8186a125f7b17e99|How were potentially hateful messages identified?|10 hashtags that can be used in an insulting or offensive way| | 
1905.09866|ad5898fa0063c8a943452f79df2f55a5531035c7|Which embeddings do they detect biases in?|Word embeddings trained on GoogleNews and Word embeddings trained on Reddit dataset| | 
1912.09152|601f96770726a0063faf9bacd5db01c4af5add1f|What does their system consist of?|rule-based and dictionary-based methods | | 
1912.09152|1c68d18b4b65c4d75dc199d2043079490f6310f8|What are the two PharmaCoNER subtasks?|Entity identification with offset mapping and concept indexing| | 
2004.02451|818c89b11471a6ca4f13c838713864fdf282c2ca|What neural language models are explored?|LSTM-LM | | 
2004.02451|7994b4001925798dfb381f9aa5c0545cdbd77220|How do they perform data augmentation?|They randomly sample sentences from Wikipedia that contains an object RC and add them to training data| | 
1702.06777|0d755ff58a7e22eb4d02fca45d4a7a3920f4e725|Do the authors mention any possible confounds in their study?|Yes| | 
1702.06777|ff2bcf2d8ffee586751ce91cf15176301267b779|What are the characteristics of the city dialect?|Lexicon of the cities tend to use most forms of a particular concept| | 
1702.06777|55588ae77496e7753bff18763a21ca07d9f93240|What are the characteristics of the rural dialect?|It uses particular forms of a concept rather than all of them uniformly| | 
1607.01759|4e9d1c73a2d032ec4e04e04921897b7cf928bda8|What are their baseline methods?|simple linear model with rank constraint, Hierarchical softmax, N-gram features| | 
1912.00582|4d5f112874250d48eb49649c4abe31d6c9236700|Which of the model yields the best performance?|GPT-2| | 
1912.00582|8985ead714236458a7496075bc15054df0e3234e|What is the performance of the models on the tasks?|Overall accuracy per model is: 5-gram (60.5), LSTM (68.9), TXL (68.7), GPT-2 (80.1)| | 
1912.00582|49aecc50823a60c852165e121dbc0ca54304e40f|How is the data automatically generated?| The data generation scripts use a basic template to create each paradigm, pulling from a vocabulary of over 3000 words annotated for morphological, syntactic, and semantic features needed to create grammatical and semantically felicitous sentences.| | 
1907.04152|0153f563f5e2680c2de1a5f6d0e443454dc1ef2a|Do they fine-tune the used word embeddings on their medical texts?|No| | 
1907.04152|670c464a5dba78e0be7ec168fe36db604e172ea7|Which word embeddings do they use to represent medical visits?|GloVe, concatenation of average embeddings calculated separately for the interview and for the medical examination| | 
1907.04152|94ec0e205790ec663a5353f3c68c8d77701573c7|Do they explore similarity of texts across different doctors?|Yes| | 
1907.04152|a91878129583fcb6d16067ba8ba3600e39d70021|Which clustering technique do they use on partients' visits texts?|k-means, hierarchical clustering with Ward's method for merging clusters BIBREF23| | 
1909.12673|ff9495982b8821240b8a65eafcc9bb8ed8b8e084|What is proof that proposed functional form approximates well generalization error in practice?|estimated test accuracy is highly correlated with actual test accuracy for various datasets, appropriateness of the proposed function for modeling the complex error landscape| | 
1705.10586|2df2f6e4efd19023434c84f5b4f29a2f00bfc9fb|What other non-neural baselines do the authors compare to? |bag of words, tf-idf, bag-of-means| | 
2003.09971|95c7b27d192ab0edcdf203a74ce24f4a9a814e6c|What baseline function is used in REINFORCE algorithm?|baseline for each sampled caption is defined as the average reward of the rest samples| | 
1909.01958|48cf360a7753a23342f53f116eeccc2014bcc8eb|Is Aristo just some modern NLP model (ex. BERT) finetuned od data specific for this task?|Yes| | 
1702.01517|33d1f53cf25a7701db605b6b7ac36946af588bb7|Does they focus on any specific product/service domain?|local businesses (i.e. restaurants)| | 
1702.01517|0aa46c132515d8830a72f263812cdf7cbd5627c6|What are the baselines?|RS-Average , RS-Linear, RS-Item, RS-MF, Sum-Opinosis, Sum-LSTM-Att| | 
1909.09986|56a3c9bd74c6573abce3805177cdebf941db0b71|How is fluency of generated text evaluated?|manually reviewed| | 
1909.09986|73cd785d474bae7af974802715ef7ba5468d9139|How is faithfulness of the resulting text evaluated?|manually inspect| | 
1909.09986|65461516098ed63c45a567648e8e47c38ea7e58a|How are typing hints suggested?| concatenating to the embedding vector| | 
1909.09986|92d09654011d424cfef5691eec28ee934f88d954|What is the effectiveness plan generation?|clear mapping between plans and text helps to reduce these issues greatly, the system in BIBREF0 still has 2% errors, work in neural text generation and summarization attempt to address these issues| | 
1909.09986|8a94766f8251fa0bce0e09e5c69ce05761811a62|How is neural planning component trained?|plan-to-DFS mapping to perform the correct sequence of traversals, and train a neural classifier to act as a controller| | 
1707.04662|92cfac12d9583747bd9be8604275b4a9ddd8afe6|How do they evaluate interpretability in this paper?|we suggest to estimate the interpretability of $k$ th component as $ \operatorname{interp}_k W = \sum _{i,j=1}^N W_{i,k} W_{j,k} \left(W_i \cdot W_j \right). $| | 
1712.00609|2e6b7afaf14871ed6db674782b93709910020b06|How much better performing is the proposed method over the baselines?|original models were better in some tasks (CR, MPQA, MRPC), utilizing self-attentive sentence representation further improves performances in 5 out of 8 tasks| | 
1712.00609|d2e409031f4512375dd5cecec639c7373025f277|What baselines are the proposed method compared against?|(Layer Normalized Skip-Thoughts, ST-LN) BIBREF31, Cap2All, Cap2Cap, Cap2Img| | 
1712.00609|5d22746b3004c5e90ea714b24bf8bc7b4d15bd88|What dataset/corpus is this work evaluated over?|Karpathy and Fei-Fei's split for MS-COCO dataset BIBREF10| | 
1909.11879|80b9600e51a823f32bbce12fc52cba9700e2b8d2|Does the dataset contain non-English reviews?|Yes| | 
1909.11879|8aeaa3ccb8e062a01007c25a510b0dc1747ce66c|Does the paper report the performance of the method when is trained for more than 8 epochs?|No| | 
1701.04653|39cfb8473c8be4e5d8ecc3227b800a10477c5f80|What do the correlation demonstrate? |the extent to which the text obtained from the two platforms of Yahoo! Answers and Twitter reflect the true attributes of neighbourhoods| | 
1701.04653|f981781021d4bacbaf3d076c895dc42d7fa108ba|On Twitter, do the demographic attributes and answers show more correlations than on Yahoo! Answers?|No| | 
1701.04653|d5105a6a6d5d1931b0729dcf15ca862d6eac770f|How many demographic attributes they try to predict?|62| | 
1910.11491|be7b375b22d95d1f6c68c48f57ea87bf82c72123|What evaluation metrics do they use?|ROUGE F1, METEOR| | 
1910.11491|c4b5cc2988a2b91534394a3a0665b0c769b598bb|How do they define local variance?|The reciprocal of the variance of the attention distribution| | 
1909.00153|ef07ec34433221d4da79d5923fb00d8ac446b92c|How do they quantify alignment between the embeddings of a document and its translation?|median cosine similarity| | 
1909.00153|5cb3d69607f60e1c5be2120462726a477ead9570|Does adversarial learning have stronger performance gains for text classification, or for NER?|classification| | 
1909.00153|3b7798a6bce1a5faf411bb12e2e011dbab1e279d|Do any of the evaluations show that adversarial learning improves performance in at least two different language families?|Yes| | 
1909.03023|65ebed1971dca992c3751ed985fbe294cbe140d7|what experiments are conducted?|a reliability study for the proposed scheme | | 
1909.03023|b24b56ccc5d4b04fee85579b2dee77306ec829b2|what opportunities are highlighted?|Our annotation scheme introduces opportunities for the educational community to conduct further research , Once automated classifiers are developed, such relations between talk and learning can be examined at scale,  automatic labeling via a standard coding scheme can support the generalization of findings across studies, and potentially lead to automated tools for teachers and students, collecting and annotating corpora that can be used by the NLP community to advance the field in this particular area| | 
1909.03023|3bfdbf2d4d68e01bef39dc3371960e25489e510e|how do they measure discussion quality?|Measuring three aspects: argumentation, specificity and knowledge domain.| | 
1909.03023|9378b41f7e888e78d667e9763883dd64ddb48728|do they use a crowdsourcing platform?|No| | 
1909.11297|1afd550cbee15b753db45d7db2c969fc3d12a7d9|Is the model evaluated against the baseline also on single-aspect sentences?|No| | 
1909.11297|2a7c40a72b6380e76511e722b4b02b3a1e5078fd|Is the accuracy of the opinion snippet detection subtask reported?|No| | 
1912.11602|f95097cf4a0dc036fd8b80c007cd8d7a157b7816|What were the baselines?|$\textsc {Lead-X}$, $\textsc {PTGen}$, $\textsc {DRM}$, $\textsc {TConvS2S}$,  $\textsc {BottomUp}$, ABS, DRGD, SEQ$^3$, BottleSum, GPT-2| | 
1912.11602|bb8a0035b767688a98602c33f4714f8ac8ae60db|What metric was used in the evaluation step?|ROUGE-1, ROUGE-2 and ROUGE-L, F-measure ROUGE on XSUM and CNN/DailyMail, and use limited-length recall-measure ROUGE on NYT and DUC| | 
1912.11602|2a0a44f169ad61774d77df65f8846bd57685bfcf|What did they pretrain the model on?|hree years of online news articles from June 2016 to June 2019| | 
1912.11602|fd6c194632230e392088fc1f574c8626c6a2fa96|What does the data cleaning and filtering process consist of?|many news articles begin with reporter names, media agencies, dates or other contents irrelevant to the content, to ensure that the summary is concise and the article contains enough salient information, we only keep articles with 10-150 words in the top three sentences and 150-1200 words in the rest, and that contain at least 6 sentences in total, we try to remove articles whose top three sentences may not form a relevant summary| | 
1912.11602|487dc65bf8a8ecbf052cf05641caf1b90a502853|What unlabeled corpus did they use?|three years of online news articles from June 2016 to June 2019| | 
1901.04085|25c4fa78299481788a19d0c25ae07dfd8cb6315c|What is the TREC-CAR dataset?|in this dataset, the input query is the concatenation of a Wikipedia article title with the title of one of its section. The relevant passages are the paragraphs within that section| | 
1911.01680|dcdcd977f18206da3ff8ad0ffb14f7bc5e126c7d|How does their model utilize contextual information for each work in the given sentence in a multi-task setting? setting?|we use the context of the word to predict its label and by doing so our model learns label-aware context for each word in the sentence| | 
1911.01680|5efa19058f815494b72c44d746c157e9403f726e|What metris are used for evaluation?|micro-averaged F1 score| | 
1911.01680|71f135be79341e61c28c3150b1822d0c4d0ca8d6|How better is proposed model compared to baselines?| improves the F1 score by almost 2%, which corresponds to a 12.3% error rate reduction| | 
1911.01680|cb8e2069218e30c643013c20e93ebe23525d9f55|What are the baselines?|Adobe internal NLU tool, Pytext, Rasa| | 
1809.08298|7633be56ae46c163fb21cd1afd018f989eb6b524|Which machine learning models do they use to correct run-on sentences?|conditional random field model, Seq2Seq attention model| | 
2002.05104|649d6dc076251547aece6532f75d00fc99081d2b|What are least important components identified in the the training of VQA models?| some configurations used in some architectures (e.g., additional RNN layers) are actually irrelevant| | 
2002.05104|92412a449c28b9121a4a4f4acca996563f107131|What type of experiments are performed?|pre-trained word embeddings BIBREF11, BIBREF12, recurrent BIBREF13, transformer-based sentence encoders BIBREF14, distinct convolutional neural networks, standard fusion strategies,  two main attention mechanisms BIBREF18, BIBREF19| | 
2002.05104|76405b76b930a5bbe895e9e96ce4a3cff1b0b1a1|What components are identified as core components for training VQA models?|pre-trained text representations, transformer-based encoders together with GRU models, attention mechanisms are paramount for learning top performing networks, Top-Down is the preferred attention method| | 
1901.10746|f08502e952e711c629d40b22ee3f5ff626d62ba8|what approaches are compared?|MT metrics, Readability metrics and other sentence-level features, Metrics based on the baseline QuEst features, Metrics based on other features| | 
2001.07615|8e5e03a5f35f0820a3a1651e148dd6faf646fb67|What model do they use a baseline to estimate satisfaction?|a BiLSTM without attention (BiLSTM) as well as a single forward-LSTM layer with attention (LSTM+att) and without attention (LSTM), baselines are defined by BIBREF32 who already proposed an LSTM-based architecture that only uses non-temporal features, and the SVM-based estimation model as originally used for reward estimation by BIBREF24| | 
1909.13717|090fd9ce9a21438cdec1ea51ed216941d52eb3b6|what semantically conditioned models did they compare with?|Hierarchical Disentangled Self-Attention| | 
1810.07091|d13efa7dee280c7c2f6dc451c4fbbf0240fc2efa|Do they differentiate insights where they are dealing with learned or engineered representations?|Yes| | 
1810.07091|6cd01609c8afb425fbed941441a2528123352940|Do they show an example of usage for INFODENS?|Yes| | 
1604.00125|03b939ad70593f6475c56e9be73ba409d33faa62|What models do they compare to?|LEAD, QUERY_SIM, MultiMR, SVR, DocEmb, ISOLATION| | 
1801.02243|3d39e57e90903b776389f1b01ca238a6feb877f3|Do the authors give any examples of major events which draw the public's attention and the impact they have on stock price?|Yes| | 
1801.02243|69ef007fc131b04b5b71b0b446db2f77f434f1b3|Which tweets are used to output the daily sentiment signal?|Tesla and Ford are investigated on how Twitter sentiment could impact the stock price| | 
1801.02243|20df24165b881f97dc1ac32f343939554dd68011|What is the baseline machine learning prediction approach?|linear logistic regression to a set of stock technical signals| | 
1711.00331|551f77b58c48ee826d78b4bf622bb42b039eca8c|What are the weaknesses of their proposed interpretability quantification method?|can be biased by dataset used and may generate categories which are suboptimal compared to human designed categories| | 
1711.00331|74cd51a5528c6c8e0b634f3ad7a9ce366dfa5706|What advantages does their proposed method of quantifying interpretability have over the human-in-the-loop evaluation they compare to?|it is less expensive and quantifies interpretability using continuous values rather than binary evaluations| | 
1808.04614|e2a637f1d93e1ea9f29c96ff0fc6bc017209065b|How do they gather data for the query explanation problem?|hand crafted by users| | 
1808.04614|b3bd217287b8c765b0d461dc283afec779dbf039|Which query explanation method was preffered by the users in terms of correctness?|hybrid approach| | 
1808.04614|e8647f9dc0986048694c34ab9ce763b3167c3deb|Do they conduct a user study where they show an NL interface with and without their explanation?|No| | 
1707.06939|84d36bca06786070e49d3db784e42a51dd573d36|What was the task given to workers?|conceptualization task| | 
1707.06939|7af01e2580c332e2b5e8094908df4e43a29c8792|How was lexical diversity measured?|By computing number of unique responses and number of responses divided by the number of unique responses to that question for each of the questions| | 
1707.06939|c78f18606524539e4c573481e5bf1e0a242cc33c|How many responses did they obtain?|1001| | 
1707.06939|0cf6d52d7eafd43ff961377572bccefc29caf612|What crowdsourcing platform was used?|AMT| | 
1805.04033|ddd6ba43c4e1138156dd2ef03c25a4c4a47adad0|Are results reported only for English data?|No| | 
1805.04033|73bb8b7d7e98ccb88bb19ecd2215d91dd212f50d|What human evaluation method is proposed?|comparing the summary with the text instead of the reference and labeling the candidate bad if it is incorrect or irrelevant| | 
1911.09845|86e3136271a7b93991c8de5d310ab15a6ac5ab8c|How is human evaluation performed, what were the criteria?|(1) Good (3 points): The response is grammatical, semantically relevant to the query, and more importantly informative and interesting, (2) Acceptable (2 points): The response is grammatical, semantically relevant to the query, but too trivial or generic, (3) Failed (1 point): The response has grammar mistakes or irrelevant to the query| | 
1911.09845|b48cd91219429f910b1ea6fcd6f4bd143ddf096f|What automatic metrics are used?|BLEU, Distinct-1 & distinct-2| | 
1911.09845|4f1a5eed730fdcf0e570f9118fc09ef2173c6a1b|What other kinds of generation models are used in experiments?| Seq2seq, CVAE, Hierarchical Gated Fusion Unit (HGFU), Mechanism-Aware Neural Machine (MANM)| | 
1911.09845|4bdad5a20750c878d1a891ef255621f6172b6a79|How does discrete latent variable has an explicit semantic meaning to improve the CVAE on short-text conversation?|we connect each latent variable with a word in the vocabulary, thus each latent variable has an exact semantic meaning.| | 
1909.07512|2e3265d83d2a595293ed458152d3ee76ad19e244|What news dataset was used?|collection of headlines published by HuffPost BIBREF12 between 2012 and 2018| | 
1909.07512|c2432884287dca4af355698a543bc0db67a8c091|How do they determine similarity between predicted word and topics?|number of relevant output words as a function of the headline’s category label| | 
1909.07512|226ae469a65611f041de3ae545be0e386dba7d19|What is the language model pre-trained on?|Wikipedea Corpus and BooksCorpus| | 
1910.06748|3f9ef59ac06db3f99b8b6f082308610eb2d3626a|Which existing language ID systems are tested?|langid.py library, encoder-decoder EquiLID system, GRU neural network LanideNN system, CLD2, CLD3| | 
1910.08418|557d1874f736d9d487eb823fe8f6dab4b17c3c42|Which language family does Mboshi belong to?|Bantu| | 
1910.08418|f41c401a4c6e1be768f8e68f774af3661c890ffd|Does the paper report any alignment-only baseline?|Yes| | 
1910.08418|09cd7ae01fe97bba230c109d0234fee80a1f013b|What is the dataset used in the paper?|French-Mboshi 5K corpus| | 
1910.08418|be3e020ba84bc53dfb90b8acaf549004b66e31e2|How is the word segmentation task evaluated?|precision, recall, and F-measure on boundaries (BP, BR, BF), and tokens (WP, WR, WF),  exact-match (X) metric| | 
1911.08673|24014a040447013a8cf0c0f196274667320db79f|What are performance compared to former models?|model overall still gives 1.0% higher average UAS and LAS than the previous best parser, BIAF, our model reports more than 1.0% higher average UAS than STACKPTR and 0.3% higher than BIAF| | 
1906.06442|c431c142f5b82374746a2b2f18b40c6874f7131d|What datasets was the method evaluated on?|WMT18 EnDe bitext, WMT16 EnRo bitext, WMT15 EnFr bitext, We perform our experiments on WMT18 EnDe bitext, WMT16 EnRo bitext, and WMT15 EnFr bitext respectively. We use WMT Newscrawl for monolingual data (2007-2017 for De, 2016 for Ro, 2007-2013 for En, and 2007-2014 for Fr). For bitext, we filter out empty sentences and sentences longer than 250 subwords. We remove pairs whose whitespace-tokenized length ratio is greater than 2. This results in about 5.0M pairs for EnDe, and 0.6M pairs for EnRo. We do not filter the EnFr bitext, resulting in 41M sentence pairs.| | 
2001.06785|7835d8f578386834c02e2c9aba78a345059d56ca|Is the model evaluated against a baseline?|No| | 
2001.06785|32e78ca99ba8b8423d4b21c54cd5309cb92191fc|How many people are employed for the subjective evaluation?|14 volunteers| | 
1707.07554|ffc5ad48b69a71e92295a66a9a0ff39548ab3cf1|What other embedding models are tested?|GloVe embeddings trained by BIBREF10 on Wikipedia and Gigaword 5 (vocab: 400K, dim: 300), w2v-gn, Word2vec BIBREF5 trained on the Google News dataset (vocab: 3M, dim: 300), DeepWalk , node2vec| | 
1707.07554|1024f22110c436aa7a62a1022819bfe62dc0d336|How is performance measured?|To verify the reliability of the transformed semantic space, we propose an evaluation benchmark on the basis of word similarity datasets. Given an enriched space INLINEFORM0 and a similarity dataset INLINEFORM1 , we compute the similarity of each word pair INLINEFORM2 as the cosine similarity of their corresponding transformed vectors INLINEFORM3 and INLINEFORM4 from the two spaces, where INLINEFORM5 and INLINEFORM6 for LS and INLINEFORM7 and INLINEFORM8 for CCA. | | 
1707.07554|f062723bda695716aa7cb0f27675b7fc0d302d4d|How are rare words defined?|judged by 10 raters on a [0,10] scale| | 
1910.09295|50e3fd6778dadf8ec0ff589aa8b18c61bdcacd41|What other datasets are used?|WikiText-TL-39| | 
1910.09295|c5980fe1a0c53bce1502cc674c8a2ed8c311f936|What is the size of the dataset?|3,206| | 
1910.09295|7d3c036ec514d9c09c612a214498fc99bf163752|What is the source of the dataset?|Online sites tagged as fake news site by Verafiles and NUJP and news website in the Philippines, including Pilipino Star Ngayon, Abante, and Bandera| | 
1910.09295|ef7b62a705f887326b7ebacbd62567ee1f2129b3|What were the baselines?|Siamese neural network consisting of an embedding layer, a LSTM layer and a feed-forward layer with ReLU activations| | 
1602.01208|23d0637f8ae72ae343556ab135eedc7f4cb58032|How do they show that acquiring names of places helps self-localization?|unsupervised morphological analyzer capable of using lattices improved the accuracy of phoneme recognition and word segmentation, Consequently, this result suggests that this word segmentation method considers the multiple hypothesis of speech recognition as a whole and reduces uncertainty such as variability in recognition| | 
1602.01208|21c104d14ba3db7fe2cd804a191f9e6258208235|How do they evaluate how their model acquired words?|PAR score| | 
1602.01208|d557752c4706b65dcdb7718272180c59d77fb7a7|Which method do they use for word segmentation?|unsupervised word segmentation method latticelm| | 
1602.01208|1bdf7e9f3f804930b2933ebd9207a3e000b27742|Does their model start with any prior knowledge of words?|No| | 
1909.03242|a74886d789a5d7ebcf7f151bdfb862c79b6b8a12|What were the baselines?|a BiLSTM over all words in the respective sequences with randomly initialised word embeddings, following BIBREF30| | 
1905.12260|c33d0bc5484c38de0119c8738ffa985d1bd64424|Do the images have multilingual annotations or monolingual ones?|monolingual| | 
1905.12260|93b1b94b301a46251695db8194a2536639a22a88|Could you learn such embedding simply from the image annotations and without using visual information?|Yes| | 
1606.01404|f4e17b14318b9f67d60a8a2dad1f6b506a10ab36|How is the generative model evaluated?|Comparing BLEU score of model with and without attention| | 
1901.00439|fac052c4ad6b19a64d7db32fd08df38ad2e22118|How do they evaluate their method?|Calinski-Harabasz score, t-SNE, UMAP| | 
1901.00439|aa54e12ff71c25b7cff1e44783d07806e89f8e54|What is an example of a health-related tweet?|The health benefits of alcohol consumption are more limited than previously thought, researchers say| | 
1811.11136|1405824a6845082eae0458c94c4affd7456ad0f7|Was the introduced LSTM+CNN model trained on annotated data in a supervised fashion?|Yes| | 
1908.04531|5be94c7c54593144ba2ac79729d7545f27c79d37|What is the challenge for other language except English|not researched as much as English| | 
1908.04531|32e8eda2183bcafbd79b22f757f8f55895a0b7b2|How many categories of offensive language were there?|3| | 
1908.04531|b69f0438c1af4b9ed89e531c056d9812d4994016|How large was the dataset of Danish comments?|3600 user-generated comments| | 
1908.04531|2e9c6e01909503020070ec4faa6c8bf2d6c0af42|Who were the annotators?|the author and the supervisor| | 
2001.08845|fc65f19a30150a0e981fb69c1f5720f0136325b0|Is is known whether Sina Weibo posts are censored by humans or some automatic classifier?|No| | 
1901.02222|5067e5eb2cddbb34b71e8b74ab9210cd46bb09c5|Which matching features do they employ?|Matching features from matching sentences from various perspectives.| | 
1810.10797|b974523a6cbd3cdc1fa924243ccc9711bbc7070d|How often are the newspaper websites crawled daily?|RSS feeds in French on a daily basis| | 
1608.02195|981443fce6167b3f6cadf44f9f108d68c1a3f4ab|Which countries and languages do the political speeches and manifestos come from?|german | | 
1608.02195|6d0f2cce46bc962c6527f7b4a77721799f2455c6|Do changes in policies of the political actors account for all of the mistakes the model made?|Yes| | 
1608.02195|5816ebf15e31bdf70e1de8234132e146d64e31eb|What model are the text features used in to provide predictions?| multinomial logistic regression| | 
1804.08050|948327d7aa9f85943aac59e3f8613765861f97ff|Does each attention head in the decoder calculate the same output?|No| | 
1806.03191|cdf7e60150a166d41baed9dad539e3b93b544624|Which distributional methods did they consider?|WeedsPrec BIBREF8, invCL BIBREF11, SLQS model, cosine similarity| | 
1806.03191|c06b5623c35b6fa7938340fa340269dc81d061e1|Which benchmark datasets are used?|noun-noun subset of bless, leds BIBREF13, bless, wbless, bibless, hyperlex BIBREF20| | 
1806.03191|d325a3c21660dbc481b4e839ff1a2d37dcc7ca46|What hypernymy tasks do they study?|Detection, Direction, Graded Entailment| | 
2002.06424|eae13c9693ace504eab1f96c91b16a0627cd1f75|Do they repot results only on English data?|Yes| | 
2002.06424|bcec22a75c1f899e9fcea4996457cf177c50c4c5|What were the variables in the ablation study?|(i) zero NER-specific BiRNN layers, (ii) zero RE-specific BiRNN layers, or (iii) zero task-specific BiRNN layers of any kind| | 
1705.03487|91c81807374f2459990e5f9f8103906401abc5c2|What is barycentric Newton diagram?| The basic idea of the visualization, drawing on Isaac Newton’s visualization of the color spectrum BIBREF8 , is to express a mixture in terms of its constituents as represented in barycentric coordinates.| | 
1912.02761|2cc42d14c8c927939a6b8d06f4fdee0913042416|Do they propose any solution to debias the embeddings?|No| | 
1909.05246|2ea382c676e418edd5327998e076a8c445d007a5|Is human evaluation performed?|No| | 
1909.05246|bd7a95b961af7caebf0430a7c9f675816c9c527f|What are the three datasets used?|DSTC2, M2M-sim-M, M2M-sim-R| | 
1810.11118|f011d6d5287339a35d00cd9ce1dfeabb1f3c0563|Did they experiment with the corpus?|Yes| | 
1906.02715|2ba0c7576eb5b84463a59ff190d4793b67f40ccc|How were the feature representations evaluated?|attention probes, using visualizations of the activations created by different pieces of text| | 
1906.02715|c58e60b99a6590e6b9a34de96c7606b004a4f169|What linguistic features were probed for?|dependency relation between two words, word sense| | 
1903.08237|6a099dfe354a79936b59d651ba0887d9f586eaaf|Does the paper describe experiments with real humans?|Yes| | 
1807.08666|f748cb05becc60e7d47d34f4c5f94189bc184d33|What are bottleneck features?|Bulgarian, Czech, French, German, Korean, Polish, Portuguese, Russian, Thai, Vietnamese, South African English, These features are typically obtained by training a deep neural network jointly on several languages for which labelled data is available., The final shared layer often has a lower dimensionality than the input layer, and is therefore referred to as a `bottleneck'.| | 
1807.08666|1a06b7a2097ebbad0afc787ea0756db6af3dadf4|What languages are considered?|Bulgarian, Czech, French, German, Korean, Polish, Portuguese, Russian, Thai, Vietnamese| | 
2002.08126|86083a02cc9a80b31cac912c42c710de2ef4adfd|How do they obtain language identities?|model is trained to predict language IDs as well as the subwords, we add language IDs in the CS point of transcriptio| | 
1809.01341|29e5e055e01fdbf7b90d5907158676dd3169732d|What other multimodal knowledge base embedding methods are there?|merging, concatenating, or averaging the entity and its features to compute its embeddings, graph embedding approaches, matrix factorization to jointly embed KB and textual relations| | 
1809.06963|6c4d121d40ce6318ecdc141395cdd2982ba46cff|What is the data selection paper in machine translation|BIBREF7, BIBREF26 | | 
1901.10826|b1457feb6cdbf4fb19c8e87e1cd43981bc991c4c|Do they compare computational time of AM-softmax versus Softmax?|No| | 
1901.10826|46bca122a87269b20e252838407a2f88f644ded8|Do they visualize the difference between AM-Softmax and regular softmax?|Yes| | 
1606.04631|7c792cda220916df40edb3107e405c86455822ed|what metrics were used for evaluation?|METEOR| | 
1708.07241|1192e9a265956aa08177caea7d3c38d501707809|What datasets do they use for the tasks?| Viet Treebank corpus for POS tagging and chunking tasks, and on VLSP shared task 2016 corpus for NER task| | 
1810.01570|864b5c1fe8c744f80a55e87421b29d6485b7efd0|What evaluation metrics do they use?|Precision, Recall and INLINEFORM0 score| | 
1810.01570|0a050658d09f3c6e21e9ab828dc18e59b147cf7c|Do they use BERT?|No| | 
1810.01570|fd80a7162fde83077ed82ae41d521d774f74340a|What is their baseline?|Burckhardt et al. BIBREF22, Liu et al. BIBREF18, Dernoncourt et al. BIBREF9, Yang et al. BIBREF10| | 
1810.01570|4d4739682d540878a94d8227412e9e1ec1bb3d39|Which two datasets is the system tested on?|2014 i2b2 de-identification challenge data set BIBREF2, nursing notes corpus BIBREF3| | 
1910.10288|e4024db40f4b8c1ce593f53b28718e52d5007cd2|How they compare varioius mechanisms in terms of naturalness?|using mean opinion score (MOS) naturalness judgments produced by a crowd-sourced pool of raters| | 
1908.06083|c84590ba32df470a7c5343d8b99e541b217f10cf|What datasets are used?|The Wikipedia Toxic Comments dataset| | 
1910.12129|88e9e5ad0e4c369b15d81a4e18f7d12ff8fa9f1b|Is the origin of the dialogues in corpus some video game and what game is that?|No| | 
1910.12129|e93b4a15b54d139b768d5913fb5fd1aed8ab25da|How the authors made sure that corpus is clean despite being crowdsourced?|manually cleaned human-produced utterances| | 
1811.01786|993ee7de848ab6adfe02fa728b3a2c896238859b|Do they build a generative probabilistic language model for sign language?|No| | 
1908.10422|702e2d02c25a2f3f6b1be8ad3d448b502b8ced9c|How do they obtain human generated policies?|derive rewards from human-human dialogues by assigning positive values to contextualised responses seen in the data, and negative values to randomly chosen responses due to lacking coherence| | 
1908.10422|a83a351539fb0b6acb5bdee32323dd924f4fd1e7|How many agents do they ensemble over?|100 | | 
1710.09753|b8ffb81e74c1c1ad552051aca8741b0141ae6e97|What is the task of slot filling?|The task aims at extracting information about persons, organizations or geo-political entities from a large collection of news, web and discussion forum documents.| | 
1710.00341|29d917cc38a56a179395d0f3a2416fca41a01659|How are the potentially relevant text fragments identified?| Generate a query out of the claim and querying a search engine, rank the words by means of TF-IDF, use IBM's AlchemyAPI to identify named entities, generate queries of 5–10 tokens, which execute against a search engine, and collect the snippets and the URLs in the results, skipping any result that points to a domain that is considered unreliable.| | 
1710.00341|ad4658c64056b6eddda00d3cbc55944ae01eb437|What algorithm and embedding dimensions are used to build the task-specific embeddings?| task-specific embedding of the claim together with all the above evidence about it, which comes from the last hidden layer of the NN| | 
1710.00341|89b9e298993dbedd3637189c3f37c0c4791041a1|What data is used to build the task-specific embeddings?|embedding of the claim, Web evidence| | 
1606.06361|75773ee868c0429ccb913eceb367ff0782eeda8a|Do they evaluate the syntactic parses?|No| | 
1606.06361|11ed8c4d98a4e8994990edba54319efe9c6745f2|What knowledge bases do they use?|NELL| | 
1910.14254|08cbc9b8a8df56ec7be626f89285a621e1350f63|Which dataset do they use?|the annotated dataset reported by degen2015investigating, a dataset of the utterances from the Switchboard corpus of telephone dialogues BIBREF21 that contain the word some| | 
1911.02821|6b4de7fef3a543215f16042ce6a29186bf84fea4|What pre-trained models did they compare to?|BERT, ERNIE, and BERT-wwm| | 
1911.02821|3a62dd5fece70f8bf876dcbb131223682e3c54b7|How does the fusion method work?|ttention vector sequence is segmented into several subsequences and each subsequence represents the attention of one word, we devise an appropriate aggregation module to fuse the inner-word character attention| | 
1911.02821|2c20426c003f7e3053f8e6c333f8bb744f6f31f8|What benchmarks did they experiment on?|Emotion Classification (EC), Named Entity Recognition (NER), Sentence Pair Matching (SPM), Natural Language Inference (NLI)| | 
1909.02265|d1909ce77d09983aa1b3ab5c56e2458caefbd442|What were the evaluation metrics used?|entity match rate, BLEU score, Success F1 score| | 
1909.02265|fc3f0eb297b2308b99eb4661a510c9cdbb6ffba2|What is the size of the dataset?|3029| | 
1909.02265|27c1c678d3862c7676320ca493537b03a9f0c77a|What multi-domain dataset is used?|KVRET| | 
1909.02265|ccb3d21885250bdbfc4c320e99f25923896e70fa|Which domains did they explored?|calendar, weather, navigation| | 
1906.10551|61b0db2b5718d409b07f83f912bad6a788bfee5a|Do they report results only on English data?|Yes| | 
1906.10551|b217d9730ba469f48426280945dbb77542b39183|Which is the best performing method?|Caravel, COAV and NNCD| | 
1906.10551|8c0846879771c8f3915cc2e0718bee448f5cb007|What size are the corpora?|80 excerpts from scientific works, collection of 1,645 chat conversations, collection of 200 aggregated postings| | 
1906.10551|3fae289ab1fc023bce2fa4f1ce4d9f828074f232|What is a self-compiled corpus?| restrict the content of each text to the abstract and conclusion of the original work, considered other parts of the original works such as introduction or discussion sections, extracted text portions are appropriate for the AV task, each original work was preprocessed manually, removed tables, formulas, citations, quotes and sentences that include non-language content such as mathematical constructs or specific names of researchers, systems or algorithms| | 
1704.02385|37c7c62c9216d6cf3d0858cf1deab6db4b815384|how was annotation done?|Annotation was done with the help of annotators from Amazon Mechanical Turk on snippets of conversations| | 
1704.02385|539eb559744641e6a4aefe267cbc4c79e2bcceae|what is the source of the new dataset?|Reddit| | 
2004.03925|d0444cbf01efdcc247b313c7487120a2f047f421|Do the authors give examples of positive and negative sentiment with regard to the virus?|No| | 
2004.03925|1f6666c2c1d1d5f66208a6fa7da3b3442a577dbc|Which word frequencies reflect on the psychology of the twitter users, according to the authors?|unigram, bigram and trigram| | 
2004.03925|a78a6fd6ca36413586836838e38f3fa9282646ee|Do they specify which countries they collected twitter data from?|No| | 
2004.03925|c4a0c7b6f1a00f3233a5fe16240a98d9975701c0|Do they collect only English data?|No| | 
1708.01776|2ec97cf890b537e393c2ce4c2b3bd05dfe46f683|How do they measure correlation between the prediction and explanation quality?|They look at the performance accuracy of explanation and the prediction performance| | 
1703.08098|47ecaca8adc7306e3014e8c4358e306a5f0e1716|What models does this overview cover?|This article presented a brief overview of embedding models of entity and relationships for KB completion. | | 
1808.03430|255fb6e20b95092c548ba47d8a295468e06698bd|What datasets are used to evaluate the introduced method?|"They used a dataset from Taobao which contained a collection of conversation records between customers and customer service staffs. It contains over five kinds of conversations,
including chit-chat, product and discount consultation, querying delivery progress and after-sales feedback. "| | 
1607.00424|496b4ae3c0e26ec95ff6ded5e6790f24c35f0f5b|How do they incorporate human advice?|by converting human advice to first-order logic format and use as an input to calculate gradient| | 
1607.00424|281cb27cfa0eea12180fd82ae33035945476609e|What do they learn jointly?|relations| | 
1901.01911|04a4b0c6c8bd4c170c93ea7ea1bf693965ef38f4|Is this an English-language dataset?|Yes| | 
1901.01911|dbfce07613e6d0d7412165e14438d5f92ad4b004|What affective-based features are used?|affective features provided by different emotion models such as Emolex, EmoSenticNet, Dictionary of Affect in Language, Affective Norms for English Words and Linguistics Inquiry and Word Count| | 
1901.01911|b7e419d2c4e24c40b8ad0fae87036110297d6752|What conversation-based features are used?|Text Similarity to Source Tweet, Text Similarity to Replied Tweet, Tweet Depth| | 
1805.03122|be9cadaebfa0ff1a3c5a5ed56ff3aae76cf5e0a4|What are the evaluation metrics used?|average accuracy over each single-language model (Avg), and accuracy obtained when training on the concatenation of all languages but the target one (All)| | 
2002.02800|aa979aed5a454b6705d0085ba2777859feb6fc62|Do they report results only on English datasets?|Yes| | 
1906.09777|2cfcc5864a30259fd35f1cc035fab956802c1c5b|What datasets or tasks do they conduct experiments on?|Language Modeling (LM), PTB BIBREF25 , WikiText-103 BIBREF26 and One-Billion Word benchmark BIBREF27 datasets, neural machine translation (NMT), WMT 2016 English-German dataset| | 
1911.05153|4bd894c365d85e20753d9d2cb6edebb8d6f422e9|How authors create adversarial test set to measure model robustness?|we devise a test set consisting of ‘adversarial’ examples, i.e, perturbed examples that can potentially change the base model's prediction. , We use two approaches described in literature: back-translation and noisy sequence autoencoder.| | 
1805.07513|5c4a2a3d6e02bcbeae784e439441524535916e85|Do they compare with the MAML algorithm?|No| | 
1811.02906|38a5cc790f66a7362f91d338f2f1d78f48c1e252|What baseline is used?|SVM| | 
1811.02906|0da6cfbc8cb134dc3d247e91262f5050a2200664|What topic clusters are identified by LDA?|Clusters of Twitter user ids from accounts of American or German political actors, musicians, media websites or sports club| | 
1811.02906|9003c7041d3d2addabc2c112fa2c7efe5fab493c|What are the near-offensive language categories?|inappropriate, discriminating| | 
1903.09588|3554ac92d4f2d00dbf58f7b4ff2b36a852854e95|How do they generate the auxiliary sentence?|The sentence we want to generate from the target-aspect pair is a question, and the format needs to be the same., For the NLI task, the conditions we set when generating sentences are less strict, and the form is much simpler., For QA-B, we add the label information and temporarily convert TABSA into a binary classification problem ( INLINEFORM0 ) to obtain the probability distribution, auxiliary sentence changes from a question to a pseudo-sentence| | 
2004.01670|7b35593033e4c6b9dccba98f22a7eeaa3385df38|Have any baseline model been trained on this abusive language dataset?|No| | 
2004.01670|334972ba967444f98865dea4c2bc0eb9416f2ff7|How big are this dataset and catalogue?| from 469 posts to 17 million| | 
2004.01670|4d4550533edb19c38cb876b1640e62e34e2b88e0|What is open website for cataloguing abusive language data?|hatespeechdata.com| | 
1906.01946|1a7d2ade16149630c0028339a816fcafa8192408|how many speeches are in the dataset?|7,507| | 
1804.05868|df2839dbd68ed9d5d186e6c148fa42fce60de64f|How big is the provided treebank?|1448 sentences more than the dataset from Bhat et al., 2017| | 
1710.09589|97159b8b1ab360c34a1114cd81e8037474bd37db|is the dataset balanced across the four languages?|No| | 
1710.09589|cb20aebfedad1a306e82966d6e9e979129fcd9f9|what evaluation metrics were used?|weighted F1-score| | 
1710.09589|45a2ce68b4a9fd4f04738085865fbefa36dd0727|what dataset was used?|The dataset from a joint ADAPT-Microsoft project| | 
1904.01608|be7f52c4f2bad20e728785a357c383853d885d94|What is the size of ACL-ARC datasets?|includes 1,941 citation instances from 186 papers| | 
1912.03184|4c50f75b1302f749c1351de0782f2d658d4bea70|How is quality of annotation measured?|Annotators went through various phases to make sure their annotations did not deviate from the mean.| | 
1807.08447|a8e0796c1ac353d428d84f4506a92b51bce51b87|On what data is the model evaluated?|D-IMDB (derived from large scale IMDB data snapshot), D-FB (derived from large scale Freebase data snapshot)| | 
1911.13066|2c88b46c7e3a632cfa10b7574276d84ecec7a0af|What is their baseline model?|the model proposed in BIBREF3| | 
1911.13066|6ff240d985bbe96b9d5042c9b372b4e8f498f264|What is the size of the dataset?|$0.3$ million records| | 
1908.05969|54c9147ffd57f1f7238917b013444a9743f0deb8|Which are the sequence model architectures this method can be transferred across?|The sequence model architectures which this method is transferred to are: LSTM and Transformer-based models| | 
1908.05969|16f71391335a5d574f01235a9c37631893cd3bb0| What percentage of improvement in inference speed is obtained by the proposed method over the newest state-of-the-art methods?|Across 4 datasets, the best performing proposed model (CNN) achieved an average of 363% improvement over the state of the art method (LR-CNN)| | 
1804.11297|33f72c8da22dd7d1378d004cbd8d2dcd814a5291|What is the metric that is measures in this paper?|error rate in a minimal pair ABX discrimination task| | 
1705.01991|4e2e19a58e1f2cc5a7b1bc666c1577922454d8c8|Do they only test on one dataset?|Yes| | 
1705.01991|69ca609e86888c7e4e2e3d33435a0a36f77601b5|What baseline decoder do they use?|a standard beam search decoder BIBREF5 with several straightforward performance optimizations| | 
1910.13215|537a786794604ecc473fb3ef6222e0c3cb81f772|What dataset was used in this work?|How2| | 
1809.02731|dc5ff2adbe1a504122e3800c9ca1d348de391c94|How do they evaluate the sentence representations?|"The unsupervised tasks include five tasks from SemEval Semantic Textual Similarity (STS) in 2012-2016 BIBREF30 , BIBREF31 , BIBREF32 , BIBREF33 , BIBREF34 and the SemEval2014 Semantic Relatedness task (SICK-R) BIBREF35 .

The cosine similarity between vector representations of two sentences determines the textual similarity of two sentences, and the performance is reported in Pearson's correlation score between human-annotated labels and the model predictions on each dataset., Supervised Evaluation
It includes Semantic relatedness (SICK) BIBREF35 , SemEval (STS-B) BIBREF36 , paraphrase detection (MRPC) BIBREF37 , question-type classification (TREC) BIBREF38 , movie review sentiment (MR) BIBREF39 , Stanford Sentiment Treebank (SST) BIBREF40 , customer product reviews (CR) BIBREF41 , subjectivity/objectivity classification (SUBJ) BIBREF42 , opinion polarity (MPQA) BIBREF43 ."| | 
1809.02731|04b43deab0fd753e3419ed8741c10f652b893f02|What are the two decoding functions?|a linear projection and a bijective function with continuous transformation though  ‘affine coupling layer’ of (Dinh et al.,2016). | | 
1912.00159|515e10a71d78ccd9c7dc93cd942924a4c85d3a30|How is language modelling evaluated?|perplexity of the models| | 
1905.10702|52e8c9ed66ace1780e41815260af1309064d20de|What datasets are used to evaluate the model?|WN18 and FB15k| | 
1909.05855|3ee721c3531bf1b9a1356a40205d088c9a7a44fc|How did they gather the data?|simulation-based dialogue generation, where the user and system roles are simulated to generate a complete conversation flow, which can then be converted to natural language using crowd workers | | 
2002.02758|544b68f6f729e5a62c2461189682f9e4307a05c6|What is their baseline?|Google's Neural Machine Translation| | 
1905.01962|ace60950ccd6076bf13e12ee2717e50bc038a175|How are the two different models trained?|They pre-train the models using 600000 articles as an unsupervised dataset and then fine-tune the models on small training set.| | 
1905.01962|2e1660405bde64fb6c211e8753e52299e269998f|How long is the dataset?|645, 600000| | 
1909.06434|82a28c1ed7988513d5984f6dcacecb7e90f64792|How big are negative effects of proposed techniques on high-resource tasks?|The negative effects were insignificant.| | 
1909.06434|d4a6f5034345036dbc2d4e634a8504f79d42ca69|What datasets are used for experiments?|the WMT'14 English-French (En-Fr) and English-German (En-De) datasets.| | 
1909.06434|54fa5196d0e6d5e84955548f4ef51bfd9b707a32|Are this techniques used in training multilingual models, on what languages?|English to French and English to German| | 
1909.06434|a997fc1a62442fd80d1873cd29a9092043f025ad|What baselines non-adaptive baselines are used?|Transformer models in their base configuration BIBREF11, using 6 encoder and decoder layers, with model and hidden dimensions of 512 and 2048 respectively, and 8 heads for all attention layers| | 
1808.09633|9a4aa0e4096c73cd2c3b1eab437c1bf24ae7bf03|What text sequences are associated with each vertex?|abstracts, sentences| | 
1703.02507|09a993756d2781a89f7ec5d7992f812d60e24232|Do they report results only on English data?|Yes| | 
1703.02507|cdf1bf4b202576c39e063921f6b63dc9e4d6b1ff|What metric is used to measure performance?|Accuracy and F1 score for supervised tasks, Pearson's and Spearman's correlation for unsupervised tasks| | 
1703.02507|03f4e5ac5a9010191098d6d66ed9bbdfafcbd013|How do the n-gram features incorporate compositionality?|by also learning source embeddings for not only unigrams but also n-grams present in each sentence, and averaging the n-gram embeddings along with the words| | 
1708.05592|9a9338d0e74fd315af643335e733445031bd7656|Which dataset do they use?| AMI IHM meeting corpus| | 
1911.08915|3103502cf07726d3eeda34f31c0bdf1fc0ae964e|How do Zipf and Herdan-Heap's laws differ?|Zipf's law describes change of word frequency rate, while Heaps-Herdan describes different word number in large texts (assumed that Hepas-Herdan is consequence of Zipf's)| | 
1810.05334|aaec98481defc4c230f84a64cdcf793d89081a76|What was the best performing baseline?|Lead-3| | 
1810.05334|69b41524dc5820143e45f2f3545cd5c0a70e2922|Which approaches did they use?|SumBasic, Lsa, LexRank, TextRank, Bayes, Hmm, MaxEnt, NeuralSum, Lead-N| | 
1810.05334|72122e0bc5da1d07c0dadb3401aab2acd748424d|What is the size of the dataset?|20K| | 
1810.05334|1af4d56eeaf74460ca2c621a2ad8a5d8dbac491c|Did they use a crowdsourcing platform for the summaries?|No| | 
2004.04696|3f5f74c39a560b5d916496e05641783c58af2c5d|How are the synthetic examples generated?|Random perturbation of Wikipedia sentences using mask-filling with BERT, backtranslation and randomly drop out| | 
1710.09340|07f5e360e91b99aa2ed0284d7d6688335ed53778|Do they measure the number of created No-Arc long sequences?|No| | 
2003.04967|bcce5eef9ddc345177b3c39c469b4f8934700f80|Do they evaluate only on English datasets?|Yes| | 
2003.04967|d3092f78bdbe7e741932e3ddf997e8db42fa044c|What experimental evaluation is used?|root mean square error between the actual and the predicted price of Bitcoin for every minute| | 
2003.04967|e2427f182d7cda24eb7197f7998a02bc80550f15|How is the architecture fault-tolerant?|By using Apache Spark which stores all executions in a lineage graph and recovers to the previous steady state from any fault| | 
2003.04967|0457242fb2ec33446799de229ff37eaad9932f2a|Which elements of the platform are modular?|handling large volume incoming data, sentiment analysis on tweets and predictive online learning| | 
1910.02334|5e997d4499b18f1ee1ef6fa145cadbc018b8dd87|What is the source of memes?|Google Images, Reddit Memes Dataset| | 
1910.02334|12c7d79d2a26af2d445229d0c8ba3ba1aab3f5b5|Is the dataset multimodal?|Yes| | 
1910.02334|98daaa9eaa1e1e574be336b8933b861bfd242e5e|How is each instance of the dataset annotated?|weakly labeled into hate or non-hate memes, depending on their source| | 
1911.05343|a93196fb0fb5f8202912971e14552fd7828976db|Which dataset do they use for text modelling?|Penn Treebank (PTB), end-to-end (E2E) text generation corpus| | 
1911.05343|983c2fe7bdbf471bb8b15db858fd2cbec86b96a5|Do they compare against state of the art text generation?|Yes| | 
1911.05343|a5418e4af99a2cbd6b7a2b8041388a2d01b8efb2|How do they evaluate generated text quality?|Loss analysis. To conduct a more thorough evaluation, we further investigate model behaviours in terms of both reconstruction loss and KL loss, as shown in Figure FIGREF14. These plots were obtained based on the E2E training set using the inputless setting.| | 
1809.01500|9e9e9e0a52563b42e96b8c89ea12f5a916daa7f0|Was the system only evaluated over the second shared task?|No| | 
1808.08780|0bd683c51a87a110b68b377e9a06f0a3e12c8da0|What are the tasks that this method has shown improvements?|bilingual dictionary induction, monolingual and cross-lingual word similarity, and cross-lingual hypernym discovery| | 
1808.08780|a979749e59e6e300a453d8a8b1627f97101799de|Why does the model improve in monolingual spaces as well? |because word pair similarity increases if the two words translate to similar parts of the cross-lingual embedding space| | 
1704.04539|8fa7011e7beaa9fb4083bf7dd75d1216f9c7b2eb|Do the authors test their annotation projection techniques on tasks other than AMR?|No| | 
1704.04539|e0b7acf4292b71725b140f089c6850aebf2828d2|How is annotation projection done when languages have different word order?|Word alignments are generated for parallel text, and aligned words are assumed to also share AMR node alignments.| | 
1906.11180|b6ffa18d49e188c454188669987b0a4807ca3018|What is the reasoning method that is used?|SPARQL| | 
1906.11180|2b61893b22ac190c94c2cb129e86086888347079|What KB is used in this work?|DBpedia| | 
1911.06747|65e2f97f2fe8eb5c2fa41cb95c02b577e8d6e5ee|How did they measure effectiveness?|number of dialogs that resulted in launching a skill divided by total number of dialogs| | 
1911.03681|83f14af3ccca4ab9deb4c6d208f624d1e79dc7eb|Which of the two ensembles yields the best performance?|Answer with content missing: (Table 2) CONCAT ensemble| | 
1911.03681|0154d8be772193bfd70194110f125813057413a4|What are the two ways of ensembling BERT and E-BERT?|mean-pooling their outputs (AVG), concatenating the entity and its name with a slash symbol (CONCAT)| | 
1911.03681|e737cfe0f6cfc6d3ac6bec32231d9c893bfc3fc9|How is it determined that a fact is easy-to-guess?| filter deletes all KB triples where the correct answer (e.g., Apple) is a case-insensitive substring of the subject entity name (e.g., Apple Watch), person name filter uses cloze-style questions to elicit name associations inherent in BERT, and deletes KB triples that correlate with them| | 
1908.06379|42be49b883eba268e3dbc5c3ff4631442657dcbb|How is dependency parsing empirically verified?| At last, our model is evaluated on two benchmark treebanks for both constituent and dependency parsing. The empirical results show that our parser reaches new state-of-the-art for all parsing tasks.| | 
1908.06379|8d4f0815f8a23fe45c298c161fc7a27f3bb0d338|How are different network components evaluated?|For different numbers of shared layers, the results are in Table TABREF14. We respectively disable the constituent and the dependency parser to obtain a separate learning setting for both parsers in our model. | | 
1908.06379|b0fbd4b0f02b877a0d3df1d8ccc47d90dd49147c|What are the models used to perform constituency and dependency parsing?|token representation, self-attention encoder,, Constituent Parsing Decoder,  Dependency Parsing Decoder| | 
1706.09147|22b8836cb00472c9780226483b29771ae3ebdc87|What is the new initialization method proposed in this paper?|They initialize their word and entity embeddings with vectors pre-trained over a large corpus of unlabeled data.| | 
1706.09147|540e9db5595009629b2af005e3c06610e1901b12|How was a quality control performed so that the text is noisy but the annotations are accurate?|The authors believe that the Wikilinks corpus  contains ground truth annotations while being noisy. They discard mentions that cannot have ground-truth verified by comparison with Wikipedia.| | 
1711.06351|bd1a3c651ca2b27f283d3f36df507ed4eb24c2b0|Is it a neural model? How is it trained?|No, it is a probabilistic model trained by finding feature weights through gradient ascent| | 
1911.08370|0c78d2fe8bc5491b5fd8a2166190c59eba069ced|How are the clusters related to security, violence and crime identified?|Yes| | 
1805.00460|d2473c039ab85f8e9e99066894658381ae852e16|What are the features of used to customize target user interaction? |image feature, question feature, label vector for the user's answer| | 
1911.03350|5d6cc65b73f428ea2a499bcf91995ef5441f63d4|How they evaluate quality of generated output?|Through human evaluation where they are asked to evaluate the generated output on a likert scale.| | 
1911.03350|0a8bc204a76041a25cee7e9f8e2af332a17da67a|What automated metrics authors investigate?|BLEU, Self-BLEU, n-gram based score, probability score| | 
1708.09609|fc06502fa62803b62f6fd84265bfcfb207c1113b|Who annotated the data?|annotators who were not security experts, researchers in either NLP or computer security| | 
1808.10267|f91835f17c0086baec65ebd99d12326ae1ae87d2|How do they obtain parsed source sentences?|Stanford CoreNLP BIBREF11 | | 
1808.10267|14e78db206a8180ea637774aa572b073e3ffa219|What kind of encoders are used for the parsed source sentence?|RNN encoders| | 
1808.10267|bc1e3f67d607bfc7c4c56d6b9763d3ae7f56ad5b|Whas is the performance drop of their model when there is no parsed input?| improvements of up to 1.5 BLEU over the seq2seq baseline| | 
1909.09779|e8e00b4c0673af5ab02ec82563105e4157cc54bb|How were their results compared to state-of-the-art?|transformer model achieves higher BLEU score than both Attention encoder-decoder and sequence-sequence model| | 
1911.05960|18ad60f97f53af64cb9db2123c0d8846c57bfa4a|What supports the claim that injected CNN into recurent units will enhance ability of the model to catch local context and reduce ambiguities?|word embeddings to generate a new feature, i.e., summarizing a local context| | 
1911.05960|87357448ce4cae3c59d4570a19c7a9df4c086bd8|How is CNN injected into recurent units?|The most simple one is to directly apply a CNN layer after the embedding layer to obtain blended contextual representations. Then a GRU layer is applied afterward.| | 
1911.05960|1ccc4f63268aa7841cc6fd23535c9cbe85791007|Are there some results better than state of the art on these tasks?|Yes| | 
1911.05960|afe34e553c3c784dbf02add675b15c27638cdd45|Do experiment results show consistent significant improvement of new approach over traditional CNN and RNN models?|Yes| | 
1911.05960|3f46d8082a753265ec2a88ae8f1beb6651e281b6|What datasets are used for testing sentiment classification and reading comprehension?|CBT NE/CN, MR Movie reviews, IMDB Movie reviews, SUBJ| | 
1805.11189|63d9b12dc3ff3ceb1aed83ce11371bca8aac4e8f|So we do not use pre-trained embedding in this case?|Yes| | 
1906.11604|0bd864f83626a0c60f5e96b73fb269607afc7c09|How are sentence embeddings incorporated into the speech recognition system?|BERT generates sentence embeddings that represent words in context. These sentence embeddings are merged into a single conversational-context vector that is used to calculate a gated embedding and is later combined with the output of the decoder h to provide the gated activations for the next hidden layer.| | 
1711.05345|c77d6061d260f627f2a29a63718243bab5a6ed5a|How different is the dataset size of source and target?|the training dataset is large while the target dataset is usually much smaller| | 
1701.02025|4c7b29f6e3cc1e902959a1985146ccc0b15fe521|How do you find the entity descriptions?|Wikipedia| | 
1907.10738|b34c60eb4738e0439523bcc679fe0fe70ceb8bde|How is OpenBookQA different from other natural language QA?|in the OpenBookQA setup the open book part is much larger, the open book part is much larger (than a small paragraph) and is not complete as additional common knowledge may be required| | 
2002.01861|9623884915b125d26e13e8eeebe9a0f79d56954b|At what text unit/level were documents processed?|documents are segmented into paragraphs and processed at the paragraph level| | 
2002.01861|77db56fee07b01015a74413ca31f19bea7203f0b|What evaluation metric were used for presenting results? |F$_1$, precision, and recall| | 
2002.01861|c309e87c9e08cf847f31e554577d6366faec1ea0|Was the structure of regulatory filings exploited when training the model? |No| | 
2002.01861|81cee2fc6edd9b7bc65bbf6b4aa35782339e6cff|What type of documents are supported by the annotation platform?|Variety of formats supported (PDF, Word...), user can define content elements of document| | 
1706.07206|2555ca85ff6b56bd09c3919aa6b277eb7a4d4631|Which datasets are used for evaluation?|Stanford Sentiment Treebank| | 
1812.06876|d028dcef22cdf0e86f62455d083581d025db1955|What are the strong baselines you have?|optimize single task with no synthetic data| | 
1812.06038|593e307d9a9d7361eba49484099c7a8147d3dade|What are causal attribution networks?|networks where nodes represent causes and effects, and directed edges represent cause-effect relationships proposed by humans| | 
1608.08738|a71ebd8dc907d470f6bd3829fa949b15b29a0631|how did they ask if a tweet was racist?|if it includes  negative utterances, negative generalizations and insults concerning ethnicity, nationality, religion and culture.| | 
1805.04570|1546356a8c5893dc2d298dcbd96d0307731dd54d|What other cross-lingual approaches is the model compared to?|The baseline model BIBREF5 we compare with regards the output space of the model as a subset INLINEFORM2 where INLINEFORM3 is the set of all tag sets seen in this training data.| | 
1805.04570|9f5507a8c835c4671020d7d310fff2930d44e75a|What languages are explored?|Danish/Swedish (da/sv), Russian/Bulgarian (ru/bg), Finnish/Hungarian (fi/hu), Spanish/Portuguese (es/pt)| | 
1808.07625|ad0a7fe75db5553652cd25555c6980f497e08113|How does the model compute the likelihood of executing to the correction semantic denotation?|By treating logical forms as a latent variable and training a discriminative log-linear model over logical form y given x.| | 
1609.04186|f268b70b08bd0436de5310e390ca5f38f7636612|Which conventional alignment models do they use as guidance?|GIZA++ BIBREF3 or fast_align BIBREF4 | | 
1609.04186|7aae4533dbf097992f23fb2e0574ec5c891ca236|Which dataset do they use?|BTEC corpus, the CSTAR03 and IWSLT04 held out sets, the NIST2008 Open Machine Translation Campaign| | 
1909.08090|10045d7dac063013a8447b5a4bc3a3c2f18f9e82|Do they compare their algorithm to voting without weights?|No| | 
2002.01030|144714fe0d5a2bb7e21a7bf50df39d790ff12916|What are state of the art methods authors compare their work with? |"ISOT dataset: LLVM
Liar dataset: Hybrid CNN and LSTM with attention"| | 
1609.00081|f01aa192d97fa3cc36b6e316355dc5da0e9b97dc|What are the baselines model?|(i) Uniform, (ii) SVR+W, (iii) SVR+O, (iv) C4.5SSL, (v) GLM| | 
1805.12070|d7d41a1b8bbb1baece89b28962d23ee4457b9c3a|What languages are explored in the work?|Mandarin, English| | 
1807.11714|b458ebca72e3013da3b4064293a0a2b4b5ef1fa6|What is the state-of-the-art neural coreference resolution model?|BIBREF2 , BIBREF1 | | 
2004.03788|018ef092ffc356a2c0e970ae64ad3c2cf8443288|How large is the dataset?|8757 news records| | 
2004.03788|de4e180f49ff187abc519d01eff14ebcd8149cad|What features do they extract?|Inconsistency in Noun Phrase Structures,  Inconsistency Between Clauses, Inconsistency Between Named Entities and Noun Phrases, Word Level Feature Using TF-IDF| | 
1910.10869|bdc1f37c8b5e96e3c29cc02dae4ce80087d83284|What they use as a metric of finding hot spots in meeting?|unweighted average recall (UAR) metric| | 
1910.10869|fdd9dea06550a2fd0df7a1e6a5109facf3601d76|How big is ICSI meeting corpus?| 75 meetings and about 70 hours of real-time audio duration| | 
1910.10869|3786164eaf3965c11c9969c4463b8c3223627067|What annotations are available in ICSI meeting corpus?|8 levels and degrees, ranging from `not hot' to `luke warm' to `hot +'. Every utterance was labeled with one of these discrete labels by a single annotator| | 
1907.04380|2fd8688c8f475ab43edaf5d189567f8799b018e1|Is such bias caused by bad annotation?|No| | 
1904.09545|b68d2549431c524a86a46c63960b3b283f61f445|How do they determine similar environments for fragments in their data augmentation scheme?|fragments are interchangeable if they occur in at least one lexical environment that is exactly the same| | 
1904.09545|7f5059b4b5e84b7705835887f02a51d4d016316a|Do they experiment with language modeling on large datasets?|No| | 
1904.09545|df79d04cc10a01d433bb558d5f8a51bfad29f46b|Which languages do they test on?|"Answer with content missing: (Applications section) We use Wikipedia articles
in five languages
(Kinyarwanda, Lao, Pashto, Tok Pisin, and a subset of English) as well as the Na dataset of Adams
et al. (2017).
Select:
Kinyarwanda, Lao, Pashto, Tok Pisin, and a subset of English"| | 
1908.02322|182b6d77b51fa83102719a81862891f49c23a025|What limitations are mentioned?|deciding publisher partisanship, risk annotator bias because of short description text provided to annotators| | 
1908.02322|441886f0497dc84f46ed8c32e8fa32983b5db42e|What examples of applications are mentioned?|partisan news detector| | 
1908.02322|62afbf8b1090e56fdd2a2fa2bdb687c3995477f6|Did they crowdsource the annotations?|Yes| | 
1910.12354|d3341eefe4188ee8a68914a2e8c9047334997e84|Why they conclude that the usage of Gated-Attention provides no competitive advantage against concatenation in this setting?|concatenation consistently outperforms the gated-attention mechanism for both training and testing instructions| | 
2002.03438|334f90bb715d8950ead1be0742d46a3b889744e7|What semantic features help in detecting whether a piece of text is genuine or generated? of |No feature is given, only discussion that semantic features are use in practice and yet to be discovered how to embed that knowledge into statistical decision theory framework.| | 
2002.03438|53c8416f2983e07a7fa33bcb4c4281bbf49c8164|Which language models generate text that can be easier to classify as genuine or generated?|Outputs from models that are better in the sense of cross-entropy or perplexity are harder to distinguish from authentic text.| | 
2002.03438|5b2480c6533696271ae6d91f2abe1e3a25c4ae73|Is the assumption that natural language is stationary and ergodic valid?|It is not completely valid for natural languages because of diversity of language - this is called smoothing requirement.| | 
2001.03632|f42e61f9ad06fb782d1574eb973c880add4f76d2|What architectural factors were investigated?|type of recurrent unit, type of attention, choice of sequential vs. tree-based model structure| | 
1709.06671|b5484a0f03d63d091398d3ce4f841a45062438a7|What is the introduced meta-embedding method introduced in this paper?|"proposed method comprises of two steps: a neighbourhood reconstruction step (Section ""Nearest Neighbour Reconstruction"" ), and a projection step (Section ""Projection to Meta-Embedding Space"" ). In the reconstruction step, we represent the embedding of a word by the linearly weighted combination of the embeddings of its nearest neighbours in each source embedding space. "| | 
1909.08103|18d8b52b4409c718bf1cc90ce9e013206034bbd9|How long are dialogue recordings used for evaluation?|average 12.8 min per recording| | 
1904.05527|43d8057ff0d3f0c745a7164aed7ed146674630e0|What do the models that they compare predict?|national dialects of English| | 
1905.01715|ebb7313eee2ea447abc83cb08b658b57c7eaa600|What SMT models did they look at?|automatic translator with Moses| | 
1905.01715|df934aa1db09c14b3bf4bc617491264e2192390b|Which NMT models did they experiment with?|2-layer LSTM model with 500 hidden units in both encoder and decoder| | 
1911.08829|346f10ddb34503dfba72b0e49afcdf6a08ecacfa|How big PIE datasets are obtained from dictionaries?|46 documents makes up our base corpus| | 
1911.08829|2480dfe2d996afef840a81bd920aeb9c26e5b31d|What compleentary PIE extraction methods are used to increase reliability further?|exact string matching, inflectional string matching| | 
1911.08829|0fec9da2bc80a12a7a6d6600b9ecf3e122732b60|Are PIEs extracted automatically subjected to human evaluation?|Yes| | 
1911.08829|5499527beadb7f5dd908bd659cad83d6a81119bd|What dictionaries are used for automatic extraction of PIEs?|Wiktionary, Oxford Dictionary of English Idioms, UsingEnglish.com (UE), Sporleder corpus, VNC dataset, SemEval-2013 Task 5 dataset| | 
1909.09524|191d4fe8a37611b2485e715bb55ff1a30038ad6a|Are experiments performed with any other pair of languages, how did proposed method perform compared to other models?|No| | 
1909.09524|6e76f114209f59b027ec3b3c8c9cdfc3e682589f|Is pivot language used in experiments English or some other language?|Yes| | 
1909.09524|6583e8bfa7bcc3a792a90b30abb316e6d423f49b|What are multilingual models that were outperformed in performed experiment?|Direct source$\rightarrow $target: A standard NMT model trained on given source$\rightarrow $target, Multilingual: A single, shared NMT model for multiple translation directions, Many-to-many: Trained for all possible directions among source, target, and pivot languages, Many-to-one: Trained for only the directions to target language| | 
1906.05963|9a5d02062fa7eec7097f1dc1c38b5e6d5c82acdf|What are the common captioning metrics?|the CIDEr-D BIBREF22 , SPICE BIBREF23 , BLEU BIBREF24 , METEOR BIBREF25 , and ROUGE-L BIBREF26 metrics| | 
1910.11235|5450f27ccc0406d3bffd08772d8b59004c2716da|What is the road exam metric?|a new metric to reveal a model's robustness against exposure bias| | 
1706.04115|0038b073b7cca847033177024f9719c971692042|How is the input triple translated to a slot-filling task?|The relation R(x,y) is mapped onto a question q whose answer is y| | 
1910.14443|ad6415f4351c44ffae237524696a3f76f383bfd5|Is model compared against state of the art models on these datasets?|Yes| | 
1910.14443|e097c2ec6021b1c1195b953bf3e930374b74d8eb|How is octave convolution concept extended to multiple resolutions and octaves?|The resolution of the low-frequency feature maps is reduced by an octave – height and width dimensions are divided by 2. In this work, we explore spatial reduction by up to 3 octaves – dividing by $2^t$, where $t=1,2,3$ – and for up to 4 groups. We refer to such a layer as a multi-octave convolutional (MultiOctConv) layer,| | 
1601.06738|320d72a9cd19b52c29dda9ddecd520c9938a717f|Does this paper address the variation among English dialects regarding these hedges?|No| | 
1909.00107|21cbcd24863211b02b436f21deaf02125f34da4c|On which dataset is model trained?|Couples Therapy Corpus (CoupTher) BIBREF21| | 
1909.00107|37bc8763eb604c14871af71cba904b7b77b6e089|How is module that analyzes behavioral state trained?|pre-trained to identify the presence of behavior from a sequence of word using the Couples Therapy Corpus| | 
1711.03438|a81941f933907e4eb848f8aa896c78c1157bff20|Can the model add new relations to the knowledge graph, or just new entities?|The model does not add new relations to the knowledge graph.| | 
2003.01006|252677c93feb2cb0379009b680f0b4562b064270|How large is the dataset?|6,127 scientific entities, including 2,112 Process, 258 Method, 2,099 Material, and 1,658 Data entities| | 
1609.01962|fe6bb55b28f14ed8ac82c122681905397e31279d|Why is a Gaussian process an especially appropriate method for this classification problem?|avoids the need for expensive cross-validation for hyperparameter selection| | 
1904.02815|b3ac67232c8c7d5a759ae025aee85e9c838584eb|Do the authors do manual evaluation?|No| | 
1811.04604|43878a6a8fc36aaae29d95815355aaa7d25c3b53|What datasets did they use?|the personalized bAbI dialog dataset| | 
1909.05890|68ff2a14e6f0e115ef12c213cf852a35a4d73863|Do twitter users tend to tweet about the DOS attack when it occurs? How much data supports this assumption?|The dataset contains about 590 tweets about DDos attacks.| | 
1909.05890|0b54032508c96ff3320c3db613aeb25d42d00490|What is the training and test data used?|Tweets related to a Bank of America DDos attack were used as training data. The test datasets contain tweets related to attacks to Bank of America, PNC and Wells Fargo.| | 
1909.05890|86be8241737dd8f7b656a3af2cd17c8d54bf1553|Was performance of the weakly-supervised model compared to the performance of a supervised model?|Yes| | 
1912.06927|a4422019d19f9c3d95ce8dc1d529bf3da5edcfb1|Do the tweets come from a specific region?|No| | 
1909.01247|bb169a0624aefe66d3b4b1116bbd152d54f9e31b|Did they experiment with the corpus?|Yes| | 
1909.01247|ca8e023d142d89557714d67739e1df54d7e5ce4b|How did they determine the distinct classes?|inspired by the OntoNotes5 corpus BIBREF7 as well as the ACE (Automatic Content Extraction) English Annotation Guidelines for Entities Version 6.6 2008.06.13 BIBREF8| | 
1706.01723|3fddd9f6707b9e40e35518dae7f6da7c4cb77d16|Do they jointly tackle multiple tagging problems?|No| | 
1706.01723|fc54736e67f748f804e8f66b3aaaea7f5e55b209|How do they confirm their model working well on out-of-vocabulary problems?|conduct experiments using artificially constructed unnormalized text by corrupting words in the normal dev set| | 
1608.06378|a53683d1a0647c80a4398ff8f4a03e11c0929be2|What approach does this work propose for the new task?|We propose a listening comprehension model for the task defined above, the Attention-based Multi-hop Recurrent Neural Network (AMRNN) framework, and show that this model is able to perform reasonably well for the task. In the proposed approach, the audio of the stories is first transcribed into text by ASR, and the proposed model is developed to process the transcriptions for selecting the correct answer out of 4 choices given the question. | | 
1608.06378|0fd7d12711dfe0e35467a7ee6525127378a1bacb|What is the new task proposed in this work?| listening comprehension task | | 
1808.02022|5dc2f79cd8078d5976f2df9ab128d4517e894257|Which news organisations are the headlines sourced from?|BBC and CNN | | 
1909.01515|4226a1830266ed5bde1b349205effafe7a0e2337|What meta-information is being transferred?|high-order representation of a relation, loss gradient of relation meta| | 
1909.01515|5fb348b2d7b012123de93e79fd46a7182fd062bd|What datasets are used to evaluate the approach?|NELL-One, Wiki-One| | 
1811.01299|7ff48fe5b7bd6b56553caacc891ce3d7e0070440|Does their solution involve connecting images and text?|Yes| | 
1811.01299|54a2c08aa55c3db9b30ae2922c96528d3f4fc733|Which model do they use to generate key messages?|ontology-based knowledge tree, heuristics-based, n-grams model| | 
1912.01220|ecb680d79e847beb7c1aa590d288a7313908d64a|What experiments they perform to demonstrate that their approach leads more accurate region based representations?| To test our proposed category induction model, we consider all BabelNet categories with fewer than 50 known instances. This is motivated by the view that conceptual neighborhood is mostly useful in cases where the number of known instances is small. For each of these categories, we split the set of known instances into 90% for training and 10% for testing.| | 
1912.01220|b622f57c4e429b458978cb8863978d7facab7cfe|How they indentify conceptual neighbours?|Once this classifier has been trained, we can then use it to predict conceptual neighborhood for categories for which only few instances are known.| | 
1908.06151|7889ec45b996be0b8bf7360d08f84daf3644f115|What was previous state of the art model for automatic post editing?|pal-EtAl:2018:WMT proposed an APE model that uses three self-attention-based encoders, tebbifakhr-EtAl:2018:WMT, the NMT-subtask winner of WMT 2018 ($wmt18^{nmt}_{best}$), employ sequence-level loss functions in order to avoid exposure bias during training and to be consistent with the automatic evaluation metrics., shin-lee:2018:WMT propose that each encoder has its own self-attention and feed-forward layer to process each input separately. , The APE PBSMT-subtask winner of WMT 2018 ($wmt18^{smt}_{best}$) BIBREF11 also presented another transformer-based multi-source APE which uses two encoders and stacks an additional cross-attention component for $src \rightarrow pe$ above the previous cross-attention for $mt \rightarrow pe$.| | 
1802.00273|41e300acec35252e23f239772cecadc0ea986071|What neural machine translation models can learn in terms of transfer learning?|Multilingual Neural Machine Translation Models| | 
1911.03514|e70236c876c94dbecd9a665d9ba8cefe7301dcfd|Did they experiment on the proposed task?|No| | 
1911.03514|aa1f605619b2487cc914fc2594c8efe2598d8555|Is annotation done manually?|Yes| | 
1911.03514|9f2634c142dc4ad2c68135dbb393ecdfd23af13f|How large is the proposed dataset?|we obtain 52,053 dialogues and 460,358 utterances| | 
1805.05581|77e57d19a0d48f46de8cbf857f5e5284bca0df2b|How large is the dataset?|30M utterances| | 
1805.05581|50c8b821191339043306fd28e6cda2db400704f9|How is the dataset created?|We collected Japanese fictional stories from the Web| | 
1708.00077|dee7383a92c78ea49859a2d5ff2a9d0a794c1f0f|What is binary variational dropout?|the dropout technique of Gal & Ghahramani gal| | 
1902.07285|a458c649a793588911cef4c421f95117d0b9c472|Which strategies show the most promise in deterring these attacks?|At the moment defender can draw on methods from image area to text for improving the robustness of DNNs, e.g. adversarial training BIBREF107 , adding extra layer BIBREF113 , optimizing cross-entropy function BIBREF114 , BIBREF115 or weakening the transferability of adversarial examples.| | 
1912.01679|04cab3325e20c61f19846674bf9a2c46ea60c449|What are baseline models on WSJ eval92 and LibriSpeech test-clean?|Wav2vec BIBREF22, a fully-supervised system using all labeled data| | 
1705.00861|76c8aac84152fc4bbc0d5faa7b46e40438353e77|Do they use the same architecture as LSTM-s and GRUs with just replacing with the LAU unit?|Yes| | 
1902.00756|6916596253d67f74dba9222f48b9e8799581bad9|So this paper turns unstructured text inputs to parameters that GNNs can read?|Yes| | 
1602.03661|4d60e9494a412d581bd5e85f4e78881914085afc|What empirical data are the Blending Game predictions compared to?|words length distribution, the frequency of use of the different forms and a measure for the combinatoriality| | 
2003.09520|cf63a4f9fe0f71779cf5a014807ae4528279c25a|How does the semi-automatic construction process work?|Automatic transcription of 5000 tokens through sequential neural models trained on the annotated part of the corpus| | 
2003.09520|8829f738bcdf05b615072724223dbd82463e5de6|Does the paper report translation accuracy for an automatic translation model for Tunisian to Arabish words?|Yes| | 
1807.09000|4b624064332072102ea674254d7098038edad572|Did participants behave unexpectedly?|No| | 
1807.09000|65ba7304838eb960e3b3de7c8a367d2c2cd64c54|Was this experiment done in a lab?|No| | 
1706.02027|3d49b678ff6b125ffe7fb614af3e187da65c6f65|"What does ""explicitly leverages their probabilistic correlation to guide the training process of both models"" mean?"|The framework jointly learns parametrized QA and QG models subject to the constraint in equation 2. In more detail, they minimize QA and QG loss functions, with a third dual loss for regularization.| | 
1704.08424|b686e10a725254695821e330a277c900792db69f|How does this compare to contextual embedding methods?| represent each word with an expressive multimodal distribution, for multiple distinct meanings, entailment, heavy tailed uncertainty, and enhanced interpretability. For example, one mode of the word `bank' could overlap with distributions for words such as `finance' and `money', and another mode could overlap with the distributions for `river' and `creek'.| | 
1702.06700|40f87db3a8d1ac49b888ce3358200f7d52903ce7|Does the new system utilize pre-extracted bounding boxes and/or features?|Yes| | 
1702.06700|36383971a852d1542e720d3ea1f5adeae0dbff18|To which previous papers does this work compare its results?|holistic, TraAtt, RegAtt, ConAtt, ConAtt, iBOWIMG , VQA, VQA, WTL , NMN , SAN , AMA , FDA , D-NMN, DMN+| | 
1909.02151|1d941d390c0ee365aa7d7c58963e646eea74cbd6|Do they consider other tasks?|No| | 
2003.04973|3ee976add83e37339715d4ae9d8aa328dd54d052|What were the model's results on flood detection?|Queensland flood which provided 96% accuracy, Alberta flood with the same configuration of train-test split which provided 95% accuracy| | 
2003.04973|ef04182b6ae73a83d52cb694cdf4d414c81bf1dc|What dataset did they use?| disaster data from BIBREF5, Queensland flood in Queensland, Australia and Alberta flood in Alberta, Canada| | 
1610.01030|decb07f9be715de024236e50dc7011a132363480|What exactly is new about this stochastic gradient descent algorithm?|"CNN model can be trained in a purely online setting. We first initialize the model parameters $\theta _0$ (line 1), which can be a trained model from other disaster events or it can be initialized randomly to start from scratch.

As a new batch of labeled tweets $B_t= \lbrace \mathbf {s}_1 \ldots \mathbf {s}_n \rbrace $ arrives, we first compute the log-loss (cross entropy) in Equation 11 for $B_t$ with respect to the current parameters $\theta _t$ (line 2a). Then, we use backpropagation to compute the gradients $f^{\prime }(\theta _{t})$ of the loss with respect to the current parameters (line 2b). Finally, we update the parameters with the learning rate $\eta _t$ and the mean of the gradients (line 2c). We take the mean of the gradients to deal with minibatches of different sizes. Notice that we take only the current minibatch into account to get an updated model. "| | 
1909.00100|63eb31f613a41a3ddd86f599e743ed10e1cd07ba|What codemixed language pairs are evaluated?|Hindi-English| | 
1909.00100|d2804ac0f068e9c498e33582af9c66906b26cac3|How do they compress the model?|we extract sentences from Wikipedia in languages for which public multilingual is pretrained. For each sentence, we use the open-source BERT wordpiece tokenizer BIBREF4 , BIBREF1 and compute cross-entropy loss for each wordpiece: INLINEFORM0| | 
1909.00100|e24fbcc8be922c43f6b6037cdf2bfd4c0a926c08|What is the multilingual baseline?| the Meta-LSTM BIBREF0| | 
1711.05568|e8c0fabae0d29491471e37dec34f652910302928|Which features do they use?|beyond localized features and have access to the entire sequence| | 
1711.05568|cafa6103e609acaf08274a2f6d8686475c6b8723|By how much do they outperform state-of-the-art solutions on SWDA and MRDA?|improves the DAR accuracy over Bi-LSTM-CRF by 2.1% and 0.8% on SwDA and MRDA respectively| | 
1810.08732|7f2fd7ab968de720082133c42c2052d351589a67|What type and size of word embeddings were used?|word2vec, 200 as the dimension of the obtained word vectors| | 
1810.08732|369b0a481a4b75439ade0ec4f12b44414c4e5164|What data was used to build the word embeddings?|Turkish news-web corpus,  TS TweetS by Sezer-2013 and 20M Turkish Tweets by Bolat and Amasyalı| | 
1906.05012|e97545f4a5e7bc96515e60f2f9b23d8023d1eed9|How are templates discovered from training data?|For each source article, Retrieve aims to return a few candidate templates from the training corpus. Then, the Fast Rerank module quickly identifies a best template from the candidates.| | 
1910.07134|aaed6e30cf16727df0075b364873df2a4ec7605b|What is WNGT 2019 shared task?|efficiency task aimed  at reducing the number of parameters while minimizing drop in performance| | 
1606.00189|66f0dee89f084fe0565539a73f5bbe65f3677814|Do they use pretrained word representations in their neural network models?|No| | 
1606.00189|8f882f414d7ea12077930451ae77c6e5f093adbc|How do they combine the two proposed neural network models?|ncorporating NNGLM and NNJM both independently and jointly into, baseline system| | 
1608.03902|8fcbae7c3bd85034ae074fa58a35e773936edb5b|what was their baseline comparison?|Support Vector Machine (SVM), Logistic Regression (LR), Random Forest (RF)| | 
2004.03061|cbbcafffda7107358fa5bf02409a01e17ee56bfd|Was any variation in results observed based on language typology?|It is observed some variability - but not significant. Bert does not seem to gain much more syntax information than with type level information.| | 
2004.03061|1e59263f7aa7dd5acb53c8749f627cf68683adee|Does the work explicitly study the relationship between model complexity and linguistic structure encoding?|No| | 
1601.04012|eac042734f76e787cb98ba3d0c13a916a49bdfb3|Which datasets are used in this work?|GENIA corpus| | 
1909.00574|9595bf228c9e859b0dc745e6c74070be2468d2cf|Does the training dataset provide logical form supervision?|Yes| | 
1909.00574|94c5f5b1eb8414ad924c3568cedd81dc35f29c48|What is the difference between the full test set and the hard test set?|3000 hard samples are selected from the test set| | 
1707.07048|ba05a53f5563b9dd51cc2db241c6e9418bc00031|How is the discriminative training formulation different from the standard ones?|the best permutation is decided by $\mathcal {J}_{\text{SEQ}}(\mathbf {L}_{un}^{(s^{\prime })},\mathbf {L}_{un}^{(r)})$ , which is the sequence discriminative criterion of taking the $s^{\prime }$ -th permutation in $n$ -th output inference stream at utterance $u$| | 
1707.07048|7bf3a7d19f17cf01f2c9fa16401ef04a3bef65d8|How are the two datasets artificially overlapped?|we sort the speech segments by length, we take segments in pairs, zero-padding the shorter segment so both have the same length, These pairs are then mixed together| | 
1804.00520|3efc0981e7f959d916aa8bb32ab1c347b8474ff8|What type of lexical, syntactic, semantic and polarity features are used?|Our lexical features include 1-, 2-, and 3-grams in both word and character levels., number of characters and the number of words, POS tags, 300-dimensional pre-trained word embeddings from GloVe, latent semantic indexing, tweet representation by applying the Brown clustering algorithm, positive words (e.g., love), negative words (e.g., awful), positive emoji icon and negative emoji icon, boolean features that check whether or not a negation word is in a tweet| | 
1901.03859|10f560fe8e1c0c7dea5e308ee4cec16d07874f1d|How does nextsum work?|selects the next summary sentence based not only on properties of the source text, but also on the previously selected sentences in the summary| | 
1804.02233|dc28ac845602904c2522f5349374153f378c42d3|How many tweets were manually labelled? |44,000 tweets| | 
1908.08566|ac148fb921cce9c8e7b559bba36e54b63ef86350|What dataset they use for evaluation?|The same 2K set from Gigaword used in BIBREF7| | 
1708.04120|094ce2f912aa3ced9eb97b171745d38f58f946dd|What is the source of the tables?|The Online Retail Data Set consists of a clean list of 25873 invoices, totaling 541909 rows and 8 columns.| | 
1803.09745|b5bfa6effdeae8ee864d7d11bc5f3e1766171c2d|Which regions of the United States do they consider?|all regions except those that are colored black| | 
1910.03771|ec62c4cdbeaafc875c695f2d4415bce285015763|What state-of-the-art general-purpose pretrained models are made available under the unified API? |BERT, RoBERTa, DistilBERT, GPT, GPT2, Transformer-XL, XLNet, XLM| | 
1904.06941|405964517f372629cda4326d8efadde0206b7751|How is performance measured?|they use ROC curves and cross-validation| | 
1906.01512|ae95a7d286cb7a0d5bc1a8283ecbf803e9305951|What models are included in the toolkit?| recurrent neural network (RNN)-based sequence-to-sequence (Seq2Seq) models for NATS| | 
1711.06288|0be0c8106df5fde4b544af766ec3d4a3d7a6c8a2|Is there any human evaluation involved in evaluating this famework?|Yes| | 
1911.00473|504a069ccda21580ccbf18c34f5eefc0088fa105|How big is dataset used for fine-tuning BERT?|hundreds of thousands of legal agreements| | 
1909.05016|2a6469f8f6bf16577b590732d30266fd2486a72e|What is novel in author's approach?|They use self-play learning , optimize the model for specific metrics, train separate models per user, use model  and response classification predictors, and filter the dataset to obtain higher quality training data.| | 
1912.00955|78577fd1c09c0766f6e7d625196adcc72ddc8438|What dataset is used for train/test of this method?|Training datasets: TTS System dataset and embedding selection dataset. Evaluation datasets: Common Prosody Errors dataset and LFR dataset.| | 
1811.09529|b2254f9dd0e416ee37b577cef75ffa36cbcb8293|How many domains of ontologies do they gather data from?|5 domains: software, stuff, african wildlife, healthcare, datatypes| | 
1604.06076|cb1126992a39555e154bedec388465b249a02ded|How is the semi-structured knowledge base created?|using a mixture of manual and semi-automatic techniques| | 
1808.04314|d5256d684b5f1b1ec648d996c358e66fe51f4904|what is the practical application for this paper?|Improve existing NLP methods. Improve linguistic analysis. Measure impact of word normalization tools.| | 
1707.06878|2a1069ae3629ae8ecc19d2305f23445c0231dc39|Do they use a neural model for their task?|No| | 
1909.08752|0b411f942c6e2e34e3d81cc855332f815b6bc123|What's the method used here?|Two neural networks: an extractor based on an encoder (BERT) and a decoder (LSTM Pointer Network BIBREF22) and an abstractor identical to the one proposed in BIBREF8.| | 
1905.10247|01123a39574bdc4684aafa59c52d956b532d2e53|By how much does their method outperform state-of-the-art OOD detection?|AE-HCN outperforms by 17%, AE-HCN-CNN outperforms by 20% on average| | 
1811.07684|954c4756e293fd5c26dc50dc74f505cc94b3f8cc|What are dilated convolutions?|Similar to standard convolutional networks but instead they skip some input values effectively operating on a broader scale.| | 
1906.08382|ee279ace5bc69d15e640da967bd4214fe264aa1a|what was the evaluation metrics studied in this work?|mean rank (MR), mean reciprocal rank (MRR), as well as Hits@1, Hits@3, and Hits@10| | 
1608.04207|beda007307c76b8ce7ffcd159a8280d2e8c7c356|Do they analyze ELMo?|No| | 
2003.02639|639c145f0bcb1dd12d08108bc7a02f9ec181552e|What are three possible phases for language formation?|Phase I: $\langle cc \rangle $ increases smoothly for $\wp < 0.4$, indicating that for this domain there is a small correlation between word neighborhoods. Full vocabularies are attained also for $\wp < 0.4$, Phase II: a drastic transition appears at the critical domain $\wp ^* \in (0.4,0.6)$, in which $\langle cc \rangle $ shifts abruptly towards 1. An abrupt change in $V(t_f)$ versus $\wp $ is also found (Fig. FIGREF16) for $\wp ^*$, Phase III: single-word languages dominate for $\wp > 0.6$. The maximum value of $\langle cc \rangle $ indicate that word neighborhoods are completely correlated| | 
1606.07298|0b8d64d6cdcfc2ba66efa41a52e09241729a697c|Do the experiments explore how various architectures and layers contribute towards certain decisions?|No| | 
1908.11279|de015276dcde4e7d1d648c6e31100ec80f61960f|Do the authors perform experiments using their proposed method?|Yes| | 
1708.00214|56836afc57cae60210fa1e5294c88e40bb10cc0e|What NLP tasks do the authors evaluate feed-forward networks on?|language identification, part-of-speech tagging, word segmentation, and preordering for statistical machine translation| | 
1909.03464|6147846520a3dc05b230241f2ad6d411d614e24c|What are three challenging tasks authors evaluated their sequentially aligned representations?|paper acceptance prediction, Named Entity Recognition (NER), author stance prediction| | 
1801.07537|99cf494714c67723692ad1279132212db29295f3|What is the difference in findings of Buck et al? It looks like the same conclusion was mentioned in Buck et al..|AQA diverges from well structured language in favour of less fluent, but more effective, classic information retrieval (IR) query operations| | 
1612.09113|85e45b37408bb353c6068ba62c18e516d4f67fe9|What is the baseline?|The baseline is a multi-task architecture inspired by another paper.| | 
1612.09113|f4e1d2276d3fc781b686d2bb44eead73e06fbf3f|What is the unsupervised task in the final layer?|Language Modeling| | 
1702.03274|ff814793387c8f3b61f09b88c73c00360a22a60e|Does the latent dialogue state heklp their model?|Yes| | 
1702.03274|059acc270062921ad27ee40a77fd50de6f02840a|Do the authors test on datasets other than bAbl?|No| | 
1702.03274|6a9eb407be6a459dc976ffeae17bdd8f71c8791c|What is the reward model for the reinforcement learning appraoch?|reward 1 for successfully completing the task, with a discount by the number of turns, and reward 0 when fail| | 
1610.03112|cacb83e15e160d700db93c3f67c79a11281d20c5|Does this paper propose a new task that others can try to improve performance on?|No, there has been previous work on recognizing social norm violation.| | 
1607.03542|33957fde72f9082a5c11844e7c47c58f8029c4ae|What knowledge base do they use?|Freebase| | 
1607.03542|1c4cd22d6eaefffd47b93c2124f6779a06d2d9e1|How big is their dataset?|3 million webpages processed with a CCG parser for training, 220 queries for development, and 307 queries for testing| | 
1607.03542|2122bd05c03dde098aa17e36773e1ac7b6011969|What task do they evaluate on?|Fill-in-the-blank natural language questions| | 
1812.10860|1d6c42e3f545d55daa86bea6fabf0b1c52a93bbb|Do some pretraining objectives perform better than others for sentence level understanding tasks?|Yes| | 
1712.02121|480e10e5a1b9c0ae9f7763b7611eeae9e925096b|Did the authors try stacking multiple convolutional layers?|No| | 
